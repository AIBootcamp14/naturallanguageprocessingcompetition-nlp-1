2025-10-14 09:40:51 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-14 09:40:53 | 📊 OPTUNA 모드 실행 중...
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | 📊 OPTUNA 튜닝 모드 시작
2025-10-14 09:40:53 | 🔧 모델: kobart
2025-10-14 09:40:53 | 🔢 시도 횟수: 100
2025-10-14 09:40:53 | ⏱ 최대 시간: 7200
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | [1/3] 데이터 로드...
2025-10-14 09:40:53 | ✅ 학습 데이터: 12457개
2025-10-14 09:40:53 | ✅ 검증 데이터: 499개
2025-10-14 09:40:53 | [2/3] Config 로드...
2025-10-14 09:40:53 | Config 로드 완료: kobart
2025-10-14 09:40:53 | [3/3] Optuna 튜닝 시작...
2025-10-14 09:40:53 | OptunaOptimizer 초기화 완료
2025-10-14 09:40:53 | - Study 이름: optuna_kobart_kobart_ultimate
2025-10-14 09:40:53 | - Trial 횟수: 100
2025-10-14 09:40:53 | - 방향: maximize
2025-10-14 09:40:53 | ======================================================================
2025-10-14 09:40:53 | Optuna 최적화 시작
2025-10-14 09:40:53 | ======================================================================
2025-10-14 09:40:53 | [I 2025-10-14 09:40:53,701] A new study created in memory with name: optuna_kobart_kobart_ultimate
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | Trial 0 시작
2025-10-14 09:40:53 | 파라미터: {'learning_rate': 5.611516415334504e-06, 'num_epochs': 10, 'warmup_ratio': 0.146398788362281, 'weight_decay': 0.05986584841970366, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.7486639612006325}
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | 모델 타입: encoder_decoder
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | 모델 및 토크나이저 로딩 시작
2025-10-14 09:40:53 | ============================================================
2025-10-14 09:40:53 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 09:40:54 | 모델 로딩: digit82/kobart-summarization
2025-10-14 09:40:54 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 09:40:55 | → 디바이스: cuda
2025-10-14 09:40:55 | → 전체 파라미터: 123,859,968
2025-10-14 09:40:55 | → 학습 가능 파라미터: 123,859,968
2025-10-14 09:40:55 | ============================================================
2025-10-14 09:40:55 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 09:40:55 | ============================================================
2025-10-14 09:40:55 | ============================================================
2025-10-14 09:40:55 | 모델 학습 시작
2025-10-14 09:40:55 | ============================================================
2025-10-14 09:40:55 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 09:40:55 | 학습 진행 중...
2025-10-14 09:41:02 | {'loss': 2.6595, 'grad_norm': 6.167272567749023, 'learning_rate': 1.1110802502362317e-06, 'epoch': 0.13}
2025-10-14 09:41:09 | {'loss': 1.9804, 'grad_norm': 5.4381256103515625, 'learning_rate': 2.2333835333031327e-06, 'epoch': 0.26}
2025-10-14 09:41:15 | {'loss': 1.7551, 'grad_norm': 4.867646217346191, 'learning_rate': 3.355686816370033e-06, 'epoch': 0.39}
2025-10-14 09:41:20 | {'loss': 1.6702, 'grad_norm': 4.237330436706543, 'learning_rate': 4.477990099436934e-06, 'epoch': 0.51}
2025-10-14 09:41:26 | {'loss': 1.6288, 'grad_norm': 4.005626678466797, 'learning_rate': 5.600293382503835e-06, 'epoch': 0.64}
2025-10-14 09:41:32 | {'loss': 1.5789, 'grad_norm': 3.7644295692443848, 'learning_rate': 5.535310636854652e-06, 'epoch': 0.77}
2025-10-14 09:41:38 | {'loss': 1.5568, 'grad_norm': 4.022952556610107, 'learning_rate': 5.4583351030366205e-06, 'epoch': 0.9}
2025-10-14 09:42:12 | {'eval_loss': 1.4557788372039795, 'eval_rouge1': 0.4009256373950428, 'eval_rouge2': 0.2578823629539612, 'eval_rougeL': 0.39567359233815574, 'eval_rouge_sum': 1.0544815926871598, 'eval_runtime': 29.3101, 'eval_samples_per_second': 17.025, 'eval_steps_per_second': 1.092, 'epoch': 1.0}
2025-10-14 09:42:15 | {'loss': 1.5491, 'grad_norm': 3.9831395149230957, 'learning_rate': 5.38135956921859e-06, 'epoch': 1.03}
2025-10-14 09:42:20 | {'loss': 1.4842, 'grad_norm': 4.324086666107178, 'learning_rate': 5.304384035400558e-06, 'epoch': 1.16}
2025-10-14 09:42:26 | {'loss': 1.477, 'grad_norm': 3.7515556812286377, 'learning_rate': 5.227408501582526e-06, 'epoch': 1.28}
2025-10-14 09:42:32 | {'loss': 1.4583, 'grad_norm': 4.33668851852417, 'learning_rate': 5.150432967764494e-06, 'epoch': 1.41}
2025-10-14 09:42:38 | {'loss': 1.4428, 'grad_norm': 3.8446431159973145, 'learning_rate': 5.073457433946463e-06, 'epoch': 1.54}
2025-10-14 09:42:44 | {'loss': 1.4606, 'grad_norm': 3.7427852153778076, 'learning_rate': 4.996481900128431e-06, 'epoch': 1.67}
2025-10-14 09:42:50 | {'loss': 1.4367, 'grad_norm': 3.576993465423584, 'learning_rate': 4.9195063663104e-06, 'epoch': 1.8}
2025-10-14 09:42:56 | {'loss': 1.4347, 'grad_norm': 4.07927131652832, 'learning_rate': 4.842530832492368e-06, 'epoch': 1.93}
2025-10-14 09:43:28 | {'eval_loss': 1.387251377105713, 'eval_rouge1': 0.4178104410710018, 'eval_rouge2': 0.2703548869107541, 'eval_rougeL': 0.4110598593986894, 'eval_rouge_sum': 1.0992251873804453, 'eval_runtime': 29.3611, 'eval_samples_per_second': 16.995, 'eval_steps_per_second': 1.09, 'epoch': 2.0}
2025-10-14 09:43:32 | {'loss': 1.3948, 'grad_norm': 3.655362367630005, 'learning_rate': 4.7655552986743365e-06, 'epoch': 2.05}
2025-10-14 09:43:38 | {'loss': 1.3693, 'grad_norm': 3.5381674766540527, 'learning_rate': 4.688579764856305e-06, 'epoch': 2.18}
2025-10-14 09:43:43 | {'loss': 1.38, 'grad_norm': 3.69254207611084, 'learning_rate': 4.611604231038274e-06, 'epoch': 2.31}
2025-10-14 09:43:49 | {'loss': 1.3681, 'grad_norm': 3.60644793510437, 'learning_rate': 4.534628697220242e-06, 'epoch': 2.44}
2025-10-14 09:43:56 | {'loss': 1.3664, 'grad_norm': 3.728604793548584, 'learning_rate': 4.45765316340221e-06, 'epoch': 2.57}
2025-10-14 09:44:01 | {'loss': 1.3495, 'grad_norm': 3.8833250999450684, 'learning_rate': 4.380677629584179e-06, 'epoch': 2.7}
2025-10-14 09:44:07 | {'loss': 1.3415, 'grad_norm': 3.898775339126587, 'learning_rate': 4.303702095766147e-06, 'epoch': 2.82}
2025-10-14 09:44:13 | {'loss': 1.3329, 'grad_norm': 3.7467939853668213, 'learning_rate': 4.226726561948115e-06, 'epoch': 2.95}
2025-10-14 09:44:46 | {'eval_loss': 1.3597710132598877, 'eval_rouge1': 0.42537631284691974, 'eval_rouge2': 0.2786187065872753, 'eval_rougeL': 0.4183212743484991, 'eval_rouge_sum': 1.1223162937826943, 'eval_runtime': 30.916, 'eval_samples_per_second': 16.141, 'eval_steps_per_second': 1.035, 'epoch': 3.0}
2025-10-14 09:44:51 | {'loss': 1.3232, 'grad_norm': 3.512653112411499, 'learning_rate': 4.149751028130083e-06, 'epoch': 3.08}
2025-10-14 09:44:58 | {'loss': 1.2979, 'grad_norm': 3.765101671218872, 'learning_rate': 4.072775494312052e-06, 'epoch': 3.21}
2025-10-14 09:45:04 | {'loss': 1.3119, 'grad_norm': 3.6802592277526855, 'learning_rate': 3.99579996049402e-06, 'epoch': 3.34}
2025-10-14 09:45:09 | {'loss': 1.2915, 'grad_norm': 3.740466356277466, 'learning_rate': 3.918824426675989e-06, 'epoch': 3.47}
2025-10-14 09:45:15 | {'loss': 1.3121, 'grad_norm': 3.5420145988464355, 'learning_rate': 3.841848892857957e-06, 'epoch': 3.59}
2025-10-14 09:45:21 | {'loss': 1.3079, 'grad_norm': 3.999253988265991, 'learning_rate': 3.764873359039926e-06, 'epoch': 3.72}
2025-10-14 09:45:27 | {'loss': 1.2837, 'grad_norm': 4.22756814956665, 'learning_rate': 3.687897825221894e-06, 'epoch': 3.85}
2025-10-14 09:45:34 | {'loss': 1.2813, 'grad_norm': 3.8044326305389404, 'learning_rate': 3.6109222914038624e-06, 'epoch': 3.98}
2025-10-14 09:46:04 | {'eval_loss': 1.3395545482635498, 'eval_rouge1': 0.4275791582483038, 'eval_rouge2': 0.2756205366900441, 'eval_rougeL': 0.4193286643588243, 'eval_rouge_sum': 1.1225283592971722, 'eval_runtime': 29.9737, 'eval_samples_per_second': 16.648, 'eval_steps_per_second': 1.068, 'epoch': 4.0}
2025-10-14 09:46:11 | {'loss': 1.2537, 'grad_norm': 4.05366849899292, 'learning_rate': 3.5339467575858307e-06, 'epoch': 4.11}
2025-10-14 09:46:16 | {'loss': 1.2417, 'grad_norm': 4.234026908874512, 'learning_rate': 3.456971223767799e-06, 'epoch': 4.24}
2025-10-14 09:46:22 | {'loss': 1.2552, 'grad_norm': 4.558953285217285, 'learning_rate': 3.3799956899497676e-06, 'epoch': 4.36}
2025-10-14 09:46:28 | {'loss': 1.2454, 'grad_norm': 3.478766441345215, 'learning_rate': 3.303020156131736e-06, 'epoch': 4.49}
2025-10-14 09:46:35 | {'loss': 1.2512, 'grad_norm': 3.626457452774048, 'learning_rate': 3.226044622313704e-06, 'epoch': 4.62}
2025-10-14 09:46:41 | {'loss': 1.2555, 'grad_norm': 4.008971214294434, 'learning_rate': 3.1490690884956732e-06, 'epoch': 4.75}
2025-10-14 09:46:47 | {'loss': 1.2453, 'grad_norm': 3.992769241333008, 'learning_rate': 3.0720935546776415e-06, 'epoch': 4.88}
2025-10-14 09:47:25 | {'eval_loss': 1.325614333152771, 'eval_rouge1': 0.4282507971598091, 'eval_rouge2': 0.2763656126679387, 'eval_rougeL': 0.41965542823619867, 'eval_rouge_sum': 1.1242718380639465, 'eval_runtime': 32.6597, 'eval_samples_per_second': 15.279, 'eval_steps_per_second': 0.98, 'epoch': 5.0}
2025-10-14 09:47:27 | {'loss': 1.2503, 'grad_norm': 3.872093915939331, 'learning_rate': 2.9951180208596098e-06, 'epoch': 5.01}
2025-10-14 09:47:33 | {'loss': 1.1998, 'grad_norm': 3.6166961193084717, 'learning_rate': 2.918142487041578e-06, 'epoch': 5.13}
2025-10-14 09:47:39 | {'loss': 1.2027, 'grad_norm': 4.2790021896362305, 'learning_rate': 2.8411669532235463e-06, 'epoch': 5.26}
2025-10-14 09:47:44 | {'loss': 1.2107, 'grad_norm': 3.7805895805358887, 'learning_rate': 2.764191419405515e-06, 'epoch': 5.39}
2025-10-14 09:47:50 | {'loss': 1.2134, 'grad_norm': 3.7741734981536865, 'learning_rate': 2.687215885587483e-06, 'epoch': 5.52}
2025-10-14 09:47:55 | {'loss': 1.2291, 'grad_norm': 3.876408100128174, 'learning_rate': 2.610240351769452e-06, 'epoch': 5.65}
2025-10-14 09:48:01 | {'loss': 1.2255, 'grad_norm': 4.104853630065918, 'learning_rate': 2.53326481795142e-06, 'epoch': 5.78}
2025-10-14 09:48:07 | {'loss': 1.2182, 'grad_norm': 3.9592373371124268, 'learning_rate': 2.4562892841333884e-06, 'epoch': 5.91}
2025-10-14 09:48:42 | {'eval_loss': 1.3205394744873047, 'eval_rouge1': 0.42328366805789147, 'eval_rouge2': 0.27358848469173846, 'eval_rougeL': 0.41503936340498004, 'eval_rouge_sum': 1.11191151615461, 'eval_runtime': 30.8199, 'eval_samples_per_second': 16.191, 'eval_steps_per_second': 1.038, 'epoch': 6.0}
2025-10-14 09:48:45 | {'loss': 1.2006, 'grad_norm': 3.5787127017974854, 'learning_rate': 2.3793137503153567e-06, 'epoch': 6.03}
2025-10-14 09:48:52 | {'loss': 1.1763, 'grad_norm': 3.3360230922698975, 'learning_rate': 2.302338216497325e-06, 'epoch': 6.16}
2025-10-14 09:48:58 | {'loss': 1.1977, 'grad_norm': 4.055524826049805, 'learning_rate': 2.2253626826792936e-06, 'epoch': 6.29}
2025-10-14 09:49:03 | {'loss': 1.1933, 'grad_norm': 3.794468879699707, 'learning_rate': 2.1483871488612623e-06, 'epoch': 6.42}
2025-10-14 09:49:09 | {'loss': 1.1901, 'grad_norm': 3.385221242904663, 'learning_rate': 2.0714116150432305e-06, 'epoch': 6.55}
2025-10-14 09:49:15 | {'loss': 1.1806, 'grad_norm': 3.960033655166626, 'learning_rate': 1.9944360812251988e-06, 'epoch': 6.68}
2025-10-14 09:49:21 | {'loss': 1.179, 'grad_norm': 3.939891815185547, 'learning_rate': 1.917460547407167e-06, 'epoch': 6.8}
2025-10-14 09:49:27 | {'loss': 1.1764, 'grad_norm': 3.9551587104797363, 'learning_rate': 1.8404850135891357e-06, 'epoch': 6.93}
2025-10-14 09:50:00 | {'eval_loss': 1.3179726600646973, 'eval_rouge1': 0.43216059778732174, 'eval_rouge2': 0.275271301885919, 'eval_rougeL': 0.42309198438288625, 'eval_rouge_sum': 1.130523884056127, 'eval_runtime': 29.873, 'eval_samples_per_second': 16.704, 'eval_steps_per_second': 1.071, 'epoch': 7.0}
2025-10-14 09:50:04 | {'loss': 1.1788, 'grad_norm': 3.618460178375244, 'learning_rate': 1.763509479771104e-06, 'epoch': 7.06}
2025-10-14 09:50:10 | {'loss': 1.1572, 'grad_norm': 3.5857369899749756, 'learning_rate': 1.6865339459530724e-06, 'epoch': 7.19}
2025-10-14 09:50:16 | {'loss': 1.1589, 'grad_norm': 3.3246991634368896, 'learning_rate': 1.6095584121350407e-06, 'epoch': 7.32}
2025-10-14 09:50:22 | {'loss': 1.1605, 'grad_norm': 3.8461153507232666, 'learning_rate': 1.5325828783170094e-06, 'epoch': 7.45}
2025-10-14 09:50:28 | {'loss': 1.1595, 'grad_norm': 3.7150845527648926, 'learning_rate': 1.4556073444989776e-06, 'epoch': 7.57}
2025-10-14 09:50:34 | {'loss': 1.169, 'grad_norm': 3.7534472942352295, 'learning_rate': 1.378631810680946e-06, 'epoch': 7.7}
2025-10-14 09:50:40 | {'loss': 1.1761, 'grad_norm': 3.9061853885650635, 'learning_rate': 1.3016562768629144e-06, 'epoch': 7.83}
2025-10-14 09:50:45 | {'loss': 1.153, 'grad_norm': 3.650891065597534, 'learning_rate': 1.2246807430448828e-06, 'epoch': 7.96}
2025-10-14 09:51:19 | {'eval_loss': 1.3175435066223145, 'eval_rouge1': 0.43324290644959035, 'eval_rouge2': 0.27859831676210567, 'eval_rougeL': 0.4235566644057539, 'eval_rouge_sum': 1.13539788761745, 'eval_runtime': 31.7579, 'eval_samples_per_second': 15.713, 'eval_steps_per_second': 1.008, 'epoch': 8.0}
2025-10-14 09:51:24 | {'loss': 1.1533, 'grad_norm': 3.952575922012329, 'learning_rate': 1.147705209226851e-06, 'epoch': 8.09}
2025-10-14 09:51:30 | {'loss': 1.1439, 'grad_norm': 4.56830358505249, 'learning_rate': 1.0707296754088198e-06, 'epoch': 8.22}
2025-10-14 09:51:37 | {'loss': 1.1354, 'grad_norm': 4.171051979064941, 'learning_rate': 9.93754141590788e-07, 'epoch': 8.34}
2025-10-14 09:51:43 | {'loss': 1.1581, 'grad_norm': 3.750812530517578, 'learning_rate': 9.167786077727564e-07, 'epoch': 8.47}
2025-10-14 09:51:49 | {'loss': 1.1306, 'grad_norm': 3.4083478450775146, 'learning_rate': 8.398030739547248e-07, 'epoch': 8.6}
2025-10-14 09:51:55 | {'loss': 1.1583, 'grad_norm': 3.6658682823181152, 'learning_rate': 7.628275401366931e-07, 'epoch': 8.73}
2025-10-14 09:52:01 | {'loss': 1.166, 'grad_norm': 3.831073045730591, 'learning_rate': 6.858520063186616e-07, 'epoch': 8.86}
2025-10-14 09:52:08 | {'loss': 1.1354, 'grad_norm': 4.390079021453857, 'learning_rate': 6.088764725006299e-07, 'epoch': 8.99}
2025-10-14 09:52:40 | {'eval_loss': 1.315134048461914, 'eval_rouge1': 0.4297478409249468, 'eval_rouge2': 0.2794930617141603, 'eval_rougeL': 0.4228222165395745, 'eval_rouge_sum': 1.1320631191786816, 'eval_runtime': 31.4272, 'eval_samples_per_second': 15.878, 'eval_steps_per_second': 1.018, 'epoch': 9.0}
2025-10-14 09:52:46 | {'loss': 1.1337, 'grad_norm': 4.772834300994873, 'learning_rate': 5.319009386825984e-07, 'epoch': 9.11}
2025-10-14 09:52:52 | {'loss': 1.1503, 'grad_norm': 3.646184206008911, 'learning_rate': 4.549254048645668e-07, 'epoch': 9.24}
2025-10-14 09:52:58 | {'loss': 1.1517, 'grad_norm': 3.987694025039673, 'learning_rate': 3.7794987104653517e-07, 'epoch': 9.37}
2025-10-14 09:53:04 | {'loss': 1.1446, 'grad_norm': 3.8871986865997314, 'learning_rate': 3.009743372285036e-07, 'epoch': 9.5}
2025-10-14 09:53:11 | {'loss': 1.1259, 'grad_norm': 4.096502304077148, 'learning_rate': 2.2399880341047197e-07, 'epoch': 9.63}
2025-10-14 09:53:17 | {'loss': 1.1146, 'grad_norm': 3.756629228591919, 'learning_rate': 1.4702326959244036e-07, 'epoch': 9.76}
2025-10-14 09:53:23 | {'loss': 1.1433, 'grad_norm': 3.9379565715789795, 'learning_rate': 7.004773577440876e-08, 'epoch': 9.88}
2025-10-14 09:53:58 | {'eval_loss': 1.3162257671356201, 'eval_rouge1': 0.4318815621669479, 'eval_rouge2': 0.27784879682327834, 'eval_rougeL': 0.4230438020012997, 'eval_rouge_sum': 1.1327741609915258, 'eval_runtime': 29.8102, 'eval_samples_per_second': 16.739, 'eval_steps_per_second': 1.073, 'epoch': 10.0}
2025-10-14 09:53:59 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 09:53:59 | {'train_runtime': 783.6938, 'train_samples_per_second': 158.952, 'train_steps_per_second': 9.94, 'train_loss': 1.3010192528309656, 'epoch': 10.0}
2025-10-14 09:53:59 | 최종 모델 저장 중...
2025-10-14 09:54:00 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 09:54:00 | 최종 평가 중...
2025-10-14 09:54:31 | 최종 평가 결과:
2025-10-14 09:54:31 | eval_rouge1: 0.4332
2025-10-14 09:54:31 | eval_rouge2: 0.2786
2025-10-14 09:54:31 | eval_rougeL: 0.4236
2025-10-14 09:54:31 | eval_rouge_sum: 1.1354
2025-10-14 09:54:31 | ============================================================
2025-10-14 09:54:31 | ✅ 학습 완료!
2025-10-14 09:54:31 | ============================================================
2025-10-14 09:54:31 | 모델 평가 중...
2025-10-14 09:55:02 | → 메트릭 'eval_rougeL' 사용: 0.4236
2025-10-14 09:55:02 | Trial 0 완료
2025-10-14 09:55:02 | - ROUGE-L F1: 0.4236
2025-10-14 09:55:02 | [I 2025-10-14 09:55:02,547] Trial 0 finished with value: 0.4235566644057539 and parameters: {'learning_rate': 5.611516415334504e-06, 'num_epochs': 10, 'warmup_ratio': 0.146398788362281, 'weight_decay': 0.05986584841970366, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.7486639612006325}. Best is trial 0 with value: 0.4235566644057539.
2025-10-14 09:55:02 | ============================================================
2025-10-14 09:55:02 | Trial 1 시작
2025-10-14 09:55:02 | 파라미터: {'learning_rate': 2.6587543983272713e-06, 'num_epochs': 4, 'warmup_ratio': 0.03668090197068676, 'weight_decay': 0.030424224295953775, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.6777639420895203}
2025-10-14 09:55:02 | ============================================================
2025-10-14 09:55:02 | 모델 타입: encoder_decoder
2025-10-14 09:55:02 | ============================================================
2025-10-14 09:55:02 | 모델 및 토크나이저 로딩 시작
2025-10-14 09:55:02 | ============================================================
2025-10-14 09:55:02 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 09:55:03 | 모델 로딩: digit82/kobart-summarization
2025-10-14 09:55:03 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 09:55:04 | → 디바이스: cuda
2025-10-14 09:55:04 | → 전체 파라미터: 123,859,968
2025-10-14 09:55:04 | → 학습 가능 파라미터: 123,859,968
2025-10-14 09:55:04 | ============================================================
2025-10-14 09:55:04 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 09:55:04 | ============================================================
2025-10-14 09:55:04 | ============================================================
2025-10-14 09:55:04 | 모델 학습 시작
2025-10-14 09:55:04 | ============================================================
2025-10-14 09:55:04 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 09:55:04 | 학습 진행 중...
2025-10-14 09:55:10 | {'loss': 2.8259, 'grad_norm': 8.172089576721191, 'learning_rate': 5.264333708687998e-07, 'epoch': 0.13}
2025-10-14 09:55:16 | {'loss': 2.1808, 'grad_norm': 5.762016773223877, 'learning_rate': 1.058184250534254e-06, 'epoch': 0.26}
2025-10-14 09:55:23 | {'loss': 1.878, 'grad_norm': 5.230891704559326, 'learning_rate': 1.589935130199708e-06, 'epoch': 0.39}
2025-10-14 09:55:29 | {'loss': 1.7612, 'grad_norm': 4.400380611419678, 'learning_rate': 2.1216860098651628e-06, 'epoch': 0.51}
2025-10-14 09:55:35 | {'loss': 1.7102, 'grad_norm': 4.181188106536865, 'learning_rate': 2.6534368895306168e-06, 'epoch': 0.64}
2025-10-14 09:55:41 | {'loss': 1.6519, 'grad_norm': 3.831162452697754, 'learning_rate': 2.5581363993080054e-06, 'epoch': 0.77}
2025-10-14 09:55:47 | {'loss': 1.6272, 'grad_norm': 4.140888214111328, 'learning_rate': 2.456502056864302e-06, 'epoch': 0.9}
2025-10-14 09:56:25 | {'eval_loss': 1.5202326774597168, 'eval_rouge1': 0.39451122937264427, 'eval_rouge2': 0.25133067126009395, 'eval_rougeL': 0.3897280061219907, 'eval_rouge_sum': 1.035569906754729, 'eval_runtime': 33.6975, 'eval_samples_per_second': 14.808, 'eval_steps_per_second': 0.95, 'epoch': 1.0}
2025-10-14 09:56:29 | {'loss': 1.625, 'grad_norm': 4.093904972076416, 'learning_rate': 2.3548677144205994e-06, 'epoch': 1.03}
2025-10-14 09:56:35 | {'loss': 1.5736, 'grad_norm': 5.849761009216309, 'learning_rate': 2.2532333719768966e-06, 'epoch': 1.16}
2025-10-14 09:56:40 | {'loss': 1.5646, 'grad_norm': 3.9377615451812744, 'learning_rate': 2.1515990295331933e-06, 'epoch': 1.28}
2025-10-14 09:56:46 | {'loss': 1.5473, 'grad_norm': 5.099858283996582, 'learning_rate': 2.04996468708949e-06, 'epoch': 1.41}
2025-10-14 09:56:52 | {'loss': 1.5277, 'grad_norm': 3.9931230545043945, 'learning_rate': 1.9483303446457872e-06, 'epoch': 1.54}
2025-10-14 09:56:58 | {'loss': 1.5469, 'grad_norm': 3.895902633666992, 'learning_rate': 1.8466960022020842e-06, 'epoch': 1.67}
2025-10-14 09:57:05 | {'loss': 1.5234, 'grad_norm': 3.7791032791137695, 'learning_rate': 1.7450616597583812e-06, 'epoch': 1.8}
2025-10-14 09:57:11 | {'loss': 1.5228, 'grad_norm': 4.161285877227783, 'learning_rate': 1.643427317314678e-06, 'epoch': 1.93}
2025-10-14 09:57:47 | {'eval_loss': 1.452023983001709, 'eval_rouge1': 0.40205051371346806, 'eval_rouge2': 0.2541075256822541, 'eval_rougeL': 0.39501214501416143, 'eval_rouge_sum': 1.0511701844098835, 'eval_runtime': 32.3227, 'eval_samples_per_second': 15.438, 'eval_steps_per_second': 0.99, 'epoch': 2.0}
2025-10-14 09:57:51 | {'loss': 1.4955, 'grad_norm': 3.9387574195861816, 'learning_rate': 1.5417929748709751e-06, 'epoch': 2.05}
2025-10-14 09:57:57 | {'loss': 1.4867, 'grad_norm': 3.6216318607330322, 'learning_rate': 1.4401586324272719e-06, 'epoch': 2.18}
2025-10-14 09:58:03 | {'loss': 1.4959, 'grad_norm': 3.8064801692962646, 'learning_rate': 1.338524289983569e-06, 'epoch': 2.31}
2025-10-14 09:58:10 | {'loss': 1.4824, 'grad_norm': 3.641301393508911, 'learning_rate': 1.2368899475398658e-06, 'epoch': 2.44}
2025-10-14 09:58:16 | {'loss': 1.4826, 'grad_norm': 4.002499580383301, 'learning_rate': 1.1352556050961628e-06, 'epoch': 2.57}
2025-10-14 09:58:21 | {'loss': 1.4639, 'grad_norm': 4.02380895614624, 'learning_rate': 1.0336212626524597e-06, 'epoch': 2.7}
2025-10-14 09:58:27 | {'loss': 1.4586, 'grad_norm': 4.093143463134766, 'learning_rate': 9.319869202087569e-07, 'epoch': 2.82}
2025-10-14 09:58:34 | {'loss': 1.4498, 'grad_norm': 3.9309113025665283, 'learning_rate': 8.303525777650538e-07, 'epoch': 2.95}
2025-10-14 09:59:08 | {'eval_loss': 1.4296870231628418, 'eval_rouge1': 0.40768317870090404, 'eval_rouge2': 0.26327741602452015, 'eval_rougeL': 0.4015239171947595, 'eval_rouge_sum': 1.0724845119201838, 'eval_runtime': 31.8209, 'eval_samples_per_second': 15.682, 'eval_steps_per_second': 1.006, 'epoch': 3.0}
2025-10-14 09:59:13 | {'loss': 1.4599, 'grad_norm': 3.5998668670654297, 'learning_rate': 7.287182353213507e-07, 'epoch': 3.08}
2025-10-14 09:59:20 | {'loss': 1.4489, 'grad_norm': 4.04705286026001, 'learning_rate': 6.270838928776477e-07, 'epoch': 3.21}
2025-10-14 09:59:26 | {'loss': 1.4619, 'grad_norm': 3.9724280834198, 'learning_rate': 5.254495504339447e-07, 'epoch': 3.34}
2025-10-14 09:59:32 | {'loss': 1.4412, 'grad_norm': 3.9732918739318848, 'learning_rate': 4.238152079902416e-07, 'epoch': 3.47}
2025-10-14 09:59:37 | {'loss': 1.463, 'grad_norm': 3.8178064823150635, 'learning_rate': 3.2218086554653864e-07, 'epoch': 3.59}
2025-10-14 09:59:43 | {'loss': 1.4595, 'grad_norm': 4.017246246337891, 'learning_rate': 2.205465231028356e-07, 'epoch': 3.72}
2025-10-14 09:59:50 | {'loss': 1.4333, 'grad_norm': 4.3628058433532715, 'learning_rate': 1.1891218065913255e-07, 'epoch': 3.85}
2025-10-14 09:59:56 | {'loss': 1.4333, 'grad_norm': 4.079375267028809, 'learning_rate': 1.7277838215429515e-08, 'epoch': 3.98}
2025-10-14 10:00:28 | {'eval_loss': 1.4221895933151245, 'eval_rouge1': 0.4083565357308172, 'eval_rouge2': 0.26483903557399724, 'eval_rougeL': 0.4009912545554714, 'eval_rouge_sum': 1.0741868258602858, 'eval_runtime': 31.3141, 'eval_samples_per_second': 15.935, 'eval_steps_per_second': 1.022, 'epoch': 4.0}
2025-10-14 10:00:29 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:00:29 | {'train_runtime': 324.9691, 'train_samples_per_second': 153.332, 'train_steps_per_second': 9.589, 'train_loss': 1.5954712756331986, 'epoch': 4.0}
2025-10-14 10:00:29 | 최종 모델 저장 중...
2025-10-14 10:00:30 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:00:30 | 최종 평가 중...
2025-10-14 10:01:02 | 최종 평가 결과:
2025-10-14 10:01:02 | eval_rouge1: 0.4084
2025-10-14 10:01:02 | eval_rouge2: 0.2648
2025-10-14 10:01:02 | eval_rougeL: 0.4010
2025-10-14 10:01:02 | eval_rouge_sum: 1.0742
2025-10-14 10:01:02 | ============================================================
2025-10-14 10:01:02 | ✅ 학습 완료!
2025-10-14 10:01:02 | ============================================================
2025-10-14 10:01:02 | 모델 평가 중...
2025-10-14 10:01:33 | → 메트릭 'eval_rougeL' 사용: 0.4010
2025-10-14 10:01:33 | Trial 1 완료
2025-10-14 10:01:33 | - ROUGE-L F1: 0.4010
2025-10-14 10:01:33 | [I 2025-10-14 10:01:33,390] Trial 1 finished with value: 0.4009912545554714 and parameters: {'learning_rate': 2.6587543983272713e-06, 'num_epochs': 4, 'warmup_ratio': 0.03668090197068676, 'weight_decay': 0.030424224295953775, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.6777639420895203}. Best is trial 0 with value: 0.4235566644057539.
2025-10-14 10:01:33 | ============================================================
2025-10-14 10:01:33 | Trial 2 시작
2025-10-14 10:01:33 | 파라미터: {'learning_rate': 2.5081156860452325e-06, 'num_epochs': 7, 'warmup_ratio': 0.1184829137724085, 'weight_decay': 0.0046450412719997725, 'scheduler_type': 'polynomial', 'num_beams': 2, 'length_penalty': 1.5263495397682354}
2025-10-14 10:01:33 | ============================================================
2025-10-14 10:01:33 | 모델 타입: encoder_decoder
2025-10-14 10:01:33 | ============================================================
2025-10-14 10:01:33 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:01:33 | ============================================================
2025-10-14 10:01:33 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:01:33 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:01:34 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:01:35 | → 디바이스: cuda
2025-10-14 10:01:35 | → 전체 파라미터: 123,859,968
2025-10-14 10:01:35 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:01:35 | ============================================================
2025-10-14 10:01:35 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:01:35 | ============================================================
2025-10-14 10:01:35 | ============================================================
2025-10-14 10:01:35 | 모델 학습 시작
2025-10-14 10:01:35 | ============================================================
2025-10-14 10:01:35 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:01:35 | 학습 진행 중...
2025-10-14 10:01:41 | {'loss': 2.8373, 'grad_norm': 8.378257751464844, 'learning_rate': 4.966069058369561e-07, 'epoch': 0.13}
2025-10-14 10:01:47 | {'loss': 2.2016, 'grad_norm': 5.808915615081787, 'learning_rate': 9.982300430460026e-07, 'epoch': 0.26}
2025-10-14 10:01:53 | {'loss': 1.8904, 'grad_norm': 5.264982223510742, 'learning_rate': 1.499853180255049e-06, 'epoch': 0.39}
2025-10-14 10:01:59 | {'loss': 1.7698, 'grad_norm': 4.407628059387207, 'learning_rate': 2.0014763174640955e-06, 'epoch': 0.51}
2025-10-14 10:02:06 | {'loss': 1.7174, 'grad_norm': 4.203019618988037, 'learning_rate': 2.503099454673142e-06, 'epoch': 0.64}
2025-10-14 10:02:12 | {'loss': 1.6582, 'grad_norm': 3.842395305633545, 'learning_rate': 2.4579837553126505e-06, 'epoch': 0.77}
2025-10-14 10:02:18 | {'loss': 1.6329, 'grad_norm': 4.148924350738525, 'learning_rate': 2.407345441441356e-06, 'epoch': 0.9}
2025-10-14 10:02:48 | {'eval_loss': 1.5246129035949707, 'eval_rouge1': 0.4242191634903024, 'eval_rouge2': 0.26260500257239866, 'eval_rougeL': 0.4162820893862193, 'eval_rouge_sum': 1.1031062554489204, 'eval_runtime': 25.5886, 'eval_samples_per_second': 19.501, 'eval_steps_per_second': 1.251, 'epoch': 1.0}
2025-10-14 10:02:51 | {'loss': 1.6301, 'grad_norm': 4.1015849113464355, 'learning_rate': 2.3567071275700607e-06, 'epoch': 1.03}
2025-10-14 10:02:57 | {'loss': 1.5786, 'grad_norm': 6.816154956817627, 'learning_rate': 2.3060688136987663e-06, 'epoch': 1.16}
2025-10-14 10:03:03 | {'loss': 1.5687, 'grad_norm': 3.947718620300293, 'learning_rate': 2.2554304998274714e-06, 'epoch': 1.28}
2025-10-14 10:03:10 | {'loss': 1.5507, 'grad_norm': 5.118168830871582, 'learning_rate': 2.2047921859561765e-06, 'epoch': 1.41}
2025-10-14 10:03:16 | {'loss': 1.5301, 'grad_norm': 3.98695969581604, 'learning_rate': 2.1541538720848816e-06, 'epoch': 1.54}
2025-10-14 10:03:22 | {'loss': 1.5485, 'grad_norm': 3.9019861221313477, 'learning_rate': 2.1035155582135867e-06, 'epoch': 1.67}
2025-10-14 10:03:28 | {'loss': 1.524, 'grad_norm': 3.776547431945801, 'learning_rate': 2.0528772443422923e-06, 'epoch': 1.8}
2025-10-14 10:03:35 | {'loss': 1.5226, 'grad_norm': 4.148220062255859, 'learning_rate': 2.002238930470997e-06, 'epoch': 1.93}
2025-10-14 10:04:03 | {'eval_loss': 1.4500929117202759, 'eval_rouge1': 0.4265841793382944, 'eval_rouge2': 0.2672539402678939, 'eval_rougeL': 0.41749252756421895, 'eval_rouge_sum': 1.1113306471704072, 'eval_runtime': 25.264, 'eval_samples_per_second': 19.751, 'eval_steps_per_second': 1.267, 'epoch': 2.0}
2025-10-14 10:04:07 | {'loss': 1.4934, 'grad_norm': 3.927990436553955, 'learning_rate': 1.9516006165997025e-06, 'epoch': 2.05}
2025-10-14 10:04:14 | {'loss': 1.4828, 'grad_norm': 3.622175931930542, 'learning_rate': 1.9009623027284078e-06, 'epoch': 2.18}
2025-10-14 10:04:20 | {'loss': 1.4913, 'grad_norm': 3.8099958896636963, 'learning_rate': 1.8503239888571127e-06, 'epoch': 2.31}
2025-10-14 10:04:27 | {'loss': 1.4768, 'grad_norm': 3.6678779125213623, 'learning_rate': 1.799685674985818e-06, 'epoch': 2.44}
2025-10-14 10:04:33 | {'loss': 1.4761, 'grad_norm': 4.0099263191223145, 'learning_rate': 1.7490473611145232e-06, 'epoch': 2.57}
2025-10-14 10:04:39 | {'loss': 1.4564, 'grad_norm': 4.024416446685791, 'learning_rate': 1.6984090472432283e-06, 'epoch': 2.7}
2025-10-14 10:04:45 | {'loss': 1.4497, 'grad_norm': 4.074224472045898, 'learning_rate': 1.6477707333719334e-06, 'epoch': 2.82}
2025-10-14 10:04:52 | {'loss': 1.4398, 'grad_norm': 3.9111688137054443, 'learning_rate': 1.5971324195006387e-06, 'epoch': 2.95}
2025-10-14 10:05:19 | {'eval_loss': 1.4217393398284912, 'eval_rouge1': 0.4396257403654821, 'eval_rouge2': 0.27632098080198614, 'eval_rougeL': 0.4297526014018091, 'eval_rouge_sum': 1.1456993225692773, 'eval_runtime': 25.1864, 'eval_samples_per_second': 19.812, 'eval_steps_per_second': 1.271, 'epoch': 3.0}
2025-10-14 10:05:25 | {'loss': 1.4452, 'grad_norm': 3.6083688735961914, 'learning_rate': 1.546494105629344e-06, 'epoch': 3.08}
2025-10-14 10:05:31 | {'loss': 1.4309, 'grad_norm': 4.032492637634277, 'learning_rate': 1.495855791758049e-06, 'epoch': 3.21}
2025-10-14 10:05:36 | {'loss': 1.4428, 'grad_norm': 3.916658401489258, 'learning_rate': 1.4452174778867543e-06, 'epoch': 3.34}
2025-10-14 10:05:42 | {'loss': 1.4211, 'grad_norm': 3.946873664855957, 'learning_rate': 1.3945791640154594e-06, 'epoch': 3.47}
2025-10-14 10:05:48 | {'loss': 1.4419, 'grad_norm': 3.772716522216797, 'learning_rate': 1.3439408501441647e-06, 'epoch': 3.59}
2025-10-14 10:05:55 | {'loss': 1.4365, 'grad_norm': 4.038565158843994, 'learning_rate': 1.2933025362728696e-06, 'epoch': 3.72}
2025-10-14 10:06:01 | {'loss': 1.4097, 'grad_norm': 4.309682369232178, 'learning_rate': 1.242664222401575e-06, 'epoch': 3.85}
2025-10-14 10:06:07 | {'loss': 1.408, 'grad_norm': 4.029696941375732, 'learning_rate': 1.19202590853028e-06, 'epoch': 3.98}
2025-10-14 10:06:31 | {'eval_loss': 1.4023377895355225, 'eval_rouge1': 0.44203574809541757, 'eval_rouge2': 0.2761167126337095, 'eval_rougeL': 0.4303707888223939, 'eval_rouge_sum': 1.148523249551521, 'eval_runtime': 23.8724, 'eval_samples_per_second': 20.903, 'eval_steps_per_second': 1.34, 'epoch': 4.0}
2025-10-14 10:06:38 | {'loss': 1.4015, 'grad_norm': 4.119922637939453, 'learning_rate': 1.1413875946589852e-06, 'epoch': 4.11}
2025-10-14 10:06:44 | {'loss': 1.391, 'grad_norm': 4.374491214752197, 'learning_rate': 1.0907492807876905e-06, 'epoch': 4.24}
2025-10-14 10:06:50 | {'loss': 1.408, 'grad_norm': 4.9678730964660645, 'learning_rate': 1.0401109669163956e-06, 'epoch': 4.36}
2025-10-14 10:06:56 | {'loss': 1.3919, 'grad_norm': 3.5322301387786865, 'learning_rate': 9.89472653045101e-07, 'epoch': 4.49}
2025-10-14 10:07:03 | {'loss': 1.4012, 'grad_norm': 3.7538974285125732, 'learning_rate': 9.38834339173806e-07, 'epoch': 4.62}
2025-10-14 10:07:09 | {'loss': 1.4031, 'grad_norm': 4.307265758514404, 'learning_rate': 8.881960253025112e-07, 'epoch': 4.75}
2025-10-14 10:07:14 | {'loss': 1.3933, 'grad_norm': 4.093590259552002, 'learning_rate': 8.375577114312164e-07, 'epoch': 4.88}
2025-10-14 10:07:44 | {'eval_loss': 1.3913109302520752, 'eval_rouge1': 0.4379143156206881, 'eval_rouge2': 0.2742892835329695, 'eval_rougeL': 0.4283969274089143, 'eval_rouge_sum': 1.1406005265625718, 'eval_runtime': 24.292, 'eval_samples_per_second': 20.542, 'eval_steps_per_second': 1.317, 'epoch': 5.0}
2025-10-14 10:07:46 | {'loss': 1.3973, 'grad_norm': 4.018677711486816, 'learning_rate': 7.869193975599215e-07, 'epoch': 5.01}
2025-10-14 10:07:52 | {'loss': 1.365, 'grad_norm': 3.789008140563965, 'learning_rate': 7.362810836886266e-07, 'epoch': 5.13}
2025-10-14 10:07:58 | {'loss': 1.3709, 'grad_norm': 4.330713272094727, 'learning_rate': 6.856427698173319e-07, 'epoch': 5.26}
2025-10-14 10:08:05 | {'loss': 1.3765, 'grad_norm': 3.996544361114502, 'learning_rate': 6.350044559460371e-07, 'epoch': 5.39}
2025-10-14 10:08:11 | {'loss': 1.3829, 'grad_norm': 3.8731961250305176, 'learning_rate': 5.843661420747423e-07, 'epoch': 5.52}
2025-10-14 10:08:17 | {'loss': 1.3984, 'grad_norm': 3.9059720039367676, 'learning_rate': 5.337278282034474e-07, 'epoch': 5.65}
2025-10-14 10:08:23 | {'loss': 1.3928, 'grad_norm': 4.401247501373291, 'learning_rate': 4.830895143321526e-07, 'epoch': 5.78}
2025-10-14 10:08:29 | {'loss': 1.3848, 'grad_norm': 3.850639820098877, 'learning_rate': 4.324512004608578e-07, 'epoch': 5.91}
2025-10-14 10:08:58 | {'eval_loss': 1.384639024734497, 'eval_rouge1': 0.43108444851987127, 'eval_rouge2': 0.2727310326694854, 'eval_rougeL': 0.4208765088984608, 'eval_rouge_sum': 1.1246919900878174, 'eval_runtime': 24.0807, 'eval_samples_per_second': 20.722, 'eval_steps_per_second': 1.329, 'epoch': 6.0}
2025-10-14 10:09:01 | {'loss': 1.373, 'grad_norm': 3.9371883869171143, 'learning_rate': 3.818128865895629e-07, 'epoch': 6.03}
2025-10-14 10:09:07 | {'loss': 1.3595, 'grad_norm': 3.527648448944092, 'learning_rate': 3.311745727182681e-07, 'epoch': 6.16}
2025-10-14 10:09:14 | {'loss': 1.3893, 'grad_norm': 4.132686138153076, 'learning_rate': 2.805362588469733e-07, 'epoch': 6.29}
2025-10-14 10:09:20 | {'loss': 1.3788, 'grad_norm': 4.095948696136475, 'learning_rate': 2.298979449756785e-07, 'epoch': 6.42}
2025-10-14 10:09:27 | {'loss': 1.3786, 'grad_norm': 3.4282639026641846, 'learning_rate': 1.792596311043837e-07, 'epoch': 6.55}
2025-10-14 10:09:33 | {'loss': 1.3681, 'grad_norm': 4.019199848175049, 'learning_rate': 1.2862131723308884e-07, 'epoch': 6.68}
2025-10-14 10:09:38 | {'loss': 1.3632, 'grad_norm': 4.060740947723389, 'learning_rate': 7.798300336179403e-08, 'epoch': 6.8}
2025-10-14 10:09:45 | {'loss': 1.3624, 'grad_norm': 3.8699235916137695, 'learning_rate': 2.7344689490499205e-08, 'epoch': 6.93}
2025-10-14 10:10:13 | {'eval_loss': 1.383307933807373, 'eval_rouge1': 0.4345986240132117, 'eval_rouge2': 0.2762639248462288, 'eval_rougeL': 0.4245321722410575, 'eval_rouge_sum': 1.135394721100498, 'eval_runtime': 24.6246, 'eval_samples_per_second': 20.264, 'eval_steps_per_second': 1.3, 'epoch': 7.0}
2025-10-14 10:10:14 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:10:14 | {'train_runtime': 519.3623, 'train_samples_per_second': 167.896, 'train_steps_per_second': 10.499, 'train_loss': 1.5023462591795622, 'epoch': 7.0}
2025-10-14 10:10:14 | 최종 모델 저장 중...
2025-10-14 10:10:15 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:10:15 | 최종 평가 중...
2025-10-14 10:10:41 | 최종 평가 결과:
2025-10-14 10:10:41 | eval_rouge1: 0.4420
2025-10-14 10:10:41 | eval_rouge2: 0.2761
2025-10-14 10:10:41 | eval_rougeL: 0.4304
2025-10-14 10:10:41 | eval_rouge_sum: 1.1485
2025-10-14 10:10:41 | ============================================================
2025-10-14 10:10:41 | ✅ 학습 완료!
2025-10-14 10:10:41 | ============================================================
2025-10-14 10:10:41 | 모델 평가 중...
2025-10-14 10:11:05 | → 메트릭 'eval_rougeL' 사용: 0.4304
2025-10-14 10:11:05 | Trial 2 완료
2025-10-14 10:11:05 | - ROUGE-L F1: 0.4304
2025-10-14 10:11:05 | [I 2025-10-14 10:11:05,769] Trial 2 finished with value: 0.4303707888223939 and parameters: {'learning_rate': 2.5081156860452325e-06, 'num_epochs': 7, 'warmup_ratio': 0.1184829137724085, 'weight_decay': 0.0046450412719997725, 'scheduler_type': 'polynomial', 'num_beams': 2, 'length_penalty': 1.5263495397682354}. Best is trial 2 with value: 0.4303707888223939.
2025-10-14 10:11:05 | ============================================================
2025-10-14 10:11:05 | Trial 3 시작
2025-10-14 10:11:05 | 파라미터: {'learning_rate': 7.5911048052827045e-06, 'num_epochs': 3, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'scheduler_type': 'linear', 'num_beams': 8, 'length_penalty': 1.6626992350416718}
2025-10-14 10:11:05 | ============================================================
2025-10-14 10:11:05 | 모델 타입: encoder_decoder
2025-10-14 10:11:05 | ============================================================
2025-10-14 10:11:05 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:11:05 | ============================================================
2025-10-14 10:11:05 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:11:06 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:11:06 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:11:07 | → 디바이스: cuda
2025-10-14 10:11:07 | → 전체 파라미터: 123,859,968
2025-10-14 10:11:07 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:11:07 | ============================================================
2025-10-14 10:11:07 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:11:07 | ============================================================
2025-10-14 10:11:07 | ============================================================
2025-10-14 10:11:07 | 모델 학습 시작
2025-10-14 10:11:07 | ============================================================
2025-10-14 10:11:07 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:11:07 | 학습 진행 중...
2025-10-14 10:11:14 | {'loss': 2.5858, 'grad_norm': 5.644097805023193, 'learning_rate': 1.5030387514459755e-06, 'epoch': 0.13}
2025-10-14 10:11:20 | {'loss': 1.9162, 'grad_norm': 5.3125739097595215, 'learning_rate': 3.0212597125025164e-06, 'epoch': 0.26}
2025-10-14 10:11:27 | {'loss': 1.7173, 'grad_norm': 4.707469940185547, 'learning_rate': 4.539480673559057e-06, 'epoch': 0.39}
2025-10-14 10:11:33 | {'loss': 1.6387, 'grad_norm': 4.166406631469727, 'learning_rate': 6.057701634615598e-06, 'epoch': 0.51}
2025-10-14 10:11:39 | {'loss': 1.5999, 'grad_norm': 3.934478282928467, 'learning_rate': 7.575922595672139e-06, 'epoch': 0.64}
2025-10-14 10:11:44 | {'loss': 1.5528, 'grad_norm': 3.7932288646698, 'learning_rate': 7.182003348710583e-06, 'epoch': 0.77}
2025-10-14 10:11:50 | {'loss': 1.533, 'grad_norm': 3.9965507984161377, 'learning_rate': 6.7687695541932885e-06, 'epoch': 0.9}
2025-10-14 10:12:26 | {'eval_loss': 1.434436559677124, 'eval_rouge1': 0.3993155473824428, 'eval_rouge2': 0.2572274162026085, 'eval_rougeL': 0.3947114221294347, 'eval_rouge_sum': 1.051254385714486, 'eval_runtime': 30.9195, 'eval_samples_per_second': 16.139, 'eval_steps_per_second': 1.035, 'epoch': 1.0}
2025-10-14 10:12:29 | {'loss': 1.5226, 'grad_norm': 3.8914005756378174, 'learning_rate': 6.355535759675993e-06, 'epoch': 1.03}
2025-10-14 10:12:36 | {'loss': 1.4518, 'grad_norm': 4.2696919441223145, 'learning_rate': 5.9423019651586985e-06, 'epoch': 1.16}
2025-10-14 10:12:42 | {'loss': 1.4469, 'grad_norm': 3.7187297344207764, 'learning_rate': 5.5290681706414035e-06, 'epoch': 1.28}
2025-10-14 10:12:48 | {'loss': 1.4297, 'grad_norm': 4.305914878845215, 'learning_rate': 5.115834376124109e-06, 'epoch': 1.41}
2025-10-14 10:12:53 | {'loss': 1.4165, 'grad_norm': 3.8206255435943604, 'learning_rate': 4.7026005816068135e-06, 'epoch': 1.54}
2025-10-14 10:13:00 | {'loss': 1.435, 'grad_norm': 3.689394950866699, 'learning_rate': 4.289366787089519e-06, 'epoch': 1.67}
2025-10-14 10:13:06 | {'loss': 1.4128, 'grad_norm': 3.556363105773926, 'learning_rate': 3.876132992572224e-06, 'epoch': 1.8}
2025-10-14 10:13:12 | {'loss': 1.4126, 'grad_norm': 3.9875056743621826, 'learning_rate': 3.4628991980549303e-06, 'epoch': 1.93}
2025-10-14 10:13:46 | {'eval_loss': 1.3771576881408691, 'eval_rouge1': 0.4115249663603202, 'eval_rouge2': 0.26805200539526775, 'eval_rougeL': 0.40442806164294237, 'eval_rouge_sum': 1.0840050333985303, 'eval_runtime': 30.5928, 'eval_samples_per_second': 16.311, 'eval_steps_per_second': 1.046, 'epoch': 2.0}
2025-10-14 10:13:50 | {'loss': 1.3752, 'grad_norm': 3.606250286102295, 'learning_rate': 3.0496654035376353e-06, 'epoch': 2.05}
2025-10-14 10:13:56 | {'loss': 1.3527, 'grad_norm': 3.4987282752990723, 'learning_rate': 2.6364316090203407e-06, 'epoch': 2.18}
2025-10-14 10:14:02 | {'loss': 1.3637, 'grad_norm': 3.6871373653411865, 'learning_rate': 2.2231978145030457e-06, 'epoch': 2.31}
2025-10-14 10:14:08 | {'loss': 1.354, 'grad_norm': 3.5807061195373535, 'learning_rate': 1.8099640199857512e-06, 'epoch': 2.44}
2025-10-14 10:14:15 | {'loss': 1.3534, 'grad_norm': 3.712566614151001, 'learning_rate': 1.3967302254684564e-06, 'epoch': 2.57}
2025-10-14 10:14:21 | {'loss': 1.3382, 'grad_norm': 3.9030144214630127, 'learning_rate': 9.834964309511616e-07, 'epoch': 2.7}
2025-10-14 10:14:27 | {'loss': 1.3332, 'grad_norm': 3.9019250869750977, 'learning_rate': 5.702626364338667e-07, 'epoch': 2.82}
2025-10-14 10:14:33 | {'loss': 1.3271, 'grad_norm': 3.799759864807129, 'learning_rate': 1.5702884191657202e-07, 'epoch': 2.95}
2025-10-14 10:15:06 | {'eval_loss': 1.3638126850128174, 'eval_rouge1': 0.42030908588419086, 'eval_rouge2': 0.2729145408129032, 'eval_rougeL': 0.4123903113340111, 'eval_rouge_sum': 1.1056139380311052, 'eval_runtime': 31.0842, 'eval_samples_per_second': 16.053, 'eval_steps_per_second': 1.029, 'epoch': 3.0}
2025-10-14 10:15:08 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:15:08 | {'train_runtime': 240.2957, 'train_samples_per_second': 155.521, 'train_steps_per_second': 9.726, 'train_loss': 1.5135829111016812, 'epoch': 3.0}
2025-10-14 10:15:08 | 최종 모델 저장 중...
2025-10-14 10:15:09 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:15:09 | 최종 평가 중...
2025-10-14 10:15:40 | 최종 평가 결과:
2025-10-14 10:15:40 | eval_rouge1: 0.4203
2025-10-14 10:15:40 | eval_rouge2: 0.2729
2025-10-14 10:15:40 | eval_rougeL: 0.4124
2025-10-14 10:15:40 | eval_rouge_sum: 1.1056
2025-10-14 10:15:40 | ============================================================
2025-10-14 10:15:40 | ✅ 학습 완료!
2025-10-14 10:15:40 | ============================================================
2025-10-14 10:15:40 | 모델 평가 중...
2025-10-14 10:16:12 | → 메트릭 'eval_rougeL' 사용: 0.4124
2025-10-14 10:16:12 | Trial 3 완료
2025-10-14 10:16:12 | - ROUGE-L F1: 0.4124
2025-10-14 10:16:12 | [I 2025-10-14 10:16:12,249] Trial 3 finished with value: 0.4123903113340111 and parameters: {'learning_rate': 7.5911048052827045e-06, 'num_epochs': 3, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'scheduler_type': 'linear', 'num_beams': 8, 'length_penalty': 1.6626992350416718}. Best is trial 2 with value: 0.4303707888223939.
2025-10-14 10:16:12 | ============================================================
2025-10-14 10:16:12 | Trial 4 시작
2025-10-14 10:16:12 | 파라미터: {'learning_rate': 7.568292060167621e-05, 'num_epochs': 10, 'warmup_ratio': 0.11957999576221703, 'weight_decay': 0.09218742350231168, 'scheduler_type': 'polynomial', 'num_beams': 6, 'length_penalty': 0.9214017645310711}
2025-10-14 10:16:12 | ============================================================
2025-10-14 10:16:12 | 모델 타입: encoder_decoder
2025-10-14 10:16:12 | ============================================================
2025-10-14 10:16:12 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:16:12 | ============================================================
2025-10-14 10:16:12 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:16:12 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:16:12 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:16:13 | → 디바이스: cuda
2025-10-14 10:16:13 | → 전체 파라미터: 123,859,968
2025-10-14 10:16:13 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:16:13 | ============================================================
2025-10-14 10:16:13 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:16:13 | ============================================================
2025-10-14 10:16:14 | ============================================================
2025-10-14 10:16:14 | 모델 학습 시작
2025-10-14 10:16:14 | ============================================================
2025-10-14 10:16:14 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:16:14 | 학습 진행 중...
2025-10-14 10:16:20 | {'loss': 2.0875, 'grad_norm': 6.247005939483643, 'learning_rate': 1.498521827913189e-05, 'epoch': 0.13}
2025-10-14 10:16:27 | {'loss': 1.6455, 'grad_norm': 3.9307451248168945, 'learning_rate': 3.0121802399467134e-05, 'epoch': 0.26}
2025-10-14 10:16:33 | {'loss': 1.5372, 'grad_norm': 3.9898908138275146, 'learning_rate': 4.525838651980238e-05, 'epoch': 0.39}
2025-10-14 10:16:39 | {'loss': 1.4979, 'grad_norm': 4.0390472412109375, 'learning_rate': 6.039497064013762e-05, 'epoch': 0.51}
2025-10-14 10:16:45 | {'loss': 1.5002, 'grad_norm': 3.5514652729034424, 'learning_rate': 7.553155476047286e-05, 'epoch': 0.64}
2025-10-14 10:16:51 | {'loss': 1.4684, 'grad_norm': 3.114147424697876, 'learning_rate': 7.465512785276456e-05, 'epoch': 0.77}
2025-10-14 10:16:59 | {'loss': 1.4614, 'grad_norm': 3.402581214904785, 'learning_rate': 7.36169533589144e-05, 'epoch': 0.9}
2025-10-14 10:17:32 | {'eval_loss': 1.3718085289001465, 'eval_rouge1': 0.38684648708602926, 'eval_rouge2': 0.250936747478903, 'eval_rougeL': 0.38154822567129476, 'eval_rouge_sum': 1.0193314602362271, 'eval_runtime': 28.8442, 'eval_samples_per_second': 17.3, 'eval_steps_per_second': 1.109, 'epoch': 1.0}
2025-10-14 10:17:35 | {'loss': 1.4057, 'grad_norm': 3.308304786682129, 'learning_rate': 7.257877886506426e-05, 'epoch': 1.03}
2025-10-14 10:17:41 | {'loss': 1.1821, 'grad_norm': 3.626399278640747, 'learning_rate': 7.154060437121411e-05, 'epoch': 1.16}
2025-10-14 10:17:47 | {'loss': 1.1954, 'grad_norm': 3.10722017288208, 'learning_rate': 7.050242987736395e-05, 'epoch': 1.28}
2025-10-14 10:17:53 | {'loss': 1.1896, 'grad_norm': 3.3975086212158203, 'learning_rate': 6.94642553835138e-05, 'epoch': 1.41}
2025-10-14 10:17:59 | {'loss': 1.1874, 'grad_norm': 3.361830949783325, 'learning_rate': 6.842608088966365e-05, 'epoch': 1.54}
2025-10-14 10:18:06 | {'loss': 1.1985, 'grad_norm': 3.0338144302368164, 'learning_rate': 6.738790639581349e-05, 'epoch': 1.67}
2025-10-14 10:18:12 | {'loss': 1.1866, 'grad_norm': 5.210069179534912, 'learning_rate': 6.634973190196334e-05, 'epoch': 1.8}
2025-10-14 10:18:18 | {'loss': 1.1835, 'grad_norm': 3.2246079444885254, 'learning_rate': 6.531155740811318e-05, 'epoch': 1.93}
2025-10-14 10:18:50 | {'eval_loss': 1.3120512962341309, 'eval_rouge1': 0.43467631667482703, 'eval_rouge2': 0.27682606213600086, 'eval_rougeL': 0.42790415963442596, 'eval_rouge_sum': 1.1394065384452539, 'eval_runtime': 28.8714, 'eval_samples_per_second': 17.284, 'eval_steps_per_second': 1.108, 'epoch': 2.0}
2025-10-14 10:18:54 | {'loss': 1.0382, 'grad_norm': 3.113018035888672, 'learning_rate': 6.427338291426303e-05, 'epoch': 2.05}
2025-10-14 10:19:00 | {'loss': 0.8585, 'grad_norm': 3.1505086421966553, 'learning_rate': 6.323520842041287e-05, 'epoch': 2.18}
2025-10-14 10:19:06 | {'loss': 0.8753, 'grad_norm': 3.126892328262329, 'learning_rate': 6.219703392656272e-05, 'epoch': 2.31}
2025-10-14 10:19:13 | {'loss': 0.8795, 'grad_norm': 3.050962448120117, 'learning_rate': 6.115885943271256e-05, 'epoch': 2.44}
2025-10-14 10:19:19 | {'loss': 0.892, 'grad_norm': 3.207716941833496, 'learning_rate': 6.012068493886241e-05, 'epoch': 2.57}
2025-10-14 10:19:25 | {'loss': 0.8852, 'grad_norm': 3.251581907272339, 'learning_rate': 5.9082510445012255e-05, 'epoch': 2.7}
2025-10-14 10:19:31 | {'loss': 0.8909, 'grad_norm': 3.1728453636169434, 'learning_rate': 5.80443359511621e-05, 'epoch': 2.82}
2025-10-14 10:19:37 | {'loss': 0.8987, 'grad_norm': 3.168172597885132, 'learning_rate': 5.7006161457311946e-05, 'epoch': 2.95}
2025-10-14 10:20:09 | {'eval_loss': 1.354703664779663, 'eval_rouge1': 0.4380596597721519, 'eval_rouge2': 0.2791349492563065, 'eval_rougeL': 0.4300350989868064, 'eval_rouge_sum': 1.147229708015265, 'eval_runtime': 30.5584, 'eval_samples_per_second': 16.329, 'eval_steps_per_second': 1.047, 'epoch': 3.0}
2025-10-14 10:20:15 | {'loss': 0.7383, 'grad_norm': 2.7468953132629395, 'learning_rate': 5.596798696346179e-05, 'epoch': 3.08}
2025-10-14 10:20:22 | {'loss': 0.6293, 'grad_norm': 2.965330123901367, 'learning_rate': 5.492981246961164e-05, 'epoch': 3.21}
2025-10-14 10:20:28 | {'loss': 0.6472, 'grad_norm': 2.9575555324554443, 'learning_rate': 5.389163797576148e-05, 'epoch': 3.34}
2025-10-14 10:20:34 | {'loss': 0.6489, 'grad_norm': 3.180429697036743, 'learning_rate': 5.2853463481911335e-05, 'epoch': 3.47}
2025-10-14 10:20:40 | {'loss': 0.6712, 'grad_norm': 3.2490451335906982, 'learning_rate': 5.181528898806118e-05, 'epoch': 3.59}
2025-10-14 10:20:46 | {'loss': 0.6685, 'grad_norm': 3.2323999404907227, 'learning_rate': 5.0777114494211026e-05, 'epoch': 3.72}
2025-10-14 10:20:53 | {'loss': 0.6695, 'grad_norm': 3.4187216758728027, 'learning_rate': 4.973894000036087e-05, 'epoch': 3.85}
2025-10-14 10:20:59 | {'loss': 0.6705, 'grad_norm': 3.3462181091308594, 'learning_rate': 4.870076550651072e-05, 'epoch': 3.98}
2025-10-14 10:21:30 | {'eval_loss': 1.4424266815185547, 'eval_rouge1': 0.4623107324471353, 'eval_rouge2': 0.3043709550667739, 'eval_rougeL': 0.45334659196764215, 'eval_rouge_sum': 1.2200282794815513, 'eval_runtime': 29.6942, 'eval_samples_per_second': 16.805, 'eval_steps_per_second': 1.078, 'epoch': 4.0}
2025-10-14 10:21:36 | {'loss': 0.4955, 'grad_norm': 2.7000174522399902, 'learning_rate': 4.766259101266056e-05, 'epoch': 4.11}
2025-10-14 10:21:42 | {'loss': 0.4663, 'grad_norm': 2.96675181388855, 'learning_rate': 4.662441651881041e-05, 'epoch': 4.24}
2025-10-14 10:21:48 | {'loss': 0.4752, 'grad_norm': 3.0537607669830322, 'learning_rate': 4.5586242024960254e-05, 'epoch': 4.36}
2025-10-14 10:21:54 | {'loss': 0.4798, 'grad_norm': 2.3898422718048096, 'learning_rate': 4.45480675311101e-05, 'epoch': 4.49}
2025-10-14 10:22:02 | {'loss': 0.4895, 'grad_norm': 3.0495245456695557, 'learning_rate': 4.3509893037259945e-05, 'epoch': 4.62}
2025-10-14 10:22:08 | {'loss': 0.4992, 'grad_norm': 2.963343858718872, 'learning_rate': 4.24717185434098e-05, 'epoch': 4.75}
2025-10-14 10:22:14 | {'loss': 0.4918, 'grad_norm': 3.358698606491089, 'learning_rate': 4.143354404955964e-05, 'epoch': 4.88}
2025-10-14 10:22:49 | {'eval_loss': 1.5216602087020874, 'eval_rouge1': 0.46538413010603585, 'eval_rouge2': 0.29408229669107466, 'eval_rougeL': 0.45471028984922435, 'eval_rouge_sum': 1.2141767166463349, 'eval_runtime': 29.81, 'eval_samples_per_second': 16.739, 'eval_steps_per_second': 1.073, 'epoch': 5.0}
2025-10-14 10:22:51 | {'loss': 0.4981, 'grad_norm': 2.4932608604431152, 'learning_rate': 4.039536955570949e-05, 'epoch': 5.01}
2025-10-14 10:22:57 | {'loss': 0.341, 'grad_norm': 2.4049222469329834, 'learning_rate': 3.9357195061859334e-05, 'epoch': 5.13}
2025-10-14 10:23:05 | {'loss': 0.3424, 'grad_norm': 2.8144850730895996, 'learning_rate': 3.831902056800918e-05, 'epoch': 5.26}
2025-10-14 10:23:11 | {'loss': 0.352, 'grad_norm': 2.5714011192321777, 'learning_rate': 3.7280846074159025e-05, 'epoch': 5.39}
2025-10-14 10:23:16 | {'loss': 0.3553, 'grad_norm': 2.863675594329834, 'learning_rate': 3.624267158030887e-05, 'epoch': 5.52}
2025-10-14 10:23:22 | {'loss': 0.3667, 'grad_norm': 2.814527750015259, 'learning_rate': 3.5204497086458716e-05, 'epoch': 5.65}
2025-10-14 10:23:28 | {'loss': 0.3677, 'grad_norm': 2.89634108543396, 'learning_rate': 3.416632259260856e-05, 'epoch': 5.78}
2025-10-14 10:23:34 | {'loss': 0.361, 'grad_norm': 2.5467114448547363, 'learning_rate': 3.312814809875841e-05, 'epoch': 5.91}
2025-10-14 10:24:07 | {'eval_loss': 1.5829493999481201, 'eval_rouge1': 0.4473012085732883, 'eval_rouge2': 0.29541612611901796, 'eval_rougeL': 0.44063999719738445, 'eval_rouge_sum': 1.1833573318896908, 'eval_runtime': 27.4667, 'eval_samples_per_second': 18.167, 'eval_steps_per_second': 1.165, 'epoch': 6.0}
2025-10-14 10:24:11 | {'loss': 0.3403, 'grad_norm': 2.742095947265625, 'learning_rate': 3.208997360490825e-05, 'epoch': 6.03}
2025-10-14 10:24:17 | {'loss': 0.2504, 'grad_norm': 2.215649127960205, 'learning_rate': 3.10517991110581e-05, 'epoch': 6.16}
2025-10-14 10:24:23 | {'loss': 0.2585, 'grad_norm': 2.3976778984069824, 'learning_rate': 3.0013624617207947e-05, 'epoch': 6.29}
2025-10-14 10:24:29 | {'loss': 0.2592, 'grad_norm': 2.7935950756073, 'learning_rate': 2.8975450123357793e-05, 'epoch': 6.42}
2025-10-14 10:24:35 | {'loss': 0.2637, 'grad_norm': 2.4112980365753174, 'learning_rate': 2.793727562950764e-05, 'epoch': 6.55}
2025-10-14 10:24:41 | {'loss': 0.2687, 'grad_norm': 2.3812432289123535, 'learning_rate': 2.6899101135657484e-05, 'epoch': 6.68}
2025-10-14 10:24:48 | {'loss': 0.2689, 'grad_norm': 2.3026392459869385, 'learning_rate': 2.586092664180733e-05, 'epoch': 6.8}
2025-10-14 10:24:54 | {'loss': 0.2642, 'grad_norm': 2.6590845584869385, 'learning_rate': 2.482275214795718e-05, 'epoch': 6.93}
2025-10-14 10:25:25 | {'eval_loss': 1.645741581916809, 'eval_rouge1': 0.4541018056232696, 'eval_rouge2': 0.2911596048927988, 'eval_rougeL': 0.4464947110923274, 'eval_rouge_sum': 1.1917561216083958, 'eval_runtime': 28.33, 'eval_samples_per_second': 17.614, 'eval_steps_per_second': 1.13, 'epoch': 7.0}
2025-10-14 10:25:27 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:25:27 | {'train_runtime': 553.1677, 'train_samples_per_second': 225.194, 'train_steps_per_second': 14.083, 'train_loss': 0.7736480549543511, 'epoch': 7.0}
2025-10-14 10:25:27 | 최종 모델 저장 중...
2025-10-14 10:25:28 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:25:28 | 최종 평가 중...
2025-10-14 10:25:57 | 최종 평가 결과:
2025-10-14 10:25:57 | eval_rouge1: 0.4623
2025-10-14 10:25:57 | eval_rouge2: 0.3044
2025-10-14 10:25:57 | eval_rougeL: 0.4533
2025-10-14 10:25:57 | eval_rouge_sum: 1.2200
2025-10-14 10:25:57 | ============================================================
2025-10-14 10:25:57 | ✅ 학습 완료!
2025-10-14 10:25:57 | ============================================================
2025-10-14 10:25:57 | 모델 평가 중...
2025-10-14 10:26:28 | → 메트릭 'eval_rougeL' 사용: 0.4533
2025-10-14 10:26:28 | Trial 4 완료
2025-10-14 10:26:28 | - ROUGE-L F1: 0.4533
2025-10-14 10:26:28 | [I 2025-10-14 10:26:28,451] Trial 4 finished with value: 0.45334659196764215 and parameters: {'learning_rate': 7.568292060167621e-05, 'num_epochs': 10, 'warmup_ratio': 0.11957999576221703, 'weight_decay': 0.09218742350231168, 'scheduler_type': 'polynomial', 'num_beams': 6, 'length_penalty': 0.9214017645310711}. Best is trial 4 with value: 0.45334659196764215.
2025-10-14 10:26:28 | ============================================================
2025-10-14 10:26:28 | Trial 5 시작
2025-10-14 10:26:28 | 파라미터: {'learning_rate': 1.2172847081122448e-05, 'num_epochs': 4, 'warmup_ratio': 0.16043939615080793, 'weight_decay': 0.007455064367977083, 'scheduler_type': 'linear', 'num_beams': 2, 'length_penalty': 0.6110669776011355}
2025-10-14 10:26:28 | ============================================================
2025-10-14 10:26:28 | 모델 타입: encoder_decoder
2025-10-14 10:26:28 | ============================================================
2025-10-14 10:26:28 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:26:28 | ============================================================
2025-10-14 10:26:28 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:26:28 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:26:29 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:26:30 | → 디바이스: cuda
2025-10-14 10:26:30 | → 전체 파라미터: 123,859,968
2025-10-14 10:26:30 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:26:30 | ============================================================
2025-10-14 10:26:30 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:26:30 | ============================================================
2025-10-14 10:26:30 | ============================================================
2025-10-14 10:26:30 | 모델 학습 시작
2025-10-14 10:26:30 | ============================================================
2025-10-14 10:26:30 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:26:30 | 학습 진행 중...
2025-10-14 10:26:36 | {'loss': 2.4704, 'grad_norm': 5.332363128662109, 'learning_rate': 2.4102237220622447e-06, 'epoch': 0.13}
2025-10-14 10:26:42 | {'loss': 1.8337, 'grad_norm': 5.318781852722168, 'learning_rate': 4.844793138286734e-06, 'epoch': 0.26}
2025-10-14 10:26:48 | {'loss': 1.6656, 'grad_norm': 4.474743366241455, 'learning_rate': 7.279362554511224e-06, 'epoch': 0.39}
2025-10-14 10:26:54 | {'loss': 1.5933, 'grad_norm': 4.063220977783203, 'learning_rate': 9.713931970735714e-06, 'epoch': 0.51}
2025-10-14 10:27:01 | {'loss': 1.56, 'grad_norm': 3.7120742797851562, 'learning_rate': 1.2148501386960202e-05, 'epoch': 0.64}
2025-10-14 10:27:07 | {'loss': 1.516, 'grad_norm': 3.5969433784484863, 'learning_rate': 1.1712177409474466e-05, 'epoch': 0.77}
2025-10-14 10:27:13 | {'loss': 1.4988, 'grad_norm': 3.8772830963134766, 'learning_rate': 1.1246854508819937e-05, 'epoch': 0.9}
2025-10-14 10:27:43 | {'eval_loss': 1.4040241241455078, 'eval_rouge1': 0.4110559392915045, 'eval_rouge2': 0.2639682929728022, 'eval_rougeL': 0.40559894894045495, 'eval_rouge_sum': 1.0806231812047618, 'eval_runtime': 25.7277, 'eval_samples_per_second': 19.395, 'eval_steps_per_second': 1.244, 'epoch': 1.0}
2025-10-14 10:27:46 | {'loss': 1.4793, 'grad_norm': 3.6356000900268555, 'learning_rate': 1.0781531608165409e-05, 'epoch': 1.03}
2025-10-14 10:27:52 | {'loss': 1.3927, 'grad_norm': 4.086313247680664, 'learning_rate': 1.0316208707510883e-05, 'epoch': 1.16}
2025-10-14 10:27:58 | {'loss': 1.3898, 'grad_norm': 3.666886568069458, 'learning_rate': 9.850885806856353e-06, 'epoch': 1.28}
2025-10-14 10:28:05 | {'loss': 1.3736, 'grad_norm': 4.2844929695129395, 'learning_rate': 9.385562906201827e-06, 'epoch': 1.41}
2025-10-14 10:28:11 | {'loss': 1.3609, 'grad_norm': 3.8502371311187744, 'learning_rate': 8.920240005547298e-06, 'epoch': 1.54}
2025-10-14 10:28:18 | {'loss': 1.378, 'grad_norm': 3.5824334621429443, 'learning_rate': 8.45491710489277e-06, 'epoch': 1.67}
2025-10-14 10:28:24 | {'loss': 1.3562, 'grad_norm': 3.4758071899414062, 'learning_rate': 7.989594204238244e-06, 'epoch': 1.8}
2025-10-14 10:28:30 | {'loss': 1.3541, 'grad_norm': 3.8909196853637695, 'learning_rate': 7.524271303583715e-06, 'epoch': 1.93}
2025-10-14 10:28:58 | {'eval_loss': 1.34260892868042, 'eval_rouge1': 0.44571689078414606, 'eval_rouge2': 0.28155857628580166, 'eval_rougeL': 0.43646288576337533, 'eval_rouge_sum': 1.163738352833323, 'eval_runtime': 25.0891, 'eval_samples_per_second': 19.889, 'eval_steps_per_second': 1.275, 'epoch': 2.0}
2025-10-14 10:29:02 | {'loss': 1.3027, 'grad_norm': 3.5188703536987305, 'learning_rate': 7.058948402929187e-06, 'epoch': 2.05}
2025-10-14 10:29:09 | {'loss': 1.2604, 'grad_norm': 3.4376277923583984, 'learning_rate': 6.593625502274658e-06, 'epoch': 2.18}
2025-10-14 10:29:15 | {'loss': 1.2716, 'grad_norm': 3.652909755706787, 'learning_rate': 6.128302601620131e-06, 'epoch': 2.31}
2025-10-14 10:29:21 | {'loss': 1.2624, 'grad_norm': 3.5436301231384277, 'learning_rate': 5.6629797009656035e-06, 'epoch': 2.44}
2025-10-14 10:29:27 | {'loss': 1.263, 'grad_norm': 3.7202351093292236, 'learning_rate': 5.197656800311076e-06, 'epoch': 2.57}
2025-10-14 10:29:34 | {'loss': 1.2477, 'grad_norm': 3.789616346359253, 'learning_rate': 4.732333899656548e-06, 'epoch': 2.7}
2025-10-14 10:29:39 | {'loss': 1.2409, 'grad_norm': 3.7148914337158203, 'learning_rate': 4.26701099900202e-06, 'epoch': 2.82}
2025-10-14 10:29:46 | {'loss': 1.2355, 'grad_norm': 3.657113790512085, 'learning_rate': 3.8016880983474925e-06, 'epoch': 2.95}
2025-10-14 10:30:12 | {'eval_loss': 1.3248090744018555, 'eval_rouge1': 0.4507658378890006, 'eval_rouge2': 0.28510657921755655, 'eval_rougeL': 0.43963169487639475, 'eval_rouge_sum': 1.175504111982952, 'eval_runtime': 23.5514, 'eval_samples_per_second': 21.188, 'eval_steps_per_second': 1.359, 'epoch': 3.0}
2025-10-14 10:30:18 | {'loss': 1.2213, 'grad_norm': 3.4080967903137207, 'learning_rate': 3.3363651976929642e-06, 'epoch': 3.08}
2025-10-14 10:30:24 | {'loss': 1.1914, 'grad_norm': 3.6896822452545166, 'learning_rate': 2.871042297038437e-06, 'epoch': 3.21}
2025-10-14 10:30:30 | {'loss': 1.2068, 'grad_norm': 3.538320302963257, 'learning_rate': 2.405719396383909e-06, 'epoch': 3.34}
2025-10-14 10:30:36 | {'loss': 1.1866, 'grad_norm': 3.6183106899261475, 'learning_rate': 1.9403964957293807e-06, 'epoch': 3.47}
2025-10-14 10:30:42 | {'loss': 1.2071, 'grad_norm': 3.511690139770508, 'learning_rate': 1.4750735950748533e-06, 'epoch': 3.59}
2025-10-14 10:30:49 | {'loss': 1.2058, 'grad_norm': 3.945871114730835, 'learning_rate': 1.0097506944203254e-06, 'epoch': 3.72}
2025-10-14 10:30:55 | {'loss': 1.1845, 'grad_norm': 4.099123477935791, 'learning_rate': 5.444277937657976e-07, 'epoch': 3.85}
2025-10-14 10:31:01 | {'loss': 1.18, 'grad_norm': 3.7127816677093506, 'learning_rate': 7.910489311126973e-08, 'epoch': 3.98}
2025-10-14 10:31:26 | {'eval_loss': 1.3178672790527344, 'eval_rouge1': 0.44454336945609035, 'eval_rouge2': 0.2848660751261619, 'eval_rougeL': 0.43671128756831185, 'eval_rouge_sum': 1.1661207321505642, 'eval_runtime': 23.6671, 'eval_samples_per_second': 21.084, 'eval_steps_per_second': 1.352, 'epoch': 4.0}
2025-10-14 10:31:27 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:31:27 | {'train_runtime': 296.8193, 'train_samples_per_second': 167.873, 'train_steps_per_second': 10.498, 'train_loss': 1.3825797851033632, 'epoch': 4.0}
2025-10-14 10:31:27 | 최종 모델 저장 중...
2025-10-14 10:31:28 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:31:28 | 최종 평가 중...
2025-10-14 10:31:51 | 최종 평가 결과:
2025-10-14 10:31:51 | eval_rouge1: 0.4508
2025-10-14 10:31:51 | eval_rouge2: 0.2851
2025-10-14 10:31:51 | eval_rougeL: 0.4396
2025-10-14 10:31:51 | eval_rouge_sum: 1.1755
2025-10-14 10:31:51 | ============================================================
2025-10-14 10:31:51 | ✅ 학습 완료!
2025-10-14 10:31:51 | ============================================================
2025-10-14 10:31:51 | 모델 평가 중...
2025-10-14 10:32:18 | → 메트릭 'eval_rougeL' 사용: 0.4396
2025-10-14 10:32:18 | Trial 5 완료
2025-10-14 10:32:18 | - ROUGE-L F1: 0.4396
2025-10-14 10:32:18 | [I 2025-10-14 10:32:18,835] Trial 5 finished with value: 0.43963169487639475 and parameters: {'learning_rate': 1.2172847081122448e-05, 'num_epochs': 4, 'warmup_ratio': 0.16043939615080793, 'weight_decay': 0.007455064367977083, 'scheduler_type': 'linear', 'num_beams': 2, 'length_penalty': 0.6110669776011355}. Best is trial 4 with value: 0.45334659196764215.
2025-10-14 10:32:18 | ============================================================
2025-10-14 10:32:18 | Trial 6 시작
2025-10-14 10:32:18 | 파라미터: {'learning_rate': 5.211124595788262e-06, 'num_epochs': 3, 'warmup_ratio': 0.17262068517511872, 'weight_decay': 0.062329812682755795, 'scheduler_type': 'linear', 'num_beams': 6, 'length_penalty': 0.6793913689074526}
2025-10-14 10:32:18 | ============================================================
2025-10-14 10:32:18 | 모델 타입: encoder_decoder
2025-10-14 10:32:18 | ============================================================
2025-10-14 10:32:18 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:32:18 | ============================================================
2025-10-14 10:32:18 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:32:19 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:32:19 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:32:20 | → 디바이스: cuda
2025-10-14 10:32:20 | → 전체 파라미터: 123,859,968
2025-10-14 10:32:20 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:32:20 | ============================================================
2025-10-14 10:32:20 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:32:20 | ============================================================
2025-10-14 10:32:20 | ============================================================
2025-10-14 10:32:20 | 모델 학습 시작
2025-10-14 10:32:20 | ============================================================
2025-10-14 10:32:20 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:32:20 | 학습 진행 중...
2025-10-14 10:32:27 | {'loss': 2.6772, 'grad_norm': 6.32676362991333, 'learning_rate': 1.0318026699660759e-06, 'epoch': 0.13}
2025-10-14 10:32:34 | {'loss': 1.997, 'grad_norm': 5.473154544830322, 'learning_rate': 2.074027589123728e-06, 'epoch': 0.26}
2025-10-14 10:32:39 | {'loss': 1.7649, 'grad_norm': 4.909174919128418, 'learning_rate': 3.1162525082813803e-06, 'epoch': 0.39}
2025-10-14 10:32:45 | {'loss': 1.6782, 'grad_norm': 4.247821807861328, 'learning_rate': 4.158477427439033e-06, 'epoch': 0.51}
2025-10-14 10:32:51 | {'loss': 1.6362, 'grad_norm': 4.015347957611084, 'learning_rate': 5.2007023465966854e-06, 'epoch': 0.64}
2025-10-14 10:32:57 | {'loss': 1.5856, 'grad_norm': 3.738262891769409, 'learning_rate': 4.930285545715841e-06, 'epoch': 0.77}
2025-10-14 10:33:04 | {'loss': 1.5637, 'grad_norm': 4.030848979949951, 'learning_rate': 4.64660973756188e-06, 'epoch': 0.9}
2025-10-14 10:33:40 | {'eval_loss': 1.4632072448730469, 'eval_rouge1': 0.40062181476750147, 'eval_rouge2': 0.2600981248795116, 'eval_rougeL': 0.39353978999250566, 'eval_rouge_sum': 1.0542597296395189, 'eval_runtime': 31.4155, 'eval_samples_per_second': 15.884, 'eval_steps_per_second': 1.019, 'epoch': 1.0}
2025-10-14 10:33:43 | {'loss': 1.5581, 'grad_norm': 4.000067234039307, 'learning_rate': 4.362933929407918e-06, 'epoch': 1.03}
2025-10-14 10:33:49 | {'loss': 1.4966, 'grad_norm': 4.320496559143066, 'learning_rate': 4.0792581212539575e-06, 'epoch': 1.16}
2025-10-14 10:33:55 | {'loss': 1.4904, 'grad_norm': 3.7791857719421387, 'learning_rate': 3.795582313099997e-06, 'epoch': 1.28}
2025-10-14 10:34:01 | {'loss': 1.4726, 'grad_norm': 4.345713138580322, 'learning_rate': 3.5119065049460364e-06, 'epoch': 1.41}
2025-10-14 10:34:07 | {'loss': 1.4579, 'grad_norm': 3.861870288848877, 'learning_rate': 3.2282306967920747e-06, 'epoch': 1.54}
2025-10-14 10:34:14 | {'loss': 1.4773, 'grad_norm': 3.745044231414795, 'learning_rate': 2.944554888638114e-06, 'epoch': 1.67}
2025-10-14 10:34:20 | {'loss': 1.4549, 'grad_norm': 3.6316897869110107, 'learning_rate': 2.660879080484153e-06, 'epoch': 1.8}
2025-10-14 10:34:26 | {'loss': 1.4552, 'grad_norm': 4.056389331817627, 'learning_rate': 2.3772032723301924e-06, 'epoch': 1.93}
2025-10-14 10:35:00 | {'eval_loss': 1.4046204090118408, 'eval_rouge1': 0.40933629806876776, 'eval_rouge2': 0.2647820536025739, 'eval_rougeL': 0.40318877199377073, 'eval_rouge_sum': 1.0773071236651124, 'eval_runtime': 30.4655, 'eval_samples_per_second': 16.379, 'eval_steps_per_second': 1.05, 'epoch': 2.0}
2025-10-14 10:35:04 | {'loss': 1.4232, 'grad_norm': 3.69181227684021, 'learning_rate': 2.0935274641762316e-06, 'epoch': 2.05}
2025-10-14 10:35:10 | {'loss': 1.4083, 'grad_norm': 3.5374019145965576, 'learning_rate': 1.8098516560222706e-06, 'epoch': 2.18}
2025-10-14 10:35:17 | {'loss': 1.4189, 'grad_norm': 3.7258968353271484, 'learning_rate': 1.5261758478683096e-06, 'epoch': 2.31}
2025-10-14 10:35:23 | {'loss': 1.4083, 'grad_norm': 3.5737650394439697, 'learning_rate': 1.2425000397143488e-06, 'epoch': 2.44}
2025-10-14 10:35:29 | {'loss': 1.4082, 'grad_norm': 3.7865967750549316, 'learning_rate': 9.588242315603878e-07, 'epoch': 2.57}
2025-10-14 10:35:34 | {'loss': 1.3921, 'grad_norm': 3.9664738178253174, 'learning_rate': 6.75148423406427e-07, 'epoch': 2.7}
2025-10-14 10:35:40 | {'loss': 1.3877, 'grad_norm': 3.9668948650360107, 'learning_rate': 3.9147261525246607e-07, 'epoch': 2.82}
2025-10-14 10:35:46 | {'loss': 1.3809, 'grad_norm': 3.8604655265808105, 'learning_rate': 1.0779680709850515e-07, 'epoch': 2.95}
2025-10-14 10:36:17 | {'eval_loss': 1.3910926580429077, 'eval_rouge1': 0.4145278311363611, 'eval_rouge2': 0.2641464765927313, 'eval_rougeL': 0.40661388254920994, 'eval_rouge_sum': 1.0852881902783023, 'eval_runtime': 28.4112, 'eval_samples_per_second': 17.563, 'eval_steps_per_second': 1.126, 'epoch': 3.0}
2025-10-14 10:36:19 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:36:19 | {'train_runtime': 238.1816, 'train_samples_per_second': 156.901, 'train_steps_per_second': 9.812, 'train_loss': 1.5625120300893658, 'epoch': 3.0}
2025-10-14 10:36:19 | 최종 모델 저장 중...
2025-10-14 10:36:19 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:36:19 | 최종 평가 중...
2025-10-14 10:36:49 | 최종 평가 결과:
2025-10-14 10:36:49 | eval_rouge1: 0.4145
2025-10-14 10:36:49 | eval_rouge2: 0.2641
2025-10-14 10:36:49 | eval_rougeL: 0.4066
2025-10-14 10:36:49 | eval_rouge_sum: 1.0853
2025-10-14 10:36:49 | ============================================================
2025-10-14 10:36:49 | ✅ 학습 완료!
2025-10-14 10:36:49 | ============================================================
2025-10-14 10:36:49 | 모델 평가 중...
2025-10-14 10:37:18 | → 메트릭 'eval_rougeL' 사용: 0.4066
2025-10-14 10:37:18 | Trial 6 완료
2025-10-14 10:37:18 | - ROUGE-L F1: 0.4066
2025-10-14 10:37:18 | Trial 6 Pruned!
2025-10-14 10:37:18 | Trial 6 실패:
2025-10-14 10:37:18 | Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/optimization/optuna_optimizer.py", line 240, in objective
optuna.exceptions.TrialPruned
2025-10-14 10:37:18 | [I 2025-10-14 10:37:18,120] Trial 6 pruned.
2025-10-14 10:37:18 | ============================================================
2025-10-14 10:37:18 | Trial 7 시작
2025-10-14 10:37:18 | 파라미터: {'learning_rate': 2.66986667427446e-05, 'num_epochs': 9, 'warmup_ratio': 0.11225543951389926, 'weight_decay': 0.0770967179954561, 'scheduler_type': 'cosine', 'num_beams': 6, 'length_penalty': 1.262856036747054}
2025-10-14 10:37:18 | ============================================================
2025-10-14 10:37:18 | 모델 타입: encoder_decoder
2025-10-14 10:37:18 | ============================================================
2025-10-14 10:37:18 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:37:18 | ============================================================
2025-10-14 10:37:18 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:37:18 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:37:18 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:37:19 | → 디바이스: cuda
2025-10-14 10:37:19 | → 전체 파라미터: 123,859,968
2025-10-14 10:37:19 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:37:19 | ============================================================
2025-10-14 10:37:19 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:37:19 | ============================================================
2025-10-14 10:37:20 | ============================================================
2025-10-14 10:37:20 | 모델 학습 시작
2025-10-14 10:37:20 | ============================================================
2025-10-14 10:37:20 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:37:20 | 학습 진행 중...
2025-10-14 10:37:26 | {'loss': 2.2903, 'grad_norm': 4.889200687408447, 'learning_rate': 5.286336015063431e-06, 'epoch': 0.13}
2025-10-14 10:37:33 | {'loss': 1.7346, 'grad_norm': 4.321145057678223, 'learning_rate': 1.0626069363612352e-05, 'epoch': 0.26}
2025-10-14 10:37:39 | {'loss': 1.5942, 'grad_norm': 4.292248249053955, 'learning_rate': 1.596580271216127e-05, 'epoch': 0.39}
2025-10-14 10:37:44 | {'loss': 1.5348, 'grad_norm': 3.949235677719116, 'learning_rate': 2.1305536060710192e-05, 'epoch': 0.51}
2025-10-14 10:37:50 | {'loss': 1.5117, 'grad_norm': 3.628368854522705, 'learning_rate': 2.6645269409259112e-05, 'epoch': 0.64}
2025-10-14 10:37:56 | {'loss': 1.4711, 'grad_norm': 3.3058156967163086, 'learning_rate': 2.629271251028696e-05, 'epoch': 0.77}
2025-10-14 10:38:03 | {'loss': 1.4587, 'grad_norm': 3.7395942211151123, 'learning_rate': 2.588265773002671e-05, 'epoch': 0.9}
2025-10-14 10:38:39 | {'eval_loss': 1.3653984069824219, 'eval_rouge1': 0.39111077739867833, 'eval_rouge2': 0.250689713797043, 'eval_rougeL': 0.386233525772666, 'eval_rouge_sum': 1.0280340169683875, 'eval_runtime': 30.7962, 'eval_samples_per_second': 16.203, 'eval_steps_per_second': 1.039, 'epoch': 1.0}
2025-10-14 10:38:41 | {'loss': 1.4227, 'grad_norm': 3.454493761062622, 'learning_rate': 2.5472602949766463e-05, 'epoch': 1.03}
2025-10-14 10:38:47 | {'loss': 1.2926, 'grad_norm': 3.9958078861236572, 'learning_rate': 2.5062548169506218e-05, 'epoch': 1.16}
2025-10-14 10:38:53 | {'loss': 1.2953, 'grad_norm': 3.4895198345184326, 'learning_rate': 2.4652493389245974e-05, 'epoch': 1.28}
2025-10-14 10:38:58 | {'loss': 1.28, 'grad_norm': 4.044815540313721, 'learning_rate': 2.424243860898573e-05, 'epoch': 1.41}
2025-10-14 10:39:04 | {'loss': 1.2728, 'grad_norm': 3.5931930541992188, 'learning_rate': 2.3832383828725485e-05, 'epoch': 1.54}
2025-10-14 10:39:12 | {'loss': 1.2859, 'grad_norm': 3.3475029468536377, 'learning_rate': 2.3422329048465237e-05, 'epoch': 1.67}
2025-10-14 10:39:18 | {'loss': 1.2674, 'grad_norm': 3.2718751430511475, 'learning_rate': 2.301227426820499e-05, 'epoch': 1.8}
2025-10-14 10:39:24 | {'loss': 1.2633, 'grad_norm': 3.5725760459899902, 'learning_rate': 2.2602219487944744e-05, 'epoch': 1.93}
2025-10-14 10:39:57 | {'eval_loss': 1.3022124767303467, 'eval_rouge1': 0.42071132529547, 'eval_rouge2': 0.27151761915112144, 'eval_rougeL': 0.41600692856941385, 'eval_rouge_sum': 1.1082358730160053, 'eval_runtime': 29.8499, 'eval_samples_per_second': 16.717, 'eval_steps_per_second': 1.072, 'epoch': 2.0}
2025-10-14 10:40:01 | {'loss': 1.1754, 'grad_norm': 3.3252336978912354, 'learning_rate': 2.21921647076845e-05, 'epoch': 2.05}
2025-10-14 10:40:07 | {'loss': 1.082, 'grad_norm': 3.271332263946533, 'learning_rate': 2.1782109927424255e-05, 'epoch': 2.18}
2025-10-14 10:40:13 | {'loss': 1.0942, 'grad_norm': 3.3820579051971436, 'learning_rate': 2.137205514716401e-05, 'epoch': 2.31}
2025-10-14 10:40:20 | {'loss': 1.088, 'grad_norm': 3.267686367034912, 'learning_rate': 2.096200036690376e-05, 'epoch': 2.44}
2025-10-14 10:40:26 | {'loss': 1.0959, 'grad_norm': 3.7848060131073, 'learning_rate': 2.0551945586643515e-05, 'epoch': 2.57}
2025-10-14 10:40:32 | {'loss': 1.0797, 'grad_norm': 3.6148109436035156, 'learning_rate': 2.014189080638327e-05, 'epoch': 2.7}
2025-10-14 10:40:38 | {'loss': 1.081, 'grad_norm': 3.4493181705474854, 'learning_rate': 1.9731836026123026e-05, 'epoch': 2.82}
2025-10-14 10:40:44 | {'loss': 1.0797, 'grad_norm': 3.418523073196411, 'learning_rate': 1.932178124586278e-05, 'epoch': 2.95}
2025-10-14 10:41:16 | {'eval_loss': 1.2947674989700317, 'eval_rouge1': 0.4269655548833099, 'eval_rouge2': 0.2737339668942139, 'eval_rougeL': 0.4188543356976198, 'eval_rouge_sum': 1.1195538574751436, 'eval_runtime': 30.1734, 'eval_samples_per_second': 16.538, 'eval_steps_per_second': 1.061, 'epoch': 3.0}
2025-10-14 10:41:23 | {'loss': 0.999, 'grad_norm': 3.1740992069244385, 'learning_rate': 1.8911726465602534e-05, 'epoch': 3.08}
2025-10-14 10:41:29 | {'loss': 0.9317, 'grad_norm': 3.5762107372283936, 'learning_rate': 1.8501671685342286e-05, 'epoch': 3.21}
2025-10-14 10:41:34 | {'loss': 0.9485, 'grad_norm': 3.5894775390625, 'learning_rate': 1.809161690508204e-05, 'epoch': 3.34}
2025-10-14 10:41:40 | {'loss': 0.9393, 'grad_norm': 3.4333484172821045, 'learning_rate': 1.7681562124821797e-05, 'epoch': 3.47}
2025-10-14 10:41:46 | {'loss': 0.9567, 'grad_norm': 3.4223005771636963, 'learning_rate': 1.7271507344561552e-05, 'epoch': 3.59}
2025-10-14 10:41:51 | {'loss': 0.9576, 'grad_norm': 3.94929575920105, 'learning_rate': 1.6861452564301305e-05, 'epoch': 3.72}
2025-10-14 10:41:58 | {'loss': 0.9442, 'grad_norm': 3.9857048988342285, 'learning_rate': 1.6451397784041057e-05, 'epoch': 3.85}
2025-10-14 10:42:04 | {'loss': 0.9436, 'grad_norm': 3.8579823970794678, 'learning_rate': 1.6041343003780812e-05, 'epoch': 3.98}
2025-10-14 10:42:34 | {'eval_loss': 1.3185343742370605, 'eval_rouge1': 0.4463174601167237, 'eval_rouge2': 0.2844411039834394, 'eval_rougeL': 0.4369500421097356, 'eval_rouge_sum': 1.1677086062098987, 'eval_runtime': 28.9688, 'eval_samples_per_second': 17.225, 'eval_steps_per_second': 1.105, 'epoch': 4.0}
2025-10-14 10:42:40 | {'loss': 0.8383, 'grad_norm': 3.61681866645813, 'learning_rate': 1.5631288223520568e-05, 'epoch': 4.11}
2025-10-14 10:42:46 | {'loss': 0.8197, 'grad_norm': 3.6207876205444336, 'learning_rate': 1.5221233443260323e-05, 'epoch': 4.24}
2025-10-14 10:42:52 | {'loss': 0.8283, 'grad_norm': 3.9099910259246826, 'learning_rate': 1.4811178663000077e-05, 'epoch': 4.36}
2025-10-14 10:42:58 | {'loss': 0.8284, 'grad_norm': 3.013993263244629, 'learning_rate': 1.440112388273983e-05, 'epoch': 4.49}
2025-10-14 10:43:05 | {'loss': 0.8342, 'grad_norm': 3.666234016418457, 'learning_rate': 1.3991069102479585e-05, 'epoch': 4.62}
2025-10-14 10:43:11 | {'loss': 0.8436, 'grad_norm': 3.6023669242858887, 'learning_rate': 1.3581014322219338e-05, 'epoch': 4.75}
2025-10-14 10:43:16 | {'loss': 0.8344, 'grad_norm': 3.5988192558288574, 'learning_rate': 1.3170959541959092e-05, 'epoch': 4.88}
2025-10-14 10:43:50 | {'eval_loss': 1.3304500579833984, 'eval_rouge1': 0.4618654691712437, 'eval_rouge2': 0.29644045817569337, 'eval_rougeL': 0.4522358151151206, 'eval_rouge_sum': 1.2105417424620577, 'eval_runtime': 28.715, 'eval_samples_per_second': 17.378, 'eval_steps_per_second': 1.114, 'epoch': 5.0}
2025-10-14 10:43:52 | {'loss': 0.8405, 'grad_norm': 3.5054931640625, 'learning_rate': 1.2760904761698848e-05, 'epoch': 5.01}
2025-10-14 10:43:58 | {'loss': 0.7291, 'grad_norm': 3.295640707015991, 'learning_rate': 1.2350849981438602e-05, 'epoch': 5.13}
2025-10-14 10:44:04 | {'loss': 0.7315, 'grad_norm': 3.7647509574890137, 'learning_rate': 1.1940795201178355e-05, 'epoch': 5.26}
2025-10-14 10:44:11 | {'loss': 0.7412, 'grad_norm': 3.391648292541504, 'learning_rate': 1.1530740420918111e-05, 'epoch': 5.39}
2025-10-14 10:44:17 | {'loss': 0.7409, 'grad_norm': 3.5845847129821777, 'learning_rate': 1.1120685640657865e-05, 'epoch': 5.52}
2025-10-14 10:44:23 | {'loss': 0.7614, 'grad_norm': 3.715017557144165, 'learning_rate': 1.0710630860397619e-05, 'epoch': 5.65}
2025-10-14 10:44:29 | {'loss': 0.7592, 'grad_norm': 3.9559357166290283, 'learning_rate': 1.0300576080137374e-05, 'epoch': 5.78}
2025-10-14 10:44:35 | {'loss': 0.7492, 'grad_norm': 3.7415168285369873, 'learning_rate': 9.890521299877126e-06, 'epoch': 5.91}
2025-10-14 10:45:09 | {'eval_loss': 1.3637834787368774, 'eval_rouge1': 0.44489024890797485, 'eval_rouge2': 0.28602128007724775, 'eval_rougeL': 0.4357873360452203, 'eval_rouge_sum': 1.1666988650304428, 'eval_runtime': 29.4376, 'eval_samples_per_second': 16.951, 'eval_steps_per_second': 1.087, 'epoch': 6.0}
2025-10-14 10:45:12 | {'loss': 0.7273, 'grad_norm': 3.4147114753723145, 'learning_rate': 9.480466519616882e-06, 'epoch': 6.03}
2025-10-14 10:45:19 | {'loss': 0.6652, 'grad_norm': 3.0790903568267822, 'learning_rate': 9.070411739356636e-06, 'epoch': 6.16}
2025-10-14 10:45:25 | {'loss': 0.679, 'grad_norm': 3.881972312927246, 'learning_rate': 8.66035695909639e-06, 'epoch': 6.29}
2025-10-14 10:45:31 | {'loss': 0.6784, 'grad_norm': 3.528822183609009, 'learning_rate': 8.250302178836145e-06, 'epoch': 6.42}
2025-10-14 10:45:36 | {'loss': 0.6764, 'grad_norm': 3.3763227462768555, 'learning_rate': 7.840247398575899e-06, 'epoch': 6.55}
2025-10-14 10:45:42 | {'loss': 0.6756, 'grad_norm': 3.461662769317627, 'learning_rate': 7.4301926183156525e-06, 'epoch': 6.68}
2025-10-14 10:45:48 | {'loss': 0.6835, 'grad_norm': 3.3351938724517822, 'learning_rate': 7.020137838055407e-06, 'epoch': 6.8}
2025-10-14 10:45:55 | {'loss': 0.6748, 'grad_norm': 3.9036104679107666, 'learning_rate': 6.610083057795162e-06, 'epoch': 6.93}
2025-10-14 10:46:28 | {'eval_loss': 1.383278250694275, 'eval_rouge1': 0.4549382831442306, 'eval_rouge2': 0.2915574547595555, 'eval_rougeL': 0.444852452064089, 'eval_rouge_sum': 1.1913481899678753, 'eval_runtime': 29.94, 'eval_samples_per_second': 16.667, 'eval_steps_per_second': 1.069, 'epoch': 7.0}
2025-10-14 10:46:32 | {'loss': 0.6552, 'grad_norm': 3.093003988265991, 'learning_rate': 6.200028277534916e-06, 'epoch': 7.06}
2025-10-14 10:46:38 | {'loss': 0.622, 'grad_norm': 3.2719287872314453, 'learning_rate': 5.7899734972746694e-06, 'epoch': 7.19}
2025-10-14 10:46:44 | {'loss': 0.6282, 'grad_norm': 2.749793291091919, 'learning_rate': 5.379918717014424e-06, 'epoch': 7.32}
2025-10-14 10:46:50 | {'loss': 0.6284, 'grad_norm': 3.6818134784698486, 'learning_rate': 4.969863936754179e-06, 'epoch': 7.45}
2025-10-14 10:46:57 | {'loss': 0.6243, 'grad_norm': 3.436155080795288, 'learning_rate': 4.5598091564939326e-06, 'epoch': 7.57}
2025-10-14 10:47:03 | {'loss': 0.6313, 'grad_norm': 3.436051845550537, 'learning_rate': 4.149754376233686e-06, 'epoch': 7.7}
2025-10-14 10:47:09 | {'loss': 0.6355, 'grad_norm': 3.323213577270508, 'learning_rate': 3.739699595973441e-06, 'epoch': 7.83}
2025-10-14 10:47:15 | {'loss': 0.6212, 'grad_norm': 3.187666654586792, 'learning_rate': 3.3296448157131953e-06, 'epoch': 7.96}
2025-10-14 10:47:45 | {'eval_loss': 1.4089348316192627, 'eval_rouge1': 0.46363957882235385, 'eval_rouge2': 0.2975759118961933, 'eval_rougeL': 0.4513710269098081, 'eval_rouge_sum': 1.2125865176283552, 'eval_runtime': 27.946, 'eval_samples_per_second': 17.856, 'eval_steps_per_second': 1.145, 'epoch': 8.0}
2025-10-14 10:47:50 | {'loss': 0.6032, 'grad_norm': 3.5098700523376465, 'learning_rate': 2.91959003545295e-06, 'epoch': 8.09}
2025-10-14 10:47:56 | {'loss': 0.5928, 'grad_norm': 3.6605849266052246, 'learning_rate': 2.5095352551927038e-06, 'epoch': 8.22}
2025-10-14 10:48:03 | {'loss': 0.584, 'grad_norm': 3.685471296310425, 'learning_rate': 2.0994804749324584e-06, 'epoch': 8.34}
2025-10-14 10:48:08 | {'loss': 0.5984, 'grad_norm': 3.3194963932037354, 'learning_rate': 1.6894256946722124e-06, 'epoch': 8.47}
2025-10-14 10:48:14 | {'loss': 0.5824, 'grad_norm': 3.1609535217285156, 'learning_rate': 1.2793709144119667e-06, 'epoch': 8.6}
2025-10-14 10:48:20 | {'loss': 0.601, 'grad_norm': 3.24438214302063, 'learning_rate': 8.693161341517209e-07, 'epoch': 8.73}
2025-10-14 10:48:26 | {'loss': 0.6059, 'grad_norm': 3.2337350845336914, 'learning_rate': 4.592613538914752e-07, 'epoch': 8.86}
2025-10-14 10:48:32 | {'loss': 0.5945, 'grad_norm': 3.8219943046569824, 'learning_rate': 4.920657363122949e-08, 'epoch': 8.99}
2025-10-14 10:49:01 | {'eval_loss': 1.4152805805206299, 'eval_rouge1': 0.45831045178889473, 'eval_rouge2': 0.29625743064726295, 'eval_rougeL': 0.4477241560853057, 'eval_rouge_sum': 1.2022920385214633, 'eval_runtime': 28.4502, 'eval_samples_per_second': 17.539, 'eval_steps_per_second': 1.125, 'epoch': 9.0}
2025-10-14 10:49:03 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:49:03 | {'train_runtime': 702.6782, 'train_samples_per_second': 159.551, 'train_steps_per_second': 9.978, 'train_loss': 0.935474023858417, 'epoch': 9.0}
2025-10-14 10:49:03 | 최종 모델 저장 중...
2025-10-14 10:49:03 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:49:03 | 최종 평가 중...
2025-10-14 10:49:31 | 최종 평가 결과:
2025-10-14 10:49:31 | eval_rouge1: 0.4636
2025-10-14 10:49:31 | eval_rouge2: 0.2976
2025-10-14 10:49:31 | eval_rougeL: 0.4514
2025-10-14 10:49:31 | eval_rouge_sum: 1.2126
2025-10-14 10:49:31 | ============================================================
2025-10-14 10:49:31 | ✅ 학습 완료!
2025-10-14 10:49:31 | ============================================================
2025-10-14 10:49:31 | 모델 평가 중...
2025-10-14 10:49:59 | → 메트릭 'eval_rougeL' 사용: 0.4514
2025-10-14 10:49:59 | Trial 7 완료
2025-10-14 10:49:59 | - ROUGE-L F1: 0.4514
2025-10-14 10:49:59 | [I 2025-10-14 10:49:59,541] Trial 7 finished with value: 0.4513710269098081 and parameters: {'learning_rate': 2.66986667427446e-05, 'num_epochs': 9, 'warmup_ratio': 0.11225543951389926, 'weight_decay': 0.0770967179954561, 'scheduler_type': 'cosine', 'num_beams': 6, 'length_penalty': 1.262856036747054}. Best is trial 4 with value: 0.45334659196764215.
2025-10-14 10:49:59 | ============================================================
2025-10-14 10:49:59 | Trial 8 시작
2025-10-14 10:49:59 | 파라미터: {'learning_rate': 6.533305220227742e-05, 'num_epochs': 4, 'warmup_ratio': 0.08207658460712595, 'weight_decay': 0.07555511385430487, 'scheduler_type': 'cosine_with_restarts', 'num_beams': 2, 'length_penalty': 1.7055081153486717}
2025-10-14 10:49:59 | ============================================================
2025-10-14 10:49:59 | 모델 타입: encoder_decoder
2025-10-14 10:49:59 | ============================================================
2025-10-14 10:49:59 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:49:59 | ============================================================
2025-10-14 10:49:59 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:50:00 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:50:00 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:50:01 | → 디바이스: cuda
2025-10-14 10:50:01 | → 전체 파라미터: 123,859,968
2025-10-14 10:50:01 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:50:01 | ============================================================
2025-10-14 10:50:01 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:50:01 | ============================================================
2025-10-14 10:50:01 | ============================================================
2025-10-14 10:50:01 | 모델 학습 시작
2025-10-14 10:50:01 | ============================================================
2025-10-14 10:50:01 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:50:01 | 학습 진행 중...
2025-10-14 10:50:08 | {'loss': 2.1156, 'grad_norm': 4.994006156921387, 'learning_rate': 1.293594433605093e-05, 'epoch': 0.13}
2025-10-14 10:50:13 | {'loss': 1.656, 'grad_norm': 3.9255776405334473, 'learning_rate': 2.6002554776506414e-05, 'epoch': 0.26}
2025-10-14 10:50:20 | {'loss': 1.5417, 'grad_norm': 4.01577615737915, 'learning_rate': 3.906916521696189e-05, 'epoch': 0.39}
2025-10-14 10:50:26 | {'loss': 1.4985, 'grad_norm': 4.073374271392822, 'learning_rate': 5.2135775657417386e-05, 'epoch': 0.51}
2025-10-14 10:50:32 | {'loss': 1.4977, 'grad_norm': 3.6413052082061768, 'learning_rate': 6.520238609787286e-05, 'epoch': 0.64}
2025-10-14 10:50:38 | {'loss': 1.4625, 'grad_norm': 3.111262321472168, 'learning_rate': 6.286058577719123e-05, 'epoch': 0.77}
2025-10-14 10:50:44 | {'loss': 1.453, 'grad_norm': 3.4735589027404785, 'learning_rate': 6.036314494377084e-05, 'epoch': 0.9}
2025-10-14 10:51:14 | {'eval_loss': 1.3608088493347168, 'eval_rouge1': 0.404578964987607, 'eval_rouge2': 0.2583714459976272, 'eval_rougeL': 0.3990655176547624, 'eval_rouge_sum': 1.0620159286399966, 'eval_runtime': 25.0052, 'eval_samples_per_second': 19.956, 'eval_steps_per_second': 1.28, 'epoch': 1.0}
2025-10-14 10:51:17 | {'loss': 1.398, 'grad_norm': 3.2673895359039307, 'learning_rate': 5.786570411035045e-05, 'epoch': 1.03}
2025-10-14 10:51:24 | {'loss': 1.1879, 'grad_norm': 3.5814871788024902, 'learning_rate': 5.5368263276930065e-05, 'epoch': 1.16}
2025-10-14 10:51:30 | {'loss': 1.1947, 'grad_norm': 3.1877567768096924, 'learning_rate': 5.287082244350967e-05, 'epoch': 1.28}
2025-10-14 10:51:36 | {'loss': 1.1834, 'grad_norm': 3.421861171722412, 'learning_rate': 5.037338161008928e-05, 'epoch': 1.41}
2025-10-14 10:51:42 | {'loss': 1.1794, 'grad_norm': 3.186278820037842, 'learning_rate': 4.7875940776668887e-05, 'epoch': 1.54}
2025-10-14 10:51:47 | {'loss': 1.1873, 'grad_norm': 3.1497912406921387, 'learning_rate': 4.53784999432485e-05, 'epoch': 1.67}
2025-10-14 10:51:53 | {'loss': 1.1691, 'grad_norm': 2.92415452003479, 'learning_rate': 4.288105910982811e-05, 'epoch': 1.8}
2025-10-14 10:52:00 | {'loss': 1.1672, 'grad_norm': 3.3403279781341553, 'learning_rate': 4.0383618276407715e-05, 'epoch': 1.93}
2025-10-14 10:52:30 | {'eval_loss': 1.2939757108688354, 'eval_rouge1': 0.4439624854031201, 'eval_rouge2': 0.2877109539196872, 'eval_rougeL': 0.4368782488203522, 'eval_rouge_sum': 1.1685516881431595, 'eval_runtime': 26.4541, 'eval_samples_per_second': 18.863, 'eval_steps_per_second': 1.21, 'epoch': 2.0}
2025-10-14 10:52:34 | {'loss': 1.0348, 'grad_norm': 3.051854133605957, 'learning_rate': 3.788617744298733e-05, 'epoch': 2.05}
2025-10-14 10:52:40 | {'loss': 0.8824, 'grad_norm': 3.1556828022003174, 'learning_rate': 3.5388736609566936e-05, 'epoch': 2.18}
2025-10-14 10:52:46 | {'loss': 0.8889, 'grad_norm': 3.0920844078063965, 'learning_rate': 3.289129577614654e-05, 'epoch': 2.31}
2025-10-14 10:52:52 | {'loss': 0.8864, 'grad_norm': 3.1711721420288086, 'learning_rate': 3.0393854942726154e-05, 'epoch': 2.44}
2025-10-14 10:52:58 | {'loss': 0.895, 'grad_norm': 3.4542899131774902, 'learning_rate': 2.789641410930576e-05, 'epoch': 2.57}
2025-10-14 10:53:05 | {'loss': 0.8807, 'grad_norm': 3.391270637512207, 'learning_rate': 2.539897327588537e-05, 'epoch': 2.7}
2025-10-14 10:53:11 | {'loss': 0.8817, 'grad_norm': 3.094219207763672, 'learning_rate': 2.2901532442464982e-05, 'epoch': 2.82}
2025-10-14 10:53:17 | {'loss': 0.8816, 'grad_norm': 3.254912853240967, 'learning_rate': 2.0404091609044592e-05, 'epoch': 2.95}
2025-10-14 10:53:43 | {'eval_loss': 1.322144627571106, 'eval_rouge1': 0.45587458676593606, 'eval_rouge2': 0.2930930140814305, 'eval_rougeL': 0.4468466113227773, 'eval_rouge_sum': 1.1958142121701438, 'eval_runtime': 24.0796, 'eval_samples_per_second': 20.723, 'eval_steps_per_second': 1.329, 'epoch': 3.0}
2025-10-14 10:53:48 | {'loss': 0.7756, 'grad_norm': 2.892106294631958, 'learning_rate': 1.79066507756242e-05, 'epoch': 3.08}
2025-10-14 10:53:54 | {'loss': 0.6928, 'grad_norm': 3.068258285522461, 'learning_rate': 1.540920994220381e-05, 'epoch': 3.21}
2025-10-14 10:54:00 | {'loss': 0.7029, 'grad_norm': 3.314455032348633, 'learning_rate': 1.2911769108783419e-05, 'epoch': 3.34}
2025-10-14 10:54:05 | {'loss': 0.6919, 'grad_norm': 3.052978515625, 'learning_rate': 1.0414328275363028e-05, 'epoch': 3.47}
2025-10-14 10:54:13 | {'loss': 0.7028, 'grad_norm': 3.129939079284668, 'learning_rate': 7.916887441942638e-06, 'epoch': 3.59}
2025-10-14 10:54:18 | {'loss': 0.7004, 'grad_norm': 3.391087055206299, 'learning_rate': 5.419446608522248e-06, 'epoch': 3.72}
2025-10-14 10:54:24 | {'loss': 0.6879, 'grad_norm': 3.3348708152770996, 'learning_rate': 2.922005775101857e-06, 'epoch': 3.85}
2025-10-14 10:54:30 | {'loss': 0.6826, 'grad_norm': 3.347597360610962, 'learning_rate': 4.245649416814664e-07, 'epoch': 3.98}
2025-10-14 10:54:56 | {'eval_loss': 1.3637382984161377, 'eval_rouge1': 0.4584817234996177, 'eval_rouge2': 0.29819167794778456, 'eval_rougeL': 0.448356200945833, 'eval_rouge_sum': 1.2050296023932352, 'eval_runtime': 24.9204, 'eval_samples_per_second': 20.024, 'eval_steps_per_second': 1.284, 'epoch': 4.0}
2025-10-14 10:54:57 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 10:54:57 | {'train_runtime': 296.0694, 'train_samples_per_second': 168.298, 'train_steps_per_second': 10.525, 'train_loss': 1.0870101372298901, 'epoch': 4.0}
2025-10-14 10:54:57 | 최종 모델 저장 중...
2025-10-14 10:54:58 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 10:54:58 | 최종 평가 중...
2025-10-14 10:55:23 | 최종 평가 결과:
2025-10-14 10:55:23 | eval_rouge1: 0.4585
2025-10-14 10:55:23 | eval_rouge2: 0.2982
2025-10-14 10:55:23 | eval_rougeL: 0.4484
2025-10-14 10:55:23 | eval_rouge_sum: 1.2050
2025-10-14 10:55:23 | ============================================================
2025-10-14 10:55:23 | ✅ 학습 완료!
2025-10-14 10:55:23 | ============================================================
2025-10-14 10:55:23 | 모델 평가 중...
2025-10-14 10:55:45 | → 메트릭 'eval_rougeL' 사용: 0.4484
2025-10-14 10:55:45 | Trial 8 완료
2025-10-14 10:55:45 | - ROUGE-L F1: 0.4484
2025-10-14 10:55:45 | [I 2025-10-14 10:55:45,731] Trial 8 finished with value: 0.448356200945833 and parameters: {'learning_rate': 6.533305220227742e-05, 'num_epochs': 4, 'warmup_ratio': 0.08207658460712595, 'weight_decay': 0.07555511385430487, 'scheduler_type': 'cosine_with_restarts', 'num_beams': 2, 'length_penalty': 1.7055081153486717}. Best is trial 4 with value: 0.45334659196764215.
2025-10-14 10:55:45 | ============================================================
2025-10-14 10:55:45 | Trial 9 시작
2025-10-14 10:55:45 | 파라미터: {'learning_rate': 2.3612399244412613e-06, 'num_epochs': 10, 'warmup_ratio': 0.10786844838313014, 'weight_decay': 0.08074401551640625, 'scheduler_type': 'linear', 'num_beams': 6, 'length_penalty': 1.2661209538663485}
2025-10-14 10:55:45 | ============================================================
2025-10-14 10:55:45 | 모델 타입: encoder_decoder
2025-10-14 10:55:45 | ============================================================
2025-10-14 10:55:45 | 모델 및 토크나이저 로딩 시작
2025-10-14 10:55:45 | ============================================================
2025-10-14 10:55:45 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 10:55:46 | 모델 로딩: digit82/kobart-summarization
2025-10-14 10:55:46 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 10:55:47 | → 디바이스: cuda
2025-10-14 10:55:47 | → 전체 파라미터: 123,859,968
2025-10-14 10:55:47 | → 학습 가능 파라미터: 123,859,968
2025-10-14 10:55:47 | ============================================================
2025-10-14 10:55:47 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 10:55:47 | ============================================================
2025-10-14 10:55:47 | ============================================================
2025-10-14 10:55:47 | 모델 학습 시작
2025-10-14 10:55:47 | ============================================================
2025-10-14 10:55:47 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 10:55:47 | 학습 진행 중...
2025-10-14 10:55:55 | {'loss': 2.8487, 'grad_norm': 8.558725357055664, 'learning_rate': 4.675255050393698e-07, 'epoch': 0.13}
2025-10-14 10:56:01 | {'loss': 2.2212, 'grad_norm': 5.8972907066345215, 'learning_rate': 9.39773489927622e-07, 'epoch': 0.26}
2025-10-14 10:56:06 | {'loss': 1.9032, 'grad_norm': 5.296200752258301, 'learning_rate': 1.4120214748158742e-06, 'epoch': 0.39}
2025-10-14 10:56:12 | {'loss': 1.7787, 'grad_norm': 4.419644832611084, 'learning_rate': 1.8842694597041266e-06, 'epoch': 0.51}
2025-10-14 10:56:18 | {'loss': 1.7249, 'grad_norm': 4.227026462554932, 'learning_rate': 2.356517444592379e-06, 'epoch': 0.64}
2025-10-14 10:56:25 | {'loss': 1.6647, 'grad_norm': 3.8530373573303223, 'learning_rate': 2.3291737032451454e-06, 'epoch': 0.77}
2025-10-14 10:56:31 | {'loss': 1.639, 'grad_norm': 4.160989284515381, 'learning_rate': 2.296783580824826e-06, 'epoch': 0.9}
2025-10-14 10:57:04 | {'eval_loss': 1.5299209356307983, 'eval_rouge1': 0.39675301540489044, 'eval_rouge2': 0.2516320491557033, 'eval_rougeL': 0.3916346785926705, 'eval_rouge_sum': 1.0400197431532643, 'eval_runtime': 27.93, 'eval_samples_per_second': 17.866, 'eval_steps_per_second': 1.146, 'epoch': 1.0}
2025-10-14 10:57:07 | {'loss': 1.6361, 'grad_norm': 4.109174728393555, 'learning_rate': 2.2643934584045074e-06, 'epoch': 1.03}
2025-10-14 10:57:12 | {'loss': 1.5849, 'grad_norm': 6.837062358856201, 'learning_rate': 2.2320033359841882e-06, 'epoch': 1.16}
2025-10-14 10:57:18 | {'loss': 1.5745, 'grad_norm': 3.964595079421997, 'learning_rate': 2.199613213563869e-06, 'epoch': 1.28}
2025-10-14 10:57:24 | {'loss': 1.5565, 'grad_norm': 5.1267781257629395, 'learning_rate': 2.16722309114355e-06, 'epoch': 1.41}
2025-10-14 10:57:32 | {'loss': 1.5351, 'grad_norm': 3.9970693588256836, 'learning_rate': 2.134832968723231e-06, 'epoch': 1.54}
2025-10-14 10:57:37 | {'loss': 1.5532, 'grad_norm': 3.9108240604400635, 'learning_rate': 2.102442846302912e-06, 'epoch': 1.67}
2025-10-14 10:57:43 | {'loss': 1.5284, 'grad_norm': 3.781934976577759, 'learning_rate': 2.0700527238825927e-06, 'epoch': 1.8}
2025-10-14 10:57:48 | {'loss': 1.5266, 'grad_norm': 4.15146017074585, 'learning_rate': 2.0376626014622735e-06, 'epoch': 1.93}
2025-10-14 10:58:22 | {'eval_loss': 1.4526753425598145, 'eval_rouge1': 0.40677774305292497, 'eval_rouge2': 0.26055585014726207, 'eval_rougeL': 0.40055159082610264, 'eval_rouge_sum': 1.0678851840262897, 'eval_runtime': 29.9504, 'eval_samples_per_second': 16.661, 'eval_steps_per_second': 1.068, 'epoch': 2.0}
2025-10-14 10:58:26 | {'loss': 1.4973, 'grad_norm': 3.941249370574951, 'learning_rate': 2.0052724790419543e-06, 'epoch': 2.05}
2025-10-14 10:58:32 | {'loss': 1.4865, 'grad_norm': 3.62648344039917, 'learning_rate': 1.9728823566216355e-06, 'epoch': 2.18}
2025-10-14 10:58:39 | {'loss': 1.4947, 'grad_norm': 3.8125054836273193, 'learning_rate': 1.9404922342013163e-06, 'epoch': 2.31}
2025-10-14 10:58:45 | {'loss': 1.4797, 'grad_norm': 3.6735167503356934, 'learning_rate': 1.9081021117809975e-06, 'epoch': 2.44}
2025-10-14 10:58:51 | {'loss': 1.4788, 'grad_norm': 4.004336357116699, 'learning_rate': 1.8757119893606783e-06, 'epoch': 2.57}
2025-10-14 10:58:57 | {'loss': 1.4588, 'grad_norm': 4.024056434631348, 'learning_rate': 1.8433218669403592e-06, 'epoch': 2.7}
2025-10-14 10:59:03 | {'loss': 1.4517, 'grad_norm': 4.075704574584961, 'learning_rate': 1.81093174452004e-06, 'epoch': 2.82}
2025-10-14 10:59:09 | {'loss': 1.4415, 'grad_norm': 3.9085752964019775, 'learning_rate': 1.778541622099721e-06, 'epoch': 2.95}
2025-10-14 10:59:40 | {'eval_loss': 1.422250747680664, 'eval_rouge1': 0.41824749485686924, 'eval_rouge2': 0.2662721151393205, 'eval_rougeL': 0.41023437319688866, 'eval_rouge_sum': 1.0947539831930784, 'eval_runtime': 27.423, 'eval_samples_per_second': 18.196, 'eval_steps_per_second': 1.167, 'epoch': 3.0}
2025-10-14 10:59:46 | {'loss': 1.4457, 'grad_norm': 3.6111152172088623, 'learning_rate': 1.7461514996794018e-06, 'epoch': 3.08}
2025-10-14 10:59:52 | {'loss': 1.4308, 'grad_norm': 4.0185394287109375, 'learning_rate': 1.7137613772590828e-06, 'epoch': 3.21}
2025-10-14 10:59:57 | {'loss': 1.4424, 'grad_norm': 3.9013211727142334, 'learning_rate': 1.6813712548387636e-06, 'epoch': 3.34}
2025-10-14 11:00:03 | {'loss': 1.4204, 'grad_norm': 3.945871353149414, 'learning_rate': 1.6489811324184448e-06, 'epoch': 3.47}
2025-10-14 11:00:09 | {'loss': 1.4409, 'grad_norm': 3.759528160095215, 'learning_rate': 1.6165910099981256e-06, 'epoch': 3.59}
2025-10-14 11:00:15 | {'loss': 1.435, 'grad_norm': 4.038158893585205, 'learning_rate': 1.5842008875778064e-06, 'epoch': 3.72}
2025-10-14 11:00:22 | {'loss': 1.408, 'grad_norm': 4.305498123168945, 'learning_rate': 1.5518107651574875e-06, 'epoch': 3.85}
2025-10-14 11:00:28 | {'loss': 1.406, 'grad_norm': 4.010516166687012, 'learning_rate': 1.5194206427371683e-06, 'epoch': 3.98}
2025-10-14 11:00:58 | {'eval_loss': 1.3999849557876587, 'eval_rouge1': 0.42398686641211264, 'eval_rouge2': 0.27463848770275867, 'eval_rougeL': 0.4156134140525456, 'eval_rouge_sum': 1.1142387681674168, 'eval_runtime': 29.0006, 'eval_samples_per_second': 17.207, 'eval_steps_per_second': 1.103, 'epoch': 4.0}
2025-10-14 11:01:04 | {'loss': 1.3969, 'grad_norm': 4.109374046325684, 'learning_rate': 1.4870305203168493e-06, 'epoch': 4.11}
2025-10-14 11:01:10 | {'loss': 1.3856, 'grad_norm': 4.362486839294434, 'learning_rate': 1.45464039789653e-06, 'epoch': 4.24}
2025-10-14 11:01:15 | {'loss': 1.4024, 'grad_norm': 5.020735263824463, 'learning_rate': 1.4222502754762109e-06, 'epoch': 4.36}
2025-10-14 11:01:21 | {'loss': 1.3862, 'grad_norm': 3.5528316497802734, 'learning_rate': 1.389860153055892e-06, 'epoch': 4.49}
2025-10-14 11:01:28 | {'loss': 1.3949, 'grad_norm': 3.7800686359405518, 'learning_rate': 1.3574700306355727e-06, 'epoch': 4.62}
2025-10-14 11:01:34 | {'loss': 1.3967, 'grad_norm': 4.375569820404053, 'learning_rate': 1.325079908215254e-06, 'epoch': 4.75}
2025-10-14 11:01:39 | {'loss': 1.3865, 'grad_norm': 4.10319185256958, 'learning_rate': 1.2926897857949347e-06, 'epoch': 4.88}
2025-10-14 11:02:15 | {'eval_loss': 1.3851984739303589, 'eval_rouge1': 0.42154832879926224, 'eval_rouge2': 0.2738969872821167, 'eval_rougeL': 0.41554213117748046, 'eval_rouge_sum': 1.1109874472588595, 'eval_runtime': 29.9582, 'eval_samples_per_second': 16.657, 'eval_steps_per_second': 1.068, 'epoch': 5.0}
2025-10-14 11:02:17 | {'loss': 1.3898, 'grad_norm': 3.9960663318634033, 'learning_rate': 1.2602996633746158e-06, 'epoch': 5.01}
2025-10-14 11:02:23 | {'loss': 1.3534, 'grad_norm': 3.764010429382324, 'learning_rate': 1.2279095409542966e-06, 'epoch': 5.13}
2025-10-14 11:02:28 | {'loss': 1.3591, 'grad_norm': 4.308669090270996, 'learning_rate': 1.1955194185339774e-06, 'epoch': 5.26}
2025-10-14 11:02:35 | {'loss': 1.3644, 'grad_norm': 3.9790384769439697, 'learning_rate': 1.1631292961136584e-06, 'epoch': 5.39}
2025-10-14 11:02:41 | {'loss': 1.37, 'grad_norm': 3.8686625957489014, 'learning_rate': 1.1307391736933392e-06, 'epoch': 5.52}
2025-10-14 11:02:47 | {'loss': 1.3852, 'grad_norm': 3.8935186862945557, 'learning_rate': 1.0983490512730202e-06, 'epoch': 5.65}
2025-10-14 11:02:52 | {'loss': 1.3794, 'grad_norm': 4.352009296417236, 'learning_rate': 1.0659589288527012e-06, 'epoch': 5.78}
2025-10-14 11:02:58 | {'loss': 1.3714, 'grad_norm': 3.871234178543091, 'learning_rate': 1.033568806432382e-06, 'epoch': 5.91}
2025-10-14 11:03:33 | {'eval_loss': 1.3747179508209229, 'eval_rouge1': 0.4176052970076539, 'eval_rouge2': 0.2702752380039718, 'eval_rougeL': 0.4096960591089649, 'eval_rouge_sum': 1.0975765941205906, 'eval_runtime': 30.9304, 'eval_samples_per_second': 16.133, 'eval_steps_per_second': 1.035, 'epoch': 6.0}
2025-10-14 11:03:37 | {'loss': 1.3573, 'grad_norm': 3.881152629852295, 'learning_rate': 1.0011786840120628e-06, 'epoch': 6.03}
2025-10-14 11:03:44 | {'loss': 1.3396, 'grad_norm': 3.499838352203369, 'learning_rate': 9.687885615917438e-07, 'epoch': 6.16}
2025-10-14 11:03:50 | {'loss': 1.3681, 'grad_norm': 4.1087799072265625, 'learning_rate': 9.363984391714247e-07, 'epoch': 6.29}
2025-10-14 11:03:56 | {'loss': 1.3579, 'grad_norm': 4.066522121429443, 'learning_rate': 9.040083167511057e-07, 'epoch': 6.42}
2025-10-14 11:04:02 | {'loss': 1.3571, 'grad_norm': 3.4211196899414062, 'learning_rate': 8.716181943307866e-07, 'epoch': 6.55}
2025-10-14 11:04:08 | {'loss': 1.346, 'grad_norm': 4.025667667388916, 'learning_rate': 8.392280719104675e-07, 'epoch': 6.68}
2025-10-14 11:04:15 | {'loss': 1.3408, 'grad_norm': 4.048281669616699, 'learning_rate': 8.068379494901484e-07, 'epoch': 6.8}
2025-10-14 11:04:21 | {'loss': 1.3397, 'grad_norm': 3.8865723609924316, 'learning_rate': 7.744478270698294e-07, 'epoch': 6.93}
2025-10-14 11:04:54 | {'eval_loss': 1.3696675300598145, 'eval_rouge1': 0.4260594379728461, 'eval_rouge2': 0.27687836344111744, 'eval_rougeL': 0.41714671983635676, 'eval_rouge_sum': 1.1200845212503203, 'eval_runtime': 29.8577, 'eval_samples_per_second': 16.713, 'eval_steps_per_second': 1.072, 'epoch': 7.0}
2025-10-14 11:04:58 | {'loss': 1.3482, 'grad_norm': 3.7053685188293457, 'learning_rate': 7.420577046495102e-07, 'epoch': 7.06}
2025-10-14 11:05:04 | {'loss': 1.3318, 'grad_norm': 3.628910541534424, 'learning_rate': 7.096675822291911e-07, 'epoch': 7.19}
2025-10-14 11:05:10 | {'loss': 1.3304, 'grad_norm': 3.4629428386688232, 'learning_rate': 6.77277459808872e-07, 'epoch': 7.32}
2025-10-14 11:05:16 | {'loss': 1.3338, 'grad_norm': 4.215921878814697, 'learning_rate': 6.44887337388553e-07, 'epoch': 7.45}
2025-10-14 11:05:23 | {'loss': 1.332, 'grad_norm': 3.749969005584717, 'learning_rate': 6.12497214968234e-07, 'epoch': 7.57}
2025-10-14 11:05:29 | {'loss': 1.3448, 'grad_norm': 3.6906991004943848, 'learning_rate': 5.801070925479149e-07, 'epoch': 7.7}
2025-10-14 11:05:35 | {'loss': 1.3496, 'grad_norm': 3.823131799697876, 'learning_rate': 5.477169701275957e-07, 'epoch': 7.83}
2025-10-14 11:05:40 | {'loss': 1.3267, 'grad_norm': 3.8084514141082764, 'learning_rate': 5.153268477072767e-07, 'epoch': 7.96}
2025-10-14 11:06:10 | {'eval_loss': 1.3658665418624878, 'eval_rouge1': 0.42898914385267084, 'eval_rouge2': 0.27804661905215955, 'eval_rougeL': 0.4206881422793662, 'eval_rouge_sum': 1.1277239051841965, 'eval_runtime': 27.8704, 'eval_samples_per_second': 17.904, 'eval_steps_per_second': 1.148, 'epoch': 8.0}
2025-10-14 11:06:15 | {'loss': 1.3326, 'grad_norm': 4.015530109405518, 'learning_rate': 4.829367252869576e-07, 'epoch': 8.09}
2025-10-14 11:06:21 | {'loss': 1.3233, 'grad_norm': 4.724436283111572, 'learning_rate': 4.505466028666385e-07, 'epoch': 8.22}
2025-10-14 11:06:28 | {'loss': 1.314, 'grad_norm': 4.2098588943481445, 'learning_rate': 4.181564804463194e-07, 'epoch': 8.34}
2025-10-14 11:06:34 | {'loss': 1.3392, 'grad_norm': 3.7973883152008057, 'learning_rate': 3.857663580260003e-07, 'epoch': 8.47}
2025-10-14 11:06:40 | {'loss': 1.3087, 'grad_norm': 3.6066701412200928, 'learning_rate': 3.5337623560568124e-07, 'epoch': 8.6}
2025-10-14 11:06:46 | {'loss': 1.3367, 'grad_norm': 3.674758195877075, 'learning_rate': 3.209861131853621e-07, 'epoch': 8.73}
2025-10-14 11:06:51 | {'loss': 1.3493, 'grad_norm': 3.726990222930908, 'learning_rate': 2.8859599076504306e-07, 'epoch': 8.86}
2025-10-14 11:06:57 | {'loss': 1.3117, 'grad_norm': 4.468875885009766, 'learning_rate': 2.5620586834472397e-07, 'epoch': 8.99}
2025-10-14 11:07:26 | {'eval_loss': 1.3624827861785889, 'eval_rouge1': 0.42340256408047533, 'eval_rouge2': 0.2747375366041154, 'eval_rougeL': 0.4159621324694114, 'eval_rouge_sum': 1.114102233154002, 'eval_runtime': 27.9588, 'eval_samples_per_second': 17.848, 'eval_steps_per_second': 1.145, 'epoch': 9.0}
2025-10-14 11:07:32 | {'loss': 1.3148, 'grad_norm': 4.937737941741943, 'learning_rate': 2.2381574592440488e-07, 'epoch': 9.11}
2025-10-14 11:07:39 | {'loss': 1.3364, 'grad_norm': 3.7500014305114746, 'learning_rate': 1.914256235040858e-07, 'epoch': 9.24}
2025-10-14 11:07:45 | {'loss': 1.338, 'grad_norm': 4.253694534301758, 'learning_rate': 1.5903550108376673e-07, 'epoch': 9.37}
2025-10-14 11:07:51 | {'loss': 1.3294, 'grad_norm': 3.995216131210327, 'learning_rate': 1.2664537866344764e-07, 'epoch': 9.5}
2025-10-14 11:07:56 | {'loss': 1.3085, 'grad_norm': 4.136703968048096, 'learning_rate': 9.425525624312855e-08, 'epoch': 9.63}
2025-10-14 11:08:02 | {'loss': 1.2993, 'grad_norm': 3.87622332572937, 'learning_rate': 6.186513382280946e-08, 'epoch': 9.76}
2025-10-14 11:08:10 | {'loss': 1.327, 'grad_norm': 3.905759572982788, 'learning_rate': 2.9475011402490367e-08, 'epoch': 9.88}
2025-10-14 11:08:44 | {'eval_loss': 1.3626197576522827, 'eval_rouge1': 0.42595607258292434, 'eval_rouge2': 0.2776214326955127, 'eval_rougeL': 0.4186963336073712, 'eval_rouge_sum': 1.1222738388858082, 'eval_runtime': 29.5825, 'eval_samples_per_second': 16.868, 'eval_steps_per_second': 1.082, 'epoch': 10.0}
2025-10-14 11:08:46 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 11:08:46 | {'train_runtime': 778.4952, 'train_samples_per_second': 160.014, 'train_steps_per_second': 10.006, 'train_loss': 1.4475993566549787, 'epoch': 10.0}
2025-10-14 11:08:46 | 최종 모델 저장 중...
2025-10-14 11:08:47 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 11:08:47 | 최종 평가 중...
2025-10-14 11:09:16 | 최종 평가 결과:
2025-10-14 11:09:16 | eval_rouge1: 0.4290
2025-10-14 11:09:16 | eval_rouge2: 0.2780
2025-10-14 11:09:16 | eval_rougeL: 0.4207
2025-10-14 11:09:16 | eval_rouge_sum: 1.1277
2025-10-14 11:09:16 | ============================================================
2025-10-14 11:09:16 | ✅ 학습 완료!
2025-10-14 11:09:16 | ============================================================
2025-10-14 11:09:16 | 모델 평가 중...
2025-10-14 11:09:45 | → 메트릭 'eval_rougeL' 사용: 0.4207
2025-10-14 11:09:45 | Trial 9 완료
2025-10-14 11:09:45 | - ROUGE-L F1: 0.4207
2025-10-14 11:09:45 | Trial 9 Pruned!
2025-10-14 11:09:45 | Trial 9 실패:
2025-10-14 11:09:45 | Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/optimization/optuna_optimizer.py", line 240, in objective
optuna.exceptions.TrialPruned
2025-10-14 11:09:45 | [I 2025-10-14 11:09:45,610] Trial 9 pruned.
2025-10-14 11:09:45 | ============================================================
2025-10-14 11:09:45 | Trial 10 시작
2025-10-14 11:09:45 | 파라미터: {'learning_rate': 9.098166997876735e-05, 'num_epochs': 7, 'warmup_ratio': 0.001509463863258606, 'weight_decay': 0.09550722292778, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.8868904289022379}
2025-10-14 11:09:45 | ============================================================
2025-10-14 11:09:45 | 모델 타입: encoder_decoder
2025-10-14 11:09:45 | ============================================================
2025-10-14 11:09:45 | 모델 및 토크나이저 로딩 시작
2025-10-14 11:09:45 | ============================================================
2025-10-14 11:09:45 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 11:09:46 | 모델 로딩: digit82/kobart-summarization
2025-10-14 11:09:46 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 11:09:47 | → 디바이스: cuda
2025-10-14 11:09:47 | → 전체 파라미터: 123,859,968
2025-10-14 11:09:47 | → 학습 가능 파라미터: 123,859,968
2025-10-14 11:09:47 | ============================================================
2025-10-14 11:09:47 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 11:09:47 | ============================================================
2025-10-14 11:09:47 | ============================================================
2025-10-14 11:09:47 | 모델 학습 시작
2025-10-14 11:09:47 | ============================================================
2025-10-14 11:09:47 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 11:09:47 | 학습 진행 중...
2025-10-14 11:09:55 | {'loss': 2.0552, 'grad_norm': 4.781301975250244, 'learning_rate': 1.8014370655795936e-05, 'epoch': 0.13}
2025-10-14 11:10:00 | {'loss': 1.634, 'grad_norm': 3.888075113296509, 'learning_rate': 3.6210704651549405e-05, 'epoch': 0.26}
2025-10-14 11:10:06 | {'loss': 1.534, 'grad_norm': 3.9665632247924805, 'learning_rate': 5.440703864730287e-05, 'epoch': 0.39}
2025-10-14 11:10:12 | {'loss': 1.4995, 'grad_norm': 3.9628312587738037, 'learning_rate': 7.260337264305634e-05, 'epoch': 0.51}
2025-10-14 11:10:18 | {'loss': 1.5065, 'grad_norm': 3.503206729888916, 'learning_rate': 9.079970663880981e-05, 'epoch': 0.64}
2025-10-14 11:10:25 | {'loss': 1.4785, 'grad_norm': 3.108351945877075, 'learning_rate': 8.916313871934921e-05, 'epoch': 0.77}
2025-10-14 11:10:31 | {'loss': 1.4715, 'grad_norm': 3.3358983993530273, 'learning_rate': 8.732623845731071e-05, 'epoch': 0.9}
2025-10-14 11:11:05 | {'eval_loss': 1.379667043685913, 'eval_rouge1': 0.38857709339297086, 'eval_rouge2': 0.24760390750885672, 'eval_rougeL': 0.383727048738489, 'eval_rouge_sum': 1.0199080496403166, 'eval_runtime': 29.0298, 'eval_samples_per_second': 17.189, 'eval_steps_per_second': 1.102, 'epoch': 1.0}
2025-10-14 11:11:07 | {'loss': 1.4145, 'grad_norm': 3.2452290058135986, 'learning_rate': 8.54893381952722e-05, 'epoch': 1.03}
2025-10-14 11:11:13 | {'loss': 1.1723, 'grad_norm': 3.678377866744995, 'learning_rate': 8.36524379332337e-05, 'epoch': 1.16}
2025-10-14 11:11:19 | {'loss': 1.1838, 'grad_norm': 3.1010000705718994, 'learning_rate': 8.181553767119519e-05, 'epoch': 1.28}
2025-10-14 11:11:25 | {'loss': 1.1825, 'grad_norm': 3.315650224685669, 'learning_rate': 7.997863740915668e-05, 'epoch': 1.41}
2025-10-14 11:11:32 | {'loss': 1.1773, 'grad_norm': 3.209153652191162, 'learning_rate': 7.814173714711818e-05, 'epoch': 1.54}
2025-10-14 11:11:38 | {'loss': 1.1881, 'grad_norm': 2.910717010498047, 'learning_rate': 7.630483688507966e-05, 'epoch': 1.67}
2025-10-14 11:11:44 | {'loss': 1.1756, 'grad_norm': 2.790555477142334, 'learning_rate': 7.446793662304115e-05, 'epoch': 1.8}
2025-10-14 11:11:50 | {'loss': 1.1739, 'grad_norm': 3.1669256687164307, 'learning_rate': 7.263103636100263e-05, 'epoch': 1.93}
2025-10-14 11:12:20 | {'eval_loss': 1.3183825016021729, 'eval_rouge1': 0.43367129661994935, 'eval_rouge2': 0.28001479343327124, 'eval_rougeL': 0.42733968879541756, 'eval_rouge_sum': 1.1410257788486382, 'eval_runtime': 27.1211, 'eval_samples_per_second': 18.399, 'eval_steps_per_second': 1.18, 'epoch': 2.0}
2025-10-14 11:12:24 | {'loss': 1.0188, 'grad_norm': 2.927886486053467, 'learning_rate': 7.079413609896413e-05, 'epoch': 2.05}
2025-10-14 11:12:30 | {'loss': 0.8279, 'grad_norm': 3.2811601161956787, 'learning_rate': 6.895723583692563e-05, 'epoch': 2.18}
2025-10-14 11:12:37 | {'loss': 0.8433, 'grad_norm': 3.0168280601501465, 'learning_rate': 6.712033557488711e-05, 'epoch': 2.31}
2025-10-14 11:12:43 | {'loss': 0.8457, 'grad_norm': 3.030961751937866, 'learning_rate': 6.528343531284861e-05, 'epoch': 2.44}
2025-10-14 11:12:49 | {'loss': 0.8592, 'grad_norm': 3.0891458988189697, 'learning_rate': 6.34465350508101e-05, 'epoch': 2.57}
2025-10-14 11:12:55 | {'loss': 0.8493, 'grad_norm': 3.045616865158081, 'learning_rate': 6.16096347887716e-05, 'epoch': 2.7}
2025-10-14 11:13:00 | {'loss': 0.8556, 'grad_norm': 2.9842634201049805, 'learning_rate': 5.9772734526733074e-05, 'epoch': 2.82}
2025-10-14 11:13:06 | {'loss': 0.8637, 'grad_norm': 3.012890100479126, 'learning_rate': 5.793583426469457e-05, 'epoch': 2.95}
2025-10-14 11:13:34 | {'eval_loss': 1.3614418506622314, 'eval_rouge1': 0.4357838009466883, 'eval_rouge2': 0.2771334307660831, 'eval_rougeL': 0.4271267125778545, 'eval_rouge_sum': 1.140043944290626, 'eval_runtime': 26.1323, 'eval_samples_per_second': 19.095, 'eval_steps_per_second': 1.225, 'epoch': 3.0}
2025-10-14 11:13:40 | {'loss': 0.6982, 'grad_norm': 2.6609373092651367, 'learning_rate': 5.6098934002656064e-05, 'epoch': 3.08}
2025-10-14 11:13:47 | {'loss': 0.5874, 'grad_norm': 2.831987142562866, 'learning_rate': 5.426203374061755e-05, 'epoch': 3.21}
2025-10-14 11:13:52 | {'loss': 0.6039, 'grad_norm': 2.870817184448242, 'learning_rate': 5.242513347857905e-05, 'epoch': 3.34}
2025-10-14 11:13:58 | {'loss': 0.6048, 'grad_norm': 2.9636263847351074, 'learning_rate': 5.058823321654053e-05, 'epoch': 3.47}
2025-10-14 11:14:04 | {'loss': 0.623, 'grad_norm': 3.1480884552001953, 'learning_rate': 4.8751332954502024e-05, 'epoch': 3.59}
2025-10-14 11:14:10 | {'loss': 0.619, 'grad_norm': 3.11663818359375, 'learning_rate': 4.6914432692463516e-05, 'epoch': 3.72}
2025-10-14 11:14:16 | {'loss': 0.6203, 'grad_norm': 3.4918460845947266, 'learning_rate': 4.507753243042501e-05, 'epoch': 3.85}
2025-10-14 11:14:23 | {'loss': 0.616, 'grad_norm': 3.126791477203369, 'learning_rate': 4.32406321683865e-05, 'epoch': 3.98}
2025-10-14 11:14:50 | {'eval_loss': 1.4555895328521729, 'eval_rouge1': 0.4573186856412549, 'eval_rouge2': 0.29620344535344817, 'eval_rougeL': 0.4477664486426728, 'eval_rouge_sum': 1.2012885796373758, 'eval_runtime': 25.9294, 'eval_samples_per_second': 19.245, 'eval_steps_per_second': 1.234, 'epoch': 4.0}
2025-10-14 11:14:58 | {'loss': 0.45, 'grad_norm': 2.513559579849243, 'learning_rate': 4.140373190634799e-05, 'epoch': 4.11}
2025-10-14 11:15:04 | {'loss': 0.4215, 'grad_norm': 2.8927347660064697, 'learning_rate': 3.9566831644309476e-05, 'epoch': 4.24}
2025-10-14 11:15:10 | {'loss': 0.4273, 'grad_norm': 2.765453577041626, 'learning_rate': 3.7729931382270974e-05, 'epoch': 4.36}
2025-10-14 11:15:16 | {'loss': 0.4282, 'grad_norm': 2.144519567489624, 'learning_rate': 3.5893031120232466e-05, 'epoch': 4.49}
2025-10-14 11:15:22 | {'loss': 0.4384, 'grad_norm': 2.5921785831451416, 'learning_rate': 3.405613085819396e-05, 'epoch': 4.62}
2025-10-14 11:15:29 | {'loss': 0.4426, 'grad_norm': 2.6917388439178467, 'learning_rate': 3.221923059615544e-05, 'epoch': 4.75}
2025-10-14 11:15:35 | {'loss': 0.4337, 'grad_norm': 2.7363522052764893, 'learning_rate': 3.0382330334116934e-05, 'epoch': 4.88}
2025-10-14 11:16:07 | {'eval_loss': 1.526965856552124, 'eval_rouge1': 0.4652173082220416, 'eval_rouge2': 0.2980020452457587, 'eval_rougeL': 0.45280833274212684, 'eval_rouge_sum': 1.216027686209927, 'eval_runtime': 26.6119, 'eval_samples_per_second': 18.751, 'eval_steps_per_second': 1.202, 'epoch': 5.0}
2025-10-14 11:16:08 | {'loss': 0.4375, 'grad_norm': 2.1729660034179688, 'learning_rate': 2.8545430072078426e-05, 'epoch': 5.01}
2025-10-14 11:16:14 | {'loss': 0.3043, 'grad_norm': 2.2886195182800293, 'learning_rate': 2.6708529810039918e-05, 'epoch': 5.13}
2025-10-14 11:16:20 | {'loss': 0.3042, 'grad_norm': 2.595693826675415, 'learning_rate': 2.4871629548001413e-05, 'epoch': 5.26}
2025-10-14 11:16:26 | {'loss': 0.3089, 'grad_norm': 2.508802652359009, 'learning_rate': 2.30347292859629e-05, 'epoch': 5.39}
2025-10-14 11:16:33 | {'loss': 0.3085, 'grad_norm': 2.374802589416504, 'learning_rate': 2.1197829023924393e-05, 'epoch': 5.52}
2025-10-14 11:16:39 | {'loss': 0.3153, 'grad_norm': 2.7502129077911377, 'learning_rate': 1.9360928761885885e-05, 'epoch': 5.65}
2025-10-14 11:16:45 | {'loss': 0.316, 'grad_norm': 2.610821008682251, 'learning_rate': 1.7524028499847373e-05, 'epoch': 5.78}
2025-10-14 11:16:51 | {'loss': 0.308, 'grad_norm': 2.4211912155151367, 'learning_rate': 1.5687128237808865e-05, 'epoch': 5.91}
2025-10-14 11:17:23 | {'eval_loss': 1.5817251205444336, 'eval_rouge1': 0.46003935973084803, 'eval_rouge2': 0.30073636818147514, 'eval_rougeL': 0.4516559142656539, 'eval_rouge_sum': 1.212431642177977, 'eval_runtime': 27.7757, 'eval_samples_per_second': 17.965, 'eval_steps_per_second': 1.152, 'epoch': 6.0}
2025-10-14 11:17:27 | {'loss': 0.293, 'grad_norm': 2.7995309829711914, 'learning_rate': 1.3850227975770356e-05, 'epoch': 6.03}
2025-10-14 11:17:32 | {'loss': 0.23, 'grad_norm': 1.963607668876648, 'learning_rate': 1.2013327713731847e-05, 'epoch': 6.16}
2025-10-14 11:17:38 | {'loss': 0.2342, 'grad_norm': 2.244426727294922, 'learning_rate': 1.0176427451693338e-05, 'epoch': 6.29}
2025-10-14 11:17:45 | {'loss': 0.2323, 'grad_norm': 2.599323272705078, 'learning_rate': 8.33952718965483e-06, 'epoch': 6.42}
2025-10-14 11:17:51 | {'loss': 0.2302, 'grad_norm': 2.062098741531372, 'learning_rate': 6.502626927616322e-06, 'epoch': 6.55}
2025-10-14 11:17:56 | {'loss': 0.2338, 'grad_norm': 2.1330807209014893, 'learning_rate': 4.6657266655778125e-06, 'epoch': 6.68}
2025-10-14 11:18:02 | {'loss': 0.2302, 'grad_norm': 2.1035101413726807, 'learning_rate': 2.828826403539304e-06, 'epoch': 6.8}
2025-10-14 11:18:08 | {'loss': 0.2266, 'grad_norm': 2.309464454650879, 'learning_rate': 9.91926141500795e-07, 'epoch': 6.93}
2025-10-14 11:18:37 | {'eval_loss': 1.6179133653640747, 'eval_rouge1': 0.4568387025519557, 'eval_rouge2': 0.2968974177003238, 'eval_rougeL': 0.4486178526872918, 'eval_rouge_sum': 1.2023539729395711, 'eval_runtime': 26.8323, 'eval_samples_per_second': 18.597, 'eval_steps_per_second': 1.193, 'epoch': 7.0}
2025-10-14 11:18:39 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 11:18:39 | {'train_runtime': 531.5667, 'train_samples_per_second': 164.041, 'train_steps_per_second': 10.258, 'train_loss': 0.7419862418399477, 'epoch': 7.0}
2025-10-14 11:18:39 | 최종 모델 저장 중...
2025-10-14 11:18:40 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 11:18:40 | 최종 평가 중...
2025-10-14 11:19:06 | 최종 평가 결과:
2025-10-14 11:19:06 | eval_rouge1: 0.4652
2025-10-14 11:19:06 | eval_rouge2: 0.2980
2025-10-14 11:19:06 | eval_rougeL: 0.4528
2025-10-14 11:19:06 | eval_rouge_sum: 1.2160
2025-10-14 11:19:06 | ============================================================
2025-10-14 11:19:06 | ✅ 학습 완료!
2025-10-14 11:19:06 | ============================================================
2025-10-14 11:19:06 | 모델 평가 중...
2025-10-14 11:19:32 | → 메트릭 'eval_rougeL' 사용: 0.4528
2025-10-14 11:19:32 | Trial 10 완료
2025-10-14 11:19:32 | - ROUGE-L F1: 0.4528
2025-10-14 11:19:32 | [I 2025-10-14 11:19:32,797] Trial 10 finished with value: 0.45280833274212684 and parameters: {'learning_rate': 9.098166997876735e-05, 'num_epochs': 7, 'warmup_ratio': 0.001509463863258606, 'weight_decay': 0.09550722292778, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.8868904289022379}. Best is trial 4 with value: 0.45334659196764215.
2025-10-14 11:19:32 | ============================================================
2025-10-14 11:19:32 | Trial 11 시작
2025-10-14 11:19:32 | 파라미터: {'learning_rate': 9.138518360133624e-05, 'num_epochs': 7, 'warmup_ratio': 0.0013572013949127268, 'weight_decay': 0.09953784597545408, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9383576982529792}
2025-10-14 11:19:32 | ============================================================
2025-10-14 11:19:32 | 모델 타입: encoder_decoder
2025-10-14 11:19:32 | ============================================================
2025-10-14 11:19:32 | 모델 및 토크나이저 로딩 시작
2025-10-14 11:19:32 | ============================================================
2025-10-14 11:19:32 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 11:19:33 | 모델 로딩: digit82/kobart-summarization
2025-10-14 11:19:33 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 11:19:34 | → 디바이스: cuda
2025-10-14 11:19:34 | → 전체 파라미터: 123,859,968
2025-10-14 11:19:34 | → 학습 가능 파라미터: 123,859,968
2025-10-14 11:19:34 | ============================================================
2025-10-14 11:19:34 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 11:19:34 | ============================================================
2025-10-14 11:19:34 | ============================================================
2025-10-14 11:19:34 | 모델 학습 시작
2025-10-14 11:19:34 | ============================================================
2025-10-14 11:19:34 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 11:19:34 | 학습 진행 중...
2025-10-14 11:19:40 | {'loss': 2.0546, 'grad_norm': 4.777803897857666, 'learning_rate': 1.8094266353064577e-05, 'epoch': 0.13}
2025-10-14 11:19:46 | {'loss': 1.6338, 'grad_norm': 3.8821945190429688, 'learning_rate': 3.637130307333183e-05, 'epoch': 0.26}
2025-10-14 11:19:51 | {'loss': 1.5338, 'grad_norm': 3.967924118041992, 'learning_rate': 5.464833979359907e-05, 'epoch': 0.39}
2025-10-14 11:19:59 | {'loss': 1.4997, 'grad_norm': 3.9809067249298096, 'learning_rate': 7.292537651386632e-05, 'epoch': 0.51}
2025-10-14 11:20:05 | {'loss': 1.5078, 'grad_norm': 3.5098376274108887, 'learning_rate': 9.120241323413357e-05, 'epoch': 0.64}
2025-10-14 11:20:10 | {'loss': 1.4781, 'grad_norm': 3.0631051063537598, 'learning_rate': 8.955858695757845e-05, 'epoch': 0.77}
2025-10-14 11:20:16 | {'loss': 1.4723, 'grad_norm': 3.3265578746795654, 'learning_rate': 8.77135398426716e-05, 'epoch': 0.9}
2025-10-14 11:20:47 | {'eval_loss': 1.3867623805999756, 'eval_rouge1': 0.3870358799207208, 'eval_rouge2': 0.24720709161587823, 'eval_rougeL': 0.3830014086223099, 'eval_rouge_sum': 1.017244380158909, 'eval_runtime': 26.2698, 'eval_samples_per_second': 18.995, 'eval_steps_per_second': 1.218, 'epoch': 1.0}
2025-10-14 11:20:50 | {'loss': 1.4151, 'grad_norm': 3.166248321533203, 'learning_rate': 8.586849272776476e-05, 'epoch': 1.03}
2025-10-14 11:20:56 | {'loss': 1.1728, 'grad_norm': 3.585434913635254, 'learning_rate': 8.402344561285791e-05, 'epoch': 1.16}
2025-10-14 11:21:03 | {'loss': 1.1834, 'grad_norm': 3.038116931915283, 'learning_rate': 8.217839849795107e-05, 'epoch': 1.28}
2025-10-14 11:21:09 | {'loss': 1.1834, 'grad_norm': 3.526827096939087, 'learning_rate': 8.033335138304421e-05, 'epoch': 1.41}
2025-10-14 11:21:15 | {'loss': 1.1794, 'grad_norm': 3.180145263671875, 'learning_rate': 7.848830426813736e-05, 'epoch': 1.54}
2025-10-14 11:21:21 | {'loss': 1.1898, 'grad_norm': 2.877589464187622, 'learning_rate': 7.66432571532305e-05, 'epoch': 1.67}
2025-10-14 11:21:27 | {'loss': 1.1782, 'grad_norm': 2.7615954875946045, 'learning_rate': 7.479821003832366e-05, 'epoch': 1.8}
2025-10-14 11:21:32 | {'loss': 1.1753, 'grad_norm': 3.2790896892547607, 'learning_rate': 7.295316292341681e-05, 'epoch': 1.93}
2025-10-14 11:22:02 | {'eval_loss': 1.3162403106689453, 'eval_rouge1': 0.4397793181856867, 'eval_rouge2': 0.2818372303165419, 'eval_rougeL': 0.4321341444665087, 'eval_rouge_sum': 1.1537506929687373, 'eval_runtime': 24.5175, 'eval_samples_per_second': 20.353, 'eval_steps_per_second': 1.305, 'epoch': 2.0}
2025-10-14 11:22:06 | {'loss': 1.021, 'grad_norm': 2.822423219680786, 'learning_rate': 7.110811580850997e-05, 'epoch': 2.05}
2025-10-14 11:22:13 | {'loss': 0.8285, 'grad_norm': 3.1694600582122803, 'learning_rate': 6.926306869360312e-05, 'epoch': 2.18}
2025-10-14 11:22:19 | {'loss': 0.8451, 'grad_norm': 2.9404654502868652, 'learning_rate': 6.741802157869627e-05, 'epoch': 2.31}
2025-10-14 11:22:25 | {'loss': 0.8483, 'grad_norm': 2.948103189468384, 'learning_rate': 6.557297446378942e-05, 'epoch': 2.44}
2025-10-14 11:22:31 | {'loss': 0.8616, 'grad_norm': 3.3761980533599854, 'learning_rate': 6.372792734888256e-05, 'epoch': 2.57}
2025-10-14 11:22:37 | {'loss': 0.8518, 'grad_norm': 3.330592155456543, 'learning_rate': 6.188288023397572e-05, 'epoch': 2.7}
2025-10-14 11:22:44 | {'loss': 0.8575, 'grad_norm': 2.970229148864746, 'learning_rate': 6.0037833119068865e-05, 'epoch': 2.82}
2025-10-14 11:22:50 | {'loss': 0.8641, 'grad_norm': 3.134647846221924, 'learning_rate': 5.819278600416202e-05, 'epoch': 2.95}
2025-10-14 11:23:20 | {'eval_loss': 1.370719313621521, 'eval_rouge1': 0.44206429642983863, 'eval_rouge2': 0.28224824251195874, 'eval_rougeL': 0.43314395146744056, 'eval_rouge_sum': 1.157456490409238, 'eval_runtime': 27.6193, 'eval_samples_per_second': 18.067, 'eval_steps_per_second': 1.159, 'epoch': 3.0}
2025-10-14 11:23:25 | {'loss': 0.699, 'grad_norm': 2.5699236392974854, 'learning_rate': 5.6347738889255175e-05, 'epoch': 3.08}
2025-10-14 11:23:31 | {'loss': 0.5868, 'grad_norm': 2.817598819732666, 'learning_rate': 5.450269177434832e-05, 'epoch': 3.21}
2025-10-14 11:23:37 | {'loss': 0.6023, 'grad_norm': 2.735569715499878, 'learning_rate': 5.265764465944148e-05, 'epoch': 3.34}
2025-10-14 11:23:42 | {'loss': 0.6055, 'grad_norm': 3.3912851810455322, 'learning_rate': 5.081259754453462e-05, 'epoch': 3.47}
2025-10-14 11:23:48 | {'loss': 0.6255, 'grad_norm': 2.991006374359131, 'learning_rate': 4.8967550429627774e-05, 'epoch': 3.59}
2025-10-14 11:23:55 | {'loss': 0.6195, 'grad_norm': 3.101865768432617, 'learning_rate': 4.712250331472092e-05, 'epoch': 3.72}
2025-10-14 11:24:01 | {'loss': 0.6179, 'grad_norm': 3.482311248779297, 'learning_rate': 4.527745619981408e-05, 'epoch': 3.85}
2025-10-14 11:24:07 | {'loss': 0.6156, 'grad_norm': 3.31427264213562, 'learning_rate': 4.3432409084907225e-05, 'epoch': 3.98}
2025-10-14 11:24:35 | {'eval_loss': 1.4557573795318604, 'eval_rouge1': 0.461028392416866, 'eval_rouge2': 0.30381040089062944, 'eval_rougeL': 0.453474189772483, 'eval_rouge_sum': 1.2183129830799784, 'eval_runtime': 27.0729, 'eval_samples_per_second': 18.432, 'eval_steps_per_second': 1.182, 'epoch': 4.0}
2025-10-14 11:24:41 | {'loss': 0.4499, 'grad_norm': 2.5097742080688477, 'learning_rate': 4.1587361970000374e-05, 'epoch': 4.11}
2025-10-14 11:24:47 | {'loss': 0.4214, 'grad_norm': 2.8542847633361816, 'learning_rate': 3.974231485509353e-05, 'epoch': 4.24}
2025-10-14 11:24:53 | {'loss': 0.4271, 'grad_norm': 2.915926933288574, 'learning_rate': 3.7897267740186683e-05, 'epoch': 4.36}
2025-10-14 11:25:00 | {'loss': 0.428, 'grad_norm': 2.364243745803833, 'learning_rate': 3.605222062527983e-05, 'epoch': 4.49}
2025-10-14 11:25:05 | {'loss': 0.4351, 'grad_norm': 2.572206497192383, 'learning_rate': 3.420717351037298e-05, 'epoch': 4.62}
2025-10-14 11:25:11 | {'loss': 0.4411, 'grad_norm': 2.909752607345581, 'learning_rate': 3.2362126395466135e-05, 'epoch': 4.75}
2025-10-14 11:25:17 | {'loss': 0.4351, 'grad_norm': 2.954559087753296, 'learning_rate': 3.0517079280559283e-05, 'epoch': 4.88}
2025-10-14 11:25:48 | {'eval_loss': 1.5297024250030518, 'eval_rouge1': 0.46950228344861744, 'eval_rouge2': 0.3062504881180933, 'eval_rougeL': 0.46158654281217215, 'eval_rouge_sum': 1.237339314378883, 'eval_runtime': 25.6006, 'eval_samples_per_second': 19.492, 'eval_steps_per_second': 1.25, 'epoch': 5.0}
2025-10-14 11:25:50 | {'loss': 0.4336, 'grad_norm': 2.3573122024536133, 'learning_rate': 2.8672032165652435e-05, 'epoch': 5.01}
2025-10-14 11:25:55 | {'loss': 0.3023, 'grad_norm': 2.2815322875976562, 'learning_rate': 2.6826985050745583e-05, 'epoch': 5.13}
2025-10-14 11:26:01 | {'loss': 0.3038, 'grad_norm': 2.4636731147766113, 'learning_rate': 2.4981937935838738e-05, 'epoch': 5.26}
2025-10-14 11:26:09 | {'loss': 0.3067, 'grad_norm': 2.5082786083221436, 'learning_rate': 2.313689082093189e-05, 'epoch': 5.39}
2025-10-14 11:26:15 | {'loss': 0.3067, 'grad_norm': 2.449707269668579, 'learning_rate': 2.1291843706025037e-05, 'epoch': 5.52}
2025-10-14 11:26:21 | {'loss': 0.315, 'grad_norm': 2.776921272277832, 'learning_rate': 1.944679659111819e-05, 'epoch': 5.65}
2025-10-14 11:26:27 | {'loss': 0.3144, 'grad_norm': 2.545933961868286, 'learning_rate': 1.760174947621134e-05, 'epoch': 5.78}
2025-10-14 11:26:32 | {'loss': 0.3064, 'grad_norm': 2.3329384326934814, 'learning_rate': 1.5756702361304492e-05, 'epoch': 5.91}
2025-10-14 11:27:03 | {'eval_loss': 1.5770409107208252, 'eval_rouge1': 0.4592963853599677, 'eval_rouge2': 0.2947606990508583, 'eval_rougeL': 0.44868374537154826, 'eval_rouge_sum': 1.2027408297823743, 'eval_runtime': 26.4745, 'eval_samples_per_second': 18.848, 'eval_steps_per_second': 1.209, 'epoch': 6.0}
2025-10-14 11:27:06 | {'loss': 0.2903, 'grad_norm': 2.465181350708008, 'learning_rate': 1.3911655246397642e-05, 'epoch': 6.03}
2025-10-14 11:27:13 | {'loss': 0.2297, 'grad_norm': 1.9995204210281372, 'learning_rate': 1.2066608131490792e-05, 'epoch': 6.16}
2025-10-14 11:27:19 | {'loss': 0.2337, 'grad_norm': 2.251715660095215, 'learning_rate': 1.0221561016583943e-05, 'epoch': 6.29}
2025-10-14 11:27:25 | {'loss': 0.2297, 'grad_norm': 2.640382766723633, 'learning_rate': 8.376513901677095e-06, 'epoch': 6.42}
2025-10-14 11:27:31 | {'loss': 0.229, 'grad_norm': 2.0017647743225098, 'learning_rate': 6.5314667867702465e-06, 'epoch': 6.55}
2025-10-14 11:27:37 | {'loss': 0.2311, 'grad_norm': 2.190603017807007, 'learning_rate': 4.686419671863396e-06, 'epoch': 6.68}
2025-10-14 11:27:42 | {'loss': 0.2293, 'grad_norm': 2.2539968490600586, 'learning_rate': 2.8413725569565475e-06, 'epoch': 6.8}
2025-10-14 11:27:49 | {'loss': 0.2242, 'grad_norm': 2.129732370376587, 'learning_rate': 9.963254420496987e-07, 'epoch': 6.93}
2025-10-14 11:28:21 | {'eval_loss': 1.616363286972046, 'eval_rouge1': 0.4638113282009434, 'eval_rouge2': 0.30088150085932025, 'eval_rougeL': 0.453236690015821, 'eval_rouge_sum': 1.2179295190760846, 'eval_runtime': 28.5297, 'eval_samples_per_second': 17.491, 'eval_steps_per_second': 1.122, 'epoch': 7.0}
2025-10-14 11:28:22 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 11:28:22 | {'train_runtime': 527.8179, 'train_samples_per_second': 165.207, 'train_steps_per_second': 10.331, 'train_loss': 0.7418325600133518, 'epoch': 7.0}
2025-10-14 11:28:22 | 최종 모델 저장 중...
2025-10-14 11:28:23 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 11:28:23 | 최종 평가 중...
2025-10-14 11:28:50 | 최종 평가 결과:
2025-10-14 11:28:50 | eval_rouge1: 0.4695
2025-10-14 11:28:50 | eval_rouge2: 0.3063
2025-10-14 11:28:50 | eval_rougeL: 0.4616
2025-10-14 11:28:50 | eval_rouge_sum: 1.2373
2025-10-14 11:28:50 | ============================================================
2025-10-14 11:28:50 | ✅ 학습 완료!
2025-10-14 11:28:50 | ============================================================
2025-10-14 11:28:50 | 모델 평가 중...
2025-10-14 11:29:18 | → 메트릭 'eval_rougeL' 사용: 0.4616
2025-10-14 11:29:18 | Trial 11 완료
2025-10-14 11:29:18 | - ROUGE-L F1: 0.4616
2025-10-14 11:29:18 | [I 2025-10-14 11:29:18,803] Trial 11 finished with value: 0.46158654281217215 and parameters: {'learning_rate': 9.138518360133624e-05, 'num_epochs': 7, 'warmup_ratio': 0.0013572013949127268, 'weight_decay': 0.09953784597545408, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9383576982529792}. Best is trial 11 with value: 0.46158654281217215.
2025-10-14 11:29:18 | ============================================================
2025-10-14 11:29:18 | Trial 12 시작
2025-10-14 11:29:18 | 파라미터: {'learning_rate': 4.0171644628004966e-05, 'num_epochs': 8, 'warmup_ratio': 0.06454328641269678, 'weight_decay': 0.09990061001173775, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9405420786530134}
2025-10-14 11:29:18 | ============================================================
2025-10-14 11:29:18 | 모델 타입: encoder_decoder
2025-10-14 11:29:18 | ============================================================
2025-10-14 11:29:18 | 모델 및 토크나이저 로딩 시작
2025-10-14 11:29:18 | ============================================================
2025-10-14 11:29:18 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 11:29:19 | 모델 로딩: digit82/kobart-summarization
2025-10-14 11:29:19 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 11:29:20 | → 디바이스: cuda
2025-10-14 11:29:20 | → 전체 파라미터: 123,859,968
2025-10-14 11:29:20 | → 학습 가능 파라미터: 123,859,968
2025-10-14 11:29:20 | ============================================================
2025-10-14 11:29:20 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 11:29:20 | ============================================================
2025-10-14 11:29:20 | ============================================================
2025-10-14 11:29:20 | 모델 학습 시작
2025-10-14 11:29:20 | ============================================================
2025-10-14 11:29:20 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 11:29:20 | 학습 진행 중...
2025-10-14 11:29:28 | {'loss': 2.2038, 'grad_norm': 4.854132175445557, 'learning_rate': 7.953985636344983e-06, 'epoch': 0.13}
2025-10-14 11:29:34 | {'loss': 1.6953, 'grad_norm': 4.194348335266113, 'learning_rate': 1.5988314561945976e-05, 'epoch': 0.26}
2025-10-14 11:29:40 | {'loss': 1.566, 'grad_norm': 4.192612171173096, 'learning_rate': 2.4022643487546967e-05, 'epoch': 0.39}
2025-10-14 11:29:46 | {'loss': 1.5128, 'grad_norm': 3.8674588203430176, 'learning_rate': 3.205697241314796e-05, 'epoch': 0.51}
2025-10-14 11:29:52 | {'loss': 1.4981, 'grad_norm': 3.7289745807647705, 'learning_rate': 4.0091301338748956e-05, 'epoch': 0.64}
2025-10-14 11:29:58 | {'loss': 1.4593, 'grad_norm': 3.1745383739471436, 'learning_rate': 3.947782173579064e-05, 'epoch': 0.77}
2025-10-14 11:30:05 | {'loss': 1.45, 'grad_norm': 3.6436502933502197, 'learning_rate': 3.877699053153375e-05, 'epoch': 0.9}
2025-10-14 11:30:36 | {'eval_loss': 1.3589816093444824, 'eval_rouge1': 0.38856486750463554, 'eval_rouge2': 0.24946399329623986, 'eval_rougeL': 0.3842749717294472, 'eval_rouge_sum': 1.0223038325303226, 'eval_runtime': 26.4892, 'eval_samples_per_second': 18.838, 'eval_steps_per_second': 1.208, 'epoch': 1.0}
2025-10-14 11:30:39 | {'loss': 1.4044, 'grad_norm': 3.314012289047241, 'learning_rate': 3.807615932727686e-05, 'epoch': 1.03}
2025-10-14 11:30:44 | {'loss': 1.2421, 'grad_norm': 3.6506216526031494, 'learning_rate': 3.737532812301997e-05, 'epoch': 1.16}
2025-10-14 11:30:50 | {'loss': 1.2472, 'grad_norm': 3.354473114013672, 'learning_rate': 3.6674496918763084e-05, 'epoch': 1.28}
2025-10-14 11:30:56 | {'loss': 1.2343, 'grad_norm': 3.63700270652771, 'learning_rate': 3.597366571450619e-05, 'epoch': 1.41}
2025-10-14 11:31:02 | {'loss': 1.2306, 'grad_norm': 3.5394279956817627, 'learning_rate': 3.5272834510249305e-05, 'epoch': 1.54}
2025-10-14 11:31:09 | {'loss': 1.2417, 'grad_norm': 3.2089200019836426, 'learning_rate': 3.457200330599241e-05, 'epoch': 1.67}
2025-10-14 11:31:14 | {'loss': 1.2253, 'grad_norm': 3.1960577964782715, 'learning_rate': 3.387117210173552e-05, 'epoch': 1.8}
2025-10-14 11:31:20 | {'loss': 1.2224, 'grad_norm': 3.4620094299316406, 'learning_rate': 3.317034089747863e-05, 'epoch': 1.93}
2025-10-14 11:31:49 | {'eval_loss': 1.294702172279358, 'eval_rouge1': 0.43407788811475445, 'eval_rouge2': 0.2785927853098673, 'eval_rougeL': 0.42697629536663645, 'eval_rouge_sum': 1.1396469687912583, 'eval_runtime': 25.5469, 'eval_samples_per_second': 19.533, 'eval_steps_per_second': 1.253, 'epoch': 2.0}
2025-10-14 11:31:53 | {'loss': 1.1122, 'grad_norm': 3.2018072605133057, 'learning_rate': 3.246950969322174e-05, 'epoch': 2.05}
2025-10-14 11:31:58 | {'loss': 0.987, 'grad_norm': 3.295247793197632, 'learning_rate': 3.1768678488964845e-05, 'epoch': 2.18}
2025-10-14 11:32:04 | {'loss': 1.0001, 'grad_norm': 3.25984263420105, 'learning_rate': 3.106784728470796e-05, 'epoch': 2.31}
2025-10-14 11:32:10 | {'loss': 0.9972, 'grad_norm': 3.3304004669189453, 'learning_rate': 3.0367016080451067e-05, 'epoch': 2.44}
2025-10-14 11:32:18 | {'loss': 1.0082, 'grad_norm': 3.660112142562866, 'learning_rate': 2.9666184876194174e-05, 'epoch': 2.57}
2025-10-14 11:32:24 | {'loss': 0.9931, 'grad_norm': 3.5926215648651123, 'learning_rate': 2.896535367193729e-05, 'epoch': 2.7}
2025-10-14 11:32:29 | {'loss': 0.9978, 'grad_norm': 3.4888570308685303, 'learning_rate': 2.8264522467680396e-05, 'epoch': 2.82}
2025-10-14 11:32:35 | {'loss': 0.9991, 'grad_norm': 3.307142972946167, 'learning_rate': 2.7563691263423503e-05, 'epoch': 2.95}
2025-10-14 11:33:04 | {'eval_loss': 1.3037593364715576, 'eval_rouge1': 0.4354196999581523, 'eval_rouge2': 0.27863990538953465, 'eval_rougeL': 0.4294759929016934, 'eval_rouge_sum': 1.1435355982493802, 'eval_runtime': 26.446, 'eval_samples_per_second': 18.869, 'eval_steps_per_second': 1.21, 'epoch': 3.0}
2025-10-14 11:33:09 | {'loss': 0.8908, 'grad_norm': 3.1054770946502686, 'learning_rate': 2.6862860059166617e-05, 'epoch': 3.08}
2025-10-14 11:33:15 | {'loss': 0.8087, 'grad_norm': 3.4271581172943115, 'learning_rate': 2.6162028854909725e-05, 'epoch': 3.21}
2025-10-14 11:33:22 | {'loss': 0.8236, 'grad_norm': 3.5101215839385986, 'learning_rate': 2.5461197650652832e-05, 'epoch': 3.34}
2025-10-14 11:33:28 | {'loss': 0.8183, 'grad_norm': 3.3384573459625244, 'learning_rate': 2.4760366446395943e-05, 'epoch': 3.47}
2025-10-14 11:33:34 | {'loss': 0.8364, 'grad_norm': 3.368056535720825, 'learning_rate': 2.4059535242139054e-05, 'epoch': 3.59}
2025-10-14 11:33:40 | {'loss': 0.8361, 'grad_norm': 3.6914312839508057, 'learning_rate': 2.335870403788216e-05, 'epoch': 3.72}
2025-10-14 11:33:45 | {'loss': 0.8259, 'grad_norm': 3.778333902359009, 'learning_rate': 2.265787283362527e-05, 'epoch': 3.85}
2025-10-14 11:33:51 | {'loss': 0.8271, 'grad_norm': 3.504272222518921, 'learning_rate': 2.195704162936838e-05, 'epoch': 3.98}
2025-10-14 11:34:20 | {'eval_loss': 1.3511154651641846, 'eval_rouge1': 0.45323870536509436, 'eval_rouge2': 0.2910239572779823, 'eval_rougeL': 0.4440890586609268, 'eval_rouge_sum': 1.1883517213040033, 'eval_runtime': 28.3811, 'eval_samples_per_second': 17.582, 'eval_steps_per_second': 1.128, 'epoch': 4.0}
2025-10-14 11:34:27 | {'loss': 0.6996, 'grad_norm': 3.5595543384552, 'learning_rate': 2.125621042511149e-05, 'epoch': 4.11}
2025-10-14 11:34:32 | {'loss': 0.6778, 'grad_norm': 3.362290382385254, 'learning_rate': 2.05553792208546e-05, 'epoch': 4.24}
2025-10-14 11:34:38 | {'loss': 0.6841, 'grad_norm': 3.7146527767181396, 'learning_rate': 1.9854548016597708e-05, 'epoch': 4.36}
2025-10-14 11:34:46 | {'loss': 0.6856, 'grad_norm': 2.8430123329162598, 'learning_rate': 1.915371681234082e-05, 'epoch': 4.49}
2025-10-14 11:34:52 | {'loss': 0.6941, 'grad_norm': 3.2361233234405518, 'learning_rate': 1.845288560808393e-05, 'epoch': 4.62}
2025-10-14 11:34:58 | {'loss': 0.7022, 'grad_norm': 3.574512481689453, 'learning_rate': 1.7752054403827037e-05, 'epoch': 4.75}
2025-10-14 11:35:04 | {'loss': 0.6937, 'grad_norm': 3.399996280670166, 'learning_rate': 1.7051223199570147e-05, 'epoch': 4.88}
2025-10-14 11:35:37 | {'eval_loss': 1.3828530311584473, 'eval_rouge1': 0.4645881978696214, 'eval_rouge2': 0.3067940045826159, 'eval_rougeL': 0.4581354437338852, 'eval_rouge_sum': 1.2295176461861226, 'eval_runtime': 27.4555, 'eval_samples_per_second': 18.175, 'eval_steps_per_second': 1.166, 'epoch': 5.0}
2025-10-14 11:35:38 | {'loss': 0.6984, 'grad_norm': 3.1881375312805176, 'learning_rate': 1.6350391995313258e-05, 'epoch': 5.01}
2025-10-14 11:35:44 | {'loss': 0.5767, 'grad_norm': 3.0676653385162354, 'learning_rate': 1.5649560791056366e-05, 'epoch': 5.13}
2025-10-14 11:35:50 | {'loss': 0.5776, 'grad_norm': 3.4762048721313477, 'learning_rate': 1.4948729586799475e-05, 'epoch': 5.26}
2025-10-14 11:35:58 | {'loss': 0.587, 'grad_norm': 3.0815370082855225, 'learning_rate': 1.4247898382542585e-05, 'epoch': 5.39}
2025-10-14 11:36:03 | {'loss': 0.5862, 'grad_norm': 3.3683011531829834, 'learning_rate': 1.3547067178285694e-05, 'epoch': 5.52}
2025-10-14 11:36:09 | {'loss': 0.6036, 'grad_norm': 3.464151620864868, 'learning_rate': 1.2846235974028804e-05, 'epoch': 5.65}
2025-10-14 11:36:15 | {'loss': 0.6013, 'grad_norm': 3.784778594970703, 'learning_rate': 1.2145404769771913e-05, 'epoch': 5.78}
2025-10-14 11:36:21 | {'loss': 0.5928, 'grad_norm': 3.5759377479553223, 'learning_rate': 1.1444573565515023e-05, 'epoch': 5.91}
2025-10-14 11:36:52 | {'eval_loss': 1.4216359853744507, 'eval_rouge1': 0.45202843968461087, 'eval_rouge2': 0.2952430370318779, 'eval_rougeL': 0.4435053891304392, 'eval_rouge_sum': 1.190776865846928, 'eval_runtime': 25.1303, 'eval_samples_per_second': 19.857, 'eval_steps_per_second': 1.273, 'epoch': 6.0}
2025-10-14 11:36:55 | {'loss': 0.5733, 'grad_norm': 3.2386181354522705, 'learning_rate': 1.074374236125813e-05, 'epoch': 6.03}
2025-10-14 11:37:02 | {'loss': 0.5072, 'grad_norm': 2.7486796379089355, 'learning_rate': 1.0042911157001241e-05, 'epoch': 6.16}
2025-10-14 11:37:08 | {'loss': 0.5181, 'grad_norm': 3.4035050868988037, 'learning_rate': 9.34207995274435e-06, 'epoch': 6.29}
2025-10-14 11:37:13 | {'loss': 0.5168, 'grad_norm': 3.2466506958007812, 'learning_rate': 8.641248748487461e-06, 'epoch': 6.42}
2025-10-14 11:37:19 | {'loss': 0.514, 'grad_norm': 3.06478214263916, 'learning_rate': 7.94041754423057e-06, 'epoch': 6.55}
2025-10-14 11:37:25 | {'loss': 0.5159, 'grad_norm': 3.1261701583862305, 'learning_rate': 7.239586339973679e-06, 'epoch': 6.68}
2025-10-14 11:37:30 | {'loss': 0.5221, 'grad_norm': 3.1428492069244385, 'learning_rate': 6.5387551357167885e-06, 'epoch': 6.8}
2025-10-14 11:37:38 | {'loss': 0.514, 'grad_norm': 3.7030868530273438, 'learning_rate': 5.8379239314598975e-06, 'epoch': 6.93}
2025-10-14 11:38:08 | {'eval_loss': 1.4499759674072266, 'eval_rouge1': 0.46235661495391484, 'eval_rouge2': 0.2960639881766966, 'eval_rougeL': 0.4526922007499261, 'eval_rouge_sum': 1.2111128038805377, 'eval_runtime': 27.1901, 'eval_samples_per_second': 18.352, 'eval_steps_per_second': 1.177, 'epoch': 7.0}
2025-10-14 11:38:13 | {'loss': 0.4952, 'grad_norm': 2.6700832843780518, 'learning_rate': 5.137092727203008e-06, 'epoch': 7.06}
2025-10-14 11:38:19 | {'loss': 0.4653, 'grad_norm': 2.9902029037475586, 'learning_rate': 4.436261522946117e-06, 'epoch': 7.19}
2025-10-14 11:38:25 | {'loss': 0.4711, 'grad_norm': 2.553849697113037, 'learning_rate': 3.735430318689227e-06, 'epoch': 7.32}
2025-10-14 11:38:30 | {'loss': 0.47, 'grad_norm': 3.269110679626465, 'learning_rate': 3.0345991144323363e-06, 'epoch': 7.45}
2025-10-14 11:38:36 | {'loss': 0.4645, 'grad_norm': 3.170816421508789, 'learning_rate': 2.3337679101754454e-06, 'epoch': 7.57}
2025-10-14 11:38:43 | {'loss': 0.4675, 'grad_norm': 3.063199996948242, 'learning_rate': 1.632936705918555e-06, 'epoch': 7.7}
2025-10-14 11:38:49 | {'loss': 0.4717, 'grad_norm': 3.1460390090942383, 'learning_rate': 9.321055016616644e-07, 'epoch': 7.83}
2025-10-14 11:38:54 | {'loss': 0.4609, 'grad_norm': 2.766338586807251, 'learning_rate': 2.3127429740477389e-07, 'epoch': 7.96}
2025-10-14 11:39:25 | {'eval_loss': 1.466196060180664, 'eval_rouge1': 0.46717541063176604, 'eval_rouge2': 0.3051769115396033, 'eval_rougeL': 0.4556030877993069, 'eval_rouge_sum': 1.2279554099706762, 'eval_runtime': 28.8452, 'eval_samples_per_second': 17.299, 'eval_steps_per_second': 1.109, 'epoch': 8.0}
2025-10-14 11:39:27 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 11:39:27 | {'train_runtime': 606.0093, 'train_samples_per_second': 164.446, 'train_steps_per_second': 10.284, 'train_loss': 0.8657258741356748, 'epoch': 8.0}
2025-10-14 11:39:27 | 최종 모델 저장 중...
2025-10-14 11:39:27 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 11:39:27 | 최종 평가 중...
2025-10-14 11:39:54 | 최종 평가 결과:
2025-10-14 11:39:54 | eval_rouge1: 0.4646
2025-10-14 11:39:54 | eval_rouge2: 0.3068
2025-10-14 11:39:54 | eval_rougeL: 0.4581
2025-10-14 11:39:54 | eval_rouge_sum: 1.2295
2025-10-14 11:39:54 | ============================================================
2025-10-14 11:39:54 | ✅ 학습 완료!
2025-10-14 11:39:54 | ============================================================
2025-10-14 11:39:54 | 모델 평가 중...
2025-10-14 11:40:20 | → 메트릭 'eval_rougeL' 사용: 0.4581
2025-10-14 11:40:20 | Trial 12 완료
2025-10-14 11:40:20 | - ROUGE-L F1: 0.4581
2025-10-14 11:40:20 | [I 2025-10-14 11:40:20,389] Trial 12 finished with value: 0.4581354437338852 and parameters: {'learning_rate': 4.0171644628004966e-05, 'num_epochs': 8, 'warmup_ratio': 0.06454328641269678, 'weight_decay': 0.09990061001173775, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9405420786530134}. Best is trial 11 with value: 0.46158654281217215.
2025-10-14 11:40:20 | ============================================================
2025-10-14 11:40:20 | Trial 13 시작
2025-10-14 11:40:20 | 파라미터: {'learning_rate': 3.816555624863532e-05, 'num_epochs': 8, 'warmup_ratio': 0.04986642306529774, 'weight_decay': 0.0991666466553262, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 1.0141989323994745}
2025-10-14 11:40:20 | ============================================================
2025-10-14 11:40:20 | 모델 타입: encoder_decoder
2025-10-14 11:40:20 | ============================================================
2025-10-14 11:40:20 | 모델 및 토크나이저 로딩 시작
2025-10-14 11:40:20 | ============================================================
2025-10-14 11:40:20 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 11:40:20 | 모델 로딩: digit82/kobart-summarization
2025-10-14 11:40:21 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 11:40:23 | → 디바이스: cuda
2025-10-14 11:40:23 | → 전체 파라미터: 123,859,968
2025-10-14 11:40:23 | → 학습 가능 파라미터: 123,859,968
2025-10-14 11:40:23 | ============================================================
2025-10-14 11:40:23 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 11:40:23 | ============================================================
2025-10-14 11:40:23 | ============================================================
2025-10-14 11:40:23 | 모델 학습 시작
2025-10-14 11:40:23 | ============================================================
2025-10-14 11:40:23 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 11:40:23 | 학습 진행 중...
2025-10-14 11:40:30 | {'loss': 2.2156, 'grad_norm': 4.847024917602539, 'learning_rate': 7.556780137229794e-06, 'epoch': 0.13}
2025-10-14 11:40:36 | {'loss': 1.7003, 'grad_norm': 4.22987174987793, 'learning_rate': 1.5189891386956858e-05, 'epoch': 0.26}
2025-10-14 11:40:41 | {'loss': 1.5696, 'grad_norm': 4.2150726318359375, 'learning_rate': 2.282300263668392e-05, 'epoch': 0.39}
2025-10-14 11:40:47 | {'loss': 1.5145, 'grad_norm': 3.8739285469055176, 'learning_rate': 3.0456113886410987e-05, 'epoch': 0.51}
2025-10-14 11:40:53 | {'loss': 1.4994, 'grad_norm': 3.707197427749634, 'learning_rate': 3.808922513613805e-05, 'epoch': 0.64}
2025-10-14 11:41:01 | {'loss': 1.4601, 'grad_norm': 3.199864149093628, 'learning_rate': 3.750638142856991e-05, 'epoch': 0.77}
2025-10-14 11:41:07 | {'loss': 1.4509, 'grad_norm': 3.6549105644226074, 'learning_rate': 3.68405482769887e-05, 'epoch': 0.9}
2025-10-14 11:41:39 | {'eval_loss': 1.358924150466919, 'eval_rouge1': 0.39452530659258966, 'eval_rouge2': 0.2523305960243302, 'eval_rougeL': 0.3885361058262184, 'eval_rouge_sum': 1.0353920084431383, 'eval_runtime': 27.9638, 'eval_samples_per_second': 17.845, 'eval_steps_per_second': 1.144, 'epoch': 1.0}
2025-10-14 11:41:42 | {'loss': 1.4064, 'grad_norm': 3.3489291667938232, 'learning_rate': 3.617471512540748e-05, 'epoch': 1.03}
2025-10-14 11:41:48 | {'loss': 1.2482, 'grad_norm': 3.6838834285736084, 'learning_rate': 3.550888197382627e-05, 'epoch': 1.16}
2025-10-14 11:41:54 | {'loss': 1.2534, 'grad_norm': 3.362152576446533, 'learning_rate': 3.484304882224505e-05, 'epoch': 1.28}
2025-10-14 11:42:00 | {'loss': 1.2395, 'grad_norm': 3.7784581184387207, 'learning_rate': 3.417721567066383e-05, 'epoch': 1.41}
2025-10-14 11:42:07 | {'loss': 1.2359, 'grad_norm': 3.562263250350952, 'learning_rate': 3.351138251908262e-05, 'epoch': 1.54}
2025-10-14 11:42:13 | {'loss': 1.247, 'grad_norm': 3.2321674823760986, 'learning_rate': 3.28455493675014e-05, 'epoch': 1.67}
2025-10-14 11:42:19 | {'loss': 1.2304, 'grad_norm': 3.2314870357513428, 'learning_rate': 3.217971621592018e-05, 'epoch': 1.8}
2025-10-14 11:42:24 | {'loss': 1.2268, 'grad_norm': 3.505366802215576, 'learning_rate': 3.151388306433897e-05, 'epoch': 1.93}
2025-10-14 11:42:55 | {'eval_loss': 1.295257806777954, 'eval_rouge1': 0.43407250287548776, 'eval_rouge2': 0.2801883057447693, 'eval_rougeL': 0.4285672298151355, 'eval_rouge_sum': 1.1428280384353926, 'eval_runtime': 26.905, 'eval_samples_per_second': 18.547, 'eval_steps_per_second': 1.189, 'epoch': 2.0}
2025-10-14 11:42:59 | {'loss': 1.119, 'grad_norm': 3.2069411277770996, 'learning_rate': 3.084804991275775e-05, 'epoch': 2.05}
2025-10-14 11:43:04 | {'loss': 0.9999, 'grad_norm': 3.270545482635498, 'learning_rate': 3.0182216761176533e-05, 'epoch': 2.18}
2025-10-14 11:43:12 | {'loss': 1.0118, 'grad_norm': 3.2929177284240723, 'learning_rate': 2.9516383609595322e-05, 'epoch': 2.31}
2025-10-14 11:43:17 | {'loss': 1.009, 'grad_norm': 3.290093421936035, 'learning_rate': 2.8850550458014104e-05, 'epoch': 2.44}
2025-10-14 11:43:23 | {'loss': 1.0188, 'grad_norm': 3.659910202026367, 'learning_rate': 2.8184717306432886e-05, 'epoch': 2.57}
2025-10-14 11:43:29 | {'loss': 1.0036, 'grad_norm': 3.593997001647949, 'learning_rate': 2.751888415485167e-05, 'epoch': 2.7}
2025-10-14 11:43:34 | {'loss': 1.0077, 'grad_norm': 3.522693634033203, 'learning_rate': 2.6853051003270453e-05, 'epoch': 2.82}
2025-10-14 11:43:40 | {'loss': 1.0083, 'grad_norm': 3.336242914199829, 'learning_rate': 2.6187217851689235e-05, 'epoch': 2.95}
2025-10-14 11:44:09 | {'eval_loss': 1.3037549257278442, 'eval_rouge1': 0.4374770117801948, 'eval_rouge2': 0.2790819090098764, 'eval_rougeL': 0.429133514995109, 'eval_rouge_sum': 1.1456924357851803, 'eval_runtime': 27.3462, 'eval_samples_per_second': 18.247, 'eval_steps_per_second': 1.17, 'epoch': 3.0}
2025-10-14 11:44:15 | {'loss': 0.9037, 'grad_norm': 3.144094228744507, 'learning_rate': 2.5521384700108024e-05, 'epoch': 3.08}
2025-10-14 11:44:22 | {'loss': 0.8244, 'grad_norm': 3.461986541748047, 'learning_rate': 2.4855551548526806e-05, 'epoch': 3.21}
2025-10-14 11:44:28 | {'loss': 0.8396, 'grad_norm': 3.6300957202911377, 'learning_rate': 2.4189718396945588e-05, 'epoch': 3.34}
2025-10-14 11:44:34 | {'loss': 0.8333, 'grad_norm': 3.3071625232696533, 'learning_rate': 2.3523885245364373e-05, 'epoch': 3.47}
2025-10-14 11:44:39 | {'loss': 0.8517, 'grad_norm': 3.3838741779327393, 'learning_rate': 2.2858052093783155e-05, 'epoch': 3.59}
2025-10-14 11:44:45 | {'loss': 0.8525, 'grad_norm': 3.746410608291626, 'learning_rate': 2.2192218942201937e-05, 'epoch': 3.72}
2025-10-14 11:44:51 | {'loss': 0.8409, 'grad_norm': 3.848827838897705, 'learning_rate': 2.1526385790620726e-05, 'epoch': 3.85}
2025-10-14 11:44:58 | {'loss': 0.842, 'grad_norm': 3.5199363231658936, 'learning_rate': 2.0860552639039508e-05, 'epoch': 3.98}
2025-10-14 11:45:28 | {'eval_loss': 1.345169186592102, 'eval_rouge1': 0.45303710718208445, 'eval_rouge2': 0.2926896114716027, 'eval_rougeL': 0.4432284054195059, 'eval_rouge_sum': 1.188955124073193, 'eval_runtime': 28.9126, 'eval_samples_per_second': 17.259, 'eval_steps_per_second': 1.107, 'epoch': 4.0}
2025-10-14 11:45:34 | {'loss': 0.7176, 'grad_norm': 3.49340558052063, 'learning_rate': 2.019471948745829e-05, 'epoch': 4.11}
2025-10-14 11:45:40 | {'loss': 0.6963, 'grad_norm': 3.3832809925079346, 'learning_rate': 1.9528886335877075e-05, 'epoch': 4.24}
2025-10-14 11:45:45 | {'loss': 0.7037, 'grad_norm': 3.733164072036743, 'learning_rate': 1.8863053184295857e-05, 'epoch': 4.36}
2025-10-14 11:45:51 | {'loss': 0.7043, 'grad_norm': 2.860649347305298, 'learning_rate': 1.8197220032714642e-05, 'epoch': 4.49}
2025-10-14 11:45:57 | {'loss': 0.7125, 'grad_norm': 3.3624753952026367, 'learning_rate': 1.7531386881133428e-05, 'epoch': 4.62}
2025-10-14 11:46:04 | {'loss': 0.72, 'grad_norm': 3.6103603839874268, 'learning_rate': 1.686555372955221e-05, 'epoch': 4.75}
2025-10-14 11:46:10 | {'loss': 0.7119, 'grad_norm': 3.45001482963562, 'learning_rate': 1.619972057797099e-05, 'epoch': 4.88}
2025-10-14 11:46:44 | {'eval_loss': 1.3746799230575562, 'eval_rouge1': 0.46485143824466435, 'eval_rouge2': 0.3032195192159083, 'eval_rougeL': 0.4560502818354435, 'eval_rouge_sum': 1.2241212392960161, 'eval_runtime': 28.3138, 'eval_samples_per_second': 17.624, 'eval_steps_per_second': 1.13, 'epoch': 5.0}
2025-10-14 11:46:46 | {'loss': 0.7166, 'grad_norm': 3.2053818702697754, 'learning_rate': 1.5533887426389777e-05, 'epoch': 5.01}
2025-10-14 11:46:51 | {'loss': 0.5974, 'grad_norm': 3.1178805828094482, 'learning_rate': 1.4868054274808559e-05, 'epoch': 5.13}
2025-10-14 11:46:57 | {'loss': 0.5987, 'grad_norm': 3.5657522678375244, 'learning_rate': 1.4202221123227344e-05, 'epoch': 5.26}
2025-10-14 11:47:03 | {'loss': 0.6083, 'grad_norm': 3.1226539611816406, 'learning_rate': 1.3536387971646128e-05, 'epoch': 5.39}
2025-10-14 11:47:11 | {'loss': 0.6071, 'grad_norm': 3.4456064701080322, 'learning_rate': 1.2870554820064912e-05, 'epoch': 5.52}
2025-10-14 11:47:17 | {'loss': 0.6245, 'grad_norm': 3.524289131164551, 'learning_rate': 1.2204721668483695e-05, 'epoch': 5.65}
2025-10-14 11:47:23 | {'loss': 0.6225, 'grad_norm': 3.834745407104492, 'learning_rate': 1.1538888516902479e-05, 'epoch': 5.78}
2025-10-14 11:47:29 | {'loss': 0.6133, 'grad_norm': 3.603945732116699, 'learning_rate': 1.0873055365321263e-05, 'epoch': 5.91}
2025-10-14 11:47:59 | {'eval_loss': 1.4134371280670166, 'eval_rouge1': 0.45250236001800376, 'eval_rouge2': 0.29629209632482434, 'eval_rougeL': 0.4440687507179557, 'eval_rouge_sum': 1.1928632070607839, 'eval_runtime': 26.1103, 'eval_samples_per_second': 19.111, 'eval_steps_per_second': 1.226, 'epoch': 6.0}
2025-10-14 11:48:02 | {'loss': 0.5941, 'grad_norm': 3.2899372577667236, 'learning_rate': 1.0207222213740046e-05, 'epoch': 6.03}
2025-10-14 11:48:08 | {'loss': 0.5292, 'grad_norm': 2.827955484390259, 'learning_rate': 9.54138906215883e-06, 'epoch': 6.16}
2025-10-14 11:48:13 | {'loss': 0.5406, 'grad_norm': 3.4474706649780273, 'learning_rate': 8.875555910577614e-06, 'epoch': 6.29}
2025-10-14 11:48:21 | {'loss': 0.5396, 'grad_norm': 3.3104896545410156, 'learning_rate': 8.209722758996397e-06, 'epoch': 6.42}
2025-10-14 11:48:27 | {'loss': 0.5366, 'grad_norm': 3.106973648071289, 'learning_rate': 7.543889607415181e-06, 'epoch': 6.55}
2025-10-14 11:48:32 | {'loss': 0.5379, 'grad_norm': 3.1902012825012207, 'learning_rate': 6.878056455833965e-06, 'epoch': 6.68}
2025-10-14 11:48:38 | {'loss': 0.5449, 'grad_norm': 3.1372594833374023, 'learning_rate': 6.212223304252748e-06, 'epoch': 6.8}
2025-10-14 11:48:44 | {'loss': 0.5365, 'grad_norm': 3.8118503093719482, 'learning_rate': 5.546390152671532e-06, 'epoch': 6.93}
2025-10-14 11:49:13 | {'eval_loss': 1.438760757446289, 'eval_rouge1': 0.45786097222220234, 'eval_rouge2': 0.2984337590751657, 'eval_rougeL': 0.4520020944371193, 'eval_rouge_sum': 1.2082968257344873, 'eval_runtime': 26.3595, 'eval_samples_per_second': 18.931, 'eval_steps_per_second': 1.214, 'epoch': 7.0}
2025-10-14 11:49:18 | {'loss': 0.5179, 'grad_norm': 2.743757724761963, 'learning_rate': 4.880557001090316e-06, 'epoch': 7.06}
2025-10-14 11:49:25 | {'loss': 0.4885, 'grad_norm': 3.025056838989258, 'learning_rate': 4.214723849509099e-06, 'epoch': 7.19}
2025-10-14 11:49:30 | {'loss': 0.4942, 'grad_norm': 2.5821075439453125, 'learning_rate': 3.548890697927883e-06, 'epoch': 7.32}
2025-10-14 11:49:36 | {'loss': 0.493, 'grad_norm': 3.2940292358398438, 'learning_rate': 2.883057546346667e-06, 'epoch': 7.45}
2025-10-14 11:49:42 | {'loss': 0.4882, 'grad_norm': 3.23544979095459, 'learning_rate': 2.2172243947654504e-06, 'epoch': 7.57}
2025-10-14 11:49:47 | {'loss': 0.4916, 'grad_norm': 3.134397506713867, 'learning_rate': 1.551391243184234e-06, 'epoch': 7.7}
2025-10-14 11:49:53 | {'loss': 0.4953, 'grad_norm': 3.221818685531616, 'learning_rate': 8.855580916030176e-07, 'epoch': 7.83}
2025-10-14 11:50:00 | {'loss': 0.4843, 'grad_norm': 2.8379874229431152, 'learning_rate': 2.197249400218014e-07, 'epoch': 7.96}
2025-10-14 11:50:32 | {'eval_loss': 1.455054759979248, 'eval_rouge1': 0.46215829372142, 'eval_rouge2': 0.2982399857777548, 'eval_rougeL': 0.4536055631871102, 'eval_rouge_sum': 1.2140038426862851, 'eval_runtime': 29.8471, 'eval_samples_per_second': 16.719, 'eval_steps_per_second': 1.072, 'epoch': 8.0}
2025-10-14 11:50:34 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 11:50:34 | {'train_runtime': 609.9035, 'train_samples_per_second': 163.396, 'train_steps_per_second': 10.218, 'train_loss': 0.8807333776672324, 'epoch': 8.0}
2025-10-14 11:50:34 | 최종 모델 저장 중...
2025-10-14 11:50:34 | → 모델 저장 위치: outputs/default/final_model
2025-10-14 11:50:34 | 최종 평가 중...
2025-10-14 11:50:59 | 최종 평가 결과:
2025-10-14 11:50:59 | eval_rouge1: 0.4649
2025-10-14 11:50:59 | eval_rouge2: 0.3032
2025-10-14 11:50:59 | eval_rougeL: 0.4561
2025-10-14 11:50:59 | eval_rouge_sum: 1.2241
2025-10-14 11:50:59 | ============================================================
2025-10-14 11:50:59 | ✅ 학습 완료!
2025-10-14 11:50:59 | ============================================================
2025-10-14 11:50:59 | 모델 평가 중...
2025-10-14 11:51:28 | → 메트릭 'eval_rougeL' 사용: 0.4561
2025-10-14 11:51:28 | Trial 13 완료
2025-10-14 11:51:28 | - ROUGE-L F1: 0.4561
2025-10-14 11:51:28 | Trial 13 Pruned!
2025-10-14 11:51:28 | Trial 13 실패:
2025-10-14 11:51:28 | Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/optimization/optuna_optimizer.py", line 240, in objective
optuna.exceptions.TrialPruned
2025-10-14 11:51:28 | [I 2025-10-14 11:51:28,423] Trial 13 pruned.
2025-10-14 11:51:28 | ======================================================================
2025-10-14 11:51:28 | Optuna 최적화 완료
2025-10-14 11:51:28 | ======================================================================
2025-10-14 11:51:28 | 최적 ROUGE-L F1: 0.4616
2025-10-14 11:51:28 | 최적 파라미터:
2025-10-14 11:51:28 | - learning_rate: 9.138518360133624e-05
2025-10-14 11:51:28 | - num_epochs: 7
2025-10-14 11:51:28 | - warmup_ratio: 0.0013572013949127268
2025-10-14 11:51:28 | - weight_decay: 0.09953784597545408
2025-10-14 11:51:28 | - scheduler_type: cosine
2025-10-14 11:51:28 | - num_beams: 4
2025-10-14 11:51:28 | - length_penalty: 0.9383576982529792
2025-10-14 11:51:28 | 결과 저장 중...
2025-10-14 11:51:28 | 최적 파라미터 저장: experiments/20251014/20251014_094051_kobart_ultimate/best_params.json
2025-10-14 11:51:28 | 전체 Trial 저장: experiments/20251014/20251014_094051_kobart_ultimate/all_trials.csv
2025-10-14 11:51:28 | Study 통계 저장: experiments/20251014/20251014_094051_kobart_ultimate/study_stats.json
2025-10-14 11:51:28 | - 완료: 11
2025-10-14 11:51:28 | - Pruned: 3
2025-10-14 11:51:28 | - 실패: 0
2025-10-14 11:51:28 | 시각화 생성 중...
2025-10-14 11:51:28 | plotly가 설치되지 않아 시각화를 건너뜁니다
2025-10-14 11:51:28 | ============================================================
2025-10-14 11:51:28 | ✅ OPTUNA 튜닝 완료!
2025-10-14 11:51:28 | 📈 최고 ROUGE-L F1: 0.4616
2025-10-14 11:51:28 | 🎯 최적 하이퍼파라미터:
2025-10-14 11:51:28 | learning_rate: 9.138518360133624e-05
2025-10-14 11:51:28 | num_epochs: 7
2025-10-14 11:51:28 | warmup_ratio: 0.0013572013949127268
2025-10-14 11:51:28 | weight_decay: 0.09953784597545408
2025-10-14 11:51:28 | scheduler_type: cosine
2025-10-14 11:51:28 | num_beams: 4
2025-10-14 11:51:28 | length_penalty: 0.9383576982529792
2025-10-14 11:51:28 | ============================================================
2025-10-14 11:51:28 | 📂 결과 저장: experiments/20251014/20251014_094051_kobart_ultimate/optuna_results.json
2025-10-14 11:51:28 | 📂 최적 Config 저장: experiments/20251014/20251014_094051_kobart_ultimate/best_config.yaml
2025-10-14 11:51:28 | 📈 시각화 생성 중...
2025-10-14 11:51:28 | ✅ 나눔고딕 폰트 로드 성공
2025-10-14 11:51:28 | ⚠️ 시각화 모듈 없음 (추후 구현 예정)
2025-10-14 11:51:28 | ============================================================
2025-10-14 11:51:28 | ✅ 학습 완료!
2025-10-14 11:51:28 | 📁 결과 저장: experiments/20251014/20251014_094051_kobart_ultimate
2025-10-14 11:51:28 | ============================================================
2025-10-14 11:51:28 | >> 로그 리디렉션 중료.
