2025-10-14 15:46:16 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-14 15:46:18 | 📊 OPTUNA 모드 실행 중...
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | 📊 OPTUNA 튜닝 모드 시작
2025-10-14 15:46:18 | 🔧 모델: kobart
2025-10-14 15:46:18 | 🔢 시도 횟수: 20
2025-10-14 15:46:18 | ⏱ 최대 시간: 7200
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | [1/3] 데이터 로드...
2025-10-14 15:46:18 | ✅ 학습 데이터: 12457개
2025-10-14 15:46:18 | ✅ 검증 데이터: 499개
2025-10-14 15:46:18 | [2/3] Config 로드...
2025-10-14 15:46:18 | Config 로드 완료: kobart
2025-10-14 15:46:18 | [3/3] Optuna 튜닝 시작...
2025-10-14 15:46:18 | OptunaOptimizer 초기화 완료
2025-10-14 15:46:18 | - Study 이름: optuna_kobart_kobart_ultimate_optuna
2025-10-14 15:46:18 | - Trial 횟수: 20
2025-10-14 15:46:18 | - 방향: maximize
2025-10-14 15:46:18 | - 체크포인트: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna_kobart_kobart_ultimate_optuna_checkpoint.pkl
2025-10-14 15:46:18 | ======================================================================
2025-10-14 15:46:18 | Optuna 최적화 시작
2025-10-14 15:46:18 | ======================================================================
2025-10-14 15:46:18 | [I 2025-10-14 15:46:18,677] A new study created in memory with name: optuna_kobart_kobart_ultimate_optuna
2025-10-14 15:46:18 | - 남은 Trial: 20개
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | Trial 0 시작
2025-10-14 15:46:18 | 파라미터: {'learning_rate': 5.611516415334504e-06, 'num_epochs': 10, 'warmup_ratio': 0.146398788362281, 'weight_decay': 0.05986584841970366, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.7486639612006325}
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | 모델 타입: encoder_decoder
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | 모델 및 토크나이저 로딩 시작
2025-10-14 15:46:18 | ============================================================
2025-10-14 15:46:18 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 15:46:19 | 모델 로딩: digit82/kobart-summarization
2025-10-14 15:46:19 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 15:46:20 | → 디바이스: cuda
2025-10-14 15:46:20 | → 전체 파라미터: 123,859,968
2025-10-14 15:46:20 | → 학습 가능 파라미터: 123,859,968
2025-10-14 15:46:20 | ============================================================
2025-10-14 15:46:20 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 15:46:20 | ============================================================
2025-10-14 15:46:20 | ============================================================
2025-10-14 15:46:20 | 모델 학습 시작
2025-10-14 15:46:20 | ============================================================
2025-10-14 15:46:20 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 15:46:20 | 학습 진행 중...
2025-10-14 15:46:27 | {'loss': 2.6596, 'grad_norm': 6.166715145111084, 'learning_rate': 1.1110802502362317e-06, 'epoch': 0.13}
2025-10-14 15:46:33 | {'loss': 1.9803, 'grad_norm': 5.437508583068848, 'learning_rate': 2.2333835333031327e-06, 'epoch': 0.26}
2025-10-14 15:46:39 | {'loss': 1.755, 'grad_norm': 4.865267276763916, 'learning_rate': 3.355686816370033e-06, 'epoch': 0.39}
2025-10-14 15:46:45 | {'loss': 1.6703, 'grad_norm': 4.233885765075684, 'learning_rate': 4.477990099436934e-06, 'epoch': 0.51}
2025-10-14 15:46:51 | {'loss': 1.6289, 'grad_norm': 4.007720470428467, 'learning_rate': 5.600293382503835e-06, 'epoch': 0.64}
2025-10-14 15:46:57 | {'loss': 1.579, 'grad_norm': 3.768005132675171, 'learning_rate': 5.535310636854652e-06, 'epoch': 0.77}
2025-10-14 15:47:03 | {'loss': 1.5569, 'grad_norm': 4.0215044021606445, 'learning_rate': 5.4583351030366205e-06, 'epoch': 0.9}
2025-10-14 15:47:39 | {'eval_loss': 1.4557002782821655, 'eval_rouge1': 0.4013122815204027, 'eval_rouge2': 0.2589339664887045, 'eval_rougeL': 0.3957558757665432, 'eval_rouge_sum': 1.0560021237756503, 'eval_runtime': 30.9793, 'eval_samples_per_second': 16.108, 'eval_steps_per_second': 1.033, 'epoch': 1.0}
2025-10-14 15:47:42 | {'loss': 1.5491, 'grad_norm': 3.9793825149536133, 'learning_rate': 5.38135956921859e-06, 'epoch': 1.03}
2025-10-14 15:47:47 | {'loss': 1.4842, 'grad_norm': 4.3871073722839355, 'learning_rate': 5.304384035400558e-06, 'epoch': 1.16}
2025-10-14 15:47:53 | {'loss': 1.4771, 'grad_norm': 3.7523040771484375, 'learning_rate': 5.227408501582526e-06, 'epoch': 1.28}
2025-10-14 15:47:59 | {'loss': 1.4583, 'grad_norm': 4.3382768630981445, 'learning_rate': 5.150432967764494e-06, 'epoch': 1.41}
2025-10-14 15:48:05 | {'loss': 1.4429, 'grad_norm': 3.843353033065796, 'learning_rate': 5.073457433946463e-06, 'epoch': 1.54}
2025-10-14 15:48:11 | {'loss': 1.4606, 'grad_norm': 3.7446320056915283, 'learning_rate': 4.996481900128431e-06, 'epoch': 1.67}
2025-10-14 15:48:17 | {'loss': 1.4368, 'grad_norm': 3.576680898666382, 'learning_rate': 4.9195063663104e-06, 'epoch': 1.8}
2025-10-14 15:48:23 | {'loss': 1.4348, 'grad_norm': 4.087047576904297, 'learning_rate': 4.842530832492368e-06, 'epoch': 1.93}
2025-10-14 15:48:56 | {'eval_loss': 1.3873867988586426, 'eval_rouge1': 0.42078268065947555, 'eval_rouge2': 0.27055289378045677, 'eval_rougeL': 0.41319118270483784, 'eval_rouge_sum': 1.10452675714477, 'eval_runtime': 29.7598, 'eval_samples_per_second': 16.768, 'eval_steps_per_second': 1.075, 'epoch': 2.0}
2025-10-14 15:49:00 | {'loss': 1.3948, 'grad_norm': 3.6544623374938965, 'learning_rate': 4.7655552986743365e-06, 'epoch': 2.05}
2025-10-14 15:49:05 | {'loss': 1.3693, 'grad_norm': 3.5354843139648438, 'learning_rate': 4.688579764856305e-06, 'epoch': 2.18}
2025-10-14 15:49:11 | {'loss': 1.3799, 'grad_norm': 3.693110704421997, 'learning_rate': 4.611604231038274e-06, 'epoch': 2.31}
2025-10-14 15:49:17 | {'loss': 1.3681, 'grad_norm': 3.611865282058716, 'learning_rate': 4.534628697220242e-06, 'epoch': 2.44}
2025-10-14 15:49:23 | {'loss': 1.3664, 'grad_norm': 3.723180055618286, 'learning_rate': 4.45765316340221e-06, 'epoch': 2.57}
2025-10-14 15:49:28 | {'loss': 1.3494, 'grad_norm': 3.8870279788970947, 'learning_rate': 4.380677629584179e-06, 'epoch': 2.7}
2025-10-14 15:49:34 | {'loss': 1.3414, 'grad_norm': 3.88988995552063, 'learning_rate': 4.303702095766147e-06, 'epoch': 2.82}
2025-10-14 15:49:40 | {'loss': 1.3328, 'grad_norm': 3.749403953552246, 'learning_rate': 4.226726561948115e-06, 'epoch': 2.95}
2025-10-14 15:50:13 | {'eval_loss': 1.3598086833953857, 'eval_rouge1': 0.42572423789368513, 'eval_rouge2': 0.2777888786653038, 'eval_rougeL': 0.41881487313813265, 'eval_rouge_sum': 1.1223279896971217, 'eval_runtime': 30.8982, 'eval_samples_per_second': 16.15, 'eval_steps_per_second': 1.036, 'epoch': 3.0}
2025-10-14 15:50:18 | {'loss': 1.3232, 'grad_norm': 3.5127151012420654, 'learning_rate': 4.149751028130083e-06, 'epoch': 3.08}
2025-10-14 15:50:24 | {'loss': 1.2979, 'grad_norm': 3.7647299766540527, 'learning_rate': 4.072775494312052e-06, 'epoch': 3.21}
2025-10-14 15:50:31 | {'loss': 1.3118, 'grad_norm': 3.676450490951538, 'learning_rate': 3.99579996049402e-06, 'epoch': 3.34}
2025-10-14 15:50:36 | {'loss': 1.2914, 'grad_norm': 3.7374160289764404, 'learning_rate': 3.918824426675989e-06, 'epoch': 3.47}
2025-10-14 15:50:42 | {'loss': 1.3121, 'grad_norm': 3.5428342819213867, 'learning_rate': 3.841848892857957e-06, 'epoch': 3.59}
2025-10-14 15:50:48 | {'loss': 1.3078, 'grad_norm': 3.997016191482544, 'learning_rate': 3.764873359039926e-06, 'epoch': 3.72}
2025-10-14 15:50:54 | {'loss': 1.2836, 'grad_norm': 4.227793216705322, 'learning_rate': 3.687897825221894e-06, 'epoch': 3.85}
2025-10-14 15:51:00 | {'loss': 1.2811, 'grad_norm': 3.806067943572998, 'learning_rate': 3.6109222914038624e-06, 'epoch': 3.98}
2025-10-14 15:51:33 | {'eval_loss': 1.3395737409591675, 'eval_rouge1': 0.42854446319343537, 'eval_rouge2': 0.2780695319833969, 'eval_rougeL': 0.4203888477054475, 'eval_rouge_sum': 1.1270028428822796, 'eval_runtime': 32.1777, 'eval_samples_per_second': 15.508, 'eval_steps_per_second': 0.994, 'epoch': 4.0}
2025-10-14 15:51:40 | {'loss': 1.2536, 'grad_norm': 4.055638313293457, 'learning_rate': 3.5339467575858307e-06, 'epoch': 4.11}
2025-10-14 15:51:46 | {'loss': 1.2416, 'grad_norm': 4.232658386230469, 'learning_rate': 3.456971223767799e-06, 'epoch': 4.24}
2025-10-14 15:51:52 | {'loss': 1.2551, 'grad_norm': 4.5638651847839355, 'learning_rate': 3.3799956899497676e-06, 'epoch': 4.36}
2025-10-14 15:51:58 | {'loss': 1.2453, 'grad_norm': 3.4803102016448975, 'learning_rate': 3.303020156131736e-06, 'epoch': 4.49}
2025-10-14 15:52:04 | {'loss': 1.2512, 'grad_norm': 3.6253466606140137, 'learning_rate': 3.226044622313704e-06, 'epoch': 4.62}
2025-10-14 15:52:10 | {'loss': 1.2554, 'grad_norm': 4.0083394050598145, 'learning_rate': 3.1490690884956732e-06, 'epoch': 4.75}
2025-10-14 15:52:16 | {'loss': 1.2453, 'grad_norm': 3.992375135421753, 'learning_rate': 3.0720935546776415e-06, 'epoch': 4.88}
2025-10-14 15:52:54 | {'eval_loss': 1.3256820440292358, 'eval_rouge1': 0.4281887729814004, 'eval_rouge2': 0.2753815597566567, 'eval_rougeL': 0.41910827146370255, 'eval_rouge_sum': 1.1226786042017598, 'eval_runtime': 32.0467, 'eval_samples_per_second': 15.571, 'eval_steps_per_second': 0.999, 'epoch': 5.0}
2025-10-14 15:52:56 | {'loss': 1.2502, 'grad_norm': 3.873300552368164, 'learning_rate': 2.9951180208596098e-06, 'epoch': 5.01}
2025-10-14 15:53:02 | {'loss': 1.1997, 'grad_norm': 3.6163954734802246, 'learning_rate': 2.918142487041578e-06, 'epoch': 5.13}
2025-10-14 15:53:08 | {'loss': 1.2026, 'grad_norm': 4.274172306060791, 'learning_rate': 2.8411669532235463e-06, 'epoch': 5.26}
2025-10-14 15:53:14 | {'loss': 1.2106, 'grad_norm': 3.7838423252105713, 'learning_rate': 2.764191419405515e-06, 'epoch': 5.39}
2025-10-14 15:53:20 | {'loss': 1.2133, 'grad_norm': 3.7744698524475098, 'learning_rate': 2.687215885587483e-06, 'epoch': 5.52}
2025-10-14 15:53:26 | {'loss': 1.2291, 'grad_norm': 3.8759357929229736, 'learning_rate': 2.610240351769452e-06, 'epoch': 5.65}
2025-10-14 15:53:32 | {'loss': 1.2255, 'grad_norm': 4.102485179901123, 'learning_rate': 2.53326481795142e-06, 'epoch': 5.78}
2025-10-14 15:53:38 | {'loss': 1.2181, 'grad_norm': 3.960752010345459, 'learning_rate': 2.4562892841333884e-06, 'epoch': 5.91}
2025-10-14 15:54:13 | {'eval_loss': 1.3206067085266113, 'eval_rouge1': 0.42183141979479843, 'eval_rouge2': 0.27164992722447656, 'eval_rougeL': 0.41335137530738447, 'eval_rouge_sum': 1.1068327223266594, 'eval_runtime': 30.7683, 'eval_samples_per_second': 16.218, 'eval_steps_per_second': 1.04, 'epoch': 6.0}
2025-10-14 15:54:16 | {'loss': 1.2006, 'grad_norm': 3.5766308307647705, 'learning_rate': 2.3793137503153567e-06, 'epoch': 6.03}
2025-10-14 15:54:22 | {'loss': 1.1763, 'grad_norm': 3.3313095569610596, 'learning_rate': 2.302338216497325e-06, 'epoch': 6.16}
2025-10-14 15:54:28 | {'loss': 1.1977, 'grad_norm': 4.054534435272217, 'learning_rate': 2.2253626826792936e-06, 'epoch': 6.29}
2025-10-14 15:54:34 | {'loss': 1.1932, 'grad_norm': 3.7930548191070557, 'learning_rate': 2.1483871488612623e-06, 'epoch': 6.42}
2025-10-14 15:54:40 | {'loss': 1.1901, 'grad_norm': 3.395163059234619, 'learning_rate': 2.0714116150432305e-06, 'epoch': 6.55}
2025-10-14 15:54:46 | {'loss': 1.1805, 'grad_norm': 3.9570295810699463, 'learning_rate': 1.9944360812251988e-06, 'epoch': 6.68}
2025-10-14 15:54:52 | {'loss': 1.1789, 'grad_norm': 3.9405500888824463, 'learning_rate': 1.917460547407167e-06, 'epoch': 6.8}
2025-10-14 15:54:58 | {'loss': 1.1763, 'grad_norm': 3.953723907470703, 'learning_rate': 1.8404850135891357e-06, 'epoch': 6.93}
2025-10-14 15:55:32 | {'eval_loss': 1.3180160522460938, 'eval_rouge1': 0.43228913213429293, 'eval_rouge2': 0.27725951237348107, 'eval_rougeL': 0.4232652969309484, 'eval_rouge_sum': 1.1328139414387224, 'eval_runtime': 30.8928, 'eval_samples_per_second': 16.153, 'eval_steps_per_second': 1.036, 'epoch': 7.0}
2025-10-14 15:55:36 | {'loss': 1.1787, 'grad_norm': 3.6181750297546387, 'learning_rate': 1.763509479771104e-06, 'epoch': 7.06}
2025-10-14 15:55:42 | {'loss': 1.1572, 'grad_norm': 3.5853848457336426, 'learning_rate': 1.6865339459530724e-06, 'epoch': 7.19}
2025-10-14 15:55:48 | {'loss': 1.1588, 'grad_norm': 3.32442307472229, 'learning_rate': 1.6095584121350407e-06, 'epoch': 7.32}
2025-10-14 15:55:53 | {'loss': 1.1605, 'grad_norm': 3.845010280609131, 'learning_rate': 1.5325828783170094e-06, 'epoch': 7.45}
2025-10-14 15:55:59 | {'loss': 1.1594, 'grad_norm': 3.715167760848999, 'learning_rate': 1.4556073444989776e-06, 'epoch': 7.57}
2025-10-14 15:56:05 | {'loss': 1.169, 'grad_norm': 3.754804849624634, 'learning_rate': 1.378631810680946e-06, 'epoch': 7.7}
2025-10-14 15:56:11 | {'loss': 1.1761, 'grad_norm': 3.9074103832244873, 'learning_rate': 1.3016562768629144e-06, 'epoch': 7.83}
2025-10-14 15:56:17 | {'loss': 1.1529, 'grad_norm': 3.650954008102417, 'learning_rate': 1.2246807430448828e-06, 'epoch': 7.96}
2025-10-14 15:56:51 | {'eval_loss': 1.317612648010254, 'eval_rouge1': 0.43423101419781285, 'eval_rouge2': 0.2781353123087676, 'eval_rougeL': 0.4250567425786824, 'eval_rouge_sum': 1.137423069085263, 'eval_runtime': 31.204, 'eval_samples_per_second': 15.992, 'eval_steps_per_second': 1.026, 'epoch': 8.0}
2025-10-14 15:56:56 | {'loss': 1.1533, 'grad_norm': 3.952631950378418, 'learning_rate': 1.147705209226851e-06, 'epoch': 8.09}
2025-10-14 15:57:02 | {'loss': 1.1439, 'grad_norm': 4.566300868988037, 'learning_rate': 1.0707296754088198e-06, 'epoch': 8.22}
2025-10-14 15:57:08 | {'loss': 1.1353, 'grad_norm': 4.166964530944824, 'learning_rate': 9.93754141590788e-07, 'epoch': 8.34}
2025-10-14 15:57:14 | {'loss': 1.158, 'grad_norm': 3.749520778656006, 'learning_rate': 9.167786077727564e-07, 'epoch': 8.47}
2025-10-14 15:57:20 | {'loss': 1.1305, 'grad_norm': 3.4192757606506348, 'learning_rate': 8.398030739547248e-07, 'epoch': 8.6}
2025-10-14 15:57:27 | {'loss': 1.1583, 'grad_norm': 3.665353298187256, 'learning_rate': 7.628275401366931e-07, 'epoch': 8.73}
2025-10-14 15:57:33 | {'loss': 1.1659, 'grad_norm': 3.8292431831359863, 'learning_rate': 6.858520063186616e-07, 'epoch': 8.86}
2025-10-14 15:57:38 | {'loss': 1.1353, 'grad_norm': 4.390174388885498, 'learning_rate': 6.088764725006299e-07, 'epoch': 8.99}
2025-10-14 15:58:10 | {'eval_loss': 1.3151743412017822, 'eval_rouge1': 0.42933749071184124, 'eval_rouge2': 0.27905534599883053, 'eval_rougeL': 0.42271225427171183, 'eval_rouge_sum': 1.1311050909823837, 'eval_runtime': 31.3744, 'eval_samples_per_second': 15.905, 'eval_steps_per_second': 1.02, 'epoch': 9.0}
2025-10-14 15:58:17 | {'loss': 1.1337, 'grad_norm': 4.772792816162109, 'learning_rate': 5.319009386825984e-07, 'epoch': 9.11}
2025-10-14 15:58:23 | {'loss': 1.1502, 'grad_norm': 3.6447670459747314, 'learning_rate': 4.549254048645668e-07, 'epoch': 9.24}
2025-10-14 15:58:30 | {'loss': 1.1516, 'grad_norm': 3.9900753498077393, 'learning_rate': 3.7794987104653517e-07, 'epoch': 9.37}
2025-10-14 15:58:36 | {'loss': 1.1446, 'grad_norm': 3.8891103267669678, 'learning_rate': 3.009743372285036e-07, 'epoch': 9.5}
2025-10-14 15:58:42 | {'loss': 1.1258, 'grad_norm': 4.096944332122803, 'learning_rate': 2.2399880341047197e-07, 'epoch': 9.63}
2025-10-14 15:58:48 | {'loss': 1.1146, 'grad_norm': 3.7545125484466553, 'learning_rate': 1.4702326959244036e-07, 'epoch': 9.76}
2025-10-14 15:58:54 | {'loss': 1.1433, 'grad_norm': 3.9387872219085693, 'learning_rate': 7.004773577440876e-08, 'epoch': 9.88}
2025-10-14 15:59:30 | {'eval_loss': 1.316236138343811, 'eval_rouge1': 0.431949784843235, 'eval_rouge2': 0.2772247778686352, 'eval_rougeL': 0.42322653942136007, 'eval_rouge_sum': 1.1324011021332303, 'eval_runtime': 31.2289, 'eval_samples_per_second': 15.979, 'eval_steps_per_second': 1.025, 'epoch': 10.0}
2025-10-14 15:59:32 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 15:59:32 | {'train_runtime': 791.4866, 'train_samples_per_second': 157.387, 'train_steps_per_second': 9.842, 'train_loss': 1.3009794712678764, 'epoch': 10.0}
2025-10-14 15:59:32 | 최종 모델 저장 중...
2025-10-14 15:59:32 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 15:59:32 | 최종 평가 중...
2025-10-14 16:00:04 | 최종 평가 결과:
2025-10-14 16:00:04 | eval_rouge1: 0.4342
2025-10-14 16:00:04 | eval_rouge2: 0.2781
2025-10-14 16:00:04 | eval_rougeL: 0.4251
2025-10-14 16:00:04 | eval_rouge_sum: 1.1374
2025-10-14 16:00:04 | ============================================================
2025-10-14 16:00:04 | ✅ 학습 완료!
2025-10-14 16:00:04 | ============================================================
2025-10-14 16:00:04 | 모델 평가 중...
2025-10-14 16:00:34 | → 메트릭 'eval_rougeL' 사용: 0.4251
2025-10-14 16:00:34 | Trial 0 완료
2025-10-14 16:00:34 | - ROUGE-L F1: 0.4251
2025-10-14 16:00:34 | [I 2025-10-14 16:00:34,856] Trial 0 finished with value: 0.4250567425786824 and parameters: {'learning_rate': 5.611516415334504e-06, 'num_epochs': 10, 'warmup_ratio': 0.146398788362281, 'weight_decay': 0.05986584841970366, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.7486639612006325}. Best is trial 0 with value: 0.4250567425786824.
2025-10-14 16:00:34 | 💾 Trial 0 체크포인트 저장
2025-10-14 16:00:34 | ============================================================
2025-10-14 16:00:34 | Trial 1 시작
2025-10-14 16:00:34 | 파라미터: {'learning_rate': 2.6587543983272713e-06, 'num_epochs': 4, 'warmup_ratio': 0.03668090197068676, 'weight_decay': 0.030424224295953775, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.6777639420895203}
2025-10-14 16:00:34 | ============================================================
2025-10-14 16:00:34 | 모델 타입: encoder_decoder
2025-10-14 16:00:34 | ============================================================
2025-10-14 16:00:34 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:00:34 | ============================================================
2025-10-14 16:00:34 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:00:35 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:00:36 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:00:37 | → 디바이스: cuda
2025-10-14 16:00:37 | → 전체 파라미터: 123,859,968
2025-10-14 16:00:37 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:00:37 | ============================================================
2025-10-14 16:00:37 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:00:37 | ============================================================
2025-10-14 16:00:37 | ============================================================
2025-10-14 16:00:37 | 모델 학습 시작
2025-10-14 16:00:37 | ============================================================
2025-10-14 16:00:37 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:00:37 | 학습 진행 중...
2025-10-14 16:00:43 | {'loss': 2.8259, 'grad_norm': 8.20439624786377, 'learning_rate': 5.264333708687998e-07, 'epoch': 0.13}
2025-10-14 16:00:49 | {'loss': 2.1812, 'grad_norm': 5.759346008300781, 'learning_rate': 1.058184250534254e-06, 'epoch': 0.26}
2025-10-14 16:00:55 | {'loss': 1.878, 'grad_norm': 5.230703830718994, 'learning_rate': 1.589935130199708e-06, 'epoch': 0.39}
2025-10-14 16:01:01 | {'loss': 1.7612, 'grad_norm': 4.39968729019165, 'learning_rate': 2.1216860098651628e-06, 'epoch': 0.51}
2025-10-14 16:01:07 | {'loss': 1.7102, 'grad_norm': 4.181340217590332, 'learning_rate': 2.6534368895306168e-06, 'epoch': 0.64}
2025-10-14 16:01:13 | {'loss': 1.6519, 'grad_norm': 3.8312482833862305, 'learning_rate': 2.5581363993080054e-06, 'epoch': 0.77}
2025-10-14 16:01:19 | {'loss': 1.6272, 'grad_norm': 4.140657901763916, 'learning_rate': 2.456502056864302e-06, 'epoch': 0.9}
2025-10-14 16:01:55 | {'eval_loss': 1.5202302932739258, 'eval_rouge1': 0.39520797825386944, 'eval_rouge2': 0.2523416676117717, 'eval_rougeL': 0.3898468592850397, 'eval_rouge_sum': 1.0373965051506808, 'eval_runtime': 31.0925, 'eval_samples_per_second': 16.049, 'eval_steps_per_second': 1.029, 'epoch': 1.0}
2025-10-14 16:01:58 | {'loss': 1.6249, 'grad_norm': 4.09303092956543, 'learning_rate': 2.3548677144205994e-06, 'epoch': 1.03}
2025-10-14 16:02:04 | {'loss': 1.5736, 'grad_norm': 5.902556896209717, 'learning_rate': 2.2532333719768966e-06, 'epoch': 1.16}
2025-10-14 16:02:11 | {'loss': 1.5646, 'grad_norm': 3.9380383491516113, 'learning_rate': 2.1515990295331933e-06, 'epoch': 1.28}
2025-10-14 16:02:17 | {'loss': 1.5473, 'grad_norm': 5.0968732833862305, 'learning_rate': 2.04996468708949e-06, 'epoch': 1.41}
2025-10-14 16:02:23 | {'loss': 1.5277, 'grad_norm': 3.9925198554992676, 'learning_rate': 1.9483303446457872e-06, 'epoch': 1.54}
2025-10-14 16:02:29 | {'loss': 1.5469, 'grad_norm': 3.8960282802581787, 'learning_rate': 1.8466960022020842e-06, 'epoch': 1.67}
2025-10-14 16:02:35 | {'loss': 1.5234, 'grad_norm': 3.7788150310516357, 'learning_rate': 1.7450616597583812e-06, 'epoch': 1.8}
2025-10-14 16:02:41 | {'loss': 1.5228, 'grad_norm': 4.161230564117432, 'learning_rate': 1.643427317314678e-06, 'epoch': 1.93}
2025-10-14 16:03:17 | {'eval_loss': 1.4520442485809326, 'eval_rouge1': 0.40146321412817254, 'eval_rouge2': 0.25424070357094297, 'eval_rougeL': 0.3943837375721268, 'eval_rouge_sum': 1.0500876552712424, 'eval_runtime': 31.9773, 'eval_samples_per_second': 15.605, 'eval_steps_per_second': 1.001, 'epoch': 2.0}
2025-10-14 16:03:21 | {'loss': 1.4955, 'grad_norm': 3.93904447555542, 'learning_rate': 1.5417929748709751e-06, 'epoch': 2.05}
2025-10-14 16:03:27 | {'loss': 1.4867, 'grad_norm': 3.6220145225524902, 'learning_rate': 1.4401586324272719e-06, 'epoch': 2.18}
2025-10-14 16:03:33 | {'loss': 1.4959, 'grad_norm': 3.806608200073242, 'learning_rate': 1.338524289983569e-06, 'epoch': 2.31}
2025-10-14 16:03:39 | {'loss': 1.4824, 'grad_norm': 3.6411659717559814, 'learning_rate': 1.2368899475398658e-06, 'epoch': 2.44}
2025-10-14 16:03:45 | {'loss': 1.4826, 'grad_norm': 4.003719806671143, 'learning_rate': 1.1352556050961628e-06, 'epoch': 2.57}
2025-10-14 16:03:51 | {'loss': 1.4639, 'grad_norm': 4.023873805999756, 'learning_rate': 1.0336212626524597e-06, 'epoch': 2.7}
2025-10-14 16:03:57 | {'loss': 1.4586, 'grad_norm': 4.0932159423828125, 'learning_rate': 9.319869202087569e-07, 'epoch': 2.82}
2025-10-14 16:04:03 | {'loss': 1.4498, 'grad_norm': 3.9311137199401855, 'learning_rate': 8.303525777650538e-07, 'epoch': 2.95}
2025-10-14 16:04:37 | {'eval_loss': 1.4296931028366089, 'eval_rouge1': 0.40703299538101995, 'eval_rouge2': 0.26253722893221704, 'eval_rougeL': 0.40104799544148695, 'eval_rouge_sum': 1.0706182197547238, 'eval_runtime': 32.1931, 'eval_samples_per_second': 15.5, 'eval_steps_per_second': 0.994, 'epoch': 3.0}
2025-10-14 16:04:43 | {'loss': 1.4599, 'grad_norm': 3.5993831157684326, 'learning_rate': 7.287182353213507e-07, 'epoch': 3.08}
2025-10-14 16:04:49 | {'loss': 1.4489, 'grad_norm': 4.047608375549316, 'learning_rate': 6.270838928776477e-07, 'epoch': 3.21}
2025-10-14 16:04:55 | {'loss': 1.4619, 'grad_norm': 3.972123384475708, 'learning_rate': 5.254495504339447e-07, 'epoch': 3.34}
2025-10-14 16:05:01 | {'loss': 1.4412, 'grad_norm': 3.972388982772827, 'learning_rate': 4.238152079902416e-07, 'epoch': 3.47}
2025-10-14 16:05:07 | {'loss': 1.4629, 'grad_norm': 3.816648483276367, 'learning_rate': 3.2218086554653864e-07, 'epoch': 3.59}
2025-10-14 16:05:14 | {'loss': 1.4595, 'grad_norm': 4.017632484436035, 'learning_rate': 2.205465231028356e-07, 'epoch': 3.72}
2025-10-14 16:05:20 | {'loss': 1.4333, 'grad_norm': 4.363552093505859, 'learning_rate': 1.1891218065913255e-07, 'epoch': 3.85}
2025-10-14 16:05:26 | {'loss': 1.4333, 'grad_norm': 4.078686237335205, 'learning_rate': 1.7277838215429515e-08, 'epoch': 3.98}
2025-10-14 16:05:58 | {'eval_loss': 1.4221712350845337, 'eval_rouge1': 0.408654667000489, 'eval_rouge2': 0.26580590690745315, 'eval_rougeL': 0.4017109283862281, 'eval_rouge_sum': 1.0761715022941702, 'eval_runtime': 30.9815, 'eval_samples_per_second': 16.106, 'eval_steps_per_second': 1.033, 'epoch': 4.0}
2025-10-14 16:05:59 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:05:59 | {'train_runtime': 322.2903, 'train_samples_per_second': 154.606, 'train_steps_per_second': 9.668, 'train_loss': 1.5954810241679631, 'epoch': 4.0}
2025-10-14 16:05:59 | 최종 모델 저장 중...
2025-10-14 16:06:00 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:06:00 | 최종 평가 중...
2025-10-14 16:06:33 | 최종 평가 결과:
2025-10-14 16:06:33 | eval_rouge1: 0.4087
2025-10-14 16:06:33 | eval_rouge2: 0.2658
2025-10-14 16:06:33 | eval_rougeL: 0.4017
2025-10-14 16:06:33 | eval_rouge_sum: 1.0762
2025-10-14 16:06:33 | ============================================================
2025-10-14 16:06:33 | ✅ 학습 완료!
2025-10-14 16:06:33 | ============================================================
2025-10-14 16:06:33 | 모델 평가 중...
2025-10-14 16:07:05 | → 메트릭 'eval_rougeL' 사용: 0.4017
2025-10-14 16:07:05 | Trial 1 완료
2025-10-14 16:07:05 | - ROUGE-L F1: 0.4017
2025-10-14 16:07:05 | [I 2025-10-14 16:07:05,077] Trial 1 finished with value: 0.4017109283862281 and parameters: {'learning_rate': 2.6587543983272713e-06, 'num_epochs': 4, 'warmup_ratio': 0.03668090197068676, 'weight_decay': 0.030424224295953775, 'scheduler_type': 'polynomial', 'num_beams': 8, 'length_penalty': 1.6777639420895203}. Best is trial 0 with value: 0.4250567425786824.
2025-10-14 16:07:05 | 💾 Trial 1 체크포인트 저장
2025-10-14 16:07:05 | ============================================================
2025-10-14 16:07:05 | Trial 2 시작
2025-10-14 16:07:05 | 파라미터: {'learning_rate': 2.5081156860452325e-06, 'num_epochs': 7, 'warmup_ratio': 0.1184829137724085, 'weight_decay': 0.0046450412719997725, 'scheduler_type': 'polynomial', 'num_beams': 2, 'length_penalty': 1.5263495397682354}
2025-10-14 16:07:05 | ============================================================
2025-10-14 16:07:05 | 모델 타입: encoder_decoder
2025-10-14 16:07:05 | ============================================================
2025-10-14 16:07:05 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:07:05 | ============================================================
2025-10-14 16:07:05 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:07:05 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:07:05 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:07:06 | → 디바이스: cuda
2025-10-14 16:07:06 | → 전체 파라미터: 123,859,968
2025-10-14 16:07:06 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:07:06 | ============================================================
2025-10-14 16:07:06 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:07:06 | ============================================================
2025-10-14 16:07:07 | ============================================================
2025-10-14 16:07:07 | 모델 학습 시작
2025-10-14 16:07:07 | ============================================================
2025-10-14 16:07:07 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:07:07 | 학습 진행 중...
2025-10-14 16:07:13 | {'loss': 2.8373, 'grad_norm': 8.38602066040039, 'learning_rate': 4.966069058369561e-07, 'epoch': 0.13}
2025-10-14 16:07:19 | {'loss': 2.2017, 'grad_norm': 5.849821090698242, 'learning_rate': 9.982300430460026e-07, 'epoch': 0.26}
2025-10-14 16:07:25 | {'loss': 1.8903, 'grad_norm': 5.264901161193848, 'learning_rate': 1.499853180255049e-06, 'epoch': 0.39}
2025-10-14 16:07:31 | {'loss': 1.7698, 'grad_norm': 4.407575607299805, 'learning_rate': 2.0014763174640955e-06, 'epoch': 0.51}
2025-10-14 16:07:37 | {'loss': 1.7174, 'grad_norm': 4.203529357910156, 'learning_rate': 2.503099454673142e-06, 'epoch': 0.64}
2025-10-14 16:07:43 | {'loss': 1.6582, 'grad_norm': 3.841837167739868, 'learning_rate': 2.4579837553126505e-06, 'epoch': 0.77}
2025-10-14 16:07:49 | {'loss': 1.6329, 'grad_norm': 4.1488728523254395, 'learning_rate': 2.407345441441356e-06, 'epoch': 0.9}
2025-10-14 16:08:20 | {'eval_loss': 1.5246272087097168, 'eval_rouge1': 0.42402510041164654, 'eval_rouge2': 0.26304997621376514, 'eval_rougeL': 0.41602956773207095, 'eval_rouge_sum': 1.1031046443574826, 'eval_runtime': 26.7269, 'eval_samples_per_second': 18.67, 'eval_steps_per_second': 1.197, 'epoch': 1.0}
2025-10-14 16:08:23 | {'loss': 1.6301, 'grad_norm': 4.101603984832764, 'learning_rate': 2.3567071275700607e-06, 'epoch': 1.03}
2025-10-14 16:08:30 | {'loss': 1.5786, 'grad_norm': 6.873513698577881, 'learning_rate': 2.3060688136987663e-06, 'epoch': 1.16}
2025-10-14 16:08:36 | {'loss': 1.5687, 'grad_norm': 3.947611093521118, 'learning_rate': 2.2554304998274714e-06, 'epoch': 1.28}
2025-10-14 16:08:41 | {'loss': 1.5508, 'grad_norm': 5.116032123565674, 'learning_rate': 2.2047921859561765e-06, 'epoch': 1.41}
2025-10-14 16:08:47 | {'loss': 1.5301, 'grad_norm': 3.987952709197998, 'learning_rate': 2.1541538720848816e-06, 'epoch': 1.54}
2025-10-14 16:08:53 | {'loss': 1.5485, 'grad_norm': 3.901937484741211, 'learning_rate': 2.1035155582135867e-06, 'epoch': 1.67}
2025-10-14 16:08:59 | {'loss': 1.524, 'grad_norm': 3.7764084339141846, 'learning_rate': 2.0528772443422923e-06, 'epoch': 1.8}
2025-10-14 16:09:05 | {'loss': 1.5225, 'grad_norm': 4.147988319396973, 'learning_rate': 2.002238930470997e-06, 'epoch': 1.93}
2025-10-14 16:09:35 | {'eval_loss': 1.4500839710235596, 'eval_rouge1': 0.42799600031245993, 'eval_rouge2': 0.2689020737237604, 'eval_rougeL': 0.41890434853838443, 'eval_rouge_sum': 1.1158024225746046, 'eval_runtime': 25.7653, 'eval_samples_per_second': 19.367, 'eval_steps_per_second': 1.242, 'epoch': 2.0}
2025-10-14 16:09:39 | {'loss': 1.4934, 'grad_norm': 3.9280641078948975, 'learning_rate': 1.9516006165997025e-06, 'epoch': 2.05}
2025-10-14 16:09:45 | {'loss': 1.4828, 'grad_norm': 3.622392177581787, 'learning_rate': 1.9009623027284078e-06, 'epoch': 2.18}
2025-10-14 16:09:51 | {'loss': 1.4913, 'grad_norm': 3.810391664505005, 'learning_rate': 1.8503239888571127e-06, 'epoch': 2.31}
2025-10-14 16:09:57 | {'loss': 1.4768, 'grad_norm': 3.6679720878601074, 'learning_rate': 1.799685674985818e-06, 'epoch': 2.44}
2025-10-14 16:10:03 | {'loss': 1.4761, 'grad_norm': 4.004766464233398, 'learning_rate': 1.7490473611145232e-06, 'epoch': 2.57}
2025-10-14 16:10:09 | {'loss': 1.4564, 'grad_norm': 4.025050163269043, 'learning_rate': 1.6984090472432283e-06, 'epoch': 2.7}
2025-10-14 16:10:15 | {'loss': 1.4497, 'grad_norm': 4.073252201080322, 'learning_rate': 1.6477707333719334e-06, 'epoch': 2.82}
2025-10-14 16:10:21 | {'loss': 1.4398, 'grad_norm': 3.9118669033050537, 'learning_rate': 1.5971324195006387e-06, 'epoch': 2.95}
2025-10-14 16:10:50 | {'eval_loss': 1.4217565059661865, 'eval_rouge1': 0.440262281103278, 'eval_rouge2': 0.27744592904587817, 'eval_rougeL': 0.4302388415384026, 'eval_rouge_sum': 1.1479470516875587, 'eval_runtime': 26.333, 'eval_samples_per_second': 18.95, 'eval_steps_per_second': 1.215, 'epoch': 3.0}
2025-10-14 16:10:56 | {'loss': 1.4452, 'grad_norm': 3.6083381175994873, 'learning_rate': 1.546494105629344e-06, 'epoch': 3.08}
2025-10-14 16:11:02 | {'loss': 1.4309, 'grad_norm': 4.0323991775512695, 'learning_rate': 1.495855791758049e-06, 'epoch': 3.21}
2025-10-14 16:11:08 | {'loss': 1.4428, 'grad_norm': 3.916313648223877, 'learning_rate': 1.4452174778867543e-06, 'epoch': 3.34}
2025-10-14 16:11:14 | {'loss': 1.4211, 'grad_norm': 3.9471921920776367, 'learning_rate': 1.3945791640154594e-06, 'epoch': 3.47}
2025-10-14 16:11:20 | {'loss': 1.4419, 'grad_norm': 3.772691249847412, 'learning_rate': 1.3439408501441647e-06, 'epoch': 3.59}
2025-10-14 16:11:26 | {'loss': 1.4365, 'grad_norm': 4.038064956665039, 'learning_rate': 1.2933025362728696e-06, 'epoch': 3.72}
2025-10-14 16:11:32 | {'loss': 1.4097, 'grad_norm': 4.309521675109863, 'learning_rate': 1.242664222401575e-06, 'epoch': 3.85}
2025-10-14 16:11:38 | {'loss': 1.408, 'grad_norm': 4.029296398162842, 'learning_rate': 1.19202590853028e-06, 'epoch': 3.98}
2025-10-14 16:12:05 | {'eval_loss': 1.4023208618164062, 'eval_rouge1': 0.44251662828618193, 'eval_rouge2': 0.2762846914059556, 'eval_rougeL': 0.43085166901315824, 'eval_rouge_sum': 1.1496529887052958, 'eval_runtime': 26.0101, 'eval_samples_per_second': 19.185, 'eval_steps_per_second': 1.23, 'epoch': 4.0}
2025-10-14 16:12:12 | {'loss': 1.4015, 'grad_norm': 4.119476318359375, 'learning_rate': 1.1413875946589852e-06, 'epoch': 4.11}
2025-10-14 16:12:18 | {'loss': 1.391, 'grad_norm': 4.374698162078857, 'learning_rate': 1.0907492807876905e-06, 'epoch': 4.24}
2025-10-14 16:12:24 | {'loss': 1.408, 'grad_norm': 4.967309474945068, 'learning_rate': 1.0401109669163956e-06, 'epoch': 4.36}
2025-10-14 16:12:30 | {'loss': 1.3919, 'grad_norm': 3.5324645042419434, 'learning_rate': 9.89472653045101e-07, 'epoch': 4.49}
2025-10-14 16:12:36 | {'loss': 1.4012, 'grad_norm': 3.7540087699890137, 'learning_rate': 9.38834339173806e-07, 'epoch': 4.62}
2025-10-14 16:12:42 | {'loss': 1.4031, 'grad_norm': 4.304897785186768, 'learning_rate': 8.881960253025112e-07, 'epoch': 4.75}
2025-10-14 16:12:48 | {'loss': 1.3933, 'grad_norm': 4.093616008758545, 'learning_rate': 8.375577114312164e-07, 'epoch': 4.88}
2025-10-14 16:13:19 | {'eval_loss': 1.3913204669952393, 'eval_rouge1': 0.4380003679269704, 'eval_rouge2': 0.2737917520170544, 'eval_rougeL': 0.4282324787131924, 'eval_rouge_sum': 1.1400245986572173, 'eval_runtime': 25.2933, 'eval_samples_per_second': 19.729, 'eval_steps_per_second': 1.265, 'epoch': 5.0}
2025-10-14 16:13:21 | {'loss': 1.3973, 'grad_norm': 4.0188727378845215, 'learning_rate': 7.869193975599215e-07, 'epoch': 5.01}
2025-10-14 16:13:27 | {'loss': 1.365, 'grad_norm': 3.7897684574127197, 'learning_rate': 7.362810836886266e-07, 'epoch': 5.13}
2025-10-14 16:13:33 | {'loss': 1.3709, 'grad_norm': 4.330937385559082, 'learning_rate': 6.856427698173319e-07, 'epoch': 5.26}
2025-10-14 16:13:39 | {'loss': 1.3765, 'grad_norm': 3.9970006942749023, 'learning_rate': 6.350044559460371e-07, 'epoch': 5.39}
2025-10-14 16:13:45 | {'loss': 1.3829, 'grad_norm': 3.8732950687408447, 'learning_rate': 5.843661420747423e-07, 'epoch': 5.52}
2025-10-14 16:13:51 | {'loss': 1.3984, 'grad_norm': 3.9058897495269775, 'learning_rate': 5.337278282034474e-07, 'epoch': 5.65}
2025-10-14 16:13:57 | {'loss': 1.3928, 'grad_norm': 4.401300430297852, 'learning_rate': 4.830895143321526e-07, 'epoch': 5.78}
2025-10-14 16:14:04 | {'loss': 1.3848, 'grad_norm': 3.8509583473205566, 'learning_rate': 4.324512004608578e-07, 'epoch': 5.91}
2025-10-14 16:14:35 | {'eval_loss': 1.3846228122711182, 'eval_rouge1': 0.4310959610358044, 'eval_rouge2': 0.2715809247057502, 'eval_rougeL': 0.42066895583224917, 'eval_rouge_sum': 1.1233458415738038, 'eval_runtime': 26.5773, 'eval_samples_per_second': 18.775, 'eval_steps_per_second': 1.204, 'epoch': 6.0}
2025-10-14 16:14:38 | {'loss': 1.373, 'grad_norm': 3.937800168991089, 'learning_rate': 3.818128865895629e-07, 'epoch': 6.03}
2025-10-14 16:14:44 | {'loss': 1.3595, 'grad_norm': 3.5264170169830322, 'learning_rate': 3.311745727182681e-07, 'epoch': 6.16}
2025-10-14 16:14:50 | {'loss': 1.3893, 'grad_norm': 4.133584022521973, 'learning_rate': 2.805362588469733e-07, 'epoch': 6.29}
2025-10-14 16:14:56 | {'loss': 1.3788, 'grad_norm': 4.095803737640381, 'learning_rate': 2.298979449756785e-07, 'epoch': 6.42}
2025-10-14 16:15:02 | {'loss': 1.3786, 'grad_norm': 3.4283089637756348, 'learning_rate': 1.792596311043837e-07, 'epoch': 6.55}
2025-10-14 16:15:08 | {'loss': 1.3681, 'grad_norm': 4.019321918487549, 'learning_rate': 1.2862131723308884e-07, 'epoch': 6.68}
2025-10-14 16:15:14 | {'loss': 1.3632, 'grad_norm': 4.06065034866333, 'learning_rate': 7.798300336179403e-08, 'epoch': 6.8}
2025-10-14 16:15:20 | {'loss': 1.3624, 'grad_norm': 3.869645833969116, 'learning_rate': 2.7344689490499205e-08, 'epoch': 6.93}
2025-10-14 16:15:48 | {'eval_loss': 1.3832988739013672, 'eval_rouge1': 0.43494196248280276, 'eval_rouge2': 0.2761645363700868, 'eval_rougeL': 0.42487551071064855, 'eval_rouge_sum': 1.1359820095635382, 'eval_runtime': 24.0846, 'eval_samples_per_second': 20.719, 'eval_steps_per_second': 1.329, 'epoch': 7.0}
2025-10-14 16:15:49 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:15:49 | {'train_runtime': 522.2425, 'train_samples_per_second': 166.97, 'train_steps_per_second': 10.442, 'train_loss': 1.5023440709578406, 'epoch': 7.0}
2025-10-14 16:15:49 | 최종 모델 저장 중...
2025-10-14 16:15:50 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:15:50 | 최종 평가 중...
2025-10-14 16:16:15 | 최종 평가 결과:
2025-10-14 16:16:15 | eval_rouge1: 0.4425
2025-10-14 16:16:15 | eval_rouge2: 0.2763
2025-10-14 16:16:15 | eval_rougeL: 0.4309
2025-10-14 16:16:15 | eval_rouge_sum: 1.1497
2025-10-14 16:16:15 | ============================================================
2025-10-14 16:16:15 | ✅ 학습 완료!
2025-10-14 16:16:15 | ============================================================
2025-10-14 16:16:15 | 모델 평가 중...
2025-10-14 16:16:40 | → 메트릭 'eval_rougeL' 사용: 0.4309
2025-10-14 16:16:40 | Trial 2 완료
2025-10-14 16:16:40 | - ROUGE-L F1: 0.4309
2025-10-14 16:16:40 | [I 2025-10-14 16:16:40,682] Trial 2 finished with value: 0.43085166901315824 and parameters: {'learning_rate': 2.5081156860452325e-06, 'num_epochs': 7, 'warmup_ratio': 0.1184829137724085, 'weight_decay': 0.0046450412719997725, 'scheduler_type': 'polynomial', 'num_beams': 2, 'length_penalty': 1.5263495397682354}. Best is trial 2 with value: 0.43085166901315824.
2025-10-14 16:16:40 | 💾 Trial 2 체크포인트 저장
2025-10-14 16:16:40 | ============================================================
2025-10-14 16:16:40 | Trial 3 시작
2025-10-14 16:16:40 | 파라미터: {'learning_rate': 7.5911048052827045e-06, 'num_epochs': 3, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'scheduler_type': 'linear', 'num_beams': 8, 'length_penalty': 1.6626992350416718}
2025-10-14 16:16:40 | ============================================================
2025-10-14 16:16:40 | 모델 타입: encoder_decoder
2025-10-14 16:16:40 | ============================================================
2025-10-14 16:16:40 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:16:40 | ============================================================
2025-10-14 16:16:40 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:16:41 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:16:41 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:16:42 | → 디바이스: cuda
2025-10-14 16:16:42 | → 전체 파라미터: 123,859,968
2025-10-14 16:16:42 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:16:42 | ============================================================
2025-10-14 16:16:42 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:16:42 | ============================================================
2025-10-14 16:16:42 | ============================================================
2025-10-14 16:16:42 | 모델 학습 시작
2025-10-14 16:16:42 | ============================================================
2025-10-14 16:16:42 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:16:42 | 학습 진행 중...
2025-10-14 16:16:49 | {'loss': 2.5857, 'grad_norm': 5.644268989562988, 'learning_rate': 1.5030387514459755e-06, 'epoch': 0.13}
2025-10-14 16:16:55 | {'loss': 1.9163, 'grad_norm': 5.3105082511901855, 'learning_rate': 3.0212597125025164e-06, 'epoch': 0.26}
2025-10-14 16:17:01 | {'loss': 1.7174, 'grad_norm': 4.7834858894348145, 'learning_rate': 4.539480673559057e-06, 'epoch': 0.39}
2025-10-14 16:17:07 | {'loss': 1.6388, 'grad_norm': 4.167281627655029, 'learning_rate': 6.057701634615598e-06, 'epoch': 0.51}
2025-10-14 16:17:13 | {'loss': 1.5999, 'grad_norm': 3.9362704753875732, 'learning_rate': 7.575922595672139e-06, 'epoch': 0.64}
2025-10-14 16:17:19 | {'loss': 1.5529, 'grad_norm': 3.7926454544067383, 'learning_rate': 7.182003348710583e-06, 'epoch': 0.77}
2025-10-14 16:17:25 | {'loss': 1.5329, 'grad_norm': 3.995436906814575, 'learning_rate': 6.7687695541932885e-06, 'epoch': 0.9}
2025-10-14 16:18:01 | {'eval_loss': 1.4345133304595947, 'eval_rouge1': 0.4003375809928773, 'eval_rouge2': 0.2580273636903251, 'eval_rougeL': 0.39533775549865746, 'eval_rouge_sum': 1.05370270018186, 'eval_runtime': 31.5483, 'eval_samples_per_second': 15.817, 'eval_steps_per_second': 1.014, 'epoch': 1.0}
2025-10-14 16:18:04 | {'loss': 1.5226, 'grad_norm': 3.8712244033813477, 'learning_rate': 6.355535759675993e-06, 'epoch': 1.03}
2025-10-14 16:18:10 | {'loss': 1.4519, 'grad_norm': 4.295738697052002, 'learning_rate': 5.9423019651586985e-06, 'epoch': 1.16}
2025-10-14 16:18:16 | {'loss': 1.4469, 'grad_norm': 3.720158100128174, 'learning_rate': 5.5290681706414035e-06, 'epoch': 1.28}
2025-10-14 16:18:22 | {'loss': 1.4297, 'grad_norm': 4.302745819091797, 'learning_rate': 5.115834376124109e-06, 'epoch': 1.41}
2025-10-14 16:18:29 | {'loss': 1.4165, 'grad_norm': 3.8192007541656494, 'learning_rate': 4.7026005816068135e-06, 'epoch': 1.54}
2025-10-14 16:18:35 | {'loss': 1.435, 'grad_norm': 3.693082332611084, 'learning_rate': 4.289366787089519e-06, 'epoch': 1.67}
2025-10-14 16:18:41 | {'loss': 1.4129, 'grad_norm': 3.5502521991729736, 'learning_rate': 3.876132992572224e-06, 'epoch': 1.8}
2025-10-14 16:18:47 | {'loss': 1.4126, 'grad_norm': 4.000442028045654, 'learning_rate': 3.4628991980549303e-06, 'epoch': 1.93}
2025-10-14 16:19:24 | {'eval_loss': 1.3771882057189941, 'eval_rouge1': 0.4113050482555612, 'eval_rouge2': 0.2676345691502941, 'eval_rougeL': 0.40498626373749885, 'eval_rouge_sum': 1.083925881143354, 'eval_runtime': 32.7611, 'eval_samples_per_second': 15.231, 'eval_steps_per_second': 0.977, 'epoch': 2.0}
2025-10-14 16:19:28 | {'loss': 1.3752, 'grad_norm': 3.594059944152832, 'learning_rate': 3.0496654035376353e-06, 'epoch': 2.05}
2025-10-14 16:19:34 | {'loss': 1.3527, 'grad_norm': 3.49635910987854, 'learning_rate': 2.6364316090203407e-06, 'epoch': 2.18}
2025-10-14 16:19:40 | {'loss': 1.3636, 'grad_norm': 3.688955068588257, 'learning_rate': 2.2231978145030457e-06, 'epoch': 2.31}
2025-10-14 16:19:46 | {'loss': 1.354, 'grad_norm': 3.5809926986694336, 'learning_rate': 1.8099640199857512e-06, 'epoch': 2.44}
2025-10-14 16:19:52 | {'loss': 1.3534, 'grad_norm': 3.7114202976226807, 'learning_rate': 1.3967302254684564e-06, 'epoch': 2.57}
2025-10-14 16:19:58 | {'loss': 1.3382, 'grad_norm': 3.902850389480591, 'learning_rate': 9.834964309511616e-07, 'epoch': 2.7}
2025-10-14 16:20:05 | {'loss': 1.3332, 'grad_norm': 3.894260883331299, 'learning_rate': 5.702626364338667e-07, 'epoch': 2.82}
2025-10-14 16:20:11 | {'loss': 1.3271, 'grad_norm': 3.800337314605713, 'learning_rate': 1.5702884191657202e-07, 'epoch': 2.95}
2025-10-14 16:20:45 | {'eval_loss': 1.3638324737548828, 'eval_rouge1': 0.4201694769469638, 'eval_rouge2': 0.2727640412170463, 'eval_rougeL': 0.4126109112793007, 'eval_rouge_sum': 1.1055444294433108, 'eval_runtime': 31.9494, 'eval_samples_per_second': 15.618, 'eval_steps_per_second': 1.002, 'epoch': 3.0}
2025-10-14 16:20:47 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:20:47 | {'train_runtime': 244.1249, 'train_samples_per_second': 153.081, 'train_steps_per_second': 9.573, 'train_loss': 1.5135836064636987, 'epoch': 3.0}
2025-10-14 16:20:47 | 최종 모델 저장 중...
2025-10-14 16:20:48 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:20:48 | 최종 평가 중...
2025-10-14 16:21:21 | 최종 평가 결과:
2025-10-14 16:21:21 | eval_rouge1: 0.4202
2025-10-14 16:21:21 | eval_rouge2: 0.2728
2025-10-14 16:21:21 | eval_rougeL: 0.4126
2025-10-14 16:21:21 | eval_rouge_sum: 1.1055
2025-10-14 16:21:21 | ============================================================
2025-10-14 16:21:21 | ✅ 학습 완료!
2025-10-14 16:21:21 | ============================================================
2025-10-14 16:21:21 | 모델 평가 중...
2025-10-14 16:21:51 | → 메트릭 'eval_rougeL' 사용: 0.4126
2025-10-14 16:21:51 | Trial 3 완료
2025-10-14 16:21:51 | - ROUGE-L F1: 0.4126
2025-10-14 16:21:51 | [I 2025-10-14 16:21:51,267] Trial 3 finished with value: 0.4126109112793007 and parameters: {'learning_rate': 7.5911048052827045e-06, 'num_epochs': 3, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'scheduler_type': 'linear', 'num_beams': 8, 'length_penalty': 1.6626992350416718}. Best is trial 2 with value: 0.43085166901315824.
2025-10-14 16:21:51 | 💾 Trial 3 체크포인트 저장
2025-10-14 16:21:51 | ============================================================
2025-10-14 16:21:51 | Trial 4 시작
2025-10-14 16:21:51 | 파라미터: {'learning_rate': 7.568292060167621e-05, 'num_epochs': 10, 'warmup_ratio': 0.11957999576221703, 'weight_decay': 0.09218742350231168, 'scheduler_type': 'polynomial', 'num_beams': 6, 'length_penalty': 0.9214017645310711}
2025-10-14 16:21:51 | ============================================================
2025-10-14 16:21:51 | 모델 타입: encoder_decoder
2025-10-14 16:21:51 | ============================================================
2025-10-14 16:21:51 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:21:51 | ============================================================
2025-10-14 16:21:51 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:21:51 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:21:52 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:21:53 | → 디바이스: cuda
2025-10-14 16:21:53 | → 전체 파라미터: 123,859,968
2025-10-14 16:21:53 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:21:53 | ============================================================
2025-10-14 16:21:53 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:21:53 | ============================================================
2025-10-14 16:21:53 | ============================================================
2025-10-14 16:21:53 | 모델 학습 시작
2025-10-14 16:21:53 | ============================================================
2025-10-14 16:21:53 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:21:53 | 학습 진행 중...
2025-10-14 16:21:59 | {'loss': 2.0875, 'grad_norm': 6.194177627563477, 'learning_rate': 1.498521827913189e-05, 'epoch': 0.13}
2025-10-14 16:22:05 | {'loss': 1.6456, 'grad_norm': 3.929314613342285, 'learning_rate': 3.0121802399467134e-05, 'epoch': 0.26}
2025-10-14 16:22:11 | {'loss': 1.5372, 'grad_norm': 4.030151844024658, 'learning_rate': 4.525838651980238e-05, 'epoch': 0.39}
2025-10-14 16:22:17 | {'loss': 1.4982, 'grad_norm': 4.034351825714111, 'learning_rate': 6.039497064013762e-05, 'epoch': 0.51}
2025-10-14 16:22:23 | {'loss': 1.5, 'grad_norm': 3.6001806259155273, 'learning_rate': 7.553155476047286e-05, 'epoch': 0.64}
2025-10-14 16:22:30 | {'loss': 1.4686, 'grad_norm': 3.1084537506103516, 'learning_rate': 7.465512785276456e-05, 'epoch': 0.77}
2025-10-14 16:22:36 | {'loss': 1.4604, 'grad_norm': 3.327594757080078, 'learning_rate': 7.36169533589144e-05, 'epoch': 0.9}
2025-10-14 16:23:11 | {'eval_loss': 1.3656458854675293, 'eval_rouge1': 0.3799292583807434, 'eval_rouge2': 0.24149790377539992, 'eval_rougeL': 0.3744330170051582, 'eval_rouge_sum': 0.9958601791613015, 'eval_runtime': 30.2107, 'eval_samples_per_second': 16.517, 'eval_steps_per_second': 1.059, 'epoch': 1.0}
2025-10-14 16:23:14 | {'loss': 1.4, 'grad_norm': 3.1646640300750732, 'learning_rate': 7.257877886506426e-05, 'epoch': 1.03}
2025-10-14 16:23:20 | {'loss': 1.1802, 'grad_norm': 3.5625572204589844, 'learning_rate': 7.154060437121411e-05, 'epoch': 1.16}
2025-10-14 16:23:26 | {'loss': 1.195, 'grad_norm': 3.1262359619140625, 'learning_rate': 7.050242987736395e-05, 'epoch': 1.28}
2025-10-14 16:23:32 | {'loss': 1.1923, 'grad_norm': 3.313553810119629, 'learning_rate': 6.94642553835138e-05, 'epoch': 1.41}
2025-10-14 16:23:38 | {'loss': 1.184, 'grad_norm': 3.0769217014312744, 'learning_rate': 6.842608088966365e-05, 'epoch': 1.54}
2025-10-14 16:23:44 | {'loss': 1.1981, 'grad_norm': 2.9716360569000244, 'learning_rate': 6.738790639581349e-05, 'epoch': 1.67}
2025-10-14 16:23:50 | {'loss': 1.1838, 'grad_norm': 2.8610644340515137, 'learning_rate': 6.634973190196334e-05, 'epoch': 1.8}
2025-10-14 16:23:55 | {'loss': 1.181, 'grad_norm': 3.045593023300171, 'learning_rate': 6.531155740811318e-05, 'epoch': 1.93}
2025-10-14 16:24:27 | {'eval_loss': 1.3063879013061523, 'eval_rouge1': 0.4399651335795734, 'eval_rouge2': 0.28407957067443224, 'eval_rougeL': 0.4337305913153867, 'eval_rouge_sum': 1.1577752955693925, 'eval_runtime': 28.3477, 'eval_samples_per_second': 17.603, 'eval_steps_per_second': 1.129, 'epoch': 2.0}
2025-10-14 16:24:31 | {'loss': 1.0371, 'grad_norm': 3.0531251430511475, 'learning_rate': 6.427338291426303e-05, 'epoch': 2.05}
2025-10-14 16:24:37 | {'loss': 0.859, 'grad_norm': 3.08077335357666, 'learning_rate': 6.323520842041287e-05, 'epoch': 2.18}
2025-10-14 16:24:43 | {'loss': 0.8753, 'grad_norm': 3.0161521434783936, 'learning_rate': 6.219703392656272e-05, 'epoch': 2.31}
2025-10-14 16:24:49 | {'loss': 0.8781, 'grad_norm': 3.057581901550293, 'learning_rate': 6.115885943271256e-05, 'epoch': 2.44}
2025-10-14 16:24:55 | {'loss': 0.8928, 'grad_norm': 3.2809462547302246, 'learning_rate': 6.012068493886241e-05, 'epoch': 2.57}
2025-10-14 16:25:01 | {'loss': 0.8826, 'grad_norm': 3.093010663986206, 'learning_rate': 5.9082510445012255e-05, 'epoch': 2.7}
2025-10-14 16:25:07 | {'loss': 0.8902, 'grad_norm': 3.052671194076538, 'learning_rate': 5.80443359511621e-05, 'epoch': 2.82}
2025-10-14 16:25:13 | {'loss': 0.8955, 'grad_norm': 3.0775647163391113, 'learning_rate': 5.7006161457311946e-05, 'epoch': 2.95}
2025-10-14 16:25:43 | {'eval_loss': 1.3539707660675049, 'eval_rouge1': 0.4374178382879997, 'eval_rouge2': 0.28072981222127813, 'eval_rougeL': 0.42757959952193625, 'eval_rouge_sum': 1.1457272500312141, 'eval_runtime': 28.1061, 'eval_samples_per_second': 17.754, 'eval_steps_per_second': 1.139, 'epoch': 3.0}
2025-10-14 16:25:48 | {'loss': 0.7349, 'grad_norm': 2.7048282623291016, 'learning_rate': 5.596798696346179e-05, 'epoch': 3.08}
2025-10-14 16:25:54 | {'loss': 0.6238, 'grad_norm': 2.8706369400024414, 'learning_rate': 5.492981246961164e-05, 'epoch': 3.21}
2025-10-14 16:26:00 | {'loss': 0.6439, 'grad_norm': 2.9570200443267822, 'learning_rate': 5.389163797576148e-05, 'epoch': 3.34}
2025-10-14 16:26:06 | {'loss': 0.6488, 'grad_norm': 3.2017152309417725, 'learning_rate': 5.2853463481911335e-05, 'epoch': 3.47}
2025-10-14 16:26:13 | {'loss': 0.6704, 'grad_norm': 3.1875860691070557, 'learning_rate': 5.181528898806118e-05, 'epoch': 3.59}
2025-10-14 16:26:19 | {'loss': 0.6675, 'grad_norm': 2.998090982437134, 'learning_rate': 5.0777114494211026e-05, 'epoch': 3.72}
2025-10-14 16:26:25 | {'loss': 0.6659, 'grad_norm': 3.5945186614990234, 'learning_rate': 4.973894000036087e-05, 'epoch': 3.85}
2025-10-14 16:26:31 | {'loss': 0.6672, 'grad_norm': 3.2378392219543457, 'learning_rate': 4.870076550651072e-05, 'epoch': 3.98}
2025-10-14 16:27:03 | {'eval_loss': 1.4408197402954102, 'eval_rouge1': 0.45630215375299027, 'eval_rouge2': 0.2923699431988118, 'eval_rougeL': 0.4483941917397589, 'eval_rouge_sum': 1.197066288691561, 'eval_runtime': 30.9049, 'eval_samples_per_second': 16.146, 'eval_steps_per_second': 1.035, 'epoch': 4.0}
2025-10-14 16:27:10 | {'loss': 0.4917, 'grad_norm': 2.9162380695343018, 'learning_rate': 4.766259101266056e-05, 'epoch': 4.11}
2025-10-14 16:27:16 | {'loss': 0.4628, 'grad_norm': 2.876704454421997, 'learning_rate': 4.662441651881041e-05, 'epoch': 4.24}
2025-10-14 16:27:22 | {'loss': 0.471, 'grad_norm': 2.8008530139923096, 'learning_rate': 4.5586242024960254e-05, 'epoch': 4.36}
2025-10-14 16:27:28 | {'loss': 0.4766, 'grad_norm': 2.553236722946167, 'learning_rate': 4.45480675311101e-05, 'epoch': 4.49}
2025-10-14 16:27:35 | {'loss': 0.4866, 'grad_norm': 2.6349995136260986, 'learning_rate': 4.3509893037259945e-05, 'epoch': 4.62}
2025-10-14 16:27:41 | {'loss': 0.4943, 'grad_norm': 2.8836042881011963, 'learning_rate': 4.24717185434098e-05, 'epoch': 4.75}
2025-10-14 16:27:47 | {'loss': 0.4898, 'grad_norm': 3.003828525543213, 'learning_rate': 4.143354404955964e-05, 'epoch': 4.88}
2025-10-14 16:28:22 | {'eval_loss': 1.517336368560791, 'eval_rouge1': 0.4691873628204494, 'eval_rouge2': 0.30009485217982707, 'eval_rougeL': 0.45980874592447585, 'eval_rouge_sum': 1.2290909609247525, 'eval_runtime': 30.1992, 'eval_samples_per_second': 16.524, 'eval_steps_per_second': 1.06, 'epoch': 5.0}
2025-10-14 16:28:24 | {'loss': 0.4959, 'grad_norm': 2.321499824523926, 'learning_rate': 4.039536955570949e-05, 'epoch': 5.01}
2025-10-14 16:28:31 | {'loss': 0.3369, 'grad_norm': 2.5637035369873047, 'learning_rate': 3.9357195061859334e-05, 'epoch': 5.13}
2025-10-14 16:28:37 | {'loss': 0.341, 'grad_norm': 2.8245012760162354, 'learning_rate': 3.831902056800918e-05, 'epoch': 5.26}
2025-10-14 16:28:43 | {'loss': 0.3492, 'grad_norm': 2.7022621631622314, 'learning_rate': 3.7280846074159025e-05, 'epoch': 5.39}
2025-10-14 16:28:49 | {'loss': 0.3524, 'grad_norm': 2.972410202026367, 'learning_rate': 3.624267158030887e-05, 'epoch': 5.52}
2025-10-14 16:28:55 | {'loss': 0.3644, 'grad_norm': 2.761857748031616, 'learning_rate': 3.5204497086458716e-05, 'epoch': 5.65}
2025-10-14 16:29:02 | {'loss': 0.3648, 'grad_norm': 2.6133217811584473, 'learning_rate': 3.416632259260856e-05, 'epoch': 5.78}
2025-10-14 16:29:08 | {'loss': 0.3571, 'grad_norm': 2.6473548412323, 'learning_rate': 3.312814809875841e-05, 'epoch': 5.91}
2025-10-14 16:29:41 | {'eval_loss': 1.5776052474975586, 'eval_rouge1': 0.44874065126578494, 'eval_rouge2': 0.2923920176169256, 'eval_rougeL': 0.4400833088359777, 'eval_rouge_sum': 1.1812159777186881, 'eval_runtime': 29.2295, 'eval_samples_per_second': 17.072, 'eval_steps_per_second': 1.095, 'epoch': 6.0}
2025-10-14 16:29:44 | {'loss': 0.3365, 'grad_norm': 2.6271748542785645, 'learning_rate': 3.208997360490825e-05, 'epoch': 6.03}
2025-10-14 16:29:50 | {'loss': 0.2483, 'grad_norm': 2.2416749000549316, 'learning_rate': 3.10517991110581e-05, 'epoch': 6.16}
2025-10-14 16:29:57 | {'loss': 0.2573, 'grad_norm': 2.6471164226531982, 'learning_rate': 3.0013624617207947e-05, 'epoch': 6.29}
2025-10-14 16:30:03 | {'loss': 0.2573, 'grad_norm': 2.665529489517212, 'learning_rate': 2.8975450123357793e-05, 'epoch': 6.42}
2025-10-14 16:30:09 | {'loss': 0.2588, 'grad_norm': 2.3339223861694336, 'learning_rate': 2.793727562950764e-05, 'epoch': 6.55}
2025-10-14 16:30:15 | {'loss': 0.2645, 'grad_norm': 2.3830840587615967, 'learning_rate': 2.6899101135657484e-05, 'epoch': 6.68}
2025-10-14 16:30:21 | {'loss': 0.2668, 'grad_norm': 2.349818706512451, 'learning_rate': 2.586092664180733e-05, 'epoch': 6.8}
2025-10-14 16:30:27 | {'loss': 0.2631, 'grad_norm': 2.9895477294921875, 'learning_rate': 2.482275214795718e-05, 'epoch': 6.93}
2025-10-14 16:31:01 | {'eval_loss': 1.6369400024414062, 'eval_rouge1': 0.4570722431988308, 'eval_rouge2': 0.28941552113236035, 'eval_rougeL': 0.4478567228181007, 'eval_rouge_sum': 1.1943444871492919, 'eval_runtime': 30.6191, 'eval_samples_per_second': 16.297, 'eval_steps_per_second': 1.045, 'epoch': 7.0}
2025-10-14 16:31:05 | {'loss': 0.23, 'grad_norm': 1.9735690355300903, 'learning_rate': 2.3784577654107024e-05, 'epoch': 7.06}
2025-10-14 16:31:11 | {'loss': 0.189, 'grad_norm': 2.1960597038269043, 'learning_rate': 2.274640316025687e-05, 'epoch': 7.19}
2025-10-14 16:31:18 | {'loss': 0.1943, 'grad_norm': 1.806089162826538, 'learning_rate': 2.1708228666406715e-05, 'epoch': 7.32}
2025-10-14 16:31:24 | {'loss': 0.1954, 'grad_norm': 2.4200828075408936, 'learning_rate': 2.0670054172556564e-05, 'epoch': 7.45}
2025-10-14 16:31:30 | {'loss': 0.1942, 'grad_norm': 2.006521224975586, 'learning_rate': 1.963187967870641e-05, 'epoch': 7.57}
2025-10-14 16:31:36 | {'loss': 0.1908, 'grad_norm': 2.2609918117523193, 'learning_rate': 1.8593705184856255e-05, 'epoch': 7.7}
2025-10-14 16:31:42 | {'loss': 0.197, 'grad_norm': 2.048027515411377, 'learning_rate': 1.75555306910061e-05, 'epoch': 7.83}
2025-10-14 16:31:48 | {'loss': 0.1946, 'grad_norm': 2.0814127922058105, 'learning_rate': 1.651735619715595e-05, 'epoch': 7.96}
2025-10-14 16:32:20 | {'eval_loss': 1.7024545669555664, 'eval_rouge1': 0.45634857240234505, 'eval_rouge2': 0.29634222080135575, 'eval_rougeL': 0.44766962446252245, 'eval_rouge_sum': 1.2003604176662233, 'eval_runtime': 30.9291, 'eval_samples_per_second': 16.134, 'eval_steps_per_second': 1.035, 'epoch': 8.0}
2025-10-14 16:32:22 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:32:22 | {'train_runtime': 628.8304, 'train_samples_per_second': 198.098, 'train_steps_per_second': 12.388, 'train_loss': 0.6992721266862822, 'epoch': 8.0}
2025-10-14 16:32:22 | 최종 모델 저장 중...
2025-10-14 16:32:23 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:32:23 | 최종 평가 중...
2025-10-14 16:32:53 | 최종 평가 결과:
2025-10-14 16:32:53 | eval_rouge1: 0.4692
2025-10-14 16:32:53 | eval_rouge2: 0.3001
2025-10-14 16:32:53 | eval_rougeL: 0.4598
2025-10-14 16:32:53 | eval_rouge_sum: 1.2291
2025-10-14 16:32:53 | ============================================================
2025-10-14 16:32:53 | ✅ 학습 완료!
2025-10-14 16:32:53 | ============================================================
2025-10-14 16:32:53 | 모델 평가 중...
2025-10-14 16:33:24 | → 메트릭 'eval_rougeL' 사용: 0.4598
2025-10-14 16:33:24 | Trial 4 완료
2025-10-14 16:33:24 | - ROUGE-L F1: 0.4598
2025-10-14 16:33:24 | [I 2025-10-14 16:33:24,230] Trial 4 finished with value: 0.45980874592447585 and parameters: {'learning_rate': 7.568292060167621e-05, 'num_epochs': 10, 'warmup_ratio': 0.11957999576221703, 'weight_decay': 0.09218742350231168, 'scheduler_type': 'polynomial', 'num_beams': 6, 'length_penalty': 0.9214017645310711}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 16:33:24 | 💾 Trial 4 체크포인트 저장
2025-10-14 16:33:24 | ============================================================
2025-10-14 16:33:24 | Trial 5 시작
2025-10-14 16:33:24 | 파라미터: {'learning_rate': 1.2172847081122448e-05, 'num_epochs': 4, 'warmup_ratio': 0.16043939615080793, 'weight_decay': 0.007455064367977083, 'scheduler_type': 'linear', 'num_beams': 2, 'length_penalty': 0.6110669776011355}
2025-10-14 16:33:24 | ============================================================
2025-10-14 16:33:24 | 모델 타입: encoder_decoder
2025-10-14 16:33:24 | ============================================================
2025-10-14 16:33:24 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:33:24 | ============================================================
2025-10-14 16:33:24 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:33:24 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:33:24 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:33:26 | → 디바이스: cuda
2025-10-14 16:33:26 | → 전체 파라미터: 123,859,968
2025-10-14 16:33:26 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:33:26 | ============================================================
2025-10-14 16:33:26 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:33:26 | ============================================================
2025-10-14 16:33:26 | ============================================================
2025-10-14 16:33:26 | 모델 학습 시작
2025-10-14 16:33:26 | ============================================================
2025-10-14 16:33:26 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:33:26 | 학습 진행 중...
2025-10-14 16:33:32 | {'loss': 2.4704, 'grad_norm': 5.331652641296387, 'learning_rate': 2.4102237220622447e-06, 'epoch': 0.13}
2025-10-14 16:33:38 | {'loss': 1.8336, 'grad_norm': 5.339574337005615, 'learning_rate': 4.844793138286734e-06, 'epoch': 0.26}
2025-10-14 16:33:45 | {'loss': 1.6656, 'grad_norm': 4.462967872619629, 'learning_rate': 7.279362554511224e-06, 'epoch': 0.39}
2025-10-14 16:33:51 | {'loss': 1.5932, 'grad_norm': 4.058704376220703, 'learning_rate': 9.713931970735714e-06, 'epoch': 0.51}
2025-10-14 16:33:56 | {'loss': 1.56, 'grad_norm': 3.713923692703247, 'learning_rate': 1.2148501386960202e-05, 'epoch': 0.64}
2025-10-14 16:34:02 | {'loss': 1.5159, 'grad_norm': 3.596620798110962, 'learning_rate': 1.1712177409474466e-05, 'epoch': 0.77}
2025-10-14 16:34:08 | {'loss': 1.4988, 'grad_norm': 3.875896453857422, 'learning_rate': 1.1246854508819937e-05, 'epoch': 0.9}
2025-10-14 16:34:39 | {'eval_loss': 1.4041211605072021, 'eval_rouge1': 0.41060604742658446, 'eval_rouge2': 0.2626633321340679, 'eval_rougeL': 0.40504053003078244, 'eval_rouge_sum': 1.0783099095914348, 'eval_runtime': 25.6828, 'eval_samples_per_second': 19.429, 'eval_steps_per_second': 1.246, 'epoch': 1.0}
2025-10-14 16:34:42 | {'loss': 1.4793, 'grad_norm': 3.633415937423706, 'learning_rate': 1.0781531608165409e-05, 'epoch': 1.03}
2025-10-14 16:34:48 | {'loss': 1.3927, 'grad_norm': 4.086988925933838, 'learning_rate': 1.0316208707510883e-05, 'epoch': 1.16}
2025-10-14 16:34:54 | {'loss': 1.3898, 'grad_norm': 3.6662731170654297, 'learning_rate': 9.850885806856353e-06, 'epoch': 1.28}
2025-10-14 16:35:00 | {'loss': 1.3736, 'grad_norm': 4.264583587646484, 'learning_rate': 9.385562906201827e-06, 'epoch': 1.41}
2025-10-14 16:35:06 | {'loss': 1.3608, 'grad_norm': 3.8379669189453125, 'learning_rate': 8.920240005547298e-06, 'epoch': 1.54}
2025-10-14 16:35:13 | {'loss': 1.3779, 'grad_norm': 3.582576036453247, 'learning_rate': 8.45491710489277e-06, 'epoch': 1.67}
2025-10-14 16:35:19 | {'loss': 1.3564, 'grad_norm': 3.4792985916137695, 'learning_rate': 7.989594204238244e-06, 'epoch': 1.8}
2025-10-14 16:35:25 | {'loss': 1.354, 'grad_norm': 3.92138934135437, 'learning_rate': 7.524271303583715e-06, 'epoch': 1.93}
2025-10-14 16:35:53 | {'eval_loss': 1.3425428867340088, 'eval_rouge1': 0.4448328045154124, 'eval_rouge2': 0.2834325683533387, 'eval_rougeL': 0.43480326292987964, 'eval_rouge_sum': 1.1630686357986308, 'eval_runtime': 25.031, 'eval_samples_per_second': 19.935, 'eval_steps_per_second': 1.278, 'epoch': 2.0}
2025-10-14 16:35:57 | {'loss': 1.3026, 'grad_norm': 3.5141403675079346, 'learning_rate': 7.058948402929187e-06, 'epoch': 2.05}
2025-10-14 16:36:03 | {'loss': 1.2603, 'grad_norm': 3.4365060329437256, 'learning_rate': 6.593625502274658e-06, 'epoch': 2.18}
2025-10-14 16:36:09 | {'loss': 1.2716, 'grad_norm': 3.657961130142212, 'learning_rate': 6.128302601620131e-06, 'epoch': 2.31}
2025-10-14 16:36:15 | {'loss': 1.2624, 'grad_norm': 3.5450122356414795, 'learning_rate': 5.6629797009656035e-06, 'epoch': 2.44}
2025-10-14 16:36:21 | {'loss': 1.2629, 'grad_norm': 3.7211720943450928, 'learning_rate': 5.197656800311076e-06, 'epoch': 2.57}
2025-10-14 16:36:27 | {'loss': 1.2477, 'grad_norm': 3.789825439453125, 'learning_rate': 4.732333899656548e-06, 'epoch': 2.7}
2025-10-14 16:36:33 | {'loss': 1.2408, 'grad_norm': 3.7121453285217285, 'learning_rate': 4.26701099900202e-06, 'epoch': 2.82}
2025-10-14 16:36:39 | {'loss': 1.2355, 'grad_norm': 3.655625343322754, 'learning_rate': 3.8016880983474925e-06, 'epoch': 2.95}
2025-10-14 16:37:08 | {'eval_loss': 1.3249287605285645, 'eval_rouge1': 0.4526482941869375, 'eval_rouge2': 0.28523810545128725, 'eval_rougeL': 0.4414966205082917, 'eval_rouge_sum': 1.1793830201465165, 'eval_runtime': 26.3569, 'eval_samples_per_second': 18.932, 'eval_steps_per_second': 1.214, 'epoch': 3.0}
2025-10-14 16:37:13 | {'loss': 1.2214, 'grad_norm': 3.409034013748169, 'learning_rate': 3.3363651976929642e-06, 'epoch': 3.08}
2025-10-14 16:37:20 | {'loss': 1.1913, 'grad_norm': 3.685657262802124, 'learning_rate': 2.871042297038437e-06, 'epoch': 3.21}
2025-10-14 16:37:26 | {'loss': 1.2068, 'grad_norm': 3.5372962951660156, 'learning_rate': 2.405719396383909e-06, 'epoch': 3.34}
2025-10-14 16:37:32 | {'loss': 1.1866, 'grad_norm': 3.608328342437744, 'learning_rate': 1.9403964957293807e-06, 'epoch': 3.47}
2025-10-14 16:37:38 | {'loss': 1.2071, 'grad_norm': 3.512242317199707, 'learning_rate': 1.4750735950748533e-06, 'epoch': 3.59}
2025-10-14 16:37:44 | {'loss': 1.2059, 'grad_norm': 3.9477317333221436, 'learning_rate': 1.0097506944203254e-06, 'epoch': 3.72}
2025-10-14 16:37:50 | {'loss': 1.1845, 'grad_norm': 4.105535507202148, 'learning_rate': 5.444277937657976e-07, 'epoch': 3.85}
2025-10-14 16:37:55 | {'loss': 1.1801, 'grad_norm': 3.710942506790161, 'learning_rate': 7.910489311126973e-08, 'epoch': 3.98}
2025-10-14 16:38:23 | {'eval_loss': 1.3179537057876587, 'eval_rouge1': 0.4427244318823615, 'eval_rouge2': 0.2849191880115957, 'eval_rougeL': 0.43443554567100623, 'eval_rouge_sum': 1.1620791655649634, 'eval_runtime': 26.7615, 'eval_samples_per_second': 18.646, 'eval_steps_per_second': 1.196, 'epoch': 4.0}
2025-10-14 16:38:25 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:38:25 | {'train_runtime': 298.5587, 'train_samples_per_second': 166.895, 'train_steps_per_second': 10.437, 'train_loss': 1.3825653324690343, 'epoch': 4.0}
2025-10-14 16:38:25 | 최종 모델 저장 중...
2025-10-14 16:38:25 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:38:25 | 최종 평가 중...
2025-10-14 16:38:51 | 최종 평가 결과:
2025-10-14 16:38:51 | eval_rouge1: 0.4526
2025-10-14 16:38:51 | eval_rouge2: 0.2852
2025-10-14 16:38:51 | eval_rougeL: 0.4415
2025-10-14 16:38:51 | eval_rouge_sum: 1.1794
2025-10-14 16:38:51 | ============================================================
2025-10-14 16:38:51 | ✅ 학습 완료!
2025-10-14 16:38:51 | ============================================================
2025-10-14 16:38:51 | 모델 평가 중...
2025-10-14 16:39:17 | → 메트릭 'eval_rougeL' 사용: 0.4415
2025-10-14 16:39:17 | Trial 5 완료
2025-10-14 16:39:17 | - ROUGE-L F1: 0.4415
2025-10-14 16:39:17 | [I 2025-10-14 16:39:17,761] Trial 5 finished with value: 0.4414966205082917 and parameters: {'learning_rate': 1.2172847081122448e-05, 'num_epochs': 4, 'warmup_ratio': 0.16043939615080793, 'weight_decay': 0.007455064367977083, 'scheduler_type': 'linear', 'num_beams': 2, 'length_penalty': 0.6110669776011355}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 16:39:17 | 💾 Trial 5 체크포인트 저장
2025-10-14 16:39:17 | ============================================================
2025-10-14 16:39:17 | Trial 6 시작
2025-10-14 16:39:17 | 파라미터: {'learning_rate': 5.211124595788262e-06, 'num_epochs': 3, 'warmup_ratio': 0.17262068517511872, 'weight_decay': 0.062329812682755795, 'scheduler_type': 'linear', 'num_beams': 6, 'length_penalty': 0.6793913689074526}
2025-10-14 16:39:17 | ============================================================
2025-10-14 16:39:17 | 모델 타입: encoder_decoder
2025-10-14 16:39:17 | ============================================================
2025-10-14 16:39:17 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:39:17 | ============================================================
2025-10-14 16:39:17 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:39:18 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:39:18 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:39:19 | → 디바이스: cuda
2025-10-14 16:39:19 | → 전체 파라미터: 123,859,968
2025-10-14 16:39:19 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:39:19 | ============================================================
2025-10-14 16:39:19 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:39:19 | ============================================================
2025-10-14 16:39:19 | ============================================================
2025-10-14 16:39:19 | 모델 학습 시작
2025-10-14 16:39:19 | ============================================================
2025-10-14 16:39:19 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:39:19 | 학습 진행 중...
2025-10-14 16:39:26 | {'loss': 2.6771, 'grad_norm': 6.325567245483398, 'learning_rate': 1.0318026699660759e-06, 'epoch': 0.13}
2025-10-14 16:39:32 | {'loss': 1.997, 'grad_norm': 5.474303245544434, 'learning_rate': 2.074027589123728e-06, 'epoch': 0.26}
2025-10-14 16:39:38 | {'loss': 1.7649, 'grad_norm': 4.909884929656982, 'learning_rate': 3.1162525082813803e-06, 'epoch': 0.39}
2025-10-14 16:39:44 | {'loss': 1.6783, 'grad_norm': 4.249346733093262, 'learning_rate': 4.158477427439033e-06, 'epoch': 0.51}
2025-10-14 16:39:50 | {'loss': 1.6363, 'grad_norm': 4.013218879699707, 'learning_rate': 5.2007023465966854e-06, 'epoch': 0.64}
2025-10-14 16:39:56 | {'loss': 1.5856, 'grad_norm': 3.7398178577423096, 'learning_rate': 4.930285545715841e-06, 'epoch': 0.77}
2025-10-14 16:40:02 | {'loss': 1.5637, 'grad_norm': 4.030646800994873, 'learning_rate': 4.64660973756188e-06, 'epoch': 0.9}
2025-10-14 16:40:37 | {'eval_loss': 1.4632319211959839, 'eval_rouge1': 0.40231030868844797, 'eval_rouge2': 0.2602585994004718, 'eval_rougeL': 0.3956539029533895, 'eval_rouge_sum': 1.0582228110423093, 'eval_runtime': 30.3822, 'eval_samples_per_second': 16.424, 'eval_steps_per_second': 1.053, 'epoch': 1.0}
2025-10-14 16:40:40 | {'loss': 1.5581, 'grad_norm': 3.9963462352752686, 'learning_rate': 4.362933929407918e-06, 'epoch': 1.03}
2025-10-14 16:40:46 | {'loss': 1.4966, 'grad_norm': 4.323423385620117, 'learning_rate': 4.0792581212539575e-06, 'epoch': 1.16}
2025-10-14 16:40:52 | {'loss': 1.4904, 'grad_norm': 3.7792351245880127, 'learning_rate': 3.795582313099997e-06, 'epoch': 1.28}
2025-10-14 16:40:58 | {'loss': 1.4726, 'grad_norm': 4.345048904418945, 'learning_rate': 3.5119065049460364e-06, 'epoch': 1.41}
2025-10-14 16:41:05 | {'loss': 1.4578, 'grad_norm': 3.8618693351745605, 'learning_rate': 3.2282306967920747e-06, 'epoch': 1.54}
2025-10-14 16:41:11 | {'loss': 1.4773, 'grad_norm': 3.747065782546997, 'learning_rate': 2.944554888638114e-06, 'epoch': 1.67}
2025-10-14 16:41:17 | {'loss': 1.4549, 'grad_norm': 3.6316041946411133, 'learning_rate': 2.660879080484153e-06, 'epoch': 1.8}
2025-10-14 16:41:23 | {'loss': 1.4552, 'grad_norm': 4.055165767669678, 'learning_rate': 2.3772032723301924e-06, 'epoch': 1.93}
2025-10-14 16:41:57 | {'eval_loss': 1.4045487642288208, 'eval_rouge1': 0.40917304295845336, 'eval_rouge2': 0.2643628397594534, 'eval_rougeL': 0.4033118037428894, 'eval_rouge_sum': 1.076847686460796, 'eval_runtime': 29.941, 'eval_samples_per_second': 16.666, 'eval_steps_per_second': 1.069, 'epoch': 2.0}
2025-10-14 16:42:01 | {'loss': 1.4232, 'grad_norm': 3.691263198852539, 'learning_rate': 2.0935274641762316e-06, 'epoch': 2.05}
2025-10-14 16:42:07 | {'loss': 1.4083, 'grad_norm': 3.5376133918762207, 'learning_rate': 1.8098516560222706e-06, 'epoch': 2.18}
2025-10-14 16:42:13 | {'loss': 1.4189, 'grad_norm': 3.7260212898254395, 'learning_rate': 1.5261758478683096e-06, 'epoch': 2.31}
2025-10-14 16:42:19 | {'loss': 1.4083, 'grad_norm': 3.573007345199585, 'learning_rate': 1.2425000397143488e-06, 'epoch': 2.44}
2025-10-14 16:42:25 | {'loss': 1.4082, 'grad_norm': 3.7865188121795654, 'learning_rate': 9.588242315603878e-07, 'epoch': 2.57}
2025-10-14 16:42:31 | {'loss': 1.3921, 'grad_norm': 3.9679276943206787, 'learning_rate': 6.75148423406427e-07, 'epoch': 2.7}
2025-10-14 16:42:37 | {'loss': 1.3877, 'grad_norm': 3.966461658477783, 'learning_rate': 3.9147261525246607e-07, 'epoch': 2.82}
2025-10-14 16:42:43 | {'loss': 1.3809, 'grad_norm': 3.8613624572753906, 'learning_rate': 1.0779680709850515e-07, 'epoch': 2.95}
2025-10-14 16:43:16 | {'eval_loss': 1.3910372257232666, 'eval_rouge1': 0.4140452061303321, 'eval_rouge2': 0.2636092475944006, 'eval_rougeL': 0.40558654870144323, 'eval_rouge_sum': 1.0832410024261758, 'eval_runtime': 29.9598, 'eval_samples_per_second': 16.656, 'eval_steps_per_second': 1.068, 'epoch': 3.0}
2025-10-14 16:43:17 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:43:17 | {'train_runtime': 237.6047, 'train_samples_per_second': 157.282, 'train_steps_per_second': 9.836, 'train_loss': 1.562524215247726, 'epoch': 3.0}
2025-10-14 16:43:17 | 최종 모델 저장 중...
2025-10-14 16:43:18 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:43:18 | 최종 평가 중...
2025-10-14 16:43:48 | 최종 평가 결과:
2025-10-14 16:43:48 | eval_rouge1: 0.4140
2025-10-14 16:43:48 | eval_rouge2: 0.2636
2025-10-14 16:43:48 | eval_rougeL: 0.4056
2025-10-14 16:43:48 | eval_rouge_sum: 1.0832
2025-10-14 16:43:48 | ============================================================
2025-10-14 16:43:48 | ✅ 학습 완료!
2025-10-14 16:43:48 | ============================================================
2025-10-14 16:43:48 | 모델 평가 중...
2025-10-14 16:44:19 | → 메트릭 'eval_rougeL' 사용: 0.4056
2025-10-14 16:44:19 | Trial 6 완료
2025-10-14 16:44:19 | - ROUGE-L F1: 0.4056
2025-10-14 16:44:19 | Trial 6 Pruned!
2025-10-14 16:44:19 | Trial 6 실패:
2025-10-14 16:44:19 | Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/optimization/optuna_optimizer.py", line 256, in objective
    # 10. Pruning 체크
        ^^^^^^^^^^^^^^^
optuna.exceptions.TrialPruned
2025-10-14 16:44:19 | [I 2025-10-14 16:44:19,130] Trial 6 pruned.
2025-10-14 16:44:19 | 💾 Trial 6 체크포인트 저장
2025-10-14 16:44:19 | ============================================================
2025-10-14 16:44:19 | Trial 7 시작
2025-10-14 16:44:19 | 파라미터: {'learning_rate': 2.66986667427446e-05, 'num_epochs': 9, 'warmup_ratio': 0.11225543951389926, 'weight_decay': 0.0770967179954561, 'scheduler_type': 'cosine', 'num_beams': 6, 'length_penalty': 1.262856036747054}
2025-10-14 16:44:19 | ============================================================
2025-10-14 16:44:19 | 모델 타입: encoder_decoder
2025-10-14 16:44:19 | ============================================================
2025-10-14 16:44:19 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:44:19 | ============================================================
2025-10-14 16:44:19 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:44:19 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:44:20 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:44:21 | → 디바이스: cuda
2025-10-14 16:44:21 | → 전체 파라미터: 123,859,968
2025-10-14 16:44:21 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:44:21 | ============================================================
2025-10-14 16:44:21 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:44:21 | ============================================================
2025-10-14 16:44:21 | ============================================================
2025-10-14 16:44:21 | 모델 학습 시작
2025-10-14 16:44:21 | ============================================================
2025-10-14 16:44:21 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:44:21 | 학습 진행 중...
2025-10-14 16:44:28 | {'loss': 2.2903, 'grad_norm': 4.888933181762695, 'learning_rate': 5.286336015063431e-06, 'epoch': 0.13}
2025-10-14 16:44:34 | {'loss': 1.7346, 'grad_norm': 4.3217363357543945, 'learning_rate': 1.0626069363612352e-05, 'epoch': 0.26}
2025-10-14 16:44:39 | {'loss': 1.5942, 'grad_norm': 4.291271209716797, 'learning_rate': 1.596580271216127e-05, 'epoch': 0.39}
2025-10-14 16:44:45 | {'loss': 1.5349, 'grad_norm': 3.956653118133545, 'learning_rate': 2.1305536060710192e-05, 'epoch': 0.51}
2025-10-14 16:44:51 | {'loss': 1.5117, 'grad_norm': 3.641862154006958, 'learning_rate': 2.6645269409259112e-05, 'epoch': 0.64}
2025-10-14 16:44:57 | {'loss': 1.4708, 'grad_norm': 3.3154025077819824, 'learning_rate': 2.629271251028696e-05, 'epoch': 0.77}
2025-10-14 16:45:04 | {'loss': 1.4582, 'grad_norm': 3.7162792682647705, 'learning_rate': 2.588265773002671e-05, 'epoch': 0.9}
2025-10-14 16:45:38 | {'eval_loss': 1.366503357887268, 'eval_rouge1': 0.39445277489188035, 'eval_rouge2': 0.24992219882340685, 'eval_rougeL': 0.388850547551379, 'eval_rouge_sum': 1.0332255212666661, 'eval_runtime': 29.9457, 'eval_samples_per_second': 16.664, 'eval_steps_per_second': 1.069, 'epoch': 1.0}
2025-10-14 16:45:41 | {'loss': 1.4215, 'grad_norm': 3.407191038131714, 'learning_rate': 2.5472602949766463e-05, 'epoch': 1.03}
2025-10-14 16:45:47 | {'loss': 1.2932, 'grad_norm': 3.9964380264282227, 'learning_rate': 2.5062548169506218e-05, 'epoch': 1.16}
2025-10-14 16:45:53 | {'loss': 1.2948, 'grad_norm': 3.5284268856048584, 'learning_rate': 2.4652493389245974e-05, 'epoch': 1.28}
2025-10-14 16:45:59 | {'loss': 1.2795, 'grad_norm': 3.886704683303833, 'learning_rate': 2.424243860898573e-05, 'epoch': 1.41}
2025-10-14 16:46:05 | {'loss': 1.2719, 'grad_norm': 3.6276416778564453, 'learning_rate': 2.3832383828725485e-05, 'epoch': 1.54}
2025-10-14 16:46:11 | {'loss': 1.285, 'grad_norm': 3.3658807277679443, 'learning_rate': 2.3422329048465237e-05, 'epoch': 1.67}
2025-10-14 16:46:17 | {'loss': 1.2662, 'grad_norm': 3.359844446182251, 'learning_rate': 2.301227426820499e-05, 'epoch': 1.8}
2025-10-14 16:46:24 | {'loss': 1.2624, 'grad_norm': 3.6096086502075195, 'learning_rate': 2.2602219487944744e-05, 'epoch': 1.93}
2025-10-14 16:46:59 | {'eval_loss': 1.301993489265442, 'eval_rouge1': 0.4220030876316391, 'eval_rouge2': 0.27312321509275794, 'eval_rougeL': 0.4162408082721654, 'eval_rouge_sum': 1.1113671109965624, 'eval_runtime': 31.6485, 'eval_samples_per_second': 15.767, 'eval_steps_per_second': 1.011, 'epoch': 2.0}
2025-10-14 16:47:03 | {'loss': 1.1743, 'grad_norm': 3.272675037384033, 'learning_rate': 2.21921647076845e-05, 'epoch': 2.05}
2025-10-14 16:47:09 | {'loss': 1.0835, 'grad_norm': 3.2286152839660645, 'learning_rate': 2.1782109927424255e-05, 'epoch': 2.18}
2025-10-14 16:47:15 | {'loss': 1.0955, 'grad_norm': 3.3507351875305176, 'learning_rate': 2.137205514716401e-05, 'epoch': 2.31}
2025-10-14 16:47:21 | {'loss': 1.0892, 'grad_norm': 3.218728542327881, 'learning_rate': 2.096200036690376e-05, 'epoch': 2.44}
2025-10-14 16:47:27 | {'loss': 1.0959, 'grad_norm': 3.9651644229888916, 'learning_rate': 2.0551945586643515e-05, 'epoch': 2.57}
2025-10-14 16:47:33 | {'loss': 1.0819, 'grad_norm': 3.610156297683716, 'learning_rate': 2.014189080638327e-05, 'epoch': 2.7}
2025-10-14 16:47:39 | {'loss': 1.0819, 'grad_norm': 3.629185914993286, 'learning_rate': 1.9731836026123026e-05, 'epoch': 2.82}
2025-10-14 16:47:45 | {'loss': 1.0804, 'grad_norm': 3.365973472595215, 'learning_rate': 1.932178124586278e-05, 'epoch': 2.95}
2025-10-14 16:48:16 | {'eval_loss': 1.2947092056274414, 'eval_rouge1': 0.43168664611713453, 'eval_rouge2': 0.2786934245808921, 'eval_rougeL': 0.42411718432343476, 'eval_rouge_sum': 1.1344972550214614, 'eval_runtime': 28.1765, 'eval_samples_per_second': 17.71, 'eval_steps_per_second': 1.136, 'epoch': 3.0}
2025-10-14 16:48:21 | {'loss': 1.0006, 'grad_norm': 3.111205816268921, 'learning_rate': 1.8911726465602534e-05, 'epoch': 3.08}
2025-10-14 16:48:27 | {'loss': 0.9352, 'grad_norm': 3.6159098148345947, 'learning_rate': 1.8501671685342286e-05, 'epoch': 3.21}
2025-10-14 16:48:33 | {'loss': 0.9519, 'grad_norm': 3.504192352294922, 'learning_rate': 1.809161690508204e-05, 'epoch': 3.34}
2025-10-14 16:48:39 | {'loss': 0.9403, 'grad_norm': 3.382237672805786, 'learning_rate': 1.7681562124821797e-05, 'epoch': 3.47}
2025-10-14 16:48:45 | {'loss': 0.9593, 'grad_norm': 3.441027879714966, 'learning_rate': 1.7271507344561552e-05, 'epoch': 3.59}
2025-10-14 16:48:51 | {'loss': 0.9582, 'grad_norm': 4.02809476852417, 'learning_rate': 1.6861452564301305e-05, 'epoch': 3.72}
2025-10-14 16:48:57 | {'loss': 0.9469, 'grad_norm': 4.232315540313721, 'learning_rate': 1.6451397784041057e-05, 'epoch': 3.85}
2025-10-14 16:49:03 | {'loss': 0.9461, 'grad_norm': 3.4818789958953857, 'learning_rate': 1.6041343003780812e-05, 'epoch': 3.98}
2025-10-14 16:49:33 | {'eval_loss': 1.3168448209762573, 'eval_rouge1': 0.44371962140189, 'eval_rouge2': 0.28652439873078756, 'eval_rougeL': 0.4351745106801349, 'eval_rouge_sum': 1.1654185308128124, 'eval_runtime': 29.0441, 'eval_samples_per_second': 17.181, 'eval_steps_per_second': 1.102, 'epoch': 4.0}
2025-10-14 16:49:39 | {'loss': 0.8425, 'grad_norm': 3.6044187545776367, 'learning_rate': 1.5631288223520568e-05, 'epoch': 4.11}
2025-10-14 16:49:45 | {'loss': 0.8223, 'grad_norm': 3.7274906635284424, 'learning_rate': 1.5221233443260323e-05, 'epoch': 4.24}
2025-10-14 16:49:51 | {'loss': 0.8312, 'grad_norm': 3.738542079925537, 'learning_rate': 1.4811178663000077e-05, 'epoch': 4.36}
2025-10-14 16:49:57 | {'loss': 0.8318, 'grad_norm': 3.017103672027588, 'learning_rate': 1.440112388273983e-05, 'epoch': 4.49}
2025-10-14 16:50:03 | {'loss': 0.8378, 'grad_norm': 3.6659364700317383, 'learning_rate': 1.3991069102479585e-05, 'epoch': 4.62}
2025-10-14 16:50:09 | {'loss': 0.8458, 'grad_norm': 3.6458139419555664, 'learning_rate': 1.3581014322219338e-05, 'epoch': 4.75}
2025-10-14 16:50:16 | {'loss': 0.8362, 'grad_norm': 3.632563829421997, 'learning_rate': 1.3170959541959092e-05, 'epoch': 4.88}
2025-10-14 16:50:52 | {'eval_loss': 1.3291215896606445, 'eval_rouge1': 0.45855968169464323, 'eval_rouge2': 0.2970314315824491, 'eval_rougeL': 0.44974985524613387, 'eval_rouge_sum': 1.2053409685232261, 'eval_runtime': 29.947, 'eval_samples_per_second': 16.663, 'eval_steps_per_second': 1.069, 'epoch': 5.0}
2025-10-14 16:50:54 | {'loss': 0.8443, 'grad_norm': 3.7597708702087402, 'learning_rate': 1.2760904761698848e-05, 'epoch': 5.01}
2025-10-14 16:51:00 | {'loss': 0.7311, 'grad_norm': 3.275942802429199, 'learning_rate': 1.2350849981438602e-05, 'epoch': 5.13}
2025-10-14 16:51:06 | {'loss': 0.7331, 'grad_norm': 3.7283499240875244, 'learning_rate': 1.1940795201178355e-05, 'epoch': 5.26}
2025-10-14 16:51:12 | {'loss': 0.7443, 'grad_norm': 3.403439998626709, 'learning_rate': 1.1530740420918111e-05, 'epoch': 5.39}
2025-10-14 16:51:18 | {'loss': 0.743, 'grad_norm': 3.6381137371063232, 'learning_rate': 1.1120685640657865e-05, 'epoch': 5.52}
2025-10-14 16:51:25 | {'loss': 0.7649, 'grad_norm': 3.7267680168151855, 'learning_rate': 1.0710630860397619e-05, 'epoch': 5.65}
2025-10-14 16:51:31 | {'loss': 0.7615, 'grad_norm': 3.9202957153320312, 'learning_rate': 1.0300576080137374e-05, 'epoch': 5.78}
2025-10-14 16:51:37 | {'loss': 0.7513, 'grad_norm': 3.7616124153137207, 'learning_rate': 9.890521299877126e-06, 'epoch': 5.91}
2025-10-14 16:52:10 | {'eval_loss': 1.362437129020691, 'eval_rouge1': 0.44179217525931813, 'eval_rouge2': 0.28691317163850466, 'eval_rougeL': 0.4332420313613283, 'eval_rouge_sum': 1.1619473782591512, 'eval_runtime': 29.1997, 'eval_samples_per_second': 17.089, 'eval_steps_per_second': 1.096, 'epoch': 6.0}
2025-10-14 16:52:14 | {'loss': 0.7289, 'grad_norm': 3.399627447128296, 'learning_rate': 9.480466519616882e-06, 'epoch': 6.03}
2025-10-14 16:52:20 | {'loss': 0.6682, 'grad_norm': 3.0125298500061035, 'learning_rate': 9.070411739356636e-06, 'epoch': 6.16}
2025-10-14 16:52:26 | {'loss': 0.6803, 'grad_norm': 3.8067362308502197, 'learning_rate': 8.66035695909639e-06, 'epoch': 6.29}
2025-10-14 16:52:32 | {'loss': 0.6802, 'grad_norm': 3.4272515773773193, 'learning_rate': 8.250302178836145e-06, 'epoch': 6.42}
2025-10-14 16:52:38 | {'loss': 0.6787, 'grad_norm': 3.3401737213134766, 'learning_rate': 7.840247398575899e-06, 'epoch': 6.55}
2025-10-14 16:52:44 | {'loss': 0.6774, 'grad_norm': 3.466825246810913, 'learning_rate': 7.4301926183156525e-06, 'epoch': 6.68}
2025-10-14 16:52:50 | {'loss': 0.6845, 'grad_norm': 3.325525999069214, 'learning_rate': 7.020137838055407e-06, 'epoch': 6.8}
2025-10-14 16:52:56 | {'loss': 0.6772, 'grad_norm': 3.8494815826416016, 'learning_rate': 6.610083057795162e-06, 'epoch': 6.93}
2025-10-14 16:53:30 | {'eval_loss': 1.3815549612045288, 'eval_rouge1': 0.4529671864155342, 'eval_rouge2': 0.2905137399170356, 'eval_rougeL': 0.444710461956436, 'eval_rouge_sum': 1.1881913882890058, 'eval_runtime': 30.0144, 'eval_samples_per_second': 16.625, 'eval_steps_per_second': 1.066, 'epoch': 7.0}
2025-10-14 16:53:34 | {'loss': 0.6574, 'grad_norm': 3.065708637237549, 'learning_rate': 6.200028277534916e-06, 'epoch': 7.06}
2025-10-14 16:53:40 | {'loss': 0.624, 'grad_norm': 4.250355243682861, 'learning_rate': 5.7899734972746694e-06, 'epoch': 7.19}
2025-10-14 16:53:46 | {'loss': 0.6297, 'grad_norm': 2.7283644676208496, 'learning_rate': 5.379918717014424e-06, 'epoch': 7.32}
2025-10-14 16:53:52 | {'loss': 0.6294, 'grad_norm': 3.5577635765075684, 'learning_rate': 4.969863936754179e-06, 'epoch': 7.45}
2025-10-14 16:53:58 | {'loss': 0.626, 'grad_norm': 3.3505821228027344, 'learning_rate': 4.5598091564939326e-06, 'epoch': 7.57}
2025-10-14 16:54:04 | {'loss': 0.634, 'grad_norm': 3.492544651031494, 'learning_rate': 4.149754376233686e-06, 'epoch': 7.7}
2025-10-14 16:54:10 | {'loss': 0.6372, 'grad_norm': 4.319864273071289, 'learning_rate': 3.739699595973441e-06, 'epoch': 7.83}
2025-10-14 16:54:16 | {'loss': 0.6231, 'grad_norm': 3.2204415798187256, 'learning_rate': 3.3296448157131953e-06, 'epoch': 7.96}
2025-10-14 16:54:47 | {'eval_loss': 1.4068365097045898, 'eval_rouge1': 0.4565544667673073, 'eval_rouge2': 0.29416830180524756, 'eval_rougeL': 0.4462705301678914, 'eval_rouge_sum': 1.1969932987404461, 'eval_runtime': 29.5026, 'eval_samples_per_second': 16.914, 'eval_steps_per_second': 1.085, 'epoch': 8.0}
2025-10-14 16:54:49 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 16:54:49 | {'train_runtime': 627.9054, 'train_samples_per_second': 178.551, 'train_steps_per_second': 11.166, 'train_loss': 0.9796011126546407, 'epoch': 8.0}
2025-10-14 16:54:49 | 최종 모델 저장 중...
2025-10-14 16:54:50 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 16:54:50 | 최종 평가 중...
2025-10-14 16:55:19 | 최종 평가 결과:
2025-10-14 16:55:19 | eval_rouge1: 0.4586
2025-10-14 16:55:19 | eval_rouge2: 0.2970
2025-10-14 16:55:19 | eval_rougeL: 0.4497
2025-10-14 16:55:19 | eval_rouge_sum: 1.2053
2025-10-14 16:55:19 | ============================================================
2025-10-14 16:55:19 | ✅ 학습 완료!
2025-10-14 16:55:19 | ============================================================
2025-10-14 16:55:19 | 모델 평가 중...
2025-10-14 16:55:48 | → 메트릭 'eval_rougeL' 사용: 0.4497
2025-10-14 16:55:48 | Trial 7 완료
2025-10-14 16:55:48 | - ROUGE-L F1: 0.4497
2025-10-14 16:55:48 | [I 2025-10-14 16:55:48,095] Trial 7 finished with value: 0.44974985524613387 and parameters: {'learning_rate': 2.66986667427446e-05, 'num_epochs': 9, 'warmup_ratio': 0.11225543951389926, 'weight_decay': 0.0770967179954561, 'scheduler_type': 'cosine', 'num_beams': 6, 'length_penalty': 1.262856036747054}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 16:55:48 | 💾 Trial 7 체크포인트 저장
2025-10-14 16:55:48 | ============================================================
2025-10-14 16:55:48 | Trial 8 시작
2025-10-14 16:55:48 | 파라미터: {'learning_rate': 6.533305220227742e-05, 'num_epochs': 4, 'warmup_ratio': 0.08207658460712595, 'weight_decay': 0.07555511385430487, 'scheduler_type': 'cosine_with_restarts', 'num_beams': 2, 'length_penalty': 1.7055081153486717}
2025-10-14 16:55:48 | ============================================================
2025-10-14 16:55:48 | 모델 타입: encoder_decoder
2025-10-14 16:55:48 | ============================================================
2025-10-14 16:55:48 | 모델 및 토크나이저 로딩 시작
2025-10-14 16:55:48 | ============================================================
2025-10-14 16:55:48 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 16:55:48 | 모델 로딩: digit82/kobart-summarization
2025-10-14 16:55:48 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 16:55:49 | → 디바이스: cuda
2025-10-14 16:55:49 | → 전체 파라미터: 123,859,968
2025-10-14 16:55:49 | → 학습 가능 파라미터: 123,859,968
2025-10-14 16:55:49 | ============================================================
2025-10-14 16:55:49 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 16:55:49 | ============================================================
2025-10-14 16:55:50 | ============================================================
2025-10-14 16:55:50 | 모델 학습 시작
2025-10-14 16:55:50 | ============================================================
2025-10-14 16:55:50 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 16:55:50 | 학습 진행 중...
2025-10-14 16:55:56 | {'loss': 2.1155, 'grad_norm': 4.992986679077148, 'learning_rate': 1.293594433605093e-05, 'epoch': 0.13}
2025-10-14 16:56:02 | {'loss': 1.6563, 'grad_norm': 3.9293742179870605, 'learning_rate': 2.6002554776506414e-05, 'epoch': 0.26}
2025-10-14 16:56:08 | {'loss': 1.5418, 'grad_norm': 4.01629638671875, 'learning_rate': 3.906916521696189e-05, 'epoch': 0.39}
2025-10-14 16:56:14 | {'loss': 1.4983, 'grad_norm': 4.0361008644104, 'learning_rate': 5.2135775657417386e-05, 'epoch': 0.51}
2025-10-14 16:56:21 | {'loss': 1.5001, 'grad_norm': 3.6316418647766113, 'learning_rate': 6.520238609787286e-05, 'epoch': 0.64}
2025-10-14 16:56:27 | {'loss': 1.4634, 'grad_norm': 3.097588300704956, 'learning_rate': 6.286058577719123e-05, 'epoch': 0.77}
2025-10-14 16:56:33 | {'loss': 1.4539, 'grad_norm': 3.4737000465393066, 'learning_rate': 6.036314494377084e-05, 'epoch': 0.9}
2025-10-14 16:57:02 | {'eval_loss': 1.3588790893554688, 'eval_rouge1': 0.40943990463179597, 'eval_rouge2': 0.26239180103596244, 'eval_rougeL': 0.404629969361964, 'eval_rouge_sum': 1.0764616750297225, 'eval_runtime': 25.2542, 'eval_samples_per_second': 19.759, 'eval_steps_per_second': 1.267, 'epoch': 1.0}
2025-10-14 16:57:05 | {'loss': 1.3985, 'grad_norm': 3.2265877723693848, 'learning_rate': 5.786570411035045e-05, 'epoch': 1.03}
2025-10-14 16:57:11 | {'loss': 1.1883, 'grad_norm': 3.6048431396484375, 'learning_rate': 5.5368263276930065e-05, 'epoch': 1.16}
2025-10-14 16:57:18 | {'loss': 1.1975, 'grad_norm': 3.203080892562866, 'learning_rate': 5.287082244350967e-05, 'epoch': 1.28}
2025-10-14 16:57:24 | {'loss': 1.1844, 'grad_norm': 3.3762941360473633, 'learning_rate': 5.037338161008928e-05, 'epoch': 1.41}
2025-10-14 16:57:30 | {'loss': 1.1794, 'grad_norm': 3.228463888168335, 'learning_rate': 4.7875940776668887e-05, 'epoch': 1.54}
2025-10-14 16:57:36 | {'loss': 1.1885, 'grad_norm': 3.125037431716919, 'learning_rate': 4.53784999432485e-05, 'epoch': 1.67}
2025-10-14 16:57:42 | {'loss': 1.1707, 'grad_norm': 2.917407274246216, 'learning_rate': 4.288105910982811e-05, 'epoch': 1.8}
2025-10-14 16:57:48 | {'loss': 1.1669, 'grad_norm': 3.2144370079040527, 'learning_rate': 4.0383618276407715e-05, 'epoch': 1.93}
2025-10-14 16:58:16 | {'eval_loss': 1.2953588962554932, 'eval_rouge1': 0.4466944934207633, 'eval_rouge2': 0.2851934504042125, 'eval_rougeL': 0.43897281431343366, 'eval_rouge_sum': 1.1708607581384094, 'eval_runtime': 25.1898, 'eval_samples_per_second': 19.81, 'eval_steps_per_second': 1.27, 'epoch': 2.0}
2025-10-14 16:58:20 | {'loss': 1.037, 'grad_norm': 2.9711356163024902, 'learning_rate': 3.788617744298733e-05, 'epoch': 2.05}
2025-10-14 16:58:27 | {'loss': 0.8827, 'grad_norm': 3.128444194793701, 'learning_rate': 3.5388736609566936e-05, 'epoch': 2.18}
2025-10-14 16:58:33 | {'loss': 0.8891, 'grad_norm': 3.159991502761841, 'learning_rate': 3.289129577614654e-05, 'epoch': 2.31}
2025-10-14 16:58:39 | {'loss': 0.8868, 'grad_norm': 3.1945629119873047, 'learning_rate': 3.0393854942726154e-05, 'epoch': 2.44}
2025-10-14 16:58:45 | {'loss': 0.8941, 'grad_norm': 3.4638373851776123, 'learning_rate': 2.789641410930576e-05, 'epoch': 2.57}
2025-10-14 16:58:51 | {'loss': 0.8813, 'grad_norm': 3.3653831481933594, 'learning_rate': 2.539897327588537e-05, 'epoch': 2.7}
2025-10-14 16:58:57 | {'loss': 0.8816, 'grad_norm': 3.1780343055725098, 'learning_rate': 2.2901532442464982e-05, 'epoch': 2.82}
2025-10-14 16:59:03 | {'loss': 0.8819, 'grad_norm': 3.3134777545928955, 'learning_rate': 2.0404091609044592e-05, 'epoch': 2.95}
2025-10-14 16:59:30 | {'eval_loss': 1.3224358558654785, 'eval_rouge1': 0.45204816755456734, 'eval_rouge2': 0.29306943928989876, 'eval_rougeL': 0.44393560543398924, 'eval_rouge_sum': 1.1890532122784552, 'eval_runtime': 24.745, 'eval_samples_per_second': 20.166, 'eval_steps_per_second': 1.293, 'epoch': 3.0}
2025-10-14 16:59:36 | {'loss': 0.7759, 'grad_norm': 2.93294095993042, 'learning_rate': 1.79066507756242e-05, 'epoch': 3.08}
2025-10-14 16:59:42 | {'loss': 0.693, 'grad_norm': 3.037580966949463, 'learning_rate': 1.540920994220381e-05, 'epoch': 3.21}
2025-10-14 16:59:48 | {'loss': 0.7029, 'grad_norm': 3.4362549781799316, 'learning_rate': 1.2911769108783419e-05, 'epoch': 3.34}
2025-10-14 16:59:53 | {'loss': 0.6922, 'grad_norm': 3.0144240856170654, 'learning_rate': 1.0414328275363028e-05, 'epoch': 3.47}
2025-10-14 16:59:59 | {'loss': 0.7031, 'grad_norm': 3.1773722171783447, 'learning_rate': 7.916887441942638e-06, 'epoch': 3.59}
2025-10-14 17:00:06 | {'loss': 0.6993, 'grad_norm': 3.4146485328674316, 'learning_rate': 5.419446608522248e-06, 'epoch': 3.72}
2025-10-14 17:00:12 | {'loss': 0.6894, 'grad_norm': 3.433464527130127, 'learning_rate': 2.922005775101857e-06, 'epoch': 3.85}
2025-10-14 17:00:17 | {'loss': 0.6831, 'grad_norm': 3.3826465606689453, 'learning_rate': 4.245649416814664e-07, 'epoch': 3.98}
2025-10-14 17:00:43 | {'eval_loss': 1.3642350435256958, 'eval_rouge1': 0.4578702734925397, 'eval_rouge2': 0.30005245794641927, 'eval_rougeL': 0.44961404519623127, 'eval_rouge_sum': 1.20753677663519, 'eval_runtime': 24.8053, 'eval_samples_per_second': 20.117, 'eval_steps_per_second': 1.29, 'epoch': 4.0}
2025-10-14 17:00:45 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 17:00:45 | {'train_runtime': 294.6854, 'train_samples_per_second': 169.089, 'train_steps_per_second': 10.574, 'train_loss': 1.0875471430963362, 'epoch': 4.0}
2025-10-14 17:00:45 | 최종 모델 저장 중...
2025-10-14 17:00:45 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 17:00:45 | 최종 평가 중...
2025-10-14 17:01:11 | 최종 평가 결과:
2025-10-14 17:01:11 | eval_rouge1: 0.4579
2025-10-14 17:01:11 | eval_rouge2: 0.3001
2025-10-14 17:01:11 | eval_rougeL: 0.4496
2025-10-14 17:01:11 | eval_rouge_sum: 1.2075
2025-10-14 17:01:11 | ============================================================
2025-10-14 17:01:11 | ✅ 학습 완료!
2025-10-14 17:01:11 | ============================================================
2025-10-14 17:01:11 | 모델 평가 중...
2025-10-14 17:01:36 | → 메트릭 'eval_rougeL' 사용: 0.4496
2025-10-14 17:01:36 | Trial 8 완료
2025-10-14 17:01:36 | - ROUGE-L F1: 0.4496
2025-10-14 17:01:36 | [I 2025-10-14 17:01:36,119] Trial 8 finished with value: 0.44961404519623127 and parameters: {'learning_rate': 6.533305220227742e-05, 'num_epochs': 4, 'warmup_ratio': 0.08207658460712595, 'weight_decay': 0.07555511385430487, 'scheduler_type': 'cosine_with_restarts', 'num_beams': 2, 'length_penalty': 1.7055081153486717}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 17:01:36 | 💾 Trial 8 체크포인트 저장
2025-10-14 17:01:36 | ============================================================
2025-10-14 17:01:36 | Trial 9 시작
2025-10-14 17:01:36 | 파라미터: {'learning_rate': 2.3612399244412613e-06, 'num_epochs': 10, 'warmup_ratio': 0.10786844838313014, 'weight_decay': 0.08074401551640625, 'scheduler_type': 'linear', 'num_beams': 6, 'length_penalty': 1.2661209538663485}
2025-10-14 17:01:36 | ============================================================
2025-10-14 17:01:36 | 모델 타입: encoder_decoder
2025-10-14 17:01:36 | ============================================================
2025-10-14 17:01:36 | 모델 및 토크나이저 로딩 시작
2025-10-14 17:01:36 | ============================================================
2025-10-14 17:01:36 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 17:01:36 | 모델 로딩: digit82/kobart-summarization
2025-10-14 17:01:36 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 17:01:38 | → 디바이스: cuda
2025-10-14 17:01:38 | → 전체 파라미터: 123,859,968
2025-10-14 17:01:38 | → 학습 가능 파라미터: 123,859,968
2025-10-14 17:01:38 | ============================================================
2025-10-14 17:01:38 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 17:01:38 | ============================================================
2025-10-14 17:01:38 | ============================================================
2025-10-14 17:01:38 | 모델 학습 시작
2025-10-14 17:01:38 | ============================================================
2025-10-14 17:01:38 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 17:01:38 | 학습 진행 중...
2025-10-14 17:01:45 | {'loss': 2.8488, 'grad_norm': 8.566364288330078, 'learning_rate': 4.675255050393698e-07, 'epoch': 0.13}
2025-10-14 17:01:50 | {'loss': 2.2212, 'grad_norm': 5.875603199005127, 'learning_rate': 9.39773489927622e-07, 'epoch': 0.26}
2025-10-14 17:01:56 | {'loss': 1.9031, 'grad_norm': 5.295422077178955, 'learning_rate': 1.4120214748158742e-06, 'epoch': 0.39}
2025-10-14 17:02:02 | {'loss': 1.7787, 'grad_norm': 4.42046594619751, 'learning_rate': 1.8842694597041266e-06, 'epoch': 0.51}
2025-10-14 17:02:08 | {'loss': 1.7249, 'grad_norm': 4.227117538452148, 'learning_rate': 2.356517444592379e-06, 'epoch': 0.64}
2025-10-14 17:02:15 | {'loss': 1.6647, 'grad_norm': 3.85404896736145, 'learning_rate': 2.3291737032451454e-06, 'epoch': 0.77}
2025-10-14 17:02:21 | {'loss': 1.639, 'grad_norm': 4.161289691925049, 'learning_rate': 2.296783580824826e-06, 'epoch': 0.9}
2025-10-14 17:02:55 | {'eval_loss': 1.529981017112732, 'eval_rouge1': 0.3974665486089801, 'eval_rouge2': 0.2521110604115313, 'eval_rougeL': 0.3920399028712169, 'eval_rouge_sum': 1.0416175118917284, 'eval_runtime': 29.3354, 'eval_samples_per_second': 17.01, 'eval_steps_per_second': 1.091, 'epoch': 1.0}
2025-10-14 17:02:58 | {'loss': 1.6361, 'grad_norm': 4.109757900238037, 'learning_rate': 2.2643934584045074e-06, 'epoch': 1.03}
2025-10-14 17:03:04 | {'loss': 1.5849, 'grad_norm': 6.886953353881836, 'learning_rate': 2.2320033359841882e-06, 'epoch': 1.16}
2025-10-14 17:03:10 | {'loss': 1.5745, 'grad_norm': 3.9650113582611084, 'learning_rate': 2.199613213563869e-06, 'epoch': 1.28}
2025-10-14 17:03:17 | {'loss': 1.5565, 'grad_norm': 5.114630222320557, 'learning_rate': 2.16722309114355e-06, 'epoch': 1.41}
2025-10-14 17:03:23 | {'loss': 1.5351, 'grad_norm': 3.9957666397094727, 'learning_rate': 2.134832968723231e-06, 'epoch': 1.54}
2025-10-14 17:03:29 | {'loss': 1.5532, 'grad_norm': 3.9114978313446045, 'learning_rate': 2.102442846302912e-06, 'epoch': 1.67}
2025-10-14 17:03:35 | {'loss': 1.5285, 'grad_norm': 3.7819085121154785, 'learning_rate': 2.0700527238825927e-06, 'epoch': 1.8}
2025-10-14 17:03:41 | {'loss': 1.5266, 'grad_norm': 4.150989532470703, 'learning_rate': 2.0376626014622735e-06, 'epoch': 1.93}
2025-10-14 17:04:13 | {'eval_loss': 1.4527119398117065, 'eval_rouge1': 0.40698605668514526, 'eval_rouge2': 0.2611024756437874, 'eval_rougeL': 0.4002914350519778, 'eval_rouge_sum': 1.0683799673809105, 'eval_runtime': 29.2843, 'eval_samples_per_second': 17.04, 'eval_steps_per_second': 1.093, 'epoch': 2.0}
2025-10-14 17:04:18 | {'loss': 1.4973, 'grad_norm': 3.9412548542022705, 'learning_rate': 2.0052724790419543e-06, 'epoch': 2.05}
2025-10-14 17:04:24 | {'loss': 1.4865, 'grad_norm': 3.6252267360687256, 'learning_rate': 1.9728823566216355e-06, 'epoch': 2.18}
2025-10-14 17:04:30 | {'loss': 1.4947, 'grad_norm': 3.8128182888031006, 'learning_rate': 1.9404922342013163e-06, 'epoch': 2.31}
2025-10-14 17:04:36 | {'loss': 1.4797, 'grad_norm': 3.673849582672119, 'learning_rate': 1.9081021117809975e-06, 'epoch': 2.44}
2025-10-14 17:04:42 | {'loss': 1.4788, 'grad_norm': 3.999621629714966, 'learning_rate': 1.8757119893606783e-06, 'epoch': 2.57}
2025-10-14 17:04:48 | {'loss': 1.4588, 'grad_norm': 4.024015426635742, 'learning_rate': 1.8433218669403592e-06, 'epoch': 2.7}
2025-10-14 17:04:55 | {'loss': 1.4517, 'grad_norm': 4.075536251068115, 'learning_rate': 1.81093174452004e-06, 'epoch': 2.82}
2025-10-14 17:05:01 | {'loss': 1.4415, 'grad_norm': 3.9090960025787354, 'learning_rate': 1.778541622099721e-06, 'epoch': 2.95}
2025-10-14 17:05:34 | {'eval_loss': 1.4222605228424072, 'eval_rouge1': 0.41816784838443716, 'eval_rouge2': 0.26624943686012925, 'eval_rougeL': 0.4100007557789025, 'eval_rouge_sum': 1.094418041023469, 'eval_runtime': 30.4923, 'eval_samples_per_second': 16.365, 'eval_steps_per_second': 1.049, 'epoch': 3.0}
2025-10-14 17:05:39 | {'loss': 1.4457, 'grad_norm': 3.610867977142334, 'learning_rate': 1.7461514996794018e-06, 'epoch': 3.08}
2025-10-14 17:05:45 | {'loss': 1.4308, 'grad_norm': 4.018589496612549, 'learning_rate': 1.7137613772590828e-06, 'epoch': 3.21}
2025-10-14 17:05:51 | {'loss': 1.4424, 'grad_norm': 3.901196002960205, 'learning_rate': 1.6813712548387636e-06, 'epoch': 3.34}
2025-10-14 17:05:57 | {'loss': 1.4204, 'grad_norm': 3.944927930831909, 'learning_rate': 1.6489811324184448e-06, 'epoch': 3.47}
2025-10-14 17:06:03 | {'loss': 1.4409, 'grad_norm': 3.7604472637176514, 'learning_rate': 1.6165910099981256e-06, 'epoch': 3.59}
2025-10-14 17:06:09 | {'loss': 1.435, 'grad_norm': 4.038198471069336, 'learning_rate': 1.5842008875778064e-06, 'epoch': 3.72}
2025-10-14 17:06:15 | {'loss': 1.408, 'grad_norm': 4.305938243865967, 'learning_rate': 1.5518107651574875e-06, 'epoch': 3.85}
2025-10-14 17:06:21 | {'loss': 1.406, 'grad_norm': 4.011630058288574, 'learning_rate': 1.5194206427371683e-06, 'epoch': 3.98}
2025-10-14 17:06:52 | {'eval_loss': 1.3999638557434082, 'eval_rouge1': 0.4234329831789389, 'eval_rouge2': 0.2742039573428978, 'eval_rougeL': 0.41541801223997754, 'eval_rouge_sum': 1.1130549527618143, 'eval_runtime': 29.7625, 'eval_samples_per_second': 16.766, 'eval_steps_per_second': 1.075, 'epoch': 4.0}
2025-10-14 17:06:59 | {'loss': 1.3969, 'grad_norm': 4.109679698944092, 'learning_rate': 1.4870305203168493e-06, 'epoch': 4.11}
2025-10-14 17:07:05 | {'loss': 1.3856, 'grad_norm': 4.362600803375244, 'learning_rate': 1.45464039789653e-06, 'epoch': 4.24}
2025-10-14 17:07:11 | {'loss': 1.4024, 'grad_norm': 5.019811153411865, 'learning_rate': 1.4222502754762109e-06, 'epoch': 4.36}
2025-10-14 17:07:17 | {'loss': 1.3862, 'grad_norm': 3.553382158279419, 'learning_rate': 1.389860153055892e-06, 'epoch': 4.49}
2025-10-14 17:07:23 | {'loss': 1.3949, 'grad_norm': 3.7809407711029053, 'learning_rate': 1.3574700306355727e-06, 'epoch': 4.62}
2025-10-14 17:07:29 | {'loss': 1.3967, 'grad_norm': 4.376664161682129, 'learning_rate': 1.325079908215254e-06, 'epoch': 4.75}
2025-10-14 17:07:35 | {'loss': 1.3865, 'grad_norm': 4.104478359222412, 'learning_rate': 1.2926897857949347e-06, 'epoch': 4.88}
2025-10-14 17:08:11 | {'eval_loss': 1.3852136135101318, 'eval_rouge1': 0.4208911757842975, 'eval_rouge2': 0.2738001935627838, 'eval_rougeL': 0.41471856346176933, 'eval_rouge_sum': 1.1094099328088507, 'eval_runtime': 30.5132, 'eval_samples_per_second': 16.354, 'eval_steps_per_second': 1.049, 'epoch': 5.0}
2025-10-14 17:08:13 | {'loss': 1.3899, 'grad_norm': 3.995919942855835, 'learning_rate': 1.2602996633746158e-06, 'epoch': 5.01}
2025-10-14 17:08:19 | {'loss': 1.3534, 'grad_norm': 3.7633535861968994, 'learning_rate': 1.2279095409542966e-06, 'epoch': 5.13}
2025-10-14 17:08:26 | {'loss': 1.3591, 'grad_norm': 4.309010028839111, 'learning_rate': 1.1955194185339774e-06, 'epoch': 5.26}
2025-10-14 17:08:32 | {'loss': 1.3644, 'grad_norm': 3.9790849685668945, 'learning_rate': 1.1631292961136584e-06, 'epoch': 5.39}
2025-10-14 17:08:38 | {'loss': 1.37, 'grad_norm': 3.8689475059509277, 'learning_rate': 1.1307391736933392e-06, 'epoch': 5.52}
2025-10-14 17:08:44 | {'loss': 1.3852, 'grad_norm': 3.8934364318847656, 'learning_rate': 1.0983490512730202e-06, 'epoch': 5.65}
2025-10-14 17:08:50 | {'loss': 1.3794, 'grad_norm': 4.35306978225708, 'learning_rate': 1.0659589288527012e-06, 'epoch': 5.78}
2025-10-14 17:08:56 | {'loss': 1.3714, 'grad_norm': 3.871971845626831, 'learning_rate': 1.033568806432382e-06, 'epoch': 5.91}
2025-10-14 17:09:31 | {'eval_loss': 1.374704122543335, 'eval_rouge1': 0.4159948097700301, 'eval_rouge2': 0.2689479914362443, 'eval_rougeL': 0.40858853858909017, 'eval_rouge_sum': 1.0935313397953645, 'eval_runtime': 30.7366, 'eval_samples_per_second': 16.235, 'eval_steps_per_second': 1.041, 'epoch': 6.0}
2025-10-14 17:09:34 | {'loss': 1.3573, 'grad_norm': 3.8811557292938232, 'learning_rate': 1.0011786840120628e-06, 'epoch': 6.03}
2025-10-14 17:09:41 | {'loss': 1.3396, 'grad_norm': 3.4998080730438232, 'learning_rate': 9.687885615917438e-07, 'epoch': 6.16}
2025-10-14 17:09:47 | {'loss': 1.3681, 'grad_norm': 4.109548091888428, 'learning_rate': 9.363984391714247e-07, 'epoch': 6.29}
2025-10-14 17:09:53 | {'loss': 1.3579, 'grad_norm': 4.066159725189209, 'learning_rate': 9.040083167511057e-07, 'epoch': 6.42}
2025-10-14 17:09:59 | {'loss': 1.3571, 'grad_norm': 3.4210453033447266, 'learning_rate': 8.716181943307866e-07, 'epoch': 6.55}
2025-10-14 17:10:05 | {'loss': 1.346, 'grad_norm': 4.0245280265808105, 'learning_rate': 8.392280719104675e-07, 'epoch': 6.68}
2025-10-14 17:10:11 | {'loss': 1.3408, 'grad_norm': 4.047568321228027, 'learning_rate': 8.068379494901484e-07, 'epoch': 6.8}
2025-10-14 17:10:18 | {'loss': 1.3397, 'grad_norm': 3.887441396713257, 'learning_rate': 7.744478270698294e-07, 'epoch': 6.93}
2025-10-14 17:10:51 | {'eval_loss': 1.3696725368499756, 'eval_rouge1': 0.42625957955358174, 'eval_rouge2': 0.2760362003126169, 'eval_rougeL': 0.41730730862730225, 'eval_rouge_sum': 1.1196030884935009, 'eval_runtime': 30.4468, 'eval_samples_per_second': 16.389, 'eval_steps_per_second': 1.051, 'epoch': 7.0}
2025-10-14 17:10:56 | {'loss': 1.3482, 'grad_norm': 3.706031322479248, 'learning_rate': 7.420577046495102e-07, 'epoch': 7.06}
2025-10-14 17:11:02 | {'loss': 1.3318, 'grad_norm': 3.6289799213409424, 'learning_rate': 7.096675822291911e-07, 'epoch': 7.19}
2025-10-14 17:11:08 | {'loss': 1.3304, 'grad_norm': 3.462770938873291, 'learning_rate': 6.77277459808872e-07, 'epoch': 7.32}
2025-10-14 17:11:14 | {'loss': 1.3338, 'grad_norm': 4.21668004989624, 'learning_rate': 6.44887337388553e-07, 'epoch': 7.45}
2025-10-14 17:11:20 | {'loss': 1.332, 'grad_norm': 3.7506344318389893, 'learning_rate': 6.12497214968234e-07, 'epoch': 7.57}
2025-10-14 17:11:26 | {'loss': 1.3448, 'grad_norm': 3.690337896347046, 'learning_rate': 5.801070925479149e-07, 'epoch': 7.7}
2025-10-14 17:11:32 | {'loss': 1.3496, 'grad_norm': 3.8234140872955322, 'learning_rate': 5.477169701275957e-07, 'epoch': 7.83}
2025-10-14 17:11:38 | {'loss': 1.3267, 'grad_norm': 3.8082940578460693, 'learning_rate': 5.153268477072767e-07, 'epoch': 7.96}
2025-10-14 17:12:10 | {'eval_loss': 1.3658807277679443, 'eval_rouge1': 0.4281636241811, 'eval_rouge2': 0.279478033962855, 'eval_rougeL': 0.4204988156287578, 'eval_rouge_sum': 1.1281404737727128, 'eval_runtime': 30.5439, 'eval_samples_per_second': 16.337, 'eval_steps_per_second': 1.048, 'epoch': 8.0}
2025-10-14 17:12:17 | {'loss': 1.3326, 'grad_norm': 4.0160675048828125, 'learning_rate': 4.829367252869576e-07, 'epoch': 8.09}
2025-10-14 17:12:23 | {'loss': 1.3233, 'grad_norm': 4.725773811340332, 'learning_rate': 4.505466028666385e-07, 'epoch': 8.22}
2025-10-14 17:12:29 | {'loss': 1.314, 'grad_norm': 4.210341453552246, 'learning_rate': 4.181564804463194e-07, 'epoch': 8.34}
2025-10-14 17:12:35 | {'loss': 1.3392, 'grad_norm': 3.798126459121704, 'learning_rate': 3.857663580260003e-07, 'epoch': 8.47}
2025-10-14 17:12:41 | {'loss': 1.3087, 'grad_norm': 3.6069071292877197, 'learning_rate': 3.5337623560568124e-07, 'epoch': 8.6}
2025-10-14 17:12:47 | {'loss': 1.3367, 'grad_norm': 3.674950361251831, 'learning_rate': 3.209861131853621e-07, 'epoch': 8.73}
2025-10-14 17:12:53 | {'loss': 1.3493, 'grad_norm': 3.72727108001709, 'learning_rate': 2.8859599076504306e-07, 'epoch': 8.86}
2025-10-14 17:12:59 | {'loss': 1.3117, 'grad_norm': 4.468277454376221, 'learning_rate': 2.5620586834472397e-07, 'epoch': 8.99}
2025-10-14 17:13:29 | {'eval_loss': 1.3624982833862305, 'eval_rouge1': 0.42359653194010694, 'eval_rouge2': 0.2745407766487077, 'eval_rougeL': 0.41574112897218796, 'eval_rouge_sum': 1.1138784375610027, 'eval_runtime': 29.4394, 'eval_samples_per_second': 16.95, 'eval_steps_per_second': 1.087, 'epoch': 9.0}
2025-10-14 17:13:36 | {'loss': 1.3148, 'grad_norm': 4.93867826461792, 'learning_rate': 2.2381574592440488e-07, 'epoch': 9.11}
2025-10-14 17:13:42 | {'loss': 1.3364, 'grad_norm': 3.7501676082611084, 'learning_rate': 1.914256235040858e-07, 'epoch': 9.24}
2025-10-14 17:13:47 | {'loss': 1.338, 'grad_norm': 4.255181312561035, 'learning_rate': 1.5903550108376673e-07, 'epoch': 9.37}
2025-10-14 17:13:54 | {'loss': 1.3294, 'grad_norm': 3.994647264480591, 'learning_rate': 1.2664537866344764e-07, 'epoch': 9.5}
2025-10-14 17:14:00 | {'loss': 1.3085, 'grad_norm': 4.137359142303467, 'learning_rate': 9.425525624312855e-08, 'epoch': 9.63}
2025-10-14 17:14:06 | {'loss': 1.2993, 'grad_norm': 3.87576961517334, 'learning_rate': 6.186513382280946e-08, 'epoch': 9.76}
2025-10-14 17:14:12 | {'loss': 1.327, 'grad_norm': 3.9057953357696533, 'learning_rate': 2.9475011402490367e-08, 'epoch': 9.88}
2025-10-14 17:14:49 | {'eval_loss': 1.362569808959961, 'eval_rouge1': 0.42668429258332047, 'eval_rouge2': 0.2785889632722618, 'eval_rougeL': 0.41959881517437875, 'eval_rouge_sum': 1.1248720710299611, 'eval_runtime': 30.9228, 'eval_samples_per_second': 16.137, 'eval_steps_per_second': 1.035, 'epoch': 10.0}
2025-10-14 17:14:50 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 17:14:50 | {'train_runtime': 791.8071, 'train_samples_per_second': 157.324, 'train_steps_per_second': 9.838, 'train_loss': 1.4475990843864706, 'epoch': 10.0}
2025-10-14 17:14:50 | 최종 모델 저장 중...
2025-10-14 17:14:51 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 17:14:51 | 최종 평가 중...
2025-10-14 17:15:21 | 최종 평가 결과:
2025-10-14 17:15:21 | eval_rouge1: 0.4282
2025-10-14 17:15:21 | eval_rouge2: 0.2795
2025-10-14 17:15:21 | eval_rougeL: 0.4205
2025-10-14 17:15:21 | eval_rouge_sum: 1.1281
2025-10-14 17:15:21 | ============================================================
2025-10-14 17:15:21 | ✅ 학습 완료!
2025-10-14 17:15:21 | ============================================================
2025-10-14 17:15:21 | 모델 평가 중...
2025-10-14 17:15:50 | → 메트릭 'eval_rougeL' 사용: 0.4205
2025-10-14 17:15:50 | Trial 9 완료
2025-10-14 17:15:50 | - ROUGE-L F1: 0.4205
2025-10-14 17:15:50 | Trial 9 Pruned!
2025-10-14 17:15:50 | Trial 9 실패:
2025-10-14 17:15:50 | Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/optimization/optuna_optimizer.py", line 256, in objective
    # 10. Pruning 체크
        ^^^^^^^^^^^^^^^
optuna.exceptions.TrialPruned
2025-10-14 17:15:50 | [I 2025-10-14 17:15:50,868] Trial 9 pruned.
2025-10-14 17:15:50 | 💾 Trial 9 체크포인트 저장
2025-10-14 17:15:50 | ============================================================
2025-10-14 17:15:50 | Trial 10 시작
2025-10-14 17:15:50 | 파라미터: {'learning_rate': 9.098166997876735e-05, 'num_epochs': 7, 'warmup_ratio': 0.001509463863258606, 'weight_decay': 0.09550722292778, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.8868904289022379}
2025-10-14 17:15:50 | ============================================================
2025-10-14 17:15:50 | 모델 타입: encoder_decoder
2025-10-14 17:15:50 | ============================================================
2025-10-14 17:15:50 | 모델 및 토크나이저 로딩 시작
2025-10-14 17:15:50 | ============================================================
2025-10-14 17:15:50 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 17:15:51 | 모델 로딩: digit82/kobart-summarization
2025-10-14 17:15:51 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 17:15:52 | → 디바이스: cuda
2025-10-14 17:15:52 | → 전체 파라미터: 123,859,968
2025-10-14 17:15:52 | → 학습 가능 파라미터: 123,859,968
2025-10-14 17:15:52 | ============================================================
2025-10-14 17:15:52 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 17:15:52 | ============================================================
2025-10-14 17:15:52 | ============================================================
2025-10-14 17:15:52 | 모델 학습 시작
2025-10-14 17:15:52 | ============================================================
2025-10-14 17:15:52 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 17:15:52 | 학습 진행 중...
2025-10-14 17:15:59 | {'loss': 2.0553, 'grad_norm': 4.7824320793151855, 'learning_rate': 1.8014370655795936e-05, 'epoch': 0.13}
2025-10-14 17:16:05 | {'loss': 1.6341, 'grad_norm': 3.8826515674591064, 'learning_rate': 3.6210704651549405e-05, 'epoch': 0.26}
2025-10-14 17:16:11 | {'loss': 1.5341, 'grad_norm': 3.918252944946289, 'learning_rate': 5.440703864730287e-05, 'epoch': 0.39}
2025-10-14 17:16:17 | {'loss': 1.4996, 'grad_norm': 3.9688429832458496, 'learning_rate': 7.260337264305634e-05, 'epoch': 0.51}
2025-10-14 17:16:23 | {'loss': 1.5071, 'grad_norm': 3.5065464973449707, 'learning_rate': 9.079970663880981e-05, 'epoch': 0.64}
2025-10-14 17:16:29 | {'loss': 1.4774, 'grad_norm': 3.0808603763580322, 'learning_rate': 8.916313871934921e-05, 'epoch': 0.77}
2025-10-14 17:16:36 | {'loss': 1.4698, 'grad_norm': 3.780785083770752, 'learning_rate': 8.732623845731071e-05, 'epoch': 0.9}
2025-10-14 17:17:09 | {'eval_loss': 1.381263017654419, 'eval_rouge1': 0.3883084765092352, 'eval_rouge2': 0.2473989339795255, 'eval_rougeL': 0.38351238603125476, 'eval_rouge_sum': 1.0192197965200156, 'eval_runtime': 28.339, 'eval_samples_per_second': 17.608, 'eval_steps_per_second': 1.129, 'epoch': 1.0}
2025-10-14 17:17:12 | {'loss': 1.4129, 'grad_norm': 3.2301957607269287, 'learning_rate': 8.54893381952722e-05, 'epoch': 1.03}
2025-10-14 17:17:18 | {'loss': 1.1719, 'grad_norm': 3.4739043712615967, 'learning_rate': 8.36524379332337e-05, 'epoch': 1.16}
2025-10-14 17:17:24 | {'loss': 1.185, 'grad_norm': 3.0407145023345947, 'learning_rate': 8.181553767119519e-05, 'epoch': 1.28}
2025-10-14 17:17:30 | {'loss': 1.18, 'grad_norm': 3.366130828857422, 'learning_rate': 7.997863740915668e-05, 'epoch': 1.41}
2025-10-14 17:17:36 | {'loss': 1.1747, 'grad_norm': 3.0559234619140625, 'learning_rate': 7.814173714711818e-05, 'epoch': 1.54}
2025-10-14 17:17:42 | {'loss': 1.189, 'grad_norm': 2.937063217163086, 'learning_rate': 7.630483688507966e-05, 'epoch': 1.67}
2025-10-14 17:17:48 | {'loss': 1.1727, 'grad_norm': 2.896420478820801, 'learning_rate': 7.446793662304115e-05, 'epoch': 1.8}
2025-10-14 17:17:54 | {'loss': 1.1724, 'grad_norm': 3.1222615242004395, 'learning_rate': 7.263103636100263e-05, 'epoch': 1.93}
2025-10-14 17:18:25 | {'eval_loss': 1.311095952987671, 'eval_rouge1': 0.43904604203917996, 'eval_rouge2': 0.2806130911732207, 'eval_rougeL': 0.4326430896796085, 'eval_rouge_sum': 1.1523022228920092, 'eval_runtime': 27.2576, 'eval_samples_per_second': 18.307, 'eval_steps_per_second': 1.174, 'epoch': 2.0}
2025-10-14 17:18:29 | {'loss': 1.0187, 'grad_norm': 2.9133598804473877, 'learning_rate': 7.079413609896413e-05, 'epoch': 2.05}
2025-10-14 17:18:35 | {'loss': 0.8276, 'grad_norm': 2.7760560512542725, 'learning_rate': 6.895723583692563e-05, 'epoch': 2.18}
2025-10-14 17:18:41 | {'loss': 0.8439, 'grad_norm': 3.016667604446411, 'learning_rate': 6.712033557488711e-05, 'epoch': 2.31}
2025-10-14 17:18:47 | {'loss': 0.8436, 'grad_norm': 3.156099796295166, 'learning_rate': 6.528343531284861e-05, 'epoch': 2.44}
2025-10-14 17:18:53 | {'loss': 0.8595, 'grad_norm': 3.2528414726257324, 'learning_rate': 6.34465350508101e-05, 'epoch': 2.57}
2025-10-14 17:18:59 | {'loss': 0.8501, 'grad_norm': 3.2194511890411377, 'learning_rate': 6.16096347887716e-05, 'epoch': 2.7}
2025-10-14 17:19:05 | {'loss': 0.8559, 'grad_norm': 3.0464119911193848, 'learning_rate': 5.9772734526733074e-05, 'epoch': 2.82}
2025-10-14 17:19:11 | {'loss': 0.8593, 'grad_norm': 2.9376745223999023, 'learning_rate': 5.793583426469457e-05, 'epoch': 2.95}
2025-10-14 17:19:40 | {'eval_loss': 1.3654756546020508, 'eval_rouge1': 0.4417563834594396, 'eval_rouge2': 0.284830479087414, 'eval_rougeL': 0.4333364746769184, 'eval_rouge_sum': 1.159923337223772, 'eval_runtime': 26.5734, 'eval_samples_per_second': 18.778, 'eval_steps_per_second': 1.204, 'epoch': 3.0}
2025-10-14 17:19:45 | {'loss': 0.6986, 'grad_norm': 2.662242889404297, 'learning_rate': 5.6098934002656064e-05, 'epoch': 3.08}
2025-10-14 17:19:51 | {'loss': 0.5843, 'grad_norm': 2.9137675762176514, 'learning_rate': 5.426203374061755e-05, 'epoch': 3.21}
2025-10-14 17:19:57 | {'loss': 0.5999, 'grad_norm': 2.862313985824585, 'learning_rate': 5.242513347857905e-05, 'epoch': 3.34}
2025-10-14 17:20:03 | {'loss': 0.607, 'grad_norm': 3.168855667114258, 'learning_rate': 5.058823321654053e-05, 'epoch': 3.47}
2025-10-14 17:20:09 | {'loss': 0.6204, 'grad_norm': 2.944788932800293, 'learning_rate': 4.8751332954502024e-05, 'epoch': 3.59}
2025-10-14 17:20:15 | {'loss': 0.6172, 'grad_norm': 3.0118844509124756, 'learning_rate': 4.6914432692463516e-05, 'epoch': 3.72}
2025-10-14 17:20:21 | {'loss': 0.6153, 'grad_norm': 3.557295799255371, 'learning_rate': 4.507753243042501e-05, 'epoch': 3.85}
2025-10-14 17:20:28 | {'loss': 0.6151, 'grad_norm': 3.1502809524536133, 'learning_rate': 4.32406321683865e-05, 'epoch': 3.98}
2025-10-14 17:20:55 | {'eval_loss': 1.4589293003082275, 'eval_rouge1': 0.4637344074731233, 'eval_rouge2': 0.302224559503979, 'eval_rougeL': 0.45561808046902075, 'eval_rouge_sum': 1.221577047446123, 'eval_runtime': 26.5424, 'eval_samples_per_second': 18.8, 'eval_steps_per_second': 1.206, 'epoch': 4.0}
2025-10-14 17:21:03 | {'loss': 0.4483, 'grad_norm': 2.8078761100769043, 'learning_rate': 4.140373190634799e-05, 'epoch': 4.11}
2025-10-14 17:21:09 | {'loss': 0.4193, 'grad_norm': 2.5768532752990723, 'learning_rate': 3.9566831644309476e-05, 'epoch': 4.24}
2025-10-14 17:21:15 | {'loss': 0.4257, 'grad_norm': 2.7174367904663086, 'learning_rate': 3.7729931382270974e-05, 'epoch': 4.36}
2025-10-14 17:21:21 | {'loss': 0.4261, 'grad_norm': 2.26850962638855, 'learning_rate': 3.5893031120232466e-05, 'epoch': 4.49}
2025-10-14 17:21:28 | {'loss': 0.4354, 'grad_norm': 2.743603467941284, 'learning_rate': 3.405613085819396e-05, 'epoch': 4.62}
2025-10-14 17:21:34 | {'loss': 0.4394, 'grad_norm': 2.46821665763855, 'learning_rate': 3.221923059615544e-05, 'epoch': 4.75}
2025-10-14 17:21:40 | {'loss': 0.4317, 'grad_norm': 2.7267889976501465, 'learning_rate': 3.0382330334116934e-05, 'epoch': 4.88}
2025-10-14 17:22:14 | {'eval_loss': 1.5312435626983643, 'eval_rouge1': 0.46605920234586234, 'eval_rouge2': 0.30247866001912793, 'eval_rougeL': 0.4576226771381787, 'eval_rouge_sum': 1.226160539503169, 'eval_runtime': 28.0929, 'eval_samples_per_second': 17.762, 'eval_steps_per_second': 1.139, 'epoch': 5.0}
2025-10-14 17:22:16 | {'loss': 0.4337, 'grad_norm': 2.267749547958374, 'learning_rate': 2.8545430072078426e-05, 'epoch': 5.01}
2025-10-14 17:22:22 | {'loss': 0.3005, 'grad_norm': 2.3187224864959717, 'learning_rate': 2.6708529810039918e-05, 'epoch': 5.13}
2025-10-14 17:22:28 | {'loss': 0.3008, 'grad_norm': 2.395148992538452, 'learning_rate': 2.4871629548001413e-05, 'epoch': 5.26}
2025-10-14 17:22:35 | {'loss': 0.3061, 'grad_norm': 2.4464597702026367, 'learning_rate': 2.30347292859629e-05, 'epoch': 5.39}
2025-10-14 17:22:41 | {'loss': 0.3068, 'grad_norm': 2.78599214553833, 'learning_rate': 2.1197829023924393e-05, 'epoch': 5.52}
2025-10-14 17:22:47 | {'loss': 0.3161, 'grad_norm': 2.5516910552978516, 'learning_rate': 1.9360928761885885e-05, 'epoch': 5.65}
2025-10-14 17:22:53 | {'loss': 0.3121, 'grad_norm': 2.4727742671966553, 'learning_rate': 1.7524028499847373e-05, 'epoch': 5.78}
2025-10-14 17:22:59 | {'loss': 0.3061, 'grad_norm': 2.3627851009368896, 'learning_rate': 1.5687128237808865e-05, 'epoch': 5.91}
2025-10-14 17:23:32 | {'eval_loss': 1.5839393138885498, 'eval_rouge1': 0.4629226882825411, 'eval_rouge2': 0.3010843620209411, 'eval_rougeL': 0.4528674195251933, 'eval_rouge_sum': 1.2168744698286755, 'eval_runtime': 28.0298, 'eval_samples_per_second': 17.803, 'eval_steps_per_second': 1.142, 'epoch': 6.0}
2025-10-14 17:23:35 | {'loss': 0.29, 'grad_norm': 2.3846893310546875, 'learning_rate': 1.3850227975770356e-05, 'epoch': 6.03}
2025-10-14 17:23:41 | {'loss': 0.2298, 'grad_norm': 1.972830891609192, 'learning_rate': 1.2013327713731847e-05, 'epoch': 6.16}
2025-10-14 17:23:47 | {'loss': 0.2307, 'grad_norm': 2.2990262508392334, 'learning_rate': 1.0176427451693338e-05, 'epoch': 6.29}
2025-10-14 17:23:53 | {'loss': 0.2296, 'grad_norm': 2.644270658493042, 'learning_rate': 8.33952718965483e-06, 'epoch': 6.42}
2025-10-14 17:23:59 | {'loss': 0.2291, 'grad_norm': 2.071981430053711, 'learning_rate': 6.502626927616322e-06, 'epoch': 6.55}
2025-10-14 17:24:05 | {'loss': 0.2295, 'grad_norm': 2.169865369796753, 'learning_rate': 4.6657266655778125e-06, 'epoch': 6.68}
2025-10-14 17:24:11 | {'loss': 0.2298, 'grad_norm': 2.1995112895965576, 'learning_rate': 2.828826403539304e-06, 'epoch': 6.8}
2025-10-14 17:24:17 | {'loss': 0.2258, 'grad_norm': 1.9575810432434082, 'learning_rate': 9.91926141500795e-07, 'epoch': 6.93}
2025-10-14 17:24:48 | {'eval_loss': 1.6161960363388062, 'eval_rouge1': 0.46663992304809543, 'eval_rouge2': 0.3076943919961224, 'eval_rougeL': 0.4578679089974126, 'eval_rouge_sum': 1.2322022240416306, 'eval_runtime': 28.0774, 'eval_samples_per_second': 17.772, 'eval_steps_per_second': 1.14, 'epoch': 7.0}
2025-10-14 17:24:50 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 17:24:50 | {'train_runtime': 537.2474, 'train_samples_per_second': 162.307, 'train_steps_per_second': 10.15, 'train_loss': 0.740433230348493, 'epoch': 7.0}
2025-10-14 17:24:50 | 최종 모델 저장 중...
2025-10-14 17:24:51 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 17:24:51 | 최종 평가 중...
2025-10-14 17:25:18 | 최종 평가 결과:
2025-10-14 17:25:18 | eval_rouge1: 0.4666
2025-10-14 17:25:18 | eval_rouge2: 0.3077
2025-10-14 17:25:18 | eval_rougeL: 0.4579
2025-10-14 17:25:18 | eval_rouge_sum: 1.2322
2025-10-14 17:25:18 | ============================================================
2025-10-14 17:25:18 | ✅ 학습 완료!
2025-10-14 17:25:18 | ============================================================
2025-10-14 17:25:18 | 모델 평가 중...
2025-10-14 17:25:45 | → 메트릭 'eval_rougeL' 사용: 0.4579
2025-10-14 17:25:45 | Trial 10 완료
2025-10-14 17:25:45 | - ROUGE-L F1: 0.4579
2025-10-14 17:25:45 | [I 2025-10-14 17:25:45,154] Trial 10 finished with value: 0.4578679089974126 and parameters: {'learning_rate': 9.098166997876735e-05, 'num_epochs': 7, 'warmup_ratio': 0.001509463863258606, 'weight_decay': 0.09550722292778, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.8868904289022379}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 17:25:45 | 💾 Trial 10 체크포인트 저장
2025-10-14 17:25:45 | ============================================================
2025-10-14 17:25:45 | Trial 11 시작
2025-10-14 17:25:45 | 파라미터: {'learning_rate': 9.138518360133624e-05, 'num_epochs': 7, 'warmup_ratio': 0.0013572013949127268, 'weight_decay': 0.09953784597545408, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9383576982529792}
2025-10-14 17:25:45 | ============================================================
2025-10-14 17:25:45 | 모델 타입: encoder_decoder
2025-10-14 17:25:45 | ============================================================
2025-10-14 17:25:45 | 모델 및 토크나이저 로딩 시작
2025-10-14 17:25:45 | ============================================================
2025-10-14 17:25:45 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 17:25:45 | 모델 로딩: digit82/kobart-summarization
2025-10-14 17:25:45 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 17:25:47 | → 디바이스: cuda
2025-10-14 17:25:47 | → 전체 파라미터: 123,859,968
2025-10-14 17:25:47 | → 학습 가능 파라미터: 123,859,968
2025-10-14 17:25:47 | ============================================================
2025-10-14 17:25:47 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 17:25:47 | ============================================================
2025-10-14 17:25:47 | ============================================================
2025-10-14 17:25:47 | 모델 학습 시작
2025-10-14 17:25:47 | ============================================================
2025-10-14 17:25:47 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 17:25:47 | 학습 진행 중...
2025-10-14 17:25:54 | {'loss': 2.0546, 'grad_norm': 4.776486873626709, 'learning_rate': 1.8094266353064577e-05, 'epoch': 0.13}
2025-10-14 17:26:00 | {'loss': 1.6338, 'grad_norm': 3.876291036605835, 'learning_rate': 3.637130307333183e-05, 'epoch': 0.26}
2025-10-14 17:26:06 | {'loss': 1.5339, 'grad_norm': 3.9868533611297607, 'learning_rate': 5.464833979359907e-05, 'epoch': 0.39}
2025-10-14 17:26:12 | {'loss': 1.5, 'grad_norm': 4.030797004699707, 'learning_rate': 7.292537651386632e-05, 'epoch': 0.51}
2025-10-14 17:26:19 | {'loss': 1.5077, 'grad_norm': 3.531557559967041, 'learning_rate': 9.120241323413357e-05, 'epoch': 0.64}
2025-10-14 17:26:25 | {'loss': 1.4795, 'grad_norm': 3.028987169265747, 'learning_rate': 8.955858695757845e-05, 'epoch': 0.77}
2025-10-14 17:26:31 | {'loss': 1.473, 'grad_norm': 3.3493900299072266, 'learning_rate': 8.77135398426716e-05, 'epoch': 0.9}
2025-10-14 17:27:03 | {'eval_loss': 1.3857598304748535, 'eval_rouge1': 0.3956786287923795, 'eval_rouge2': 0.2501286945866507, 'eval_rougeL': 0.390324308207598, 'eval_rouge_sum': 1.0361316315866282, 'eval_runtime': 26.6469, 'eval_samples_per_second': 18.726, 'eval_steps_per_second': 1.201, 'epoch': 1.0}
2025-10-14 17:27:06 | {'loss': 1.4129, 'grad_norm': 3.1809346675872803, 'learning_rate': 8.586849272776476e-05, 'epoch': 1.03}
2025-10-14 17:27:12 | {'loss': 1.1732, 'grad_norm': 3.552659273147583, 'learning_rate': 8.402344561285791e-05, 'epoch': 1.16}
2025-10-14 17:27:18 | {'loss': 1.1866, 'grad_norm': 2.9907643795013428, 'learning_rate': 8.217839849795107e-05, 'epoch': 1.28}
2025-10-14 17:27:24 | {'loss': 1.1801, 'grad_norm': 3.317565679550171, 'learning_rate': 8.033335138304421e-05, 'epoch': 1.41}
2025-10-14 17:27:30 | {'loss': 1.1766, 'grad_norm': 3.0339739322662354, 'learning_rate': 7.848830426813736e-05, 'epoch': 1.54}
2025-10-14 17:27:36 | {'loss': 1.1895, 'grad_norm': 3.4165613651275635, 'learning_rate': 7.66432571532305e-05, 'epoch': 1.67}
2025-10-14 17:27:42 | {'loss': 1.176, 'grad_norm': 2.756385087966919, 'learning_rate': 7.479821003832366e-05, 'epoch': 1.8}
2025-10-14 17:27:48 | {'loss': 1.1746, 'grad_norm': 3.6988868713378906, 'learning_rate': 7.295316292341681e-05, 'epoch': 1.93}
2025-10-14 17:28:19 | {'eval_loss': 1.3144266605377197, 'eval_rouge1': 0.4408872609612695, 'eval_rouge2': 0.2863495589281526, 'eval_rougeL': 0.4355619013931534, 'eval_rouge_sum': 1.1627987212825754, 'eval_runtime': 27.5216, 'eval_samples_per_second': 18.131, 'eval_steps_per_second': 1.163, 'epoch': 2.0}
2025-10-14 17:28:23 | {'loss': 1.0199, 'grad_norm': 2.855410575866699, 'learning_rate': 7.110811580850997e-05, 'epoch': 2.05}
2025-10-14 17:28:29 | {'loss': 0.8304, 'grad_norm': 3.128227949142456, 'learning_rate': 6.926306869360312e-05, 'epoch': 2.18}
2025-10-14 17:28:35 | {'loss': 0.8469, 'grad_norm': 2.9511334896087646, 'learning_rate': 6.741802157869627e-05, 'epoch': 2.31}
2025-10-14 17:28:41 | {'loss': 0.8493, 'grad_norm': 2.98684024810791, 'learning_rate': 6.557297446378942e-05, 'epoch': 2.44}
2025-10-14 17:28:48 | {'loss': 0.8627, 'grad_norm': 3.4433000087738037, 'learning_rate': 6.372792734888256e-05, 'epoch': 2.57}
2025-10-14 17:28:54 | {'loss': 0.8548, 'grad_norm': 3.0677473545074463, 'learning_rate': 6.188288023397572e-05, 'epoch': 2.7}
2025-10-14 17:28:59 | {'loss': 0.8566, 'grad_norm': 2.9999608993530273, 'learning_rate': 6.0037833119068865e-05, 'epoch': 2.82}
2025-10-14 17:29:05 | {'loss': 0.8634, 'grad_norm': 3.009517192840576, 'learning_rate': 5.819278600416202e-05, 'epoch': 2.95}
2025-10-14 17:29:36 | {'eval_loss': 1.3671983480453491, 'eval_rouge1': 0.4414622421566651, 'eval_rouge2': 0.28554361842082787, 'eval_rougeL': 0.4338882742826854, 'eval_rouge_sum': 1.1608941348601785, 'eval_runtime': 28.1564, 'eval_samples_per_second': 17.722, 'eval_steps_per_second': 1.137, 'epoch': 3.0}
2025-10-14 17:29:41 | {'loss': 0.7001, 'grad_norm': 2.7176690101623535, 'learning_rate': 5.6347738889255175e-05, 'epoch': 3.08}
2025-10-14 17:29:48 | {'loss': 0.5861, 'grad_norm': 2.8029849529266357, 'learning_rate': 5.450269177434832e-05, 'epoch': 3.21}
2025-10-14 17:29:54 | {'loss': 0.6045, 'grad_norm': 2.7916183471679688, 'learning_rate': 5.265764465944148e-05, 'epoch': 3.34}
2025-10-14 17:29:59 | {'loss': 0.6086, 'grad_norm': 2.9302730560302734, 'learning_rate': 5.081259754453462e-05, 'epoch': 3.47}
2025-10-14 17:30:05 | {'loss': 0.6247, 'grad_norm': 3.0230183601379395, 'learning_rate': 4.8967550429627774e-05, 'epoch': 3.59}
2025-10-14 17:30:11 | {'loss': 0.6166, 'grad_norm': 3.0489261150360107, 'learning_rate': 4.712250331472092e-05, 'epoch': 3.72}
2025-10-14 17:30:18 | {'loss': 0.6184, 'grad_norm': 3.5907206535339355, 'learning_rate': 4.527745619981408e-05, 'epoch': 3.85}
2025-10-14 17:30:23 | {'loss': 0.6158, 'grad_norm': 3.1700727939605713, 'learning_rate': 4.3432409084907225e-05, 'epoch': 3.98}
2025-10-14 17:30:51 | {'eval_loss': 1.462158203125, 'eval_rouge1': 0.46626197165418015, 'eval_rouge2': 0.3024821242229744, 'eval_rougeL': 0.45698482841623617, 'eval_rouge_sum': 1.2257289242933906, 'eval_runtime': 26.6369, 'eval_samples_per_second': 18.733, 'eval_steps_per_second': 1.201, 'epoch': 4.0}
2025-10-14 17:30:58 | {'loss': 0.4492, 'grad_norm': 2.6890337467193604, 'learning_rate': 4.1587361970000374e-05, 'epoch': 4.11}
2025-10-14 17:31:03 | {'loss': 0.4226, 'grad_norm': 2.7429916858673096, 'learning_rate': 3.974231485509353e-05, 'epoch': 4.24}
2025-10-14 17:31:09 | {'loss': 0.4265, 'grad_norm': 2.641629219055176, 'learning_rate': 3.7897267740186683e-05, 'epoch': 4.36}
2025-10-14 17:31:15 | {'loss': 0.4301, 'grad_norm': 2.2063984870910645, 'learning_rate': 3.605222062527983e-05, 'epoch': 4.49}
2025-10-14 17:31:21 | {'loss': 0.4376, 'grad_norm': 2.765047788619995, 'learning_rate': 3.420717351037298e-05, 'epoch': 4.62}
2025-10-14 17:31:27 | {'loss': 0.4433, 'grad_norm': 2.9463772773742676, 'learning_rate': 3.2362126395466135e-05, 'epoch': 4.75}
2025-10-14 17:31:33 | {'loss': 0.4324, 'grad_norm': 2.8159565925598145, 'learning_rate': 3.0517079280559283e-05, 'epoch': 4.88}
2025-10-14 17:32:04 | {'eval_loss': 1.5353574752807617, 'eval_rouge1': 0.46300848338235806, 'eval_rouge2': 0.2979376912721931, 'eval_rougeL': 0.45384393817575963, 'eval_rouge_sum': 1.2147901128303107, 'eval_runtime': 25.9902, 'eval_samples_per_second': 19.2, 'eval_steps_per_second': 1.231, 'epoch': 5.0}
2025-10-14 17:32:06 | {'loss': 0.4352, 'grad_norm': 2.175798177719116, 'learning_rate': 2.8672032165652435e-05, 'epoch': 5.01}
2025-10-14 17:32:12 | {'loss': 0.303, 'grad_norm': 2.2855560779571533, 'learning_rate': 2.6826985050745583e-05, 'epoch': 5.13}
2025-10-14 17:32:18 | {'loss': 0.3027, 'grad_norm': 2.430454969406128, 'learning_rate': 2.4981937935838738e-05, 'epoch': 5.26}
2025-10-14 17:32:24 | {'loss': 0.3072, 'grad_norm': 2.697831392288208, 'learning_rate': 2.313689082093189e-05, 'epoch': 5.39}
2025-10-14 17:32:31 | {'loss': 0.3087, 'grad_norm': 2.3698670864105225, 'learning_rate': 2.1291843706025037e-05, 'epoch': 5.52}
2025-10-14 17:32:36 | {'loss': 0.3153, 'grad_norm': 2.5333001613616943, 'learning_rate': 1.944679659111819e-05, 'epoch': 5.65}
2025-10-14 17:32:42 | {'loss': 0.3136, 'grad_norm': 2.499483823776245, 'learning_rate': 1.760174947621134e-05, 'epoch': 5.78}
2025-10-14 17:32:48 | {'loss': 0.3061, 'grad_norm': 2.360344171524048, 'learning_rate': 1.5756702361304492e-05, 'epoch': 5.91}
2025-10-14 17:33:20 | {'eval_loss': 1.5820239782333374, 'eval_rouge1': 0.46011721054438065, 'eval_rouge2': 0.2959809635171194, 'eval_rougeL': 0.45147669041090555, 'eval_rouge_sum': 1.2075748644724056, 'eval_runtime': 27.9412, 'eval_samples_per_second': 17.859, 'eval_steps_per_second': 1.145, 'epoch': 6.0}
2025-10-14 17:33:23 | {'loss': 0.2922, 'grad_norm': 2.437575101852417, 'learning_rate': 1.3911655246397642e-05, 'epoch': 6.03}
2025-10-14 17:33:29 | {'loss': 0.2296, 'grad_norm': 2.0708963871002197, 'learning_rate': 1.2066608131490792e-05, 'epoch': 6.16}
2025-10-14 17:33:36 | {'loss': 0.2317, 'grad_norm': 2.223762035369873, 'learning_rate': 1.0221561016583943e-05, 'epoch': 6.29}
2025-10-14 17:33:42 | {'loss': 0.2301, 'grad_norm': 2.559574604034424, 'learning_rate': 8.376513901677095e-06, 'epoch': 6.42}
2025-10-14 17:33:48 | {'loss': 0.2307, 'grad_norm': 2.2423274517059326, 'learning_rate': 6.5314667867702465e-06, 'epoch': 6.55}
2025-10-14 17:33:54 | {'loss': 0.2311, 'grad_norm': 2.2250888347625732, 'learning_rate': 4.686419671863396e-06, 'epoch': 6.68}
2025-10-14 17:34:00 | {'loss': 0.23, 'grad_norm': 2.176206350326538, 'learning_rate': 2.8413725569565475e-06, 'epoch': 6.8}
2025-10-14 17:34:07 | {'loss': 0.2241, 'grad_norm': 2.070352554321289, 'learning_rate': 9.963254420496987e-07, 'epoch': 6.93}
2025-10-14 17:34:37 | {'eval_loss': 1.6216187477111816, 'eval_rouge1': 0.46418379200134663, 'eval_rouge2': 0.29900655396507786, 'eval_rougeL': 0.45406971881112107, 'eval_rouge_sum': 1.2172600647775456, 'eval_runtime': 27.344, 'eval_samples_per_second': 18.249, 'eval_steps_per_second': 1.17, 'epoch': 7.0}
2025-10-14 17:34:39 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 17:34:39 | {'train_runtime': 532.0072, 'train_samples_per_second': 163.906, 'train_steps_per_second': 10.25, 'train_loss': 0.7420596333134181, 'epoch': 7.0}
2025-10-14 17:34:39 | 최종 모델 저장 중...
2025-10-14 17:34:40 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 17:34:40 | 최종 평가 중...
2025-10-14 17:35:08 | 최종 평가 결과:
2025-10-14 17:35:08 | eval_rouge1: 0.4663
2025-10-14 17:35:08 | eval_rouge2: 0.3025
2025-10-14 17:35:08 | eval_rougeL: 0.4570
2025-10-14 17:35:08 | eval_rouge_sum: 1.2257
2025-10-14 17:35:08 | ============================================================
2025-10-14 17:35:08 | ✅ 학습 완료!
2025-10-14 17:35:08 | ============================================================
2025-10-14 17:35:08 | 모델 평가 중...
2025-10-14 17:35:35 | → 메트릭 'eval_rougeL' 사용: 0.4570
2025-10-14 17:35:35 | Trial 11 완료
2025-10-14 17:35:35 | - ROUGE-L F1: 0.4570
2025-10-14 17:35:35 | [I 2025-10-14 17:35:35,660] Trial 11 finished with value: 0.45698482841623617 and parameters: {'learning_rate': 9.138518360133624e-05, 'num_epochs': 7, 'warmup_ratio': 0.0013572013949127268, 'weight_decay': 0.09953784597545408, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9383576982529792}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 17:35:35 | 💾 Trial 11 체크포인트 저장
2025-10-14 17:35:35 | ============================================================
2025-10-14 17:35:35 | Trial 12 시작
2025-10-14 17:35:35 | 파라미터: {'learning_rate': 4.0171644628004966e-05, 'num_epochs': 8, 'warmup_ratio': 0.0645170114468336, 'weight_decay': 0.09988633091284026, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9377994135645444}
2025-10-14 17:35:35 | ============================================================
2025-10-14 17:35:35 | 모델 타입: encoder_decoder
2025-10-14 17:35:35 | ============================================================
2025-10-14 17:35:35 | 모델 및 토크나이저 로딩 시작
2025-10-14 17:35:35 | ============================================================
2025-10-14 17:35:35 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 17:35:36 | 모델 로딩: digit82/kobart-summarization
2025-10-14 17:35:36 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 17:35:37 | → 디바이스: cuda
2025-10-14 17:35:37 | → 전체 파라미터: 123,859,968
2025-10-14 17:35:37 | → 학습 가능 파라미터: 123,859,968
2025-10-14 17:35:37 | ============================================================
2025-10-14 17:35:37 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 17:35:37 | ============================================================
2025-10-14 17:35:37 | ============================================================
2025-10-14 17:35:37 | 모델 학습 시작
2025-10-14 17:35:37 | ============================================================
2025-10-14 17:35:37 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 17:35:37 | 학습 진행 중...
2025-10-14 17:35:43 | {'loss': 2.2038, 'grad_norm': 4.853784084320068, 'learning_rate': 7.953985636344983e-06, 'epoch': 0.13}
2025-10-14 17:35:50 | {'loss': 1.6953, 'grad_norm': 4.193289756774902, 'learning_rate': 1.5988314561945976e-05, 'epoch': 0.26}
2025-10-14 17:35:56 | {'loss': 1.566, 'grad_norm': 4.187561988830566, 'learning_rate': 2.4022643487546967e-05, 'epoch': 0.39}
2025-10-14 17:36:02 | {'loss': 1.5127, 'grad_norm': 3.8635079860687256, 'learning_rate': 3.205697241314796e-05, 'epoch': 0.51}
2025-10-14 17:36:07 | {'loss': 1.4982, 'grad_norm': 3.7268128395080566, 'learning_rate': 4.0091301338748956e-05, 'epoch': 0.64}
2025-10-14 17:36:13 | {'loss': 1.4591, 'grad_norm': 3.178403854370117, 'learning_rate': 3.947782173579064e-05, 'epoch': 0.77}
2025-10-14 17:36:20 | {'loss': 1.4483, 'grad_norm': 3.6180436611175537, 'learning_rate': 3.877699053153375e-05, 'epoch': 0.9}
2025-10-14 17:36:52 | {'eval_loss': 1.3545100688934326, 'eval_rouge1': 0.39151434617087283, 'eval_rouge2': 0.2542481300525252, 'eval_rougeL': 0.3868466067636855, 'eval_rouge_sum': 1.0326090829870835, 'eval_runtime': 27.0739, 'eval_samples_per_second': 18.431, 'eval_steps_per_second': 1.182, 'epoch': 1.0}
2025-10-14 17:36:55 | {'loss': 1.4031, 'grad_norm': 3.3116300106048584, 'learning_rate': 3.807615932727686e-05, 'epoch': 1.03}
2025-10-14 17:37:01 | {'loss': 1.2415, 'grad_norm': 3.6472525596618652, 'learning_rate': 3.737532812301997e-05, 'epoch': 1.16}
2025-10-14 17:37:06 | {'loss': 1.249, 'grad_norm': 3.419543504714966, 'learning_rate': 3.6674496918763084e-05, 'epoch': 1.28}
2025-10-14 17:37:12 | {'loss': 1.2365, 'grad_norm': 3.8838658332824707, 'learning_rate': 3.597366571450619e-05, 'epoch': 1.41}
2025-10-14 17:37:18 | {'loss': 1.2299, 'grad_norm': 3.394847869873047, 'learning_rate': 3.5272834510249305e-05, 'epoch': 1.54}
2025-10-14 17:37:24 | {'loss': 1.2418, 'grad_norm': 3.138421058654785, 'learning_rate': 3.457200330599241e-05, 'epoch': 1.67}
2025-10-14 17:37:30 | {'loss': 1.2249, 'grad_norm': 3.1301143169403076, 'learning_rate': 3.387117210173552e-05, 'epoch': 1.8}
2025-10-14 17:37:36 | {'loss': 1.2208, 'grad_norm': 3.4183571338653564, 'learning_rate': 3.317034089747863e-05, 'epoch': 1.93}
2025-10-14 17:38:07 | {'eval_loss': 1.2917762994766235, 'eval_rouge1': 0.43396419045328927, 'eval_rouge2': 0.27921891832623513, 'eval_rougeL': 0.42779700515446606, 'eval_rouge_sum': 1.1409801139339906, 'eval_runtime': 27.4417, 'eval_samples_per_second': 18.184, 'eval_steps_per_second': 1.166, 'epoch': 2.0}
2025-10-14 17:38:11 | {'loss': 1.1138, 'grad_norm': 3.197613477706909, 'learning_rate': 3.246950969322174e-05, 'epoch': 2.05}
2025-10-14 17:38:17 | {'loss': 0.9918, 'grad_norm': 3.210344076156616, 'learning_rate': 3.1768678488964845e-05, 'epoch': 2.18}
2025-10-14 17:38:23 | {'loss': 1.0035, 'grad_norm': 3.2544009685516357, 'learning_rate': 3.106784728470796e-05, 'epoch': 2.31}
2025-10-14 17:38:30 | {'loss': 1.0014, 'grad_norm': 3.1776230335235596, 'learning_rate': 3.0367016080451067e-05, 'epoch': 2.44}
2025-10-14 17:38:35 | {'loss': 1.012, 'grad_norm': 3.6453797817230225, 'learning_rate': 2.9666184876194174e-05, 'epoch': 2.57}
2025-10-14 17:38:41 | {'loss': 0.9943, 'grad_norm': 3.4667820930480957, 'learning_rate': 2.896535367193729e-05, 'epoch': 2.7}
2025-10-14 17:38:47 | {'loss': 0.9989, 'grad_norm': 3.3208415508270264, 'learning_rate': 2.8264522467680396e-05, 'epoch': 2.82}
2025-10-14 17:38:53 | {'loss': 1.001, 'grad_norm': 3.299968957901001, 'learning_rate': 2.7563691263423503e-05, 'epoch': 2.95}
2025-10-14 17:39:25 | {'eval_loss': 1.3001456260681152, 'eval_rouge1': 0.43274278530906335, 'eval_rouge2': 0.2770096359462398, 'eval_rougeL': 0.4266194448199325, 'eval_rouge_sum': 1.1363718660752355, 'eval_runtime': 29.8677, 'eval_samples_per_second': 16.707, 'eval_steps_per_second': 1.071, 'epoch': 3.0}
2025-10-14 17:39:30 | {'loss': 0.8938, 'grad_norm': 2.987555503845215, 'learning_rate': 2.6862860059166617e-05, 'epoch': 3.08}
2025-10-14 17:39:37 | {'loss': 0.8121, 'grad_norm': 3.4140396118164062, 'learning_rate': 2.6162028854909725e-05, 'epoch': 3.21}
2025-10-14 17:39:42 | {'loss': 0.8281, 'grad_norm': 3.4006028175354004, 'learning_rate': 2.5461197650652832e-05, 'epoch': 3.34}
2025-10-14 17:39:48 | {'loss': 0.8229, 'grad_norm': 3.308950185775757, 'learning_rate': 2.4760366446395943e-05, 'epoch': 3.47}
2025-10-14 17:39:54 | {'loss': 0.8417, 'grad_norm': 3.4004452228546143, 'learning_rate': 2.4059535242139054e-05, 'epoch': 3.59}
2025-10-14 17:40:00 | {'loss': 0.8401, 'grad_norm': 3.8159217834472656, 'learning_rate': 2.335870403788216e-05, 'epoch': 3.72}
2025-10-14 17:40:06 | {'loss': 0.8302, 'grad_norm': 3.7556653022766113, 'learning_rate': 2.265787283362527e-05, 'epoch': 3.85}
2025-10-14 17:40:13 | {'loss': 0.8318, 'grad_norm': 3.4574391841888428, 'learning_rate': 2.195704162936838e-05, 'epoch': 3.98}
2025-10-14 17:40:43 | {'eval_loss': 1.3465967178344727, 'eval_rouge1': 0.44922926191720747, 'eval_rouge2': 0.28864885648877653, 'eval_rougeL': 0.44313316122665475, 'eval_rouge_sum': 1.1810112796326386, 'eval_runtime': 29.5511, 'eval_samples_per_second': 16.886, 'eval_steps_per_second': 1.083, 'epoch': 4.0}
2025-10-14 17:40:50 | {'loss': 0.7029, 'grad_norm': 3.4403858184814453, 'learning_rate': 2.125621042511149e-05, 'epoch': 4.11}
2025-10-14 17:40:56 | {'loss': 0.681, 'grad_norm': 3.344799518585205, 'learning_rate': 2.05553792208546e-05, 'epoch': 4.24}
2025-10-14 17:41:02 | {'loss': 0.6868, 'grad_norm': 3.6291024684906006, 'learning_rate': 1.9854548016597708e-05, 'epoch': 4.36}
2025-10-14 17:41:08 | {'loss': 0.6894, 'grad_norm': 3.1607909202575684, 'learning_rate': 1.915371681234082e-05, 'epoch': 4.49}
2025-10-14 17:41:14 | {'loss': 0.6976, 'grad_norm': 3.561340570449829, 'learning_rate': 1.845288560808393e-05, 'epoch': 4.62}
2025-10-14 17:41:20 | {'loss': 0.7048, 'grad_norm': 3.4850809574127197, 'learning_rate': 1.7752054403827037e-05, 'epoch': 4.75}
2025-10-14 17:41:26 | {'loss': 0.696, 'grad_norm': 3.4920904636383057, 'learning_rate': 1.7051223199570147e-05, 'epoch': 4.88}
2025-10-14 17:41:59 | {'eval_loss': 1.3788566589355469, 'eval_rouge1': 0.4640684467489107, 'eval_rouge2': 0.3010018477110556, 'eval_rougeL': 0.45291871360518954, 'eval_rouge_sum': 1.2179890080651559, 'eval_runtime': 26.8173, 'eval_samples_per_second': 18.607, 'eval_steps_per_second': 1.193, 'epoch': 5.0}
2025-10-14 17:42:01 | {'loss': 0.703, 'grad_norm': 3.446899890899658, 'learning_rate': 1.6350391995313258e-05, 'epoch': 5.01}
2025-10-14 17:42:07 | {'loss': 0.5782, 'grad_norm': 3.024078607559204, 'learning_rate': 1.5649560791056366e-05, 'epoch': 5.13}
2025-10-14 17:42:12 | {'loss': 0.5795, 'grad_norm': 3.407186985015869, 'learning_rate': 1.4948729586799475e-05, 'epoch': 5.26}
2025-10-14 17:42:18 | {'loss': 0.5911, 'grad_norm': 3.102543354034424, 'learning_rate': 1.4247898382542585e-05, 'epoch': 5.39}
2025-10-14 17:42:25 | {'loss': 0.5895, 'grad_norm': 3.429455518722534, 'learning_rate': 1.3547067178285694e-05, 'epoch': 5.52}
2025-10-14 17:42:31 | {'loss': 0.6066, 'grad_norm': 3.519929885864258, 'learning_rate': 1.2846235974028804e-05, 'epoch': 5.65}
2025-10-14 17:42:37 | {'loss': 0.6051, 'grad_norm': 3.6948673725128174, 'learning_rate': 1.2145404769771913e-05, 'epoch': 5.78}
2025-10-14 17:42:43 | {'loss': 0.5942, 'grad_norm': 3.3696954250335693, 'learning_rate': 1.1444573565515023e-05, 'epoch': 5.91}
2025-10-14 17:43:13 | {'eval_loss': 1.420929193496704, 'eval_rouge1': 0.45559095822215956, 'eval_rouge2': 0.29447008284481996, 'eval_rougeL': 0.4458197306099868, 'eval_rouge_sum': 1.1958807716769662, 'eval_runtime': 26.4942, 'eval_samples_per_second': 18.834, 'eval_steps_per_second': 1.208, 'epoch': 6.0}
2025-10-14 17:43:16 | {'loss': 0.5774, 'grad_norm': 4.728013038635254, 'learning_rate': 1.074374236125813e-05, 'epoch': 6.03}
2025-10-14 17:43:22 | {'loss': 0.5112, 'grad_norm': 2.865344524383545, 'learning_rate': 1.0042911157001241e-05, 'epoch': 6.16}
2025-10-14 17:43:29 | {'loss': 0.5205, 'grad_norm': 3.414278984069824, 'learning_rate': 9.34207995274435e-06, 'epoch': 6.29}
2025-10-14 17:43:35 | {'loss': 0.5187, 'grad_norm': 3.2660505771636963, 'learning_rate': 8.641248748487461e-06, 'epoch': 6.42}
2025-10-14 17:43:40 | {'loss': 0.5182, 'grad_norm': 3.0864391326904297, 'learning_rate': 7.94041754423057e-06, 'epoch': 6.55}
2025-10-14 17:43:46 | {'loss': 0.519, 'grad_norm': 3.1216845512390137, 'learning_rate': 7.239586339973679e-06, 'epoch': 6.68}
2025-10-14 17:43:52 | {'loss': 0.5245, 'grad_norm': 2.948829412460327, 'learning_rate': 6.5387551357167885e-06, 'epoch': 6.8}
2025-10-14 17:43:58 | {'loss': 0.5162, 'grad_norm': 3.6478271484375, 'learning_rate': 5.8379239314598975e-06, 'epoch': 6.93}
2025-10-14 17:44:32 | {'eval_loss': 1.4471133947372437, 'eval_rouge1': 0.4591203866823757, 'eval_rouge2': 0.29434134659275346, 'eval_rougeL': 0.4510817636814752, 'eval_rouge_sum': 1.2045434969566045, 'eval_runtime': 30.4865, 'eval_samples_per_second': 16.368, 'eval_steps_per_second': 1.05, 'epoch': 7.0}
2025-10-14 17:44:36 | {'loss': 0.4977, 'grad_norm': 2.5828866958618164, 'learning_rate': 5.137092727203008e-06, 'epoch': 7.06}
2025-10-14 17:44:42 | {'loss': 0.4683, 'grad_norm': 2.9783973693847656, 'learning_rate': 4.436261522946117e-06, 'epoch': 7.19}
2025-10-14 17:44:48 | {'loss': 0.474, 'grad_norm': 2.4646072387695312, 'learning_rate': 3.735430318689227e-06, 'epoch': 7.32}
2025-10-14 17:44:54 | {'loss': 0.4725, 'grad_norm': 3.2309811115264893, 'learning_rate': 3.0345991144323363e-06, 'epoch': 7.45}
2025-10-14 17:44:59 | {'loss': 0.4681, 'grad_norm': 3.0867416858673096, 'learning_rate': 2.3337679101754454e-06, 'epoch': 7.57}
2025-10-14 17:45:06 | {'loss': 0.4713, 'grad_norm': 3.007303476333618, 'learning_rate': 1.632936705918555e-06, 'epoch': 7.7}
2025-10-14 17:45:12 | {'loss': 0.4758, 'grad_norm': 3.160212278366089, 'learning_rate': 9.321055016616644e-07, 'epoch': 7.83}
2025-10-14 17:45:18 | {'loss': 0.4642, 'grad_norm': 2.7316982746124268, 'learning_rate': 2.3127429740477389e-07, 'epoch': 7.96}
2025-10-14 17:45:47 | {'eval_loss': 1.4637062549591064, 'eval_rouge1': 0.4613169303278624, 'eval_rouge2': 0.29852450805517927, 'eval_rougeL': 0.45304666779380914, 'eval_rouge_sum': 1.2128881061768508, 'eval_runtime': 26.6374, 'eval_samples_per_second': 18.733, 'eval_steps_per_second': 1.201, 'epoch': 8.0}
2025-10-14 17:45:48 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 17:45:48 | {'train_runtime': 610.8867, 'train_samples_per_second': 163.133, 'train_steps_per_second': 10.202, 'train_loss': 0.8681096427706914, 'epoch': 8.0}
2025-10-14 17:45:48 | 최종 모델 저장 중...
2025-10-14 17:45:49 | → 모델 저장 위치: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna/final_model
2025-10-14 17:45:49 | 최종 평가 중...
2025-10-14 17:46:17 | 최종 평가 결과:
2025-10-14 17:46:17 | eval_rouge1: 0.4641
2025-10-14 17:46:17 | eval_rouge2: 0.3010
2025-10-14 17:46:17 | eval_rougeL: 0.4529
2025-10-14 17:46:17 | eval_rouge_sum: 1.2180
2025-10-14 17:46:17 | ============================================================
2025-10-14 17:46:17 | ✅ 학습 완료!
2025-10-14 17:46:17 | ============================================================
2025-10-14 17:46:17 | 모델 평가 중...
2025-10-14 17:46:47 | → 메트릭 'eval_rougeL' 사용: 0.4529
2025-10-14 17:46:47 | Trial 12 완료
2025-10-14 17:46:47 | - ROUGE-L F1: 0.4529
2025-10-14 17:46:47 | [I 2025-10-14 17:46:47,291] Trial 12 finished with value: 0.45291871360518954 and parameters: {'learning_rate': 4.0171644628004966e-05, 'num_epochs': 8, 'warmup_ratio': 0.0645170114468336, 'weight_decay': 0.09988633091284026, 'scheduler_type': 'cosine', 'num_beams': 4, 'length_penalty': 0.9377994135645444}. Best is trial 4 with value: 0.45980874592447585.
2025-10-14 17:46:47 | 💾 Trial 12 체크포인트 저장
2025-10-14 17:46:47 | ======================================================================
2025-10-14 17:46:47 | Optuna 최적화 완료
2025-10-14 17:46:47 | ======================================================================
2025-10-14 17:46:47 | 최적 ROUGE-L F1: 0.4598
2025-10-14 17:46:47 | 최적 파라미터:
2025-10-14 17:46:47 | - learning_rate: 7.568292060167621e-05
2025-10-14 17:46:47 | - num_epochs: 10
2025-10-14 17:46:47 | - warmup_ratio: 0.11957999576221703
2025-10-14 17:46:47 | - weight_decay: 0.09218742350231168
2025-10-14 17:46:47 | - scheduler_type: polynomial
2025-10-14 17:46:47 | - num_beams: 6
2025-10-14 17:46:47 | - length_penalty: 0.9214017645310711
2025-10-14 17:46:47 | 결과 저장 중...
2025-10-14 17:46:47 | 최적 파라미터 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna/best_params.json
2025-10-14 17:46:47 | 전체 Trial 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna/all_trials.csv
2025-10-14 17:46:47 | Study 통계 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna/study_stats.json
2025-10-14 17:46:47 | - 완료: 11
2025-10-14 17:46:47 | - Pruned: 2
2025-10-14 17:46:47 | - 실패: 0
2025-10-14 17:46:47 | ============================================================
2025-10-14 17:46:47 | ✅ OPTUNA 튜닝 완료!
2025-10-14 17:46:47 | 📈 최고 ROUGE-L F1: 0.4598
2025-10-14 17:46:47 | 🎯 최적 하이퍼파라미터:
2025-10-14 17:46:47 | learning_rate: 7.568292060167621e-05
2025-10-14 17:46:47 | num_epochs: 10
2025-10-14 17:46:47 | warmup_ratio: 0.11957999576221703
2025-10-14 17:46:47 | weight_decay: 0.09218742350231168
2025-10-14 17:46:47 | scheduler_type: polynomial
2025-10-14 17:46:47 | num_beams: 6
2025-10-14 17:46:47 | length_penalty: 0.9214017645310711
2025-10-14 17:46:47 | ============================================================
2025-10-14 17:46:47 | 📂 결과 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna/optuna_results.json
2025-10-14 17:46:47 | 📂 최적 Config 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna/best_config.yaml
2025-10-14 17:46:47 | ============================================================
2025-10-14 17:46:47 | ✅ 학습 완료!
2025-10-14 17:46:47 | 📁 결과 저장: experiments/20251014/20251014_154616_kobart_ultimate_optuna
2025-10-14 17:46:47 | ============================================================
2025-10-14 17:46:47 | >> 로그 리디렉션 중료.
