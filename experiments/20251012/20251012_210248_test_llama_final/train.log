2025-10-12 21:02:48 | >> í‘œì¤€ ì¶œë ¥ ë° ì˜¤ë¥˜ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ë¦¬ë””ë ‰ì…˜ ì‹œì‘
2025-10-12 21:02:51 | ğŸ“Š SINGLE ëª¨ë“œ ì‹¤í–‰ ì¤‘...
2025-10-12 21:02:51 | ============================================================
2025-10-12 21:02:51 | ğŸš€ SINGLE MODEL ëª¨ë“œ í•™ìŠµ ì‹œì‘
2025-10-12 21:02:51 | ğŸ“‹ ëª¨ë¸: llama-3.2-korean-3b
2025-10-12 21:02:51 | ============================================================
2025-10-12 21:02:51 | [1/5] ë°ì´í„° ë¡œë”©...
2025-10-12 21:02:51 | âš ï¸ ë””ë²„ê·¸ ëª¨ë“œ: í•™ìŠµ 100ê°œ, ê²€ì¦ 20ê°œ
2025-10-12 21:02:51 | [2/5] Config ë¡œë”©...
2025-10-12 21:02:51 | âš™ï¸ Epochs ì˜¤ë²„ë¼ì´ë“œ: 1
2025-10-12 21:02:51 | âš™ï¸ Batch size ì˜¤ë²„ë¼ì´ë“œ: 4
2025-10-12 21:02:51 | âœ… Config ë¡œë“œ ì™„ë£Œ: llama-3.2-korean-3b
2025-10-12 21:02:51 | [3/5] ëª¨ë¸ ë¡œë”©...
2025-10-12 21:02:51 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-12 21:02:51 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 21:02:51 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 21:02:51 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 21:02:51 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 21:02:55 | Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.64s/it]
2025-10-12 21:02:56 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.88s/it]
2025-10-12 21:02:56 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 21:02:57 | íŒ¨ë”© í† í° ì„¤ì •: <|eot_id|>
2025-10-12 21:02:57 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 21:02:57 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 21:02:57 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 24,313,856 (0.75%)
2025-10-12 21:02:57 | ì „ì²´ íŒŒë¼ë¯¸í„°: 3,237,063,680
2025-10-12 21:02:57 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 21:02:57 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 21:02:57 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 21:02:57 | âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ
2025-10-12 21:02:57 | [4/5] Dataset ìƒì„±...
2025-10-12 21:02:57 | âœ… í•™ìŠµ Dataset: 100ê°œ
2025-10-12 21:02:57 | âœ… ê²€ì¦ Dataset: 20ê°œ
2025-10-12 21:02:57 | [5/5] í•™ìŠµ ì‹œì‘...
2025-10-12 21:02:57 | ============================================================
2025-10-12 21:02:57 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 21:02:57 | ============================================================
2025-10-12 21:02:57 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 21:02:58 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 21:02:58 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 21:02:58 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 21:02:58 | 0%|          | 0/25 [00:00<?, ?it/s]
2025-10-12 21:03:01 | 4%|â–         | 1/25 [00:03<01:18,  3.27s/it]
2025-10-12 21:03:03 | 8%|â–Š         | 2/25 [00:05<01:00,  2.63s/it]
2025-10-12 21:03:05 | 12%|â–ˆâ–        | 3/25 [00:07<00:48,  2.21s/it]
2025-10-12 21:03:06 | 16%|â–ˆâ–Œ        | 4/25 [00:08<00:41,  1.97s/it]
2025-10-12 21:03:08 | 20%|â–ˆâ–ˆ        | 5/25 [00:10<00:36,  1.84s/it]
2025-10-12 21:03:10 | 24%|â–ˆâ–ˆâ–       | 6/25 [00:12<00:33,  1.77s/it]
2025-10-12 21:03:11 | 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:13<00:31,  1.73s/it]
2025-10-12 21:03:13 | 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:15<00:28,  1.70s/it]
2025-10-12 21:03:15 | 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:16<00:26,  1.69s/it]
2025-10-12 21:03:16 | 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:18<00:25,  1.68s/it]
2025-10-12 21:03:16 | {'loss': 1.5811, 'grad_norm': 2.3844869136810303, 'learning_rate': 3.6e-07, 'epoch': 0.4}
2025-10-12 21:03:16 | 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:18<00:25,  1.68s/it]
2025-10-12 21:03:18 | 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:20<00:23,  1.67s/it]
2025-10-12 21:03:20 | 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:21<00:21,  1.66s/it]
2025-10-12 21:03:22 | 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:24<00:21,  1.83s/it]
2025-10-12 21:03:23 | 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:25<00:19,  1.77s/it]
2025-10-12 21:03:25 | 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:27<00:17,  1.72s/it]
2025-10-12 21:03:27 | 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:28<00:15,  1.69s/it]
2025-10-12 21:03:28 | 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:30<00:13,  1.67s/it]
2025-10-12 21:03:30 | 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:32<00:11,  1.66s/it]
2025-10-12 21:03:32 | 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:33<00:09,  1.65s/it]
2025-10-12 21:03:33 | 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:35<00:08,  1.65s/it]
2025-10-12 21:03:33 | {'loss': 1.6233, 'grad_norm': 2.510491132736206, 'learning_rate': 7.6e-07, 'epoch': 0.8}
2025-10-12 21:03:33 | 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:35<00:08,  1.65s/it]
2025-10-12 21:03:35 | 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:37<00:06,  1.64s/it]
2025-10-12 21:03:36 | 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:38<00:04,  1.64s/it]
2025-10-12 21:03:38 | 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:40<00:03,  1.64s/it]
2025-10-12 21:03:40 | 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:42<00:01,  1.64s/it]
2025-10-12 21:03:41 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:43<00:00,  1.65s/it]
2025-10-12 21:03:42 | A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
2025-10-12 21:03:42 | âŒ ì˜¤ë¥˜ ë°œìƒ: Input length of input_ids is 1024, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
2025-10-12 21:03:42 | >> ë¡œê·¸ ë¦¬ë””ë ‰ì…˜ ì¤‘ë£Œ.
