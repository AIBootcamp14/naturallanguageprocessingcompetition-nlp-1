2025-10-12 21:02:48 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-12 21:02:51 | 📊 SINGLE 모드 실행 중...
2025-10-12 21:02:51 | ============================================================
2025-10-12 21:02:51 | 🚀 SINGLE MODEL 모드 학습 시작
2025-10-12 21:02:51 | 📋 모델: llama-3.2-korean-3b
2025-10-12 21:02:51 | ============================================================
2025-10-12 21:02:51 | [1/5] 데이터 로딩...
2025-10-12 21:02:51 | ⚠️ 디버그 모드: 학습 100개, 검증 20개
2025-10-12 21:02:51 | [2/5] Config 로딩...
2025-10-12 21:02:51 | ⚙️ Epochs 오버라이드: 1
2025-10-12 21:02:51 | ⚙️ Batch size 오버라이드: 4
2025-10-12 21:02:51 | ✅ Config 로드 완료: llama-3.2-korean-3b
2025-10-12 21:02:51 | [3/5] 모델 로딩...
2025-10-12 21:02:51 | 모델 타입: causal_lm
2025-10-12 21:02:51 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 21:02:51 | 모델 로딩 중...
2025-10-12 21:02:51 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 21:02:51 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 21:02:55 | Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.64s/it]
2025-10-12 21:02:56 | Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.88s/it]
2025-10-12 21:02:56 | 토크나이저 로딩 중...
2025-10-12 21:02:57 | 패딩 토큰 설정: <|eot_id|>
2025-10-12 21:02:57 | LoRA 설정 적용 중...
2025-10-12 21:02:57 | ✅ LoRA 적용 완료
2025-10-12 21:02:57 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-12 21:02:57 | 전체 파라미터: 3,237,063,680
2025-10-12 21:02:57 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 21:02:57 | ✅ Gradient Checkpointing 활성화
2025-10-12 21:02:57 | ✅ Causal LM 로드 완료
2025-10-12 21:02:57 | ✅ 모델 로드 완료
2025-10-12 21:02:57 | [4/5] Dataset 생성...
2025-10-12 21:02:57 | ✅ 학습 Dataset: 100개
2025-10-12 21:02:57 | ✅ 검증 Dataset: 20개
2025-10-12 21:02:57 | [5/5] 학습 시작...
2025-10-12 21:02:57 | ============================================================
2025-10-12 21:02:57 | 모델 학습 시작
2025-10-12 21:02:57 | ============================================================
2025-10-12 21:02:57 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 21:02:58 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 21:02:58 | 학습 진행 중...
2025-10-12 21:02:58 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 21:02:58 | 0%|          | 0/25 [00:00<?, ?it/s]
2025-10-12 21:03:01 | 4%|▍         | 1/25 [00:03<01:18,  3.27s/it]
2025-10-12 21:03:03 | 8%|▊         | 2/25 [00:05<01:00,  2.63s/it]
2025-10-12 21:03:05 | 12%|█▏        | 3/25 [00:07<00:48,  2.21s/it]
2025-10-12 21:03:06 | 16%|█▌        | 4/25 [00:08<00:41,  1.97s/it]
2025-10-12 21:03:08 | 20%|██        | 5/25 [00:10<00:36,  1.84s/it]
2025-10-12 21:03:10 | 24%|██▍       | 6/25 [00:12<00:33,  1.77s/it]
2025-10-12 21:03:11 | 28%|██▊       | 7/25 [00:13<00:31,  1.73s/it]
2025-10-12 21:03:13 | 32%|███▏      | 8/25 [00:15<00:28,  1.70s/it]
2025-10-12 21:03:15 | 36%|███▌      | 9/25 [00:16<00:26,  1.69s/it]
2025-10-12 21:03:16 | 40%|████      | 10/25 [00:18<00:25,  1.68s/it]
2025-10-12 21:03:16 | {'loss': 1.5811, 'grad_norm': 2.3844869136810303, 'learning_rate': 3.6e-07, 'epoch': 0.4}
2025-10-12 21:03:16 | 40%|████      | 10/25 [00:18<00:25,  1.68s/it]
2025-10-12 21:03:18 | 44%|████▍     | 11/25 [00:20<00:23,  1.67s/it]
2025-10-12 21:03:20 | 48%|████▊     | 12/25 [00:21<00:21,  1.66s/it]
2025-10-12 21:03:22 | 52%|█████▏    | 13/25 [00:24<00:21,  1.83s/it]
2025-10-12 21:03:23 | 56%|█████▌    | 14/25 [00:25<00:19,  1.77s/it]
2025-10-12 21:03:25 | 60%|██████    | 15/25 [00:27<00:17,  1.72s/it]
2025-10-12 21:03:27 | 64%|██████▍   | 16/25 [00:28<00:15,  1.69s/it]
2025-10-12 21:03:28 | 68%|██████▊   | 17/25 [00:30<00:13,  1.67s/it]
2025-10-12 21:03:30 | 72%|███████▏  | 18/25 [00:32<00:11,  1.66s/it]
2025-10-12 21:03:32 | 76%|███████▌  | 19/25 [00:33<00:09,  1.65s/it]
2025-10-12 21:03:33 | 80%|████████  | 20/25 [00:35<00:08,  1.65s/it]
2025-10-12 21:03:33 | {'loss': 1.6233, 'grad_norm': 2.510491132736206, 'learning_rate': 7.6e-07, 'epoch': 0.8}
2025-10-12 21:03:33 | 80%|████████  | 20/25 [00:35<00:08,  1.65s/it]
2025-10-12 21:03:35 | 84%|████████▍ | 21/25 [00:37<00:06,  1.64s/it]
2025-10-12 21:03:36 | 88%|████████▊ | 22/25 [00:38<00:04,  1.64s/it]
2025-10-12 21:03:38 | 92%|█████████▏| 23/25 [00:40<00:03,  1.64s/it]
2025-10-12 21:03:40 | 96%|█████████▌| 24/25 [00:42<00:01,  1.64s/it]
2025-10-12 21:03:41 | 100%|██████████| 25/25 [00:43<00:00,  1.65s/it]
2025-10-12 21:03:42 | A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
2025-10-12 21:03:42 | ❌ 오류 발생: Input length of input_ids is 1024, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
2025-10-12 21:03:42 | >> 로그 리디렉션 중료.
