{
  "mode": "full",
  "models": [
    "kobart",
    "llama-3.2-korean-3b",
    "qwen3-4b",
    "solar-10.7b",
    "polyglot-ko-12.8b",
    "kullm-v2"
  ],
  "ensemble_strategy": "stacking",
  "use_tta": true,
  "model_results": [
    {
      "model_name": "kobart",
      "model_path": "experiments/20251012/20251012_131535_test_full_pipeline_quick/model_0_kobart/default/final_model",
      "eval_metrics": {
        "eval_loss": 1.4561381340026855,
        "eval_rouge1": 0.4133965995229025,
        "eval_rouge2": 0.25518309577197984,
        "eval_rougeL": 0.4064726843949632,
        "eval_rouge_sum": 1.0750523796898457,
        "eval_runtime": 78.3688,
        "eval_samples_per_second": 6.367,
        "eval_steps_per_second": 0.804,
        "epoch": 1.0
      },
      "status": "success"
    },
    {
      "model_name": "llama-3.2-korean-3b",
      "model_path": null,
      "eval_metrics": {},
      "status": "failed",
      "error": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
    },
    {
      "model_name": "qwen3-4b",
      "model_path": null,
      "eval_metrics": {},
      "status": "failed",
      "error": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
    },
    {
      "model_name": "solar-10.7b",
      "model_path": null,
      "eval_metrics": {},
      "status": "failed",
      "error": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
    },
    {
      "model_name": "polyglot-ko-12.8b",
      "model_path": null,
      "eval_metrics": {},
      "status": "failed",
      "error": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
    },
    {
      "model_name": "kullm-v2",
      "model_path": null,
      "eval_metrics": {},
      "status": "failed",
      "error": "The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder` for them. Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format."
    }
  ],
  "ensemble_results": {},
  "solar_results": {
    "solar_rouge_1_f1": 0.22721529000252147,
    "solar_rouge_2_f1": 0.07650273711618417,
    "solar_rouge_l_f1": 0.21771327705012092,
    "n_samples": 50
  },
  "tta_results": {
    "tta_applied": false,
    "strategies": [
      "paraphrase",
      "reorder",
      "synonym",
      "mask"
    ],
    "num_aug": 2
  },
  "inference_results": {
    "submission_path": null,
    "num_predictions": 0,
    "error": "The following `model_kwargs` are not used by the model: ['token_type_ids'] (note: typos in the generate arguments will also show up in this list)"
  }
}