2025-10-12 22:01:57 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-12 22:01:59 | 📊 FULL 모드 실행 중...
2025-10-12 22:01:59 | ============================================================
2025-10-12 22:01:59 | = FULL PIPELINE 실행 시작
2025-10-12 22:01:59 | =대상 모델: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-12 22:01:59 | =앙상블 앙상블 전략: stacking
2025-10-12 22:01:59 | = TTA 사용: True
2025-10-12 22:01:59 | ============================================================
2025-10-12 22:01:59 | [1/6] 데이터 로딩...
2025-10-12 22:02:00 | ✅ 학습 데이터: 12457개
2025-10-12 22:02:00 | ✅ 검증 데이터: 499개
2025-10-12 22:02:00 | [2/6] 다중 모델 학습 (6 모델)...
2025-10-12 22:02:00 | ==================================================
2025-10-12 22:02:00 | 모델 1/6: kobart
2025-10-12 22:02:00 | ==================================================
2025-10-12 22:02:00 | 모델 타입: encoder_decoder
2025-10-12 22:02:00 | ============================================================
2025-10-12 22:02:00 | 모델 및 토크나이저 로딩 시작
2025-10-12 22:02:00 | ============================================================
2025-10-12 22:02:00 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-12 22:02:00 | 모델 로딩: digit82/kobart-summarization
2025-10-12 22:02:00 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-12 22:02:02 | → 디바이스: cuda
2025-10-12 22:02:02 | → 전체 파라미터: 123,859,968
2025-10-12 22:02:02 | → 학습 가능 파라미터: 123,859,968
2025-10-12 22:02:02 | ============================================================
2025-10-12 22:02:02 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-12 22:02:02 | ============================================================
2025-10-12 22:02:02 | ============================================================
2025-10-12 22:02:02 | 모델 학습 시작
2025-10-12 22:02:02 | ============================================================
2025-10-12 22:02:02 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:02:02 | 학습 진행 중...
2025-10-12 22:02:02 | 0%|          | 0/1558 [00:00<?, ?it/s]
2025-10-12 22:02:03 | 1%|          | 10/1558 [00:00<01:46, 14.59it/s]
2025-10-12 22:02:04 | 2%|▏         | 25/1558 [00:01<01:12, 21.27it/s]
2025-10-12 22:02:04 | 3%|▎         | 40/1558 [00:02<01:07, 22.49it/s]
2025-10-12 22:02:06 | 4%|▎         | 55/1558 [00:03<01:35, 15.76it/s]
2025-10-12 22:02:07 | 5%|▍         | 73/1558 [00:04<01:04, 23.09it/s]
2025-10-12 22:02:07 | 6%|▌         | 88/1558 [00:05<01:01, 24.02it/s]
2025-10-12 22:02:08 | {'loss': 2.747, 'grad_norm': 8.180926322937012, 'learning_rate': 9.9e-07, 'epoch': 0.06}
2025-10-12 22:02:08 | 6%|▋         | 100/1558 [00:05<00:59, 24.52it/s]
2025-10-12 22:02:08 | 7%|▋         | 103/1558 [00:05<00:59, 24.39it/s]
2025-10-12 22:02:08 | 8%|▊         | 118/1558 [00:06<01:00, 23.87it/s]
2025-10-12 22:02:09 | 9%|▊         | 133/1558 [00:06<01:01, 23.03it/s]
2025-10-12 22:02:10 | 10%|▉         | 151/1558 [00:07<01:00, 23.25it/s]
2025-10-12 22:02:11 | 11%|█         | 166/1558 [00:08<01:02, 22.22it/s]
2025-10-12 22:02:11 | 12%|█▏        | 181/1558 [00:09<01:03, 21.71it/s]
2025-10-12 22:02:12 | 13%|█▎        | 196/1558 [00:09<01:00, 22.65it/s]
2025-10-12 22:02:12 | {'loss': 2.0334, 'grad_norm': 10.215290069580078, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.13}
2025-10-12 22:02:12 | 13%|█▎        | 200/1558 [00:09<01:02, 21.75it/s]
2025-10-12 22:02:13 | 14%|█▎        | 211/1558 [00:10<00:59, 22.55it/s]
2025-10-12 22:02:13 | 15%|█▍        | 226/1558 [00:11<00:55, 23.79it/s]
2025-10-12 22:02:14 | 16%|█▌        | 244/1558 [00:11<00:57, 22.80it/s]
2025-10-12 22:02:15 | 17%|█▋        | 259/1558 [00:12<00:55, 23.60it/s]
2025-10-12 22:02:15 | 18%|█▊        | 274/1558 [00:13<00:57, 22.14it/s]
2025-10-12 22:02:16 | 19%|█▊        | 289/1558 [00:13<00:57, 22.14it/s]
2025-10-12 22:02:16 | {'loss': 1.854, 'grad_norm': 6.359881401062012, 'learning_rate': 2.99e-06, 'epoch': 0.19}
2025-10-12 22:02:16 | 19%|█▉        | 300/1558 [00:14<00:57, 21.93it/s]
2025-10-12 22:02:17 | 20%|█▉        | 304/1558 [00:14<00:55, 22.47it/s]
2025-10-12 22:02:17 | 21%|██        | 322/1558 [00:15<00:54, 22.71it/s]
2025-10-12 22:02:18 | 22%|██▏       | 337/1558 [00:15<00:53, 22.89it/s]
2025-10-12 22:02:19 | 23%|██▎       | 352/1558 [00:16<00:55, 21.65it/s]
2025-10-12 22:02:19 | 24%|██▎       | 367/1558 [00:17<00:52, 22.47it/s]
2025-10-12 22:02:20 | 25%|██▍       | 382/1558 [00:17<00:53, 21.80it/s]
2025-10-12 22:02:21 | 26%|██▌       | 400/1558 [00:18<00:51, 22.54it/s]
2025-10-12 22:02:21 | {'loss': 1.7594, 'grad_norm': 6.687454700469971, 'learning_rate': 3.990000000000001e-06, 'epoch': 0.26}
2025-10-12 22:02:21 | 26%|██▌       | 400/1558 [00:18<00:51, 22.54it/s]
2025-10-12 22:02:22 | 27%|██▋       | 415/1558 [00:19<00:48, 23.49it/s]
2025-10-12 22:02:22 | 28%|██▊       | 430/1558 [00:20<00:51, 21.90it/s]
2025-10-12 22:02:23 | 29%|██▊       | 445/1558 [00:20<00:47, 23.21it/s]
2025-10-12 22:02:24 | 30%|██▉       | 460/1558 [00:21<00:45, 23.87it/s]
2025-10-12 22:02:24 | 31%|███       | 478/1558 [00:22<00:45, 23.50it/s]
2025-10-12 22:02:25 | 32%|███▏      | 493/1558 [00:22<00:47, 22.36it/s]
2025-10-12 22:02:25 | {'loss': 1.6645, 'grad_norm': 5.66191291809082, 'learning_rate': 4.9900000000000005e-06, 'epoch': 0.32}
2025-10-12 22:02:25 | 32%|███▏      | 500/1558 [00:23<00:47, 22.44it/s]
2025-10-12 22:02:26 | 33%|███▎      | 508/1558 [00:23<00:47, 22.04it/s]
2025-10-12 22:02:26 | 34%|███▎      | 523/1558 [00:24<00:46, 22.23it/s]
2025-10-12 22:02:27 | 35%|███▍      | 538/1558 [00:24<00:42, 23.94it/s]
2025-10-12 22:02:28 | 36%|███▌      | 556/1558 [00:25<00:42, 23.50it/s]
2025-10-12 22:02:28 | 37%|███▋      | 571/1558 [00:26<00:42, 23.35it/s]
2025-10-12 22:02:29 | 38%|███▊      | 586/1558 [00:26<00:41, 23.16it/s]
2025-10-12 22:02:30 | {'loss': 1.6316, 'grad_norm': 6.635172367095947, 'learning_rate': 4.532136105860114e-06, 'epoch': 0.39}
2025-10-12 22:02:30 | 39%|███▊      | 600/1558 [00:27<00:43, 22.02it/s]
2025-10-12 22:02:30 | 40%|███▉      | 616/1558 [00:28<00:40, 23.04it/s]
2025-10-12 22:02:31 | 41%|████      | 631/1558 [00:28<00:39, 23.76it/s]
2025-10-12 22:02:32 | 42%|████▏     | 649/1558 [00:29<00:41, 21.99it/s]
2025-10-12 22:02:32 | 43%|████▎     | 664/1558 [00:30<00:38, 23.18it/s]
2025-10-12 22:02:33 | 44%|████▎     | 679/1558 [00:30<00:38, 23.13it/s]
2025-10-12 22:02:34 | 45%|████▍     | 694/1558 [00:31<00:35, 24.65it/s]
2025-10-12 22:02:34 | {'loss': 1.6048, 'grad_norm': 7.678183555603027, 'learning_rate': 4.059546313799622e-06, 'epoch': 0.45}
2025-10-12 22:02:34 | 45%|████▍     | 700/1558 [00:31<00:35, 24.22it/s]
2025-10-12 22:02:34 | 46%|████▌     | 709/1558 [00:32<00:34, 24.78it/s]
2025-10-12 22:02:35 | 47%|████▋     | 727/1558 [00:32<00:34, 23.74it/s]
2025-10-12 22:02:36 | 48%|████▊     | 742/1558 [00:33<00:37, 22.02it/s]
2025-10-12 22:02:36 | 49%|████▊     | 757/1558 [00:34<00:35, 22.75it/s]
2025-10-12 22:02:37 | 50%|████▉     | 772/1558 [00:34<00:34, 22.62it/s]
2025-10-12 22:02:39 | 51%|█████     | 787/1558 [00:36<01:15, 10.23it/s]
2025-10-12 22:02:39 | {'loss': 1.5706, 'grad_norm': 5.509920597076416, 'learning_rate': 3.5869565217391305e-06, 'epoch': 0.51}
2025-10-12 22:02:39 | 51%|█████▏    | 800/1558 [00:37<00:43, 17.49it/s]
2025-10-12 22:02:39 | 52%|█████▏    | 805/1558 [00:37<00:38, 19.34it/s]
2025-10-12 22:02:40 | 53%|█████▎    | 820/1558 [00:37<00:34, 21.60it/s]
2025-10-12 22:02:41 | 54%|█████▎    | 835/1558 [00:38<00:34, 21.20it/s]
2025-10-12 22:02:41 | 55%|█████▍    | 850/1558 [00:39<00:33, 21.28it/s]
2025-10-12 22:02:42 | 56%|█████▌    | 865/1558 [00:39<00:31, 22.28it/s]
2025-10-12 22:02:43 | 57%|█████▋    | 883/1558 [00:40<00:29, 23.01it/s]
2025-10-12 22:02:44 | 58%|█████▊    | 898/1558 [00:41<00:29, 22.12it/s]
2025-10-12 22:02:44 | {'loss': 1.5792, 'grad_norm': 6.100025653839111, 'learning_rate': 3.114366729678639e-06, 'epoch': 0.58}
2025-10-12 22:02:44 | 58%|█████▊    | 900/1558 [00:41<00:29, 22.12it/s]
2025-10-12 22:02:44 | 59%|█████▊    | 913/1558 [00:42<00:28, 22.59it/s]
2025-10-12 22:02:45 | 60%|█████▉    | 928/1558 [00:42<00:28, 22.36it/s]
2025-10-12 22:02:46 | 61%|██████    | 943/1558 [00:43<00:27, 22.43it/s]
2025-10-12 22:02:46 | 62%|██████▏   | 961/1558 [00:44<00:26, 22.78it/s]
2025-10-12 22:02:47 | 63%|██████▎   | 976/1558 [00:44<00:27, 21.32it/s]
2025-10-12 22:02:48 | 64%|██████▎   | 991/1558 [00:45<00:25, 22.42it/s]
2025-10-12 22:02:48 | {'loss': 1.5576, 'grad_norm': 5.244892597198486, 'learning_rate': 2.641776937618148e-06, 'epoch': 0.64}
2025-10-12 22:02:48 | 64%|██████▍   | 1000/1558 [00:46<00:24, 22.44it/s]
2025-10-12 22:02:48 | 65%|██████▍   | 1006/1558 [00:46<00:24, 22.68it/s]
2025-10-12 22:02:49 | 66%|██████▌   | 1021/1558 [00:46<00:23, 22.60it/s]
2025-10-12 22:02:50 | 67%|██████▋   | 1039/1558 [00:47<00:23, 22.22it/s]
2025-10-12 22:02:51 | 68%|██████▊   | 1054/1558 [00:48<00:22, 22.23it/s]
2025-10-12 22:02:51 | 69%|██████▊   | 1069/1558 [00:49<00:22, 21.93it/s]
2025-10-12 22:02:52 | 70%|██████▉   | 1084/1558 [00:49<00:20, 23.67it/s]
2025-10-12 22:02:52 | 71%|███████   | 1099/1558 [00:50<00:18, 24.36it/s]
2025-10-12 22:02:53 | {'loss': 1.5503, 'grad_norm': 5.734854221343994, 'learning_rate': 2.169187145557656e-06, 'epoch': 0.71}
2025-10-12 22:02:53 | 71%|███████   | 1100/1558 [00:50<00:18, 24.36it/s]
2025-10-12 22:02:53 | 72%|███████▏  | 1114/1558 [00:51<00:17, 24.75it/s]
2025-10-12 22:02:54 | 73%|███████▎  | 1132/1558 [00:51<00:19, 21.77it/s]
2025-10-12 22:02:55 | 74%|███████▎  | 1147/1558 [00:52<00:17, 23.44it/s]
2025-10-12 22:02:55 | 75%|███████▍  | 1162/1558 [00:53<00:15, 25.07it/s]
2025-10-12 22:02:56 | 76%|███████▌  | 1177/1558 [00:53<00:16, 23.41it/s]
2025-10-12 22:02:57 | 77%|███████▋  | 1192/1558 [00:54<00:16, 21.86it/s]
2025-10-12 22:02:57 | {'loss': 1.5246, 'grad_norm': 4.482044696807861, 'learning_rate': 1.6965973534971647e-06, 'epoch': 0.77}
2025-10-12 22:02:57 | 77%|███████▋  | 1200/1558 [00:54<00:16, 22.09it/s]
2025-10-12 22:02:57 | 78%|███████▊  | 1210/1558 [00:55<00:16, 21.55it/s]
2025-10-12 22:02:58 | 79%|███████▊  | 1225/1558 [00:55<00:13, 24.24it/s]
2025-10-12 22:02:59 | 80%|███████▉  | 1240/1558 [00:56<00:12, 24.81it/s]
2025-10-12 22:02:59 | 81%|████████  | 1255/1558 [00:57<00:13, 23.15it/s]
2025-10-12 22:03:00 | 82%|████████▏ | 1270/1558 [00:57<00:11, 25.07it/s]
2025-10-12 22:03:01 | 83%|████████▎ | 1288/1558 [00:58<00:11, 23.35it/s]
2025-10-12 22:03:01 | {'loss': 1.5155, 'grad_norm': 5.404173851013184, 'learning_rate': 1.224007561436673e-06, 'epoch': 0.83}
2025-10-12 22:03:01 | 83%|████████▎ | 1300/1558 [00:59<00:11, 23.31it/s]
2025-10-12 22:03:01 | 84%|████████▎ | 1303/1558 [00:59<00:11, 22.94it/s]
2025-10-12 22:03:02 | 85%|████████▍ | 1318/1558 [00:59<00:09, 24.31it/s]
2025-10-12 22:03:03 | 86%|████████▌ | 1333/1558 [01:00<00:09, 23.27it/s]
2025-10-12 22:03:03 | 87%|████████▋ | 1348/1558 [01:01<00:08, 24.91it/s]
2025-10-12 22:03:04 | 88%|████████▊ | 1366/1558 [01:01<00:08, 23.79it/s]
2025-10-12 22:03:05 | 89%|████████▊ | 1381/1558 [01:02<00:07, 22.78it/s]
2025-10-12 22:03:05 | 90%|████████▉ | 1396/1558 [01:03<00:06, 23.55it/s]
2025-10-12 22:03:05 | {'loss': 1.5504, 'grad_norm': 5.750039577484131, 'learning_rate': 7.514177693761815e-07, 'epoch': 0.9}
2025-10-12 22:03:05 | 90%|████████▉ | 1400/1558 [01:03<00:06, 23.33it/s]
2025-10-12 22:03:06 | 91%|█████████ | 1411/1558 [01:03<00:06, 24.07it/s]
2025-10-12 22:03:06 | 92%|█████████▏| 1426/1558 [01:04<00:05, 22.35it/s]
2025-10-12 22:03:07 | 93%|█████████▎| 1444/1558 [01:05<00:04, 23.95it/s]
2025-10-12 22:03:08 | 94%|█████████▎| 1459/1558 [01:05<00:04, 23.98it/s]
2025-10-12 22:03:09 | 95%|█████████▍| 1474/1558 [01:06<00:03, 23.32it/s]
2025-10-12 22:03:09 | 96%|█████████▌| 1489/1558 [01:07<00:02, 23.64it/s]
2025-10-12 22:03:10 | {'loss': 1.5458, 'grad_norm': 4.620453834533691, 'learning_rate': 2.7882797731569e-07, 'epoch': 0.96}
2025-10-12 22:03:10 | 96%|█████████▋| 1500/1558 [01:07<00:02, 22.58it/s]
2025-10-12 22:03:10 | 97%|█████████▋| 1504/1558 [01:07<00:02, 22.89it/s]
2025-10-12 22:03:11 | 98%|█████████▊| 1522/1558 [01:09<00:04,  8.11it/s]
2025-10-12 22:03:12 | 99%|█████████▊| 1537/1558 [01:09<00:01, 17.56it/s]
2025-10-12 22:03:13 | 100%|█████████▉| 1552/1558 [01:10<00:00, 22.03it/s]
2025-10-12 22:03:14 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 22:03:14 | [A
2025-10-12 22:03:15 | 3%|▎         | 2/63 [00:00<00:24,  2.48it/s]
2025-10-12 22:03:15 | [A
2025-10-12 22:03:16 | 5%|▍         | 3/63 [00:01<00:32,  1.87it/s]
2025-10-12 22:03:16 | [A
2025-10-12 22:03:16 | 6%|▋         | 4/63 [00:02<00:38,  1.53it/s]
2025-10-12 22:03:16 | [A
2025-10-12 22:03:17 | 8%|▊         | 5/63 [00:03<00:39,  1.48it/s]
2025-10-12 22:03:17 | [A
2025-10-12 22:03:18 | 10%|▉         | 6/63 [00:03<00:39,  1.46it/s]
2025-10-12 22:03:18 | [A
2025-10-12 22:03:19 | 11%|█         | 7/63 [00:04<00:40,  1.37it/s]
2025-10-12 22:03:19 | [A
2025-10-12 22:03:19 | 13%|█▎        | 8/63 [00:05<00:42,  1.30it/s]
2025-10-12 22:03:19 | [A
2025-10-12 22:03:20 | 14%|█▍        | 9/63 [00:06<00:43,  1.25it/s]
2025-10-12 22:03:20 | [A
2025-10-12 22:03:21 | 16%|█▌        | 10/63 [00:07<00:45,  1.17it/s]
2025-10-12 22:03:21 | [A
2025-10-12 22:03:22 | 17%|█▋        | 11/63 [00:08<00:44,  1.17it/s]
2025-10-12 22:03:22 | [A
2025-10-12 22:03:23 | 19%|█▉        | 12/63 [00:09<00:44,  1.14it/s]
2025-10-12 22:03:23 | [A
2025-10-12 22:03:24 | 21%|██        | 13/63 [00:09<00:44,  1.14it/s]
2025-10-12 22:03:24 | [A
2025-10-12 22:03:24 | 100%|██████████| 1558/1558 [01:22<00:00, 21.36it/s]
2025-10-12 22:03:25 | 22%|██▏       | 14/63 [00:10<00:43,  1.14it/s]
2025-10-12 22:03:25 | [A
2025-10-12 22:03:26 | 24%|██▍       | 15/63 [00:11<00:41,  1.15it/s]
2025-10-12 22:03:26 | [A
2025-10-12 22:03:26 | 25%|██▌       | 16/63 [00:12<00:38,  1.21it/s]
2025-10-12 22:03:26 | [A
2025-10-12 22:03:27 | 27%|██▋       | 17/63 [00:13<00:37,  1.22it/s]
2025-10-12 22:03:27 | [A
2025-10-12 22:03:28 | 29%|██▊       | 18/63 [00:13<00:35,  1.27it/s]
2025-10-12 22:03:28 | [A
2025-10-12 22:03:29 | 30%|███       | 19/63 [00:14<00:35,  1.23it/s]
2025-10-12 22:03:29 | [A
2025-10-12 22:03:30 | 32%|███▏      | 20/63 [00:15<00:34,  1.24it/s]
2025-10-12 22:03:30 | [A
2025-10-12 22:03:30 | 33%|███▎      | 21/63 [00:16<00:34,  1.22it/s]
2025-10-12 22:03:30 | [A
2025-10-12 22:03:31 | 35%|███▍      | 22/63 [00:17<00:32,  1.27it/s]
2025-10-12 22:03:31 | [A
2025-10-12 22:03:32 | 37%|███▋      | 23/63 [00:18<00:32,  1.22it/s]
2025-10-12 22:03:32 | [A
2025-10-12 22:03:33 | 38%|███▊      | 24/63 [00:18<00:32,  1.21it/s]
2025-10-12 22:03:33 | [A
2025-10-12 22:03:34 | 40%|███▉      | 25/63 [00:19<00:30,  1.24it/s]
2025-10-12 22:03:34 | [A
2025-10-12 22:03:34 | 41%|████▏     | 26/63 [00:20<00:29,  1.26it/s]
2025-10-12 22:03:34 | [A
2025-10-12 22:03:35 | 43%|████▎     | 27/63 [00:21<00:28,  1.26it/s]
2025-10-12 22:03:35 | [A
2025-10-12 22:03:36 | 44%|████▍     | 28/63 [00:22<00:28,  1.25it/s]
2025-10-12 22:03:36 | [A
2025-10-12 22:03:37 | 46%|████▌     | 29/63 [00:22<00:27,  1.22it/s]
2025-10-12 22:03:37 | [A
2025-10-12 22:03:38 | 48%|████▊     | 30/63 [00:23<00:28,  1.17it/s]
2025-10-12 22:03:38 | [A
2025-10-12 22:03:39 | 49%|████▉     | 31/63 [00:24<00:26,  1.19it/s]
2025-10-12 22:03:39 | [A
2025-10-12 22:03:39 | 51%|█████     | 32/63 [00:25<00:25,  1.21it/s]
2025-10-12 22:03:39 | [A
2025-10-12 22:03:40 | 52%|█████▏    | 33/63 [00:26<00:24,  1.23it/s]
2025-10-12 22:03:40 | [A
2025-10-12 22:03:41 | 54%|█████▍    | 34/63 [00:27<00:24,  1.17it/s]
2025-10-12 22:03:41 | [A
2025-10-12 22:03:42 | 56%|█████▌    | 35/63 [00:28<00:23,  1.17it/s]
2025-10-12 22:03:42 | [A
2025-10-12 22:03:43 | 57%|█████▋    | 36/63 [00:28<00:23,  1.16it/s]
2025-10-12 22:03:43 | [A
2025-10-12 22:03:45 | 59%|█████▊    | 37/63 [00:30<00:29,  1.12s/it]
2025-10-12 22:03:45 | [A
2025-10-12 22:03:46 | 60%|██████    | 38/63 [00:31<00:26,  1.04s/it]
2025-10-12 22:03:46 | [A
2025-10-12 22:03:46 | 62%|██████▏   | 39/63 [00:32<00:23,  1.03it/s]
2025-10-12 22:03:46 | [A
2025-10-12 22:03:47 | 63%|██████▎   | 40/63 [00:33<00:21,  1.07it/s]
2025-10-12 22:03:47 | [A
2025-10-12 22:03:48 | 65%|██████▌   | 41/63 [00:33<00:19,  1.15it/s]
2025-10-12 22:03:48 | [A
2025-10-12 22:03:49 | 67%|██████▋   | 42/63 [00:34<00:17,  1.20it/s]
2025-10-12 22:03:49 | [A
2025-10-12 22:03:50 | 68%|██████▊   | 43/63 [00:35<00:17,  1.17it/s]
2025-10-12 22:03:50 | [A
2025-10-12 22:03:50 | 70%|██████▉   | 44/63 [00:36<00:15,  1.22it/s]
2025-10-12 22:03:50 | [A
2025-10-12 22:03:51 | 71%|███████▏  | 45/63 [00:37<00:14,  1.25it/s]
2025-10-12 22:03:51 | [A
2025-10-12 22:03:52 | 73%|███████▎  | 46/63 [00:37<00:13,  1.31it/s]
2025-10-12 22:03:52 | [A
2025-10-12 22:03:52 | 75%|███████▍  | 47/63 [00:38<00:12,  1.32it/s]
2025-10-12 22:03:52 | [A
2025-10-12 22:03:53 | 76%|███████▌  | 48/63 [00:39<00:11,  1.25it/s]
2025-10-12 22:03:53 | [A
2025-10-12 22:03:54 | 78%|███████▊  | 49/63 [00:40<00:11,  1.22it/s]
2025-10-12 22:03:54 | [A
2025-10-12 22:03:55 | 79%|███████▉  | 50/63 [00:41<00:10,  1.23it/s]
2025-10-12 22:03:55 | [A
2025-10-12 22:03:56 | 81%|████████  | 51/63 [00:41<00:09,  1.23it/s]
2025-10-12 22:03:56 | [A
2025-10-12 22:03:57 | 83%|████████▎ | 52/63 [00:42<00:08,  1.25it/s]
2025-10-12 22:03:57 | [A
2025-10-12 22:03:57 | 84%|████████▍ | 53/63 [00:43<00:07,  1.25it/s]
2025-10-12 22:03:57 | [A
2025-10-12 22:03:58 | 86%|████████▌ | 54/63 [00:44<00:07,  1.21it/s]
2025-10-12 22:03:58 | [A
2025-10-12 22:03:59 | 87%|████████▋ | 55/63 [00:45<00:06,  1.18it/s]
2025-10-12 22:03:59 | [A
2025-10-12 22:04:00 | 89%|████████▉ | 56/63 [00:45<00:05,  1.21it/s]
2025-10-12 22:04:00 | [A
2025-10-12 22:04:01 | 90%|█████████ | 57/63 [00:46<00:05,  1.17it/s]
2025-10-12 22:04:01 | [A
2025-10-12 22:04:02 | 92%|█████████▏| 58/63 [00:47<00:04,  1.14it/s]
2025-10-12 22:04:02 | [A
2025-10-12 22:04:03 | 94%|█████████▎| 59/63 [00:48<00:03,  1.14it/s]
2025-10-12 22:04:03 | [A
2025-10-12 22:04:03 | 95%|█████████▌| 60/63 [00:49<00:02,  1.20it/s]
2025-10-12 22:04:03 | [A
2025-10-12 22:04:04 | 97%|█████████▋| 61/63 [00:50<00:01,  1.21it/s]
2025-10-12 22:04:04 | [A
2025-10-12 22:04:05 | 98%|█████████▊| 62/63 [00:51<00:00,  1.21it/s]
2025-10-12 22:04:05 | [A
2025-10-12 22:04:06 | 100%|██████████| 63/63 [00:51<00:00,  1.20it/s]
2025-10-12 22:04:06 | [A
2025-10-12 22:04:06 | [A
2025-10-12 22:04:06 | {'eval_loss': 1.4561806917190552, 'eval_rouge1': 0.4140241857774703, 'eval_rouge2': 0.2557054666592291, 'eval_rougeL': 0.4066672851640876, 'eval_rouge_sum': 1.076396937600787, 'eval_runtime': 52.9524, 'eval_samples_per_second': 9.424, 'eval_steps_per_second': 1.19, 'epoch': 1.0}
2025-10-12 22:04:06 | 100%|██████████| 1558/1558 [02:03<00:00, 21.36it/s]
2025-10-12 22:04:06 | [A
2025-10-12 22:04:06 | [A
2025-10-12 22:04:07 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-12 22:04:07 | {'train_runtime': 125.27, 'train_samples_per_second': 99.441, 'train_steps_per_second': 12.437, 'train_loss': 1.7073205157024105, 'epoch': 1.0}
2025-10-12 22:04:07 | 100%|██████████| 1558/1558 [02:05<00:00, 21.36it/s]
2025-10-12 22:04:07 | 최종 모델 저장 중...
2025-10-12 22:04:08 | → 모델 저장 위치: experiments/20251012/20251012_220157_test_full_pipeline_verified/model_0_kobart/default/final_model
2025-10-12 22:04:08 | 최종 평가 중...
2025-10-12 22:04:09 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 22:04:10 | 3%|▎         | 2/63 [00:00<00:21,  2.84it/s]
2025-10-12 22:04:10 | 5%|▍         | 3/63 [00:01<00:31,  1.93it/s]
2025-10-12 22:04:11 | 6%|▋         | 4/63 [00:02<00:34,  1.71it/s]
2025-10-12 22:04:12 | 8%|▊         | 5/63 [00:02<00:38,  1.51it/s]
2025-10-12 22:04:13 | 10%|▉         | 6/63 [00:03<00:39,  1.44it/s]
2025-10-12 22:04:14 | 11%|█         | 7/63 [00:04<00:42,  1.30it/s]
2025-10-12 22:04:14 | 13%|█▎        | 8/63 [00:05<00:42,  1.29it/s]
2025-10-12 22:04:15 | 14%|█▍        | 9/63 [00:06<00:43,  1.24it/s]
2025-10-12 22:04:16 | 16%|█▌        | 10/63 [00:07<00:43,  1.22it/s]
2025-10-12 22:04:18 | 17%|█▋        | 11/63 [00:08<00:56,  1.08s/it]
2025-10-12 22:04:18 | 19%|█▉        | 12/63 [00:09<00:48,  1.04it/s]
2025-10-12 22:04:19 | 21%|██        | 13/63 [00:10<00:46,  1.08it/s]
2025-10-12 22:04:20 | 22%|██▏       | 14/63 [00:11<00:43,  1.13it/s]
2025-10-12 22:04:21 | 24%|██▍       | 15/63 [00:11<00:41,  1.15it/s]
2025-10-12 22:04:22 | 25%|██▌       | 16/63 [00:12<00:38,  1.22it/s]
2025-10-12 22:04:22 | 27%|██▋       | 17/63 [00:13<00:36,  1.25it/s]
2025-10-12 22:04:23 | 29%|██▊       | 18/63 [00:14<00:34,  1.29it/s]
2025-10-12 22:04:24 | 30%|███       | 19/63 [00:14<00:34,  1.28it/s]
2025-10-12 22:04:25 | 32%|███▏      | 20/63 [00:15<00:33,  1.28it/s]
2025-10-12 22:04:25 | 33%|███▎      | 21/63 [00:16<00:32,  1.31it/s]
2025-10-12 22:04:26 | 35%|███▍      | 22/63 [00:17<00:30,  1.35it/s]
2025-10-12 22:04:27 | 37%|███▋      | 23/63 [00:17<00:29,  1.37it/s]
2025-10-12 22:04:28 | 38%|███▊      | 24/63 [00:18<00:28,  1.38it/s]
2025-10-12 22:04:28 | 40%|███▉      | 25/63 [00:19<00:28,  1.32it/s]
2025-10-12 22:04:29 | 41%|████▏     | 26/63 [00:20<00:28,  1.30it/s]
2025-10-12 22:04:30 | 43%|████▎     | 27/63 [00:21<00:28,  1.26it/s]
2025-10-12 22:04:31 | 44%|████▍     | 28/63 [00:21<00:27,  1.25it/s]
2025-10-12 22:04:32 | 46%|████▌     | 29/63 [00:22<00:28,  1.21it/s]
2025-10-12 22:04:33 | 48%|████▊     | 30/63 [00:23<00:27,  1.21it/s]
2025-10-12 22:04:33 | 49%|████▉     | 31/63 [00:24<00:26,  1.21it/s]
2025-10-12 22:04:34 | 51%|█████     | 32/63 [00:25<00:26,  1.17it/s]
2025-10-12 22:04:35 | 52%|█████▏    | 33/63 [00:26<00:24,  1.24it/s]
2025-10-12 22:04:36 | 54%|█████▍    | 34/63 [00:26<00:22,  1.27it/s]
2025-10-12 22:04:36 | 56%|█████▌    | 35/63 [00:27<00:21,  1.28it/s]
2025-10-12 22:04:37 | 57%|█████▋    | 36/63 [00:28<00:20,  1.33it/s]
2025-10-12 22:04:38 | 59%|█████▊    | 37/63 [00:28<00:19,  1.32it/s]
2025-10-12 22:04:39 | 60%|██████    | 38/63 [00:29<00:19,  1.30it/s]
2025-10-12 22:04:39 | 62%|██████▏   | 39/63 [00:30<00:18,  1.33it/s]
2025-10-12 22:04:40 | 63%|██████▎   | 40/63 [00:31<00:17,  1.32it/s]
2025-10-12 22:04:41 | 65%|██████▌   | 41/63 [00:32<00:16,  1.30it/s]
2025-10-12 22:04:42 | 67%|██████▋   | 42/63 [00:32<00:15,  1.32it/s]
2025-10-12 22:04:43 | 68%|██████▊   | 43/63 [00:33<00:15,  1.27it/s]
2025-10-12 22:04:43 | 70%|██████▉   | 44/63 [00:34<00:14,  1.30it/s]
2025-10-12 22:04:44 | 71%|███████▏  | 45/63 [00:35<00:14,  1.28it/s]
2025-10-12 22:04:45 | 73%|███████▎  | 46/63 [00:35<00:13,  1.27it/s]
2025-10-12 22:04:46 | 75%|███████▍  | 47/63 [00:36<00:12,  1.28it/s]
2025-10-12 22:04:46 | 76%|███████▌  | 48/63 [00:37<00:11,  1.31it/s]
2025-10-12 22:04:47 | 78%|███████▊  | 49/63 [00:38<00:10,  1.37it/s]
2025-10-12 22:04:48 | 79%|███████▉  | 50/63 [00:38<00:09,  1.34it/s]
2025-10-12 22:04:49 | 81%|████████  | 51/63 [00:39<00:08,  1.35it/s]
2025-10-12 22:04:49 | 83%|████████▎ | 52/63 [00:40<00:08,  1.33it/s]
2025-10-12 22:04:51 | 84%|████████▍ | 53/63 [00:41<00:10,  1.00s/it]
2025-10-12 22:04:52 | 86%|████████▌ | 54/63 [00:42<00:08,  1.07it/s]
2025-10-12 22:04:52 | 87%|████████▋ | 55/63 [00:43<00:06,  1.15it/s]
2025-10-12 22:04:53 | 89%|████████▉ | 56/63 [00:44<00:05,  1.20it/s]
2025-10-12 22:04:54 | 90%|█████████ | 57/63 [00:45<00:04,  1.22it/s]
2025-10-12 22:04:55 | 92%|█████████▏| 58/63 [00:45<00:04,  1.23it/s]
2025-10-12 22:04:56 | 94%|█████████▎| 59/63 [00:46<00:03,  1.23it/s]
2025-10-12 22:04:56 | 95%|█████████▌| 60/63 [00:47<00:02,  1.20it/s]
2025-10-12 22:04:57 | 97%|█████████▋| 61/63 [00:48<00:01,  1.22it/s]
2025-10-12 22:04:58 | 98%|█████████▊| 62/63 [00:49<00:00,  1.23it/s]
2025-10-12 22:04:59 | 100%|██████████| 63/63 [00:49<00:00,  1.25it/s]
2025-10-12 22:04:59 | 최종 평가 결과:
2025-10-12 22:04:59 | eval_rouge1: 0.4140
2025-10-12 22:04:59 | eval_rouge2: 0.2557
2025-10-12 22:04:59 | eval_rougeL: 0.4067
2025-10-12 22:04:59 | eval_rouge_sum: 1.0764
2025-10-12 22:04:59 | ============================================================
2025-10-12 22:04:59 | ✅ 학습 완료!
2025-10-12 22:04:59 | ============================================================
2025-10-12 22:04:59 | ✅ kobart 학습 완료
2025-10-12 22:04:59 | ==================================================
2025-10-12 22:04:59 | 모델 2/6: llama-3.2-korean-3b
2025-10-12 22:04:59 | ==================================================
2025-10-12 22:04:59 | 모델 타입: causal_lm
2025-10-12 22:04:59 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 22:04:59 | 모델 로딩 중...
2025-10-12 22:04:59 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 22:04:59 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 22:05:00 | Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.06s/it]
2025-10-12 22:05:01 | Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]
2025-10-12 22:05:01 | 토크나이저 로딩 중...
2025-10-12 22:05:02 | 패딩 토큰 설정: <|eot_id|>
2025-10-12 22:05:02 | LoRA 설정 적용 중...
2025-10-12 22:05:02 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 22:05:02 | ✅ LoRA 적용 완료
2025-10-12 22:05:02 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-12 22:05:02 | 전체 파라미터: 3,237,063,680
2025-10-12 22:05:02 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 22:05:02 | ✅ Gradient Checkpointing 활성화
2025-10-12 22:05:02 | ✅ Causal LM 로드 완료
2025-10-12 22:05:02 | ============================================================
2025-10-12 22:05:02 | 모델 학습 시작
2025-10-12 22:05:02 | ============================================================
2025-10-12 22:05:03 | WandB 로그인 상태: ieyeppo-job
2025-10-12 22:05:03 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-12 22:05:03 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-12 22:05:04 | wandb: setting up run kwr8gvvo
2025-10-12 22:05:04 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 22:05:04 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_220503-kwr8gvvo
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 22:05:04 | wandb: Syncing run 1012-2205-llama_3.2_3b_qlora
2025-10-12 22:05:04 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 22:05:04 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/kwr8gvvo
2025-10-12 22:05:04 | wandb: Detected [openai] in use.
2025-10-12 22:05:04 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 22:05:04 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 22:05:04 | 📋 실험명: 1012-2205-llama_3.2_3b_qlora
2025-10-12 22:05:04 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/kwr8gvvo
2025-10-12 22:05:04 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:05:04 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 22:05:04 | 학습 진행 중...
2025-10-12 22:05:04 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 22:05:04 | 0%|          | 0/1558 [00:00<?, ?it/s]
2025-10-12 22:05:29 | 1%|          | 8/1558 [00:24<1:18:54,  3.05s/it]
2025-10-12 22:05:34 | {'loss': 1.6248, 'grad_norm': 2.0260937213897705, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-12 22:05:34 | 1%|          | 10/1558 [00:30<1:16:13,  2.95s/it]
2025-10-12 22:06:05 | {'loss': 1.5723, 'grad_norm': 2.1121692657470703, 'learning_rate': 1.9e-07, 'epoch': 0.01}
2025-10-12 22:06:05 | 1%|▏         | 20/1558 [01:00<1:18:26,  3.06s/it]
2025-10-12 22:06:16 | 2%|▏         | 24/1558 [01:11<1:14:37,  2.92s/it]
2025-10-12 22:06:36 | {'loss': 1.6231, 'grad_norm': 1.6558647155761719, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.02}
2025-10-12 22:06:36 | 2%|▏         | 30/1558 [01:32<1:26:44,  3.41s/it]
2025-10-12 22:07:07 | 3%|▎         | 39/1558 [02:03<1:29:12,  3.52s/it]
2025-10-12 22:07:11 | {'loss': 1.6449, 'grad_norm': 1.982552409172058, 'learning_rate': 3.9e-07, 'epoch': 0.03}
2025-10-12 22:07:11 | 3%|▎         | 40/1558 [02:06<1:28:46,  3.51s/it]
2025-10-12 22:07:45 | {'loss': 1.5791, 'grad_norm': 1.8338807821273804, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.03}
2025-10-12 22:07:45 | 3%|▎         | 50/1558 [02:40<1:25:48,  3.41s/it]
2025-10-12 22:08:02 | 4%|▎         | 55/1558 [02:57<1:22:34,  3.30s/it]
2025-10-12 22:08:20 | {'loss': 1.6087, 'grad_norm': 1.7342654466629028, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:08:20 | 4%|▍         | 60/1558 [03:15<1:29:22,  3.58s/it]
2025-10-12 22:08:53 | {'loss': 1.6157, 'grad_norm': 2.483124017715454, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:08:53 | 4%|▍         | 70/1558 [03:48<1:20:14,  3.24s/it]
2025-10-12 22:08:56 | 5%|▍         | 71/1558 [03:51<1:17:51,  3.14s/it]
2025-10-12 22:09:27 | {'loss': 1.6359, 'grad_norm': 2.1355130672454834, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.05}
2025-10-12 22:09:27 | 5%|▌         | 80/1558 [04:22<1:23:14,  3.38s/it]
2025-10-12 22:09:47 | 6%|▌         | 86/1558 [04:42<1:18:07,  3.18s/it]
2025-10-12 22:10:01 | {'loss': 1.566, 'grad_norm': 2.1057934761047363, 'learning_rate': 8.900000000000001e-07, 'epoch': 0.06}
2025-10-12 22:10:01 | 6%|▌         | 90/1558 [04:57<1:23:45,  3.42s/it]
2025-10-12 22:10:36 | {'loss': 1.6224, 'grad_norm': 2.015709400177002, 'learning_rate': 9.9e-07, 'epoch': 0.06}
2025-10-12 22:10:36 | 6%|▋         | 100/1558 [05:31<1:26:59,  3.58s/it]
2025-10-12 22:10:42 | 7%|▋         | 102/1558 [05:38<1:23:47,  3.45s/it]
2025-10-12 22:11:11 | {'loss': 1.5647, 'grad_norm': 1.6748429536819458, 'learning_rate': 1.0900000000000002e-06, 'epoch': 0.07}
2025-10-12 22:11:11 | 7%|▋         | 110/1558 [06:06<1:23:15,  3.45s/it]
2025-10-12 22:11:34 | 8%|▊         | 117/1558 [06:29<1:20:29,  3.35s/it]
2025-10-12 22:11:44 | {'loss': 1.5591, 'grad_norm': 1.9124680757522583, 'learning_rate': 1.19e-06, 'epoch': 0.08}
2025-10-12 22:11:44 | 8%|▊         | 120/1558 [06:39<1:17:31,  3.23s/it]
2025-10-12 22:12:16 | {'loss': 1.5636, 'grad_norm': 1.8828574419021606, 'learning_rate': 1.2900000000000001e-06, 'epoch': 0.08}
2025-10-12 22:12:16 | 8%|▊         | 130/1558 [07:11<1:13:57,  3.11s/it]
2025-10-12 22:12:25 | 9%|▊         | 133/1558 [07:20<1:13:58,  3.11s/it]
2025-10-12 22:12:48 | {'loss': 1.4501, 'grad_norm': 1.7867604494094849, 'learning_rate': 1.3900000000000002e-06, 'epoch': 0.09}
2025-10-12 22:12:48 | 9%|▉         | 140/1558 [07:43<1:13:38,  3.12s/it]
2025-10-12 22:13:19 | 10%|▉         | 149/1558 [08:14<1:20:18,  3.42s/it]
2025-10-12 22:13:23 | {'loss': 1.5337, 'grad_norm': 1.7558077573776245, 'learning_rate': 1.4900000000000001e-06, 'epoch': 0.1}
2025-10-12 22:13:23 | 10%|▉         | 150/1558 [08:18<1:22:56,  3.53s/it]
2025-10-12 22:13:56 | {'loss': 1.411, 'grad_norm': 1.825000524520874, 'learning_rate': 1.5900000000000002e-06, 'epoch': 0.1}
2025-10-12 22:13:56 | 10%|█         | 160/1558 [08:51<1:19:47,  3.42s/it]
2025-10-12 22:14:09 | 11%|█         | 164/1558 [09:04<1:15:29,  3.25s/it]
2025-10-12 22:14:30 | {'loss': 1.4298, 'grad_norm': 1.9290547370910645, 'learning_rate': 1.6900000000000003e-06, 'epoch': 0.11}
2025-10-12 22:14:30 | 11%|█         | 170/1558 [09:25<1:17:27,  3.35s/it]
2025-10-12 22:15:03 | 12%|█▏        | 180/1558 [09:58<1:17:04,  3.36s/it]
2025-10-12 22:15:03 | {'loss': 1.369, 'grad_norm': 1.9755674600601196, 'learning_rate': 1.79e-06, 'epoch': 0.12}
2025-10-12 22:15:03 | 12%|█▏        | 180/1558 [09:58<1:17:04,  3.36s/it]
2025-10-12 22:15:39 | {'loss': 1.3959, 'grad_norm': 2.018749713897705, 'learning_rate': 1.8900000000000001e-06, 'epoch': 0.12}
2025-10-12 22:15:39 | 12%|█▏        | 190/1558 [10:34<1:24:08,  3.69s/it]
2025-10-12 22:15:56 | 13%|█▎        | 195/1558 [10:51<1:20:03,  3.52s/it]
2025-10-12 22:16:13 | {'loss': 1.3611, 'grad_norm': 1.7554312944412231, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.13}
2025-10-12 22:16:13 | 13%|█▎        | 200/1558 [11:08<1:14:33,  3.29s/it]
2025-10-12 22:16:45 | {'loss': 1.4014, 'grad_norm': 1.555174469947815, 'learning_rate': 2.09e-06, 'epoch': 0.13}
2025-10-12 22:16:45 | 13%|█▎        | 210/1558 [11:40<1:11:15,  3.17s/it]
2025-10-12 22:16:49 | 14%|█▎        | 211/1558 [11:44<1:12:25,  3.23s/it]
2025-10-12 22:17:19 | {'loss': 1.4433, 'grad_norm': 2.096745491027832, 'learning_rate': 2.19e-06, 'epoch': 0.14}
2025-10-12 22:17:19 | 14%|█▍        | 220/1558 [12:15<1:14:44,  3.35s/it]
2025-10-12 22:17:40 | 15%|█▍        | 226/1558 [12:35<1:15:05,  3.38s/it]
2025-10-12 22:17:55 | {'loss': 1.3526, 'grad_norm': 1.9017237424850464, 'learning_rate': 2.29e-06, 'epoch': 0.15}
2025-10-12 22:17:55 | 15%|█▍        | 230/1558 [12:50<1:20:19,  3.63s/it]
2025-10-12 22:18:29 | {'loss': 1.4157, 'grad_norm': 1.548293948173523, 'learning_rate': 2.39e-06, 'epoch': 0.15}
2025-10-12 22:18:29 | 15%|█▌        | 240/1558 [13:24<1:10:05,  3.19s/it]
2025-10-12 22:18:36 | 16%|█▌        | 242/1558 [13:31<1:17:08,  3.52s/it]
2025-10-12 22:19:04 | {'loss': 1.3324, 'grad_norm': 1.7229104042053223, 'learning_rate': 2.4900000000000003e-06, 'epoch': 0.16}
2025-10-12 22:19:04 | 16%|█▌        | 250/1558 [13:59<1:17:48,  3.57s/it]
2025-10-12 22:19:32 | 17%|█▋        | 258/1558 [14:28<1:13:24,  3.39s/it]
2025-10-12 22:19:39 | {'loss': 1.2984, 'grad_norm': 1.9131561517715454, 'learning_rate': 2.59e-06, 'epoch': 0.17}
2025-10-12 22:19:39 | 17%|█▋        | 260/1558 [14:34<1:11:14,  3.29s/it]
