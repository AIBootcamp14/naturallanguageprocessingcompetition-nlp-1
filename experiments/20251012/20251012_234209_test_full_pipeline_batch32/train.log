2025-10-12 23:42:09 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-12 23:42:11 | 📊 FULL 모드 실행 중...
2025-10-12 23:42:11 | ============================================================
2025-10-12 23:42:11 | = FULL PIPELINE 실행 시작
2025-10-12 23:42:11 | =대상 모델: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-12 23:42:11 | =앙상블 앙상블 전략: stacking
2025-10-12 23:42:11 | = TTA 사용: True
2025-10-12 23:42:11 | ============================================================
2025-10-12 23:42:11 | [1/6] 데이터 로딩...
2025-10-12 23:42:12 | ✅ 학습 데이터: 12457개
2025-10-12 23:42:12 | ✅ 검증 데이터: 499개
2025-10-12 23:42:12 | ⚙️ max_train_samples 적용: 학습 데이터 2000개로 제한
2025-10-12 23:42:12 | [2/6] 다중 모델 학습 (6 모델)...
2025-10-12 23:42:12 | ==================================================
2025-10-12 23:42:12 | 모델 1/6: kobart
2025-10-12 23:42:12 | ==================================================
2025-10-12 23:42:12 | 모델 타입: encoder_decoder
2025-10-12 23:42:12 | ============================================================
2025-10-12 23:42:12 | 모델 및 토크나이저 로딩 시작
2025-10-12 23:42:12 | ============================================================
2025-10-12 23:42:12 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-12 23:42:13 | 모델 로딩: digit82/kobart-summarization
2025-10-12 23:42:13 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-12 23:42:14 | → 디바이스: cuda
2025-10-12 23:42:14 | → 전체 파라미터: 123,859,968
2025-10-12 23:42:14 | → 학습 가능 파라미터: 123,859,968
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | 모델 학습 시작
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:14 | 학습 진행 중...
2025-10-12 23:42:14 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:42:15 | 2%|▏         | 1/63 [00:00<00:36,  1.70it/s]
2025-10-12 23:42:15 | 3%|▎         | 2/63 [00:00<00:19,  3.21it/s]
2025-10-12 23:42:15 | 6%|▋         | 4/63 [00:00<00:10,  5.59it/s]
2025-10-12 23:42:15 | 10%|▉         | 6/63 [00:01<00:08,  7.10it/s]
2025-10-12 23:42:15 | 11%|█         | 7/63 [00:01<00:07,  7.66it/s]
2025-10-12 23:42:15 | 14%|█▍        | 9/63 [00:01<00:06,  8.50it/s]
2025-10-12 23:42:15 | 16%|█▌        | 10/63 [00:01<00:06,  8.79it/s]
2025-10-12 23:42:16 | 17%|█▋        | 11/63 [00:01<00:05,  9.06it/s]
2025-10-12 23:42:16 | 19%|█▉        | 12/63 [00:01<00:05,  9.25it/s]
2025-10-12 23:42:16 | 22%|██▏       | 14/63 [00:01<00:05,  9.60it/s]
2025-10-12 23:42:16 | 25%|██▌       | 16/63 [00:02<00:04,  9.82it/s]
2025-10-12 23:42:16 | 29%|██▊       | 18/63 [00:02<00:04,  9.88it/s]
2025-10-12 23:42:16 | 32%|███▏      | 20/63 [00:02<00:04,  9.97it/s]
2025-10-12 23:42:17 | 35%|███▍      | 22/63 [00:02<00:04, 10.03it/s]
2025-10-12 23:42:17 | 38%|███▊      | 24/63 [00:02<00:03, 10.09it/s]
2025-10-12 23:42:17 | 41%|████▏     | 26/63 [00:03<00:03, 10.05it/s]
2025-10-12 23:42:17 | 44%|████▍     | 28/63 [00:03<00:03, 10.09it/s]
2025-10-12 23:42:17 | 48%|████▊     | 30/63 [00:03<00:03, 10.00it/s]
2025-10-12 23:42:18 | 51%|█████     | 32/63 [00:03<00:03, 10.08it/s]
2025-10-12 23:42:18 | 54%|█████▍    | 34/63 [00:03<00:02, 10.00it/s]
2025-10-12 23:42:18 | 57%|█████▋    | 36/63 [00:04<00:02, 10.05it/s]
2025-10-12 23:42:18 | 60%|██████    | 38/63 [00:04<00:02, 10.17it/s]
2025-10-12 23:42:18 | 63%|██████▎   | 40/63 [00:04<00:02, 10.18it/s]
2025-10-12 23:42:19 | 67%|██████▋   | 42/63 [00:04<00:02, 10.18it/s]
2025-10-12 23:42:19 | 70%|██████▉   | 44/63 [00:04<00:01, 10.23it/s]
2025-10-12 23:42:19 | 73%|███████▎  | 46/63 [00:05<00:01, 10.20it/s]
2025-10-12 23:42:19 | 76%|███████▌  | 48/63 [00:05<00:01, 10.22it/s]
2025-10-12 23:42:19 | 79%|███████▉  | 50/63 [00:05<00:01, 10.21it/s]
2025-10-12 23:42:20 | 83%|████████▎ | 52/63 [00:05<00:01, 10.25it/s]
2025-10-12 23:42:20 | 86%|████████▌ | 54/63 [00:05<00:00, 10.23it/s]
2025-10-12 23:42:20 | 89%|████████▉ | 56/63 [00:06<00:00, 10.21it/s]
2025-10-12 23:42:20 | 92%|█████████▏| 58/63 [00:06<00:00, 10.20it/s]
2025-10-12 23:42:20 | 95%|█████████▌| 60/63 [00:06<00:00, 10.26it/s]
2025-10-12 23:42:21 | 98%|█████████▊| 62/63 [00:06<00:00, 10.32it/s]
2025-10-12 23:42:22 | 0%|          | 0/16 [00:00<?, ?it/s]
2025-10-12 23:42:22 | [A
2025-10-12 23:42:23 | 12%|█▎        | 2/16 [00:00<00:06,  2.19it/s]
2025-10-12 23:42:23 | [A
2025-10-12 23:42:24 | 19%|█▉        | 3/16 [00:01<00:08,  1.51it/s]
2025-10-12 23:42:24 | [A
2025-10-12 23:42:25 | 25%|██▌       | 4/16 [00:02<00:09,  1.30it/s]
2025-10-12 23:42:25 | [A
2025-10-12 23:42:26 | 31%|███▏      | 5/16 [00:03<00:09,  1.16it/s]
2025-10-12 23:42:26 | [A
2025-10-12 23:42:27 | 38%|███▊      | 6/16 [00:04<00:08,  1.13it/s]
2025-10-12 23:42:27 | [A
2025-10-12 23:42:28 | 44%|████▍     | 7/16 [00:05<00:08,  1.10it/s]
2025-10-12 23:42:28 | [A
2025-10-12 23:42:29 | 50%|█████     | 8/16 [00:06<00:07,  1.06it/s]
2025-10-12 23:42:29 | [A
2025-10-12 23:42:30 | 56%|█████▋    | 9/16 [00:07<00:06,  1.07it/s]
2025-10-12 23:42:30 | [A
2025-10-12 23:42:30 | 62%|██████▎   | 10/16 [00:08<00:05,  1.08it/s]
2025-10-12 23:42:30 | [A
2025-10-12 23:42:31 | 69%|██████▉   | 11/16 [00:09<00:04,  1.07it/s]
2025-10-12 23:42:31 | [A
2025-10-12 23:42:32 | 75%|███████▌  | 12/16 [00:10<00:03,  1.06it/s]
2025-10-12 23:42:32 | [A
2025-10-12 23:42:33 | 81%|████████▏ | 13/16 [00:11<00:02,  1.03it/s]
2025-10-12 23:42:33 | [A
2025-10-12 23:42:34 | 100%|██████████| 63/63 [00:20<00:00, 10.32it/s]
2025-10-12 23:42:34 | 88%|████████▊ | 14/16 [00:12<00:01,  1.02it/s]
2025-10-12 23:42:34 | [A
2025-10-12 23:42:35 | 94%|█████████▍| 15/16 [00:13<00:00,  1.04it/s]
2025-10-12 23:42:35 | [A
2025-10-12 23:42:36 | 100%|██████████| 16/16 [00:14<00:00,  1.11it/s]
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | {'eval_loss': 2.4057695865631104, 'eval_rouge1': 0.3766561944581691, 'eval_rouge2': 0.12390926708561978, 'eval_rougeL': 0.34691153104617106, 'eval_rouge_sum': 0.8474769925899599, 'eval_runtime': 15.487, 'eval_samples_per_second': 32.221, 'eval_steps_per_second': 1.033, 'epoch': 1.0}
2025-10-12 23:42:36 | 100%|██████████| 63/63 [00:22<00:00, 10.32it/s]
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | [A
2025-10-12 23:42:37 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-12 23:42:37 | {'train_runtime': 23.5121, 'train_samples_per_second': 85.063, 'train_steps_per_second': 2.679, 'train_loss': 2.867183140345982, 'epoch': 1.0}
2025-10-12 23:42:37 | 100%|██████████| 63/63 [00:23<00:00, 10.32it/s]
2025-10-12 23:42:37 | 최종 모델 저장 중...
2025-10-12 23:42:38 | → 모델 저장 위치: experiments/20251012/20251012_234209_test_full_pipeline_batch32/model_0_kobart/default/final_model
2025-10-12 23:42:38 | 최종 평가 중...
2025-10-12 23:42:39 | 0%|          | 0/16 [00:00<?, ?it/s]
2025-10-12 23:42:40 | 12%|█▎        | 2/16 [00:00<00:06,  2.08it/s]
2025-10-12 23:42:41 | 19%|█▉        | 3/16 [00:01<00:08,  1.50it/s]
2025-10-12 23:42:42 | 25%|██▌       | 4/16 [00:02<00:09,  1.27it/s]
2025-10-12 23:42:44 | 31%|███▏      | 5/16 [00:04<00:12,  1.17s/it]
2025-10-12 23:42:45 | 38%|███▊      | 6/16 [00:05<00:10,  1.10s/it]
2025-10-12 23:42:46 | 44%|████▍     | 7/16 [00:06<00:09,  1.05s/it]
2025-10-12 23:42:47 | 50%|█████     | 8/16 [00:07<00:08,  1.00s/it]
2025-10-12 23:42:48 | 56%|█████▋    | 9/16 [00:08<00:06,  1.01it/s]
2025-10-12 23:42:49 | 62%|██████▎   | 10/16 [00:09<00:06,  1.02s/it]
2025-10-12 23:42:50 | 69%|██████▉   | 11/16 [00:10<00:05,  1.01s/it]
2025-10-12 23:42:51 | 75%|███████▌  | 12/16 [00:11<00:03,  1.02it/s]
2025-10-12 23:42:52 | 81%|████████▏ | 13/16 [00:12<00:02,  1.04it/s]
2025-10-12 23:42:53 | 88%|████████▊ | 14/16 [00:13<00:01,  1.05it/s]
2025-10-12 23:42:54 | 94%|█████████▍| 15/16 [00:14<00:00,  1.00it/s]
2025-10-12 23:42:55 | 100%|██████████| 16/16 [00:15<00:00,  1.00it/s]
2025-10-12 23:42:55 | 최종 평가 결과:
2025-10-12 23:42:55 | eval_rouge1: 0.3767
2025-10-12 23:42:55 | eval_rouge2: 0.1239
2025-10-12 23:42:55 | eval_rougeL: 0.3469
2025-10-12 23:42:55 | eval_rouge_sum: 0.8475
2025-10-12 23:42:55 | ============================================================
2025-10-12 23:42:55 | ✅ 학습 완료!
2025-10-12 23:42:55 | ============================================================
2025-10-12 23:42:55 | ✅ kobart 학습 완료
2025-10-12 23:42:55 | ==================================================
2025-10-12 23:42:55 | 모델 2/6: llama-3.2-korean-3b
2025-10-12 23:42:55 | ==================================================
2025-10-12 23:42:55 | 모델 타입: causal_lm
2025-10-12 23:42:55 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 23:42:55 | 모델 로딩 중...
2025-10-12 23:42:55 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 23:42:55 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 23:42:56 | Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]
2025-10-12 23:42:56 | Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  2.13it/s]
2025-10-12 23:42:56 | 토크나이저 로딩 중...
2025-10-12 23:42:57 | 패딩 토큰 설정: <|eot_id|>
2025-10-12 23:42:57 | LoRA 설정 적용 중...
2025-10-12 23:42:57 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:42:57 | ✅ LoRA 적용 완료
2025-10-12 23:42:57 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-12 23:42:57 | 전체 파라미터: 3,237,063,680
2025-10-12 23:42:57 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 23:42:57 | ✅ Gradient Checkpointing 활성화
2025-10-12 23:42:57 | ✅ Causal LM 로드 완료
2025-10-12 23:42:57 | ============================================================
2025-10-12 23:42:57 | 모델 학습 시작
2025-10-12 23:42:57 | ============================================================
2025-10-12 23:42:58 | WandB 로그인 상태: ieyeppo-job
2025-10-12 23:42:58 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-12 23:42:59 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-12 23:42:59 | wandb: setting up run e8phcsen
2025-10-12 23:42:59 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:42:59 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234258-e8phcsen
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:42:59 | wandb: Syncing run 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:42:59 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | wandb: Detected [openai] in use.
2025-10-12 23:42:59 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:42:59 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:42:59 | 📋 실험명: 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:59 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:42:59 | 학습 진행 중...
2025-10-12 23:42:59 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:43:00 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:43:12 | ❌ llama-3.2-korean-3b 학습 실패: OutOfMemoryError: CUDA out of memory. Tried to allocate 15.66 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 35.47 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:12 | 오류 로그 저장: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/llama-3.2-korean-3b_error.log
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | 모델 3/6: qwen3-4b
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | 모델 타입: causal_lm
2025-10-12 23:43:12 | Loading Causal LM: Qwen/Qwen3-4B-Instruct-2507
2025-10-12 23:43:12 | 모델 로딩 중...
2025-10-12 23:43:12 | Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
2025-10-12 23:43:12 | [A
2025-10-12 23:43:15 | Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.71s/it]
2025-10-12 23:43:15 | [A
2025-10-12 23:43:20 | Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.21s/it]
2025-10-12 23:43:20 | [A
2025-10-12 23:43:20 | Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.68s/it]
2025-10-12 23:43:21 | 토크나이저 로딩 중...
2025-10-12 23:43:21 | LoRA 설정 적용 중...
2025-10-12 23:43:21 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:43:22 | ✅ LoRA 적용 완료
2025-10-12 23:43:22 | 학습 가능 파라미터: 33,030,144 (0.81%)
2025-10-12 23:43:22 | 전체 파라미터: 4,055,498,240
2025-10-12 23:43:22 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 23:43:22 | ✅ Gradient Checkpointing 활성화
2025-10-12 23:43:22 | ✅ Causal LM 로드 완료
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | 모델 학습 시작
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | WandB 로그인 상태: ieyeppo-job
2025-10-12 23:43:22 | wandb: Finishing previous runs because reinit is set to True.
2025-10-12 23:43:23 | wandb: uploading summary, console lines 20-36
2025-10-12 23:43:23 | wandb: 🚀 View run 1012-2342-llama_3.2_3b_qlora at: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-12 23:43:23 | wandb: Find logs at: ./wandb/wandb/run-20251012_234258-e8phcsen/logs
2025-10-12 23:43:24 | wandb: setting up run jl8vntvn
2025-10-12 23:43:24 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:43:24 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234322-jl8vntvn
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:43:24 | wandb: Syncing run 1012-2343-qwen3_4b_qlora
2025-10-12 23:43:24 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:43:24 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
2025-10-12 23:43:24 | 📋 실험명: 1012-2343-qwen3_4b_qlora
2025-10-12 23:43:24 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
2025-10-12 23:43:24 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:43:24 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:43:24 | 학습 진행 중...
2025-10-12 23:43:24 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
2025-10-12 23:43:24 | 0%|          | 0/63 [00:24<?, ?it/s]
2025-10-12 23:43:39 | ❌ qwen3-4b 학습 실패: OutOfMemoryError: CUDA out of memory. Tried to allocate 18.55 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:39 | 오류 로그 저장: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/qwen3-4b_error.log
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | 모델 4/6: solar-10.7b
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | 모델 타입: causal_lm
2025-10-12 23:43:39 | Loading Causal LM: upstage/solar-10.7b-instruct-v1.0
2025-10-12 23:43:39 | 모델 로딩 중...
2025-10-12 23:43:40 | Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
2025-10-12 23:43:40 | [A
2025-10-12 23:43:51 | Loading checkpoint shards:  20%|██        | 1/5 [00:11<00:46, 11.57s/it]
2025-10-12 23:43:51 | [A
2025-10-12 23:44:06 | Loading checkpoint shards:  40%|████      | 2/5 [00:26<00:39, 13.32s/it]
2025-10-12 23:44:06 | [A
2025-10-12 23:44:11 | Loading checkpoint shards:  60%|██████    | 3/5 [00:31<00:19,  9.60s/it]
2025-10-12 23:44:11 | [A
2025-10-12 23:44:16 | Loading checkpoint shards:  80%|████████  | 4/5 [00:35<00:07,  7.64s/it]
2025-10-12 23:44:16 | [A
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|██████████| 5/5 [00:37<00:00,  5.29s/it]
2025-10-12 23:44:17 | [A
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|██████████| 5/5 [00:37<00:00,  7.41s/it]
2025-10-12 23:44:18 | WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.
2025-10-12 23:44:18 | 토크나이저 로딩 중...
2025-10-12 23:44:19 | LoRA 설정 적용 중...
2025-10-12 23:44:19 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:44:19 | ✅ LoRA 적용 완료
2025-10-12 23:44:19 | 학습 가능 파라미터: 62,914,560 (0.58%)
2025-10-12 23:44:19 | 전체 파라미터: 10,794,438,656
2025-10-12 23:44:19 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 23:44:19 | ✅ Gradient Checkpointing 활성화
2025-10-12 23:44:19 | ✅ Causal LM 로드 완료
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | 모델 학습 시작
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | WandB 로그인 상태: ieyeppo-job
2025-10-12 23:44:19 | wandb: Finishing previous runs because reinit is set to True.
2025-10-12 23:44:20 | wandb: updating run metadata
2025-10-12 23:44:20 | wandb: uploading summary, console lines 19-37
2025-10-12 23:44:21 | wandb: 🚀 View run 1012-2343-qwen3_4b_qlora at: https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-12 23:44:21 | wandb: Find logs at: ./wandb/wandb/run-20251012_234322-jl8vntvn/logs
2025-10-12 23:44:21 | wandb: setting up run 8qkqj7y3
2025-10-12 23:44:21 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:44:21 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234419-8qkqj7y3
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:44:21 | wandb: Syncing run 1012-2344-solar_10.7b_qlora
2025-10-12 23:44:21 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:44:21 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/8qkqj7y3
2025-10-12 23:44:21 | 📋 실험명: 1012-2344-solar_10.7b_qlora
2025-10-12 23:44:21 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/8qkqj7y3
2025-10-12 23:44:21 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:44:21 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:44:22 | 학습 진행 중...
2025-10-12 23:44:22 | 0%|          | 0/63 [00:56<?, ?it/s]
