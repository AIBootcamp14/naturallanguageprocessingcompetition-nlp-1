2025-10-12 23:42:09 | >> í‘œì¤€ ì¶œë ¥ ë° ì˜¤ë¥˜ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ë¦¬ë””ë ‰ì…˜ ì‹œì‘
2025-10-12 23:42:11 | ğŸ“Š FULL ëª¨ë“œ ì‹¤í–‰ ì¤‘...
2025-10-12 23:42:11 | ============================================================
2025-10-12 23:42:11 | = FULL PIPELINE ì‹¤í–‰ ì‹œì‘
2025-10-12 23:42:11 | =ëŒ€ìƒ ëª¨ë¸: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-12 23:42:11 | =ì•™ìƒë¸” ì•™ìƒë¸” ì „ëµ: stacking
2025-10-12 23:42:11 | = TTA ì‚¬ìš©: True
2025-10-12 23:42:11 | ============================================================
2025-10-12 23:42:11 | [1/6] ë°ì´í„° ë¡œë”©...
2025-10-12 23:42:12 | âœ… í•™ìŠµ ë°ì´í„°: 12457ê°œ
2025-10-12 23:42:12 | âœ… ê²€ì¦ ë°ì´í„°: 499ê°œ
2025-10-12 23:42:12 | âš™ï¸ max_train_samples ì ìš©: í•™ìŠµ ë°ì´í„° 2000ê°œë¡œ ì œí•œ
2025-10-12 23:42:12 | [2/6] ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ (6 ëª¨ë¸)...
2025-10-12 23:42:12 | ==================================================
2025-10-12 23:42:12 | ëª¨ë¸ 1/6: kobart
2025-10-12 23:42:12 | ==================================================
2025-10-12 23:42:12 | ëª¨ë¸ íƒ€ì…: encoder_decoder
2025-10-12 23:42:12 | ============================================================
2025-10-12 23:42:12 | ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘
2025-10-12 23:42:12 | ============================================================
2025-10-12 23:42:12 | í† í¬ë‚˜ì´ì € ë¡œë”©: digit82/kobart-summarization
2025-10-12 23:42:13 | ëª¨ë¸ ë¡œë”©: digit82/kobart-summarization
2025-10-12 23:42:13 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-12 23:42:14 | â†’ ë””ë°”ì´ìŠ¤: cuda
2025-10-12 23:42:14 | â†’ ì „ì²´ íŒŒë¼ë¯¸í„°: 123,859,968
2025-10-12 23:42:14 | â†’ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 123,859,968
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 23:42:14 | ============================================================
2025-10-12 23:42:14 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:14 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:42:14 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:42:15 | 2%|â–         | 1/63 [00:00<00:36,  1.70it/s]
2025-10-12 23:42:15 | 3%|â–         | 2/63 [00:00<00:19,  3.21it/s]
2025-10-12 23:42:15 | 6%|â–‹         | 4/63 [00:00<00:10,  5.59it/s]
2025-10-12 23:42:15 | 10%|â–‰         | 6/63 [00:01<00:08,  7.10it/s]
2025-10-12 23:42:15 | 11%|â–ˆ         | 7/63 [00:01<00:07,  7.66it/s]
2025-10-12 23:42:15 | 14%|â–ˆâ–        | 9/63 [00:01<00:06,  8.50it/s]
2025-10-12 23:42:15 | 16%|â–ˆâ–Œ        | 10/63 [00:01<00:06,  8.79it/s]
2025-10-12 23:42:16 | 17%|â–ˆâ–‹        | 11/63 [00:01<00:05,  9.06it/s]
2025-10-12 23:42:16 | 19%|â–ˆâ–‰        | 12/63 [00:01<00:05,  9.25it/s]
2025-10-12 23:42:16 | 22%|â–ˆâ–ˆâ–       | 14/63 [00:01<00:05,  9.60it/s]
2025-10-12 23:42:16 | 25%|â–ˆâ–ˆâ–Œ       | 16/63 [00:02<00:04,  9.82it/s]
2025-10-12 23:42:16 | 29%|â–ˆâ–ˆâ–Š       | 18/63 [00:02<00:04,  9.88it/s]
2025-10-12 23:42:16 | 32%|â–ˆâ–ˆâ–ˆâ–      | 20/63 [00:02<00:04,  9.97it/s]
2025-10-12 23:42:17 | 35%|â–ˆâ–ˆâ–ˆâ–      | 22/63 [00:02<00:04, 10.03it/s]
2025-10-12 23:42:17 | 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/63 [00:02<00:03, 10.09it/s]
2025-10-12 23:42:17 | 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 26/63 [00:03<00:03, 10.05it/s]
2025-10-12 23:42:17 | 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/63 [00:03<00:03, 10.09it/s]
2025-10-12 23:42:17 | 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 30/63 [00:03<00:03, 10.00it/s]
2025-10-12 23:42:18 | 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/63 [00:03<00:03, 10.08it/s]
2025-10-12 23:42:18 | 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/63 [00:03<00:02, 10.00it/s]
2025-10-12 23:42:18 | 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/63 [00:04<00:02, 10.05it/s]
2025-10-12 23:42:18 | 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 38/63 [00:04<00:02, 10.17it/s]
2025-10-12 23:42:18 | 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/63 [00:04<00:02, 10.18it/s]
2025-10-12 23:42:19 | 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 42/63 [00:04<00:02, 10.18it/s]
2025-10-12 23:42:19 | 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/63 [00:04<00:01, 10.23it/s]
2025-10-12 23:42:19 | 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/63 [00:05<00:01, 10.20it/s]
2025-10-12 23:42:19 | 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/63 [00:05<00:01, 10.22it/s]
2025-10-12 23:42:19 | 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 50/63 [00:05<00:01, 10.21it/s]
2025-10-12 23:42:20 | 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/63 [00:05<00:01, 10.25it/s]
2025-10-12 23:42:20 | 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 54/63 [00:05<00:00, 10.23it/s]
2025-10-12 23:42:20 | 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 56/63 [00:06<00:00, 10.21it/s]
2025-10-12 23:42:20 | 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 58/63 [00:06<00:00, 10.20it/s]
2025-10-12 23:42:20 | 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 60/63 [00:06<00:00, 10.26it/s]
2025-10-12 23:42:21 | 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 62/63 [00:06<00:00, 10.32it/s]
2025-10-12 23:42:22 | 0%|          | 0/16 [00:00<?, ?it/s]
2025-10-12 23:42:22 | [A
2025-10-12 23:42:23 | 12%|â–ˆâ–        | 2/16 [00:00<00:06,  2.19it/s]
2025-10-12 23:42:23 | [A
2025-10-12 23:42:24 | 19%|â–ˆâ–‰        | 3/16 [00:01<00:08,  1.51it/s]
2025-10-12 23:42:24 | [A
2025-10-12 23:42:25 | 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:09,  1.30it/s]
2025-10-12 23:42:25 | [A
2025-10-12 23:42:26 | 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:03<00:09,  1.16it/s]
2025-10-12 23:42:26 | [A
2025-10-12 23:42:27 | 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:04<00:08,  1.13it/s]
2025-10-12 23:42:27 | [A
2025-10-12 23:42:28 | 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:05<00:08,  1.10it/s]
2025-10-12 23:42:28 | [A
2025-10-12 23:42:29 | 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:06<00:07,  1.06it/s]
2025-10-12 23:42:29 | [A
2025-10-12 23:42:30 | 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:07<00:06,  1.07it/s]
2025-10-12 23:42:30 | [A
2025-10-12 23:42:30 | 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10/16 [00:08<00:05,  1.08it/s]
2025-10-12 23:42:30 | [A
2025-10-12 23:42:31 | 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:09<00:04,  1.07it/s]
2025-10-12 23:42:31 | [A
2025-10-12 23:42:32 | 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:10<00:03,  1.06it/s]
2025-10-12 23:42:32 | [A
2025-10-12 23:42:33 | 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:11<00:02,  1.03it/s]
2025-10-12 23:42:33 | [A
2025-10-12 23:42:34 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:20<00:00, 10.32it/s]
2025-10-12 23:42:34 | 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:12<00:01,  1.02it/s]
2025-10-12 23:42:34 | [A
2025-10-12 23:42:35 | 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:13<00:00,  1.04it/s]
2025-10-12 23:42:35 | [A
2025-10-12 23:42:36 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:14<00:00,  1.11it/s]
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | {'eval_loss': 2.4057695865631104, 'eval_rouge1': 0.3766561944581691, 'eval_rouge2': 0.12390926708561978, 'eval_rougeL': 0.34691153104617106, 'eval_rouge_sum': 0.8474769925899599, 'eval_runtime': 15.487, 'eval_samples_per_second': 32.221, 'eval_steps_per_second': 1.033, 'epoch': 1.0}
2025-10-12 23:42:36 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:22<00:00, 10.32it/s]
2025-10-12 23:42:36 | [A
2025-10-12 23:42:36 | [A
2025-10-12 23:42:37 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-12 23:42:37 | {'train_runtime': 23.5121, 'train_samples_per_second': 85.063, 'train_steps_per_second': 2.679, 'train_loss': 2.867183140345982, 'epoch': 1.0}
2025-10-12 23:42:37 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:23<00:00, 10.32it/s]
2025-10-12 23:42:37 | ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...
2025-10-12 23:42:38 | â†’ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: experiments/20251012/20251012_234209_test_full_pipeline_batch32/model_0_kobart/default/final_model
2025-10-12 23:42:38 | ìµœì¢… í‰ê°€ ì¤‘...
2025-10-12 23:42:39 | 0%|          | 0/16 [00:00<?, ?it/s]
2025-10-12 23:42:40 | 12%|â–ˆâ–        | 2/16 [00:00<00:06,  2.08it/s]
2025-10-12 23:42:41 | 19%|â–ˆâ–‰        | 3/16 [00:01<00:08,  1.50it/s]
2025-10-12 23:42:42 | 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:09,  1.27it/s]
2025-10-12 23:42:44 | 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:04<00:12,  1.17s/it]
2025-10-12 23:42:45 | 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:05<00:10,  1.10s/it]
2025-10-12 23:42:46 | 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:06<00:09,  1.05s/it]
2025-10-12 23:42:47 | 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:07<00:08,  1.00s/it]
2025-10-12 23:42:48 | 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:08<00:06,  1.01it/s]
2025-10-12 23:42:49 | 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10/16 [00:09<00:06,  1.02s/it]
2025-10-12 23:42:50 | 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:10<00:05,  1.01s/it]
2025-10-12 23:42:51 | 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:11<00:03,  1.02it/s]
2025-10-12 23:42:52 | 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:12<00:02,  1.04it/s]
2025-10-12 23:42:53 | 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:13<00:01,  1.05it/s]
2025-10-12 23:42:54 | 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:14<00:00,  1.00it/s]
2025-10-12 23:42:55 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:15<00:00,  1.00it/s]
2025-10-12 23:42:55 | ìµœì¢… í‰ê°€ ê²°ê³¼:
2025-10-12 23:42:55 | eval_rouge1: 0.3767
2025-10-12 23:42:55 | eval_rouge2: 0.1239
2025-10-12 23:42:55 | eval_rougeL: 0.3469
2025-10-12 23:42:55 | eval_rouge_sum: 0.8475
2025-10-12 23:42:55 | ============================================================
2025-10-12 23:42:55 | âœ… í•™ìŠµ ì™„ë£Œ!
2025-10-12 23:42:55 | ============================================================
2025-10-12 23:42:55 | âœ… kobart í•™ìŠµ ì™„ë£Œ
2025-10-12 23:42:55 | ==================================================
2025-10-12 23:42:55 | ëª¨ë¸ 2/6: llama-3.2-korean-3b
2025-10-12 23:42:55 | ==================================================
2025-10-12 23:42:55 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-12 23:42:55 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 23:42:55 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 23:42:55 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 23:42:55 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 23:42:56 | Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.25it/s]
2025-10-12 23:42:56 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  2.13it/s]
2025-10-12 23:42:56 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 23:42:57 | íŒ¨ë”© í† í° ì„¤ì •: <|eot_id|>
2025-10-12 23:42:57 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 23:42:57 | ğŸ” ìë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:42:57 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 23:42:57 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 24,313,856 (0.75%)
2025-10-12 23:42:57 | ì „ì²´ íŒŒë¼ë¯¸í„°: 3,237,063,680
2025-10-12 23:42:57 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 23:42:57 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 23:42:57 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 23:42:57 | ============================================================
2025-10-12 23:42:57 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 23:42:57 | ============================================================
2025-10-12 23:42:58 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-12 23:42:58 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-12 23:42:59 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-12 23:42:59 | wandb: setting up run e8phcsen
2025-10-12 23:42:59 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:42:59 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234258-e8phcsen
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:42:59 | wandb: Syncing run 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | wandb: â­ï¸ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:42:59 | wandb: ğŸš€ View run at https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | wandb: Detected [openai] in use.
2025-10-12 23:42:59 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:42:59 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:42:59 | ğŸ“‹ ì‹¤í—˜ëª…: 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | ğŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:59 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:42:59 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:42:59 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:43:00 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:43:12 | âŒ llama-3.2-korean-3b í•™ìŠµ ì‹¤íŒ¨: OutOfMemoryError: CUDA out of memory. Tried to allocate 15.66 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 35.47 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:12 | ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/llama-3.2-korean-3b_error.log
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | ëª¨ë¸ 3/6: qwen3-4b
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-12 23:43:12 | Loading Causal LM: Qwen/Qwen3-4B-Instruct-2507
2025-10-12 23:43:12 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 23:43:12 | Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
2025-10-12 23:43:12 | [A
2025-10-12 23:43:15 | Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.71s/it]
2025-10-12 23:43:15 | [A
2025-10-12 23:43:20 | Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:04,  4.21s/it]
2025-10-12 23:43:20 | [A
2025-10-12 23:43:20 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.68s/it]
2025-10-12 23:43:21 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 23:43:21 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 23:43:21 | ğŸ” ìë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:43:22 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 23:43:22 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 33,030,144 (0.81%)
2025-10-12 23:43:22 | ì „ì²´ íŒŒë¼ë¯¸í„°: 4,055,498,240
2025-10-12 23:43:22 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 23:43:22 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 23:43:22 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-12 23:43:22 | wandb: Finishing previous runs because reinit is set to True.
2025-10-12 23:43:23 | wandb: uploading summary, console lines 20-36
2025-10-12 23:43:23 | wandb: ğŸš€ View run 1012-2342-llama_3.2_3b_qlora at: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
wandb: â­ï¸ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-12 23:43:23 | wandb: Find logs at: ./wandb/wandb/run-20251012_234258-e8phcsen/logs
2025-10-12 23:43:24 | wandb: setting up run jl8vntvn
2025-10-12 23:43:24 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:43:24 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234322-jl8vntvn
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:43:24 | wandb: Syncing run 1012-2343-qwen3_4b_qlora
2025-10-12 23:43:24 | wandb: â­ï¸ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:43:24 | wandb: ğŸš€ View run at https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
2025-10-12 23:43:24 | ğŸ“‹ ì‹¤í—˜ëª…: 1012-2343-qwen3_4b_qlora
2025-10-12 23:43:24 | ğŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
2025-10-12 23:43:24 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:43:24 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:43:24 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:43:24 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
2025-10-12 23:43:24 | 0%|          | 0/63 [00:24<?, ?it/s]
2025-10-12 23:43:39 | âŒ qwen3-4b í•™ìŠµ ì‹¤íŒ¨: OutOfMemoryError: CUDA out of memory. Tried to allocate 18.55 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:39 | ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/qwen3-4b_error.log
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | ëª¨ë¸ 4/6: solar-10.7b
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-12 23:43:39 | Loading Causal LM: upstage/solar-10.7b-instruct-v1.0
2025-10-12 23:43:39 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 23:43:40 | Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
2025-10-12 23:43:40 | [A
2025-10-12 23:43:51 | Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:11<00:46, 11.57s/it]
2025-10-12 23:43:51 | [A
2025-10-12 23:44:06 | Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:26<00:39, 13.32s/it]
2025-10-12 23:44:06 | [A
2025-10-12 23:44:11 | Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:31<00:19,  9.60s/it]
2025-10-12 23:44:11 | [A
2025-10-12 23:44:16 | Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:35<00:07,  7.64s/it]
2025-10-12 23:44:16 | [A
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:37<00:00,  5.29s/it]
2025-10-12 23:44:17 | [A
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:37<00:00,  7.41s/it]
2025-10-12 23:44:18 | WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.
2025-10-12 23:44:18 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 23:44:19 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 23:44:19 | ğŸ” ìë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:44:19 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 23:44:19 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 62,914,560 (0.58%)
2025-10-12 23:44:19 | ì „ì²´ íŒŒë¼ë¯¸í„°: 10,794,438,656
2025-10-12 23:44:19 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 23:44:19 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 23:44:19 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-12 23:44:19 | wandb: Finishing previous runs because reinit is set to True.
2025-10-12 23:44:20 | wandb: updating run metadata
2025-10-12 23:44:20 | wandb: uploading summary, console lines 19-37
2025-10-12 23:44:21 | wandb: ğŸš€ View run 1012-2343-qwen3_4b_qlora at: https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
wandb: â­ï¸ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-12 23:44:21 | wandb: Find logs at: ./wandb/wandb/run-20251012_234322-jl8vntvn/logs
2025-10-12 23:44:21 | wandb: setting up run 8qkqj7y3
2025-10-12 23:44:21 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:44:21 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234419-8qkqj7y3
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:44:21 | wandb: Syncing run 1012-2344-solar_10.7b_qlora
2025-10-12 23:44:21 | wandb: â­ï¸ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:44:21 | wandb: ğŸš€ View run at https://wandb.ai/ieyeppo/nlp-competition/runs/8qkqj7y3
2025-10-12 23:44:21 | ğŸ“‹ ì‹¤í—˜ëª…: 1012-2344-solar_10.7b_qlora
2025-10-12 23:44:21 | ğŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/8qkqj7y3
2025-10-12 23:44:21 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:44:21 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:44:22 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:44:22 | 0%|          | 0/63 [00:56<?, ?it/s]
