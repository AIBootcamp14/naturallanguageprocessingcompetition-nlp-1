2025-10-12 22:20:59 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-12 22:21:01 | 📊 FULL 모드 실행 중...
2025-10-12 22:21:01 | ============================================================
2025-10-12 22:21:01 | = FULL PIPELINE 실행 시작
2025-10-12 22:21:01 | =대상 모델: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-12 22:21:01 | =앙상블 앙상블 전략: stacking
2025-10-12 22:21:01 | = TTA 사용: True
2025-10-12 22:21:01 | ============================================================
2025-10-12 22:21:01 | [1/6] 데이터 로딩...
2025-10-12 22:21:01 | ✅ 학습 데이터: 12457개
2025-10-12 22:21:01 | ✅ 검증 데이터: 499개
2025-10-12 22:21:01 | [2/6] 다중 모델 학습 (6 모델)...
2025-10-12 22:21:01 | ==================================================
2025-10-12 22:21:01 | 모델 1/6: kobart
2025-10-12 22:21:01 | ==================================================
2025-10-12 22:21:01 | 모델 타입: encoder_decoder
2025-10-12 22:21:01 | ============================================================
2025-10-12 22:21:01 | 모델 및 토크나이저 로딩 시작
2025-10-12 22:21:01 | ============================================================
2025-10-12 22:21:01 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-12 22:21:02 | 모델 로딩: digit82/kobart-summarization
2025-10-12 22:21:02 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-12 22:21:03 | → 디바이스: cuda
2025-10-12 22:21:03 | → 전체 파라미터: 123,859,968
2025-10-12 22:21:03 | → 학습 가능 파라미터: 123,859,968
2025-10-12 22:21:03 | ============================================================
2025-10-12 22:21:03 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-12 22:21:03 | ============================================================
2025-10-12 22:21:03 | ============================================================
2025-10-12 22:21:03 | 모델 학습 시작
2025-10-12 22:21:03 | ============================================================
2025-10-12 22:21:03 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:21:03 | 학습 진행 중...
2025-10-12 22:21:03 | 0%|          | 0/779 [00:00<?, ?it/s]
2025-10-12 22:21:04 | 1%|          | 5/779 [00:00<01:31,  8.42it/s]
2025-10-12 22:21:05 | 2%|▏         | 13/779 [00:01<00:51, 14.93it/s]
2025-10-12 22:21:05 | 3%|▎         | 21/779 [00:01<00:45, 16.58it/s]
2025-10-12 22:21:06 | 4%|▎         | 29/779 [00:02<00:43, 17.41it/s]
2025-10-12 22:21:06 | 5%|▍         | 37/779 [00:02<00:43, 17.08it/s]
2025-10-12 22:21:06 | 6%|▌         | 43/779 [00:02<00:42, 17.41it/s]
2025-10-12 22:21:07 | 7%|▋         | 51/779 [00:03<00:42, 17.32it/s]
2025-10-12 22:21:07 | 8%|▊         | 59/779 [00:03<00:42, 17.06it/s]
2025-10-12 22:21:08 | 9%|▊         | 67/779 [00:04<00:42, 16.95it/s]
2025-10-12 22:21:08 | 10%|▉         | 75/779 [00:04<00:40, 17.29it/s]
2025-10-12 22:21:09 | 11%|█         | 83/779 [00:05<00:40, 17.07it/s]
2025-10-12 22:21:09 | 12%|█▏        | 91/779 [00:05<00:40, 16.98it/s]
2025-10-12 22:21:10 | 13%|█▎        | 99/779 [00:06<00:38, 17.62it/s]
2025-10-12 22:21:10 | {'loss': 2.6869, 'grad_norm': 6.419677734375, 'learning_rate': 9.9e-07, 'epoch': 0.13}
2025-10-12 22:21:10 | 13%|█▎        | 100/779 [00:06<00:38, 17.62it/s]
2025-10-12 22:21:10 | 14%|█▎        | 107/779 [00:06<00:39, 17.01it/s]
2025-10-12 22:21:10 | 15%|█▍        | 113/779 [00:07<00:38, 17.21it/s]
2025-10-12 22:21:11 | 16%|█▌        | 121/779 [00:07<00:38, 16.89it/s]
2025-10-12 22:21:11 | 17%|█▋        | 129/779 [00:07<00:37, 17.45it/s]
2025-10-12 22:21:12 | 18%|█▊        | 137/779 [00:08<00:38, 16.75it/s]
2025-10-12 22:21:12 | 19%|█▊        | 145/779 [00:08<00:36, 17.42it/s]
2025-10-12 22:21:13 | 20%|█▉        | 153/779 [00:09<00:35, 17.57it/s]
2025-10-12 22:21:13 | 21%|██        | 161/779 [00:09<00:35, 17.43it/s]
2025-10-12 22:21:14 | 22%|██▏       | 169/779 [00:10<00:35, 17.40it/s]
2025-10-12 22:21:14 | 23%|██▎       | 177/779 [00:10<00:34, 17.25it/s]
2025-10-12 22:21:15 | 24%|██▎       | 185/779 [00:11<00:34, 17.32it/s]
2025-10-12 22:21:15 | 25%|██▍       | 191/779 [00:11<00:33, 17.43it/s]
2025-10-12 22:21:15 | 26%|██▌       | 199/779 [00:12<00:33, 17.28it/s]
2025-10-12 22:21:16 | {'loss': 2.0069, 'grad_norm': 5.499480724334717, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.26}
2025-10-12 22:21:16 | 26%|██▌       | 200/779 [00:12<00:33, 17.28it/s]
2025-10-12 22:21:16 | 27%|██▋       | 207/779 [00:12<00:33, 17.27it/s]
2025-10-12 22:21:16 | 28%|██▊       | 215/779 [00:12<00:32, 17.44it/s]
2025-10-12 22:21:17 | 29%|██▊       | 223/779 [00:13<00:33, 16.77it/s]
2025-10-12 22:21:17 | 30%|██▉       | 231/779 [00:13<00:32, 16.95it/s]
2025-10-12 22:21:18 | 31%|███       | 239/779 [00:14<00:32, 16.87it/s]
2025-10-12 22:21:18 | 32%|███▏      | 247/779 [00:14<00:30, 17.25it/s]
2025-10-12 22:21:19 | 33%|███▎      | 255/779 [00:15<00:30, 17.34it/s]
2025-10-12 22:21:19 | 34%|███▎      | 261/779 [00:15<00:30, 16.87it/s]
2025-10-12 22:21:20 | 35%|███▍      | 269/779 [00:16<00:30, 16.48it/s]
2025-10-12 22:21:20 | 36%|███▌      | 277/779 [00:16<00:30, 16.52it/s]
2025-10-12 22:21:21 | 37%|███▋      | 285/779 [00:17<00:29, 16.61it/s]
2025-10-12 22:21:21 | 38%|███▊      | 293/779 [00:17<00:29, 16.41it/s]
2025-10-12 22:21:21 | {'loss': 1.7707, 'grad_norm': 4.936250686645508, 'learning_rate': 2.99e-06, 'epoch': 0.39}
2025-10-12 22:21:21 | 39%|███▊      | 300/779 [00:18<00:28, 16.94it/s]
2025-10-12 22:21:23 | 40%|███▉      | 309/779 [00:19<01:07,  7.01it/s]
2025-10-12 22:21:23 | 41%|████      | 317/779 [00:19<00:36, 12.68it/s]
2025-10-12 22:21:24 | 42%|████▏     | 325/779 [00:20<00:28, 15.72it/s]
2025-10-12 22:21:24 | 43%|████▎     | 333/779 [00:20<00:26, 17.00it/s]
2025-10-12 22:21:25 | 44%|████▎     | 339/779 [00:21<00:25, 17.29it/s]
2025-10-12 22:21:25 | 45%|████▍     | 347/779 [00:21<00:24, 17.42it/s]
2025-10-12 22:21:25 | 46%|████▌     | 355/779 [00:21<00:23, 17.68it/s]
2025-10-12 22:21:26 | 47%|████▋     | 363/779 [00:22<00:23, 17.66it/s]
2025-10-12 22:21:26 | 48%|████▊     | 371/779 [00:22<00:23, 17.46it/s]
2025-10-12 22:21:27 | 49%|████▊     | 379/779 [00:23<00:22, 17.51it/s]
2025-10-12 22:21:27 | 50%|████▉     | 387/779 [00:23<00:22, 17.23it/s]
2025-10-12 22:21:28 | 51%|█████     | 395/779 [00:24<00:21, 17.46it/s]
2025-10-12 22:21:28 | {'loss': 1.683, 'grad_norm': 4.260188579559326, 'learning_rate': 3.990000000000001e-06, 'epoch': 0.51}
2025-10-12 22:21:28 | 51%|█████▏    | 400/779 [00:24<00:21, 17.64it/s]
2025-10-12 22:21:28 | 52%|█████▏    | 403/779 [00:24<00:20, 18.00it/s]
2025-10-12 22:21:29 | 53%|█████▎    | 409/779 [00:25<00:20, 18.23it/s]
2025-10-12 22:21:29 | 54%|█████▎    | 417/779 [00:25<00:19, 18.30it/s]
2025-10-12 22:21:29 | 55%|█████▍    | 425/779 [00:25<00:19, 18.30it/s]
2025-10-12 22:21:30 | 56%|█████▌    | 433/779 [00:26<00:18, 18.51it/s]
2025-10-12 22:21:30 | 57%|█████▋    | 441/779 [00:26<00:18, 18.34it/s]
2025-10-12 22:21:31 | 58%|█████▊    | 449/779 [00:27<00:18, 18.03it/s]
2025-10-12 22:21:31 | 59%|█████▊    | 457/779 [00:27<00:17, 18.30it/s]
2025-10-12 22:21:32 | 60%|█████▉    | 465/779 [00:28<00:17, 18.29it/s]
2025-10-12 22:21:32 | 61%|██████    | 473/779 [00:28<00:16, 18.17it/s]
2025-10-12 22:21:32 | 62%|██████▏   | 481/779 [00:28<00:16, 18.15it/s]
2025-10-12 22:21:33 | 63%|██████▎   | 487/779 [00:29<00:16, 17.81it/s]
2025-10-12 22:21:33 | 64%|██████▎   | 495/779 [00:29<00:16, 17.18it/s]
2025-10-12 22:21:34 | {'loss': 1.6406, 'grad_norm': 4.014793395996094, 'learning_rate': 4.9900000000000005e-06, 'epoch': 0.64}
2025-10-12 22:21:34 | 64%|██████▍   | 500/779 [00:30<00:16, 17.28it/s]
2025-10-12 22:21:34 | 65%|██████▍   | 503/779 [00:30<00:15, 17.53it/s]
2025-10-12 22:21:34 | 66%|██████▌   | 511/779 [00:30<00:15, 17.83it/s]
2025-10-12 22:21:35 | 67%|██████▋   | 519/779 [00:31<00:14, 17.78it/s]
2025-10-12 22:21:35 | 68%|██████▊   | 527/779 [00:31<00:14, 17.57it/s]
2025-10-12 22:21:36 | 69%|██████▊   | 535/779 [00:32<00:14, 17.41it/s]
2025-10-12 22:21:36 | 70%|██████▉   | 543/779 [00:32<00:13, 17.55it/s]
2025-10-12 22:21:36 | 71%|███████   | 551/779 [00:32<00:12, 17.75it/s]
2025-10-12 22:21:37 | 72%|███████▏  | 557/779 [00:33<00:12, 17.88it/s]
2025-10-12 22:21:37 | 73%|███████▎  | 565/779 [00:33<00:12, 17.61it/s]
2025-10-12 22:21:38 | 74%|███████▎  | 573/779 [00:34<00:11, 17.26it/s]
2025-10-12 22:21:38 | 75%|███████▍  | 581/779 [00:34<00:11, 17.64it/s]
2025-10-12 22:21:39 | 76%|███████▌  | 589/779 [00:35<00:10, 17.41it/s]
2025-10-12 22:21:39 | 77%|███████▋  | 597/779 [00:35<00:10, 17.31it/s]
2025-10-12 22:21:39 | {'loss': 1.5902, 'grad_norm': 3.673412561416626, 'learning_rate': 3.225806451612903e-06, 'epoch': 0.77}
2025-10-12 22:21:39 | 77%|███████▋  | 600/779 [00:35<00:10, 17.50it/s]
2025-10-12 22:21:40 | 78%|███████▊  | 605/779 [00:36<00:09, 17.43it/s]
2025-10-12 22:21:40 | 79%|███████▊  | 613/779 [00:36<00:09, 17.29it/s]
2025-10-12 22:21:40 | 80%|███████▉  | 621/779 [00:36<00:08, 17.67it/s]
2025-10-12 22:21:41 | 81%|████████  | 629/779 [00:37<00:08, 17.70it/s]
2025-10-12 22:21:41 | 82%|████████▏ | 635/779 [00:37<00:08, 17.72it/s]
2025-10-12 22:21:42 | 83%|████████▎ | 643/779 [00:38<00:07, 17.51it/s]
2025-10-12 22:21:42 | 84%|████████▎ | 651/779 [00:38<00:07, 17.19it/s]
2025-10-12 22:21:43 | 85%|████████▍ | 659/779 [00:39<00:07, 16.71it/s]
2025-10-12 22:21:43 | 86%|████████▌ | 667/779 [00:39<00:06, 17.35it/s]
2025-10-12 22:21:44 | 87%|████████▋ | 675/779 [00:40<00:05, 17.48it/s]
2025-10-12 22:21:44 | 88%|████████▊ | 683/779 [00:40<00:05, 17.63it/s]
2025-10-12 22:21:44 | 89%|████████▊ | 691/779 [00:40<00:05, 17.53it/s]
2025-10-12 22:21:45 | 90%|████████▉ | 699/779 [00:41<00:04, 17.54it/s]
2025-10-12 22:21:45 | {'loss': 1.5734, 'grad_norm': 4.099255561828613, 'learning_rate': 1.4336917562724014e-06, 'epoch': 0.9}
2025-10-12 22:21:45 | 90%|████████▉ | 700/779 [00:41<00:04, 17.54it/s]
2025-10-12 22:21:45 | 91%|█████████ | 705/779 [00:41<00:04, 17.61it/s]
2025-10-12 22:21:46 | 92%|█████████▏| 713/779 [00:42<00:03, 17.33it/s]
2025-10-12 22:21:46 | 93%|█████████▎| 721/779 [00:42<00:03, 17.21it/s]
2025-10-12 22:21:47 | 94%|█████████▎| 729/779 [00:43<00:02, 17.20it/s]
2025-10-12 22:21:47 | 95%|█████████▍| 737/779 [00:43<00:02, 17.31it/s]
2025-10-12 22:21:48 | 96%|█████████▌| 745/779 [00:44<00:01, 17.33it/s]
2025-10-12 22:21:48 | 97%|█████████▋| 753/779 [00:44<00:01, 17.49it/s]
2025-10-12 22:21:48 | 98%|█████████▊| 761/779 [00:45<00:01, 17.38it/s]
2025-10-12 22:21:49 | 99%|█████████▊| 769/779 [00:45<00:00, 17.20it/s]
2025-10-12 22:21:49 | 100%|█████████▉| 777/779 [00:45<00:00, 17.31it/s]
2025-10-12 22:21:51 | 0%|          | 0/32 [00:00<?, ?it/s]
2025-10-12 22:21:51 | [A
2025-10-12 22:21:51 | 6%|▋         | 2/32 [00:00<00:13,  2.17it/s]
2025-10-12 22:21:51 | [A
2025-10-12 22:21:52 | 9%|▉         | 3/32 [00:01<00:18,  1.56it/s]
2025-10-12 22:21:52 | [A
2025-10-12 22:21:53 | 12%|█▎        | 4/32 [00:02<00:20,  1.34it/s]
2025-10-12 22:21:53 | [A
2025-10-12 22:21:54 | 16%|█▌        | 5/32 [00:03<00:21,  1.26it/s]
2025-10-12 22:21:54 | [A
2025-10-12 22:21:56 | 19%|█▉        | 6/32 [00:05<00:28,  1.11s/it]
2025-10-12 22:21:56 | [A
2025-10-12 22:21:57 | 22%|██▏       | 7/32 [00:06<00:25,  1.03s/it]
2025-10-12 22:21:57 | [A
2025-10-12 22:21:58 | 25%|██▌       | 8/32 [00:07<00:23,  1.03it/s]
2025-10-12 22:21:58 | [A
2025-10-12 22:21:58 | 28%|██▊       | 9/32 [00:07<00:20,  1.10it/s]
2025-10-12 22:21:58 | [A
2025-10-12 22:21:59 | 31%|███▏      | 10/32 [00:08<00:20,  1.10it/s]
2025-10-12 22:21:59 | [A
2025-10-12 22:22:00 | 34%|███▍      | 11/32 [00:09<00:18,  1.12it/s]
2025-10-12 22:22:00 | [A
2025-10-12 22:22:01 | 38%|███▊      | 12/32 [00:10<00:17,  1.17it/s]
2025-10-12 22:22:01 | [A
2025-10-12 22:22:02 | 41%|████      | 13/32 [00:11<00:15,  1.20it/s]
2025-10-12 22:22:02 | [A
2025-10-12 22:22:03 | 44%|████▍     | 14/32 [00:11<00:15,  1.20it/s]
2025-10-12 22:22:03 | [A
2025-10-12 22:22:03 | 47%|████▋     | 15/32 [00:12<00:13,  1.22it/s]
2025-10-12 22:22:03 | [A
2025-10-12 22:22:04 | 50%|█████     | 16/32 [00:13<00:13,  1.22it/s]
2025-10-12 22:22:04 | [A
2025-10-12 22:22:05 | 53%|█████▎    | 17/32 [00:14<00:12,  1.24it/s]
2025-10-12 22:22:05 | [A
2025-10-12 22:22:05 | 100%|██████████| 779/779 [01:01<00:00, 17.65it/s]
2025-10-12 22:22:06 | 56%|█████▋    | 18/32 [00:15<00:11,  1.24it/s]
2025-10-12 22:22:06 | [A
2025-10-12 22:22:07 | 59%|█████▉    | 19/32 [00:16<00:10,  1.23it/s]
2025-10-12 22:22:07 | [A
2025-10-12 22:22:07 | 62%|██████▎   | 20/32 [00:16<00:09,  1.23it/s]
2025-10-12 22:22:07 | [A
2025-10-12 22:22:08 | 66%|██████▌   | 21/32 [00:17<00:09,  1.22it/s]
2025-10-12 22:22:08 | [A
2025-10-12 22:22:09 | 69%|██████▉   | 22/32 [00:18<00:08,  1.19it/s]
2025-10-12 22:22:09 | [A
2025-10-12 22:22:10 | 72%|███████▏  | 23/32 [00:19<00:07,  1.16it/s]
2025-10-12 22:22:10 | [A
2025-10-12 22:22:11 | 75%|███████▌  | 24/32 [00:20<00:06,  1.18it/s]
2025-10-12 22:22:11 | [A
2025-10-12 22:22:12 | 78%|███████▊  | 25/32 [00:21<00:05,  1.19it/s]
2025-10-12 22:22:12 | [A
2025-10-12 22:22:12 | 81%|████████▏ | 26/32 [00:21<00:04,  1.20it/s]
2025-10-12 22:22:12 | [A
2025-10-12 22:22:13 | 84%|████████▍ | 27/32 [00:22<00:04,  1.18it/s]
2025-10-12 22:22:13 | [A
2025-10-12 22:22:14 | 88%|████████▊ | 28/32 [00:23<00:03,  1.20it/s]
2025-10-12 22:22:14 | [A
2025-10-12 22:22:15 | 91%|█████████ | 29/32 [00:24<00:02,  1.14it/s]
2025-10-12 22:22:15 | [A
2025-10-12 22:22:16 | 94%|█████████▍| 30/32 [00:25<00:01,  1.14it/s]
2025-10-12 22:22:16 | [A
2025-10-12 22:22:17 | 97%|█████████▋| 31/32 [00:26<00:00,  1.17it/s]
2025-10-12 22:22:17 | [A
2025-10-12 22:22:18 | 100%|██████████| 32/32 [00:26<00:00,  1.23it/s]
2025-10-12 22:22:18 | [A
2025-10-12 22:22:18 | [A
2025-10-12 22:22:18 | {'eval_loss': 1.487852692604065, 'eval_rouge1': 0.41008346416314057, 'eval_rouge2': 0.25818988609731086, 'eval_rougeL': 0.40318697139257764, 'eval_rouge_sum': 1.0714603216530292, 'eval_runtime': 28.02, 'eval_samples_per_second': 17.809, 'eval_steps_per_second': 1.142, 'epoch': 1.0}
2025-10-12 22:22:18 | 100%|██████████| 779/779 [01:14<00:00, 17.65it/s]
2025-10-12 22:22:18 | [A
2025-10-12 22:22:18 | [A
2025-10-12 22:22:19 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-12 22:22:19 | {'train_runtime': 75.463, 'train_samples_per_second': 165.074, 'train_steps_per_second': 10.323, 'train_loss': 1.8235316343882884, 'epoch': 1.0}
2025-10-12 22:22:19 | 100%|██████████| 779/779 [01:15<00:00, 17.65it/s]
2025-10-12 22:22:19 | 최종 모델 저장 중...
2025-10-12 22:22:19 | → 모델 저장 위치: experiments/20251012/20251012_222059_test_full_pipeline_optimized/model_0_kobart/default/final_model
2025-10-12 22:22:19 | 최종 평가 중...
2025-10-12 22:22:20 | 0%|          | 0/32 [00:00<?, ?it/s]
2025-10-12 22:22:21 | 6%|▋         | 2/32 [00:00<00:12,  2.41it/s]
2025-10-12 22:22:22 | 9%|▉         | 3/32 [00:01<00:18,  1.55it/s]
2025-10-12 22:22:23 | 12%|█▎        | 4/32 [00:02<00:20,  1.39it/s]
2025-10-12 22:22:24 | 16%|█▌        | 5/32 [00:03<00:21,  1.27it/s]
2025-10-12 22:22:25 | 19%|█▉        | 6/32 [00:04<00:21,  1.23it/s]
2025-10-12 22:22:26 | 22%|██▏       | 7/32 [00:05<00:21,  1.17it/s]
2025-10-12 22:22:27 | 25%|██▌       | 8/32 [00:06<00:20,  1.18it/s]
2025-10-12 22:22:28 | 28%|██▊       | 9/32 [00:07<00:20,  1.14it/s]
2025-10-12 22:22:29 | 31%|███▏      | 10/32 [00:08<00:24,  1.11s/it]
2025-10-12 22:22:30 | 34%|███▍      | 11/32 [00:09<00:21,  1.04s/it]
2025-10-12 22:22:31 | 38%|███▊      | 12/32 [00:10<00:19,  1.02it/s]
2025-10-12 22:22:32 | 41%|████      | 13/32 [00:11<00:17,  1.08it/s]
2025-10-12 22:22:33 | 44%|████▍     | 14/32 [00:12<00:16,  1.08it/s]
2025-10-12 22:22:34 | 47%|████▋     | 15/32 [00:13<00:15,  1.09it/s]
2025-10-12 22:22:34 | 50%|█████     | 16/32 [00:13<00:14,  1.12it/s]
2025-10-12 22:22:35 | 53%|█████▎    | 17/32 [00:14<00:12,  1.20it/s]
2025-10-12 22:22:36 | 56%|█████▋    | 18/32 [00:15<00:11,  1.22it/s]
2025-10-12 22:22:37 | 59%|█████▉    | 19/32 [00:16<00:10,  1.22it/s]
2025-10-12 22:22:37 | 62%|██████▎   | 20/32 [00:17<00:09,  1.25it/s]
2025-10-12 22:22:38 | 66%|██████▌   | 21/32 [00:17<00:09,  1.19it/s]
2025-10-12 22:22:39 | 69%|██████▉   | 22/32 [00:18<00:08,  1.20it/s]
2025-10-12 22:22:40 | 72%|███████▏  | 23/32 [00:19<00:07,  1.23it/s]
2025-10-12 22:22:41 | 75%|███████▌  | 24/32 [00:20<00:06,  1.21it/s]
2025-10-12 22:22:42 | 78%|███████▊  | 25/32 [00:21<00:05,  1.19it/s]
2025-10-12 22:22:43 | 81%|████████▏ | 26/32 [00:22<00:05,  1.17it/s]
2025-10-12 22:22:44 | 84%|████████▍ | 27/32 [00:23<00:04,  1.12it/s]
2025-10-12 22:22:45 | 88%|████████▊ | 28/32 [00:24<00:03,  1.12it/s]
2025-10-12 22:22:45 | 91%|█████████ | 29/32 [00:24<00:02,  1.10it/s]
2025-10-12 22:22:46 | 94%|█████████▍| 30/32 [00:25<00:01,  1.15it/s]
2025-10-12 22:22:47 | 97%|█████████▋| 31/32 [00:26<00:00,  1.16it/s]
2025-10-12 22:22:48 | 100%|██████████| 32/32 [00:27<00:00,  1.24it/s]
2025-10-12 22:22:48 | 최종 평가 결과:
2025-10-12 22:22:48 | eval_rouge1: 0.4101
2025-10-12 22:22:48 | eval_rouge2: 0.2582
2025-10-12 22:22:48 | eval_rougeL: 0.4032
2025-10-12 22:22:48 | eval_rouge_sum: 1.0715
2025-10-12 22:22:48 | ============================================================
2025-10-12 22:22:48 | ✅ 학습 완료!
2025-10-12 22:22:48 | ============================================================
2025-10-12 22:22:48 | ✅ kobart 학습 완료
2025-10-12 22:22:48 | ==================================================
2025-10-12 22:22:48 | 모델 2/6: llama-3.2-korean-3b
2025-10-12 22:22:48 | ==================================================
2025-10-12 22:22:48 | 모델 타입: causal_lm
2025-10-12 22:22:48 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 22:22:48 | 모델 로딩 중...
2025-10-12 22:22:48 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 22:22:48 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 22:22:49 | Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]
2025-10-12 22:22:50 | Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]
2025-10-12 22:22:50 | 토크나이저 로딩 중...
2025-10-12 22:22:51 | 패딩 토큰 설정: <|eot_id|>
2025-10-12 22:22:51 | LoRA 설정 적용 중...
2025-10-12 22:22:51 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 22:22:51 | ✅ LoRA 적용 완료
2025-10-12 22:22:51 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-12 22:22:51 | 전체 파라미터: 3,237,063,680
2025-10-12 22:22:51 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 22:22:51 | ✅ Gradient Checkpointing 활성화
2025-10-12 22:22:51 | ✅ Causal LM 로드 완료
2025-10-12 22:22:51 | ============================================================
2025-10-12 22:22:51 | 모델 학습 시작
2025-10-12 22:22:51 | ============================================================
2025-10-12 22:22:51 | WandB 로그인 상태: ieyeppo-job
2025-10-12 22:22:52 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-12 22:22:52 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-12 22:22:53 | wandb: setting up run 9z9kt5rc
2025-10-12 22:22:53 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 22:22:53 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_222252-9z9kt5rc
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 22:22:53 | wandb: Syncing run 1012-2222-llama_3.2_3b_qlora
2025-10-12 22:22:53 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 22:22:53 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/9z9kt5rc
2025-10-12 22:22:53 | wandb: Detected [openai] in use.
2025-10-12 22:22:53 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 22:22:53 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 22:22:53 | 📋 실험명: 1012-2222-llama_3.2_3b_qlora
2025-10-12 22:22:53 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/9z9kt5rc
2025-10-12 22:22:53 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:22:53 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 22:22:53 | 학습 진행 중...
2025-10-12 22:22:53 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 22:22:53 | 0%|          | 0/779 [00:00<?, ?it/s]
2025-10-12 22:23:36 | 1%|          | 4/779 [00:42<2:04:06,  9.61s/it]
2025-10-12 22:24:23 | {'loss': 1.5992, 'grad_norm': 1.5635446310043335, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-12 22:24:23 | 1%|▏         | 10/779 [01:29<1:42:59,  8.04s/it]
2025-10-12 22:24:38 | 2%|▏         | 12/779 [01:45<1:40:57,  7.90s/it]
2025-10-12 22:25:42 | 3%|▎         | 20/779 [02:48<1:39:09,  7.84s/it]
2025-10-12 22:25:42 | {'loss': 1.6342, 'grad_norm': 1.586462140083313, 'learning_rate': 1.9e-07, 'epoch': 0.03}
2025-10-12 22:25:42 | 3%|▎         | 20/779 [02:48<1:39:09,  7.84s/it]
2025-10-12 22:26:45 | 4%|▎         | 28/779 [03:51<1:37:47,  7.81s/it]
2025-10-12 22:27:01 | {'loss': 1.6011, 'grad_norm': 1.480494499206543, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.04}
2025-10-12 22:27:01 | 4%|▍         | 30/779 [04:08<1:38:58,  7.93s/it]
2025-10-12 22:27:48 | 5%|▍         | 36/779 [04:55<1:37:13,  7.85s/it]
2025-10-12 22:28:20 | {'loss': 1.6386, 'grad_norm': 2.0113680362701416, 'learning_rate': 3.9e-07, 'epoch': 0.05}
2025-10-12 22:28:20 | 5%|▌         | 40/779 [05:26<1:36:34,  7.84s/it]
2025-10-12 22:28:44 | 6%|▌         | 43/779 [05:51<1:37:36,  7.96s/it]
2025-10-12 22:29:40 | {'loss': 1.6184, 'grad_norm': 1.5652515888214111, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.06}
2025-10-12 22:29:40 | 6%|▋         | 50/779 [06:46<1:37:34,  8.03s/it]
2025-10-12 22:29:47 | 7%|▋         | 51/779 [06:54<1:36:20,  7.94s/it]
