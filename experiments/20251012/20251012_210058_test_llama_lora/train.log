2025-10-12 21:00:58 | >> í‘œì¤€ ì¶œë ¥ ë° ì˜¤ë¥˜ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ë¦¬ë””ë ‰ì…˜ ì‹œì‘
2025-10-12 21:01:01 | ğŸ“Š SINGLE ëª¨ë“œ ì‹¤í–‰ ì¤‘...
2025-10-12 21:01:01 | ============================================================
2025-10-12 21:01:01 | ğŸš€ SINGLE MODEL ëª¨ë“œ í•™ìŠµ ì‹œì‘
2025-10-12 21:01:01 | ğŸ“‹ ëª¨ë¸: llama-3.2-korean-3b
2025-10-12 21:01:01 | ============================================================
2025-10-12 21:01:01 | [1/5] ë°ì´í„° ë¡œë”©...
2025-10-12 21:01:01 | âš ï¸ ë””ë²„ê·¸ ëª¨ë“œ: í•™ìŠµ 100ê°œ, ê²€ì¦ 20ê°œ
2025-10-12 21:01:01 | [2/5] Config ë¡œë”©...
2025-10-12 21:01:01 | âš™ï¸ Epochs ì˜¤ë²„ë¼ì´ë“œ: 1
2025-10-12 21:01:01 | âš™ï¸ Batch size ì˜¤ë²„ë¼ì´ë“œ: 4
2025-10-12 21:01:01 | âœ… Config ë¡œë“œ ì™„ë£Œ: llama-3.2-korean-3b
2025-10-12 21:01:01 | [3/5] ëª¨ë¸ ë¡œë”©...
2025-10-12 21:01:01 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-12 21:01:01 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 21:01:01 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 21:01:01 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 21:01:02 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 21:01:16 | Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:14<00:14, 14.11s/it]
2025-10-12 21:01:21 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  8.66s/it]
2025-10-12 21:01:21 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 21:01:22 | íŒ¨ë”© í† í° ì„¤ì •: <|eot_id|>
2025-10-12 21:01:22 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 21:01:23 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 21:01:23 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 24,313,856 (0.75%)
2025-10-12 21:01:23 | ì „ì²´ íŒŒë¼ë¯¸í„°: 3,237,063,680
2025-10-12 21:01:23 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 21:01:23 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 21:01:23 | âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ
2025-10-12 21:01:23 | [4/5] Dataset ìƒì„±...
2025-10-12 21:01:23 | âœ… í•™ìŠµ Dataset: 100ê°œ
2025-10-12 21:01:23 | âœ… ê²€ì¦ Dataset: 20ê°œ
2025-10-12 21:01:23 | [5/5] í•™ìŠµ ì‹œì‘...
2025-10-12 21:01:23 | ============================================================
2025-10-12 21:01:23 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-12 21:01:23 | ============================================================
2025-10-12 21:01:23 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 21:01:23 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 21:01:23 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 21:01:23 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 21:01:23 | 0%|          | 0/25 [00:00<?, ?it/s]
2025-10-12 21:01:23 | /home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2025-10-12 21:01:25 | âŒ ì˜¤ë¥˜ ë°œìƒ: element 0 of tensors does not require grad and does not have a grad_fn
2025-10-12 21:01:25 | >> ë¡œê·¸ ë¦¬ë””ë ‰ì…˜ ì¤‘ë£Œ.
