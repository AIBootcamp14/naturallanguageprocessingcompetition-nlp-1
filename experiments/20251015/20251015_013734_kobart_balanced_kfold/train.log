2025-10-15 01:37:34 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-15 01:37:35 | WandB 로그인 상태: ieyeppo-job
2025-10-15 01:37:35 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-15 01:37:35 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-15 01:37:36 | wandb: setting up run 7aw7qgki
2025-10-15 01:37:37 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 01:37:37 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_013735-7aw7qgki
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 01:37:37 | wandb: Syncing run 1015-0137-kobart_balanced_kfold
2025-10-15 01:37:37 | wandb: ⭐️ View project at https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization
2025-10-15 01:37:37 | wandb: 🚀 View run at https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/7aw7qgki
2025-10-15 01:37:37 | 📋 실험명: 1015-0137-kobart_balanced_kfold
2025-10-15 01:37:37 | 🔗 WandB URL: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/7aw7qgki
2025-10-15 01:37:37 | ✅ WandB 초기화 완료
2025-10-15 01:37:37 | 프로젝트: dialogue-summarization
2025-10-15 01:37:37 | 실험명: kobart_balanced_kfold
2025-10-15 01:37:42 | 📊 KFOLD 모드 실행 중...
2025-10-15 01:37:42 | ============================================================
2025-10-15 01:37:42 | 🔄 K-FOLD 교차검증 모드 학습 시작
2025-10-15 01:37:42 | 📋 K-Folds: 5
2025-10-15 01:37:42 | 📋 모델: kobart
2025-10-15 01:37:42 | 📋 Fold Seed: 42
2025-10-15 01:37:42 | ============================================================
2025-10-15 01:37:42 | [1/3] 전체 데이터 로딩...
2025-10-15 01:37:43 | ✅ 학습 데이터: 12457개
2025-10-15 01:37:43 | ✅ 검증 데이터: 499개
2025-10-15 01:37:43 | ✅ 전체 데이터: 12457개
2025-10-15 01:37:43 | [2/3] Config 로딩...
2025-10-15 01:37:43 | ✅ Config 로드 완료: kobart
2025-10-15 01:37:43 | [3/3] K-Fold 교차검증 실행...
2025-10-15 01:37:43 | ========================================
2025-10-15 01:37:43 | 📌 Fold 1/5 학습 시작
2025-10-15 01:37:43 | ========================================
2025-10-15 01:37:43 | 학습: 9965개
2025-10-15 01:37:43 | 검증: 2492개
2025-10-15 01:37:43 | 모델 타입: encoder_decoder
2025-10-15 01:37:43 | ============================================================
2025-10-15 01:37:43 | 모델 및 토크나이저 로딩 시작
2025-10-15 01:37:43 | ============================================================
2025-10-15 01:37:43 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-15 01:37:43 | 모델 로딩: digit82/kobart-summarization
2025-10-15 01:37:43 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-15 01:37:46 | → 디바이스: cuda
2025-10-15 01:37:46 | → 전체 파라미터: 123,859,968
2025-10-15 01:37:46 | → 학습 가능 파라미터: 123,859,968
2025-10-15 01:37:46 | ============================================================
2025-10-15 01:37:46 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-15 01:37:46 | ============================================================
2025-10-15 01:37:46 | ============================================================
2025-10-15 01:37:46 | 모델 학습 시작
2025-10-15 01:37:46 | ============================================================
2025-10-15 01:37:46 | WandB 로그인 상태: ieyeppo-job
2025-10-15 01:37:46 | wandb: Finishing previous runs because reinit is set to True.
2025-10-15 01:37:46 | wandb: updating run metadata
2025-10-15 01:37:47 | wandb: uploading summary, console lines 5-41
2025-10-15 01:37:47 | wandb: 🚀 View run 1015-0137-kobart_balanced_kfold at: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/7aw7qgki
wandb: ⭐️ View project at: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 01:37:47 | wandb: Find logs at: ./wandb/wandb/run-20251015_013735-7aw7qgki/logs
2025-10-15 01:37:48 | wandb: setting up run lpnmwho8
2025-10-15 01:37:48 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 01:37:48 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_013746-lpnmwho8
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 01:37:48 | wandb: Syncing run 1015-0137-kfold
2025-10-15 01:37:48 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-15 01:37:48 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/lpnmwho8
2025-10-15 01:37:49 | wandb: Detected [openai] in use.
2025-10-15 01:37:49 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-15 01:37:49 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-15 01:37:49 | 📋 실험명: 1015-0137-kfold
2025-10-15 01:37:49 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/lpnmwho8
2025-10-15 01:37:49 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 01:37:49 | 학습 진행 중...
2025-10-15 01:38:11 | {'loss': 2.0439, 'grad_norm': 3.8435347080230713, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 01:38:32 | {'loss': 1.6079, 'grad_norm': 3.825542688369751, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 01:38:52 | {'loss': 1.5516, 'grad_norm': 3.601555109024048, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 01:39:13 | {'loss': 1.531, 'grad_norm': 3.2691919803619385, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 01:39:34 | {'loss': 1.5104, 'grad_norm': 3.7309181690216064, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 01:39:54 | {'loss': 1.4654, 'grad_norm': 3.524055004119873, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 01:45:14 | {'eval_loss': 1.4193016290664673, 'eval_rouge1': 0.3904751452546782, 'eval_rouge2': 0.24210926104784167, 'eval_rougeL': 0.38288769060067424, 'eval_rouge_sum': 1.015472096903194, 'eval_runtime': 315.5053, 'eval_samples_per_second': 7.898, 'eval_steps_per_second': 0.494, 'epoch': 1.0}
2025-10-15 01:45:34 | {'loss': 1.282, 'grad_norm': 3.3895342350006104, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 01:45:55 | {'loss': 1.2288, 'grad_norm': 3.0341920852661133, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 01:46:17 | {'loss': 1.2169, 'grad_norm': 2.882014513015747, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 01:46:41 | {'loss': 1.2469, 'grad_norm': 3.897798538208008, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 01:47:05 | {'loss': 1.2076, 'grad_norm': 3.171177625656128, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 01:47:28 | {'loss': 1.2235, 'grad_norm': 3.8137927055358887, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 01:53:36 | {'eval_loss': 1.3633685111999512, 'eval_rouge1': 0.41159217242413026, 'eval_rouge2': 0.25836360674272013, 'eval_rougeL': 0.4051230017851383, 'eval_rouge_sum': 1.0750787809519888, 'eval_runtime': 357.6155, 'eval_samples_per_second': 6.968, 'eval_steps_per_second': 0.436, 'epoch': 2.0}
2025-10-15 01:54:01 | {'loss': 1.016, 'grad_norm': 2.626234292984009, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 01:54:41 | {'loss': 0.8556, 'grad_norm': 2.7687644958496094, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 01:55:23 | {'loss': 0.8728, 'grad_norm': 3.1205925941467285, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 01:55:50 | {'loss': 0.8835, 'grad_norm': 3.2679688930511475, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 01:56:14 | {'loss': 0.8932, 'grad_norm': 3.312293767929077, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 01:56:37 | {'loss': 0.897, 'grad_norm': 2.923285722732544, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 02:02:49 | {'eval_loss': 1.3991811275482178, 'eval_rouge1': 0.4258183301059979, 'eval_rouge2': 0.26959563526083313, 'eval_rougeL': 0.4181247394195853, 'eval_rouge_sum': 1.1135387047864163, 'eval_runtime': 355.1773, 'eval_samples_per_second': 7.016, 'eval_steps_per_second': 0.439, 'epoch': 3.0}
2025-10-15 02:03:00 | {'loss': 0.8276, 'grad_norm': 2.9799489974975586, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 02:03:23 | {'loss': 0.6097, 'grad_norm': 3.2190732955932617, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 02:03:47 | {'loss': 0.6165, 'grad_norm': 3.6121230125427246, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 02:04:11 | {'loss': 0.6268, 'grad_norm': 2.7466907501220703, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 02:04:36 | {'loss': 0.6392, 'grad_norm': 3.183790445327759, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 02:05:00 | {'loss': 0.6426, 'grad_norm': 3.0941646099090576, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 02:11:22 | {'eval_loss': 1.5070770978927612, 'eval_rouge1': 0.45847481899666953, 'eval_rouge2': 0.29005141326036615, 'eval_rougeL': 0.4455885658563098, 'eval_rouge_sum': 1.1941147981133455, 'eval_runtime': 360.4501, 'eval_samples_per_second': 6.914, 'eval_steps_per_second': 0.433, 'epoch': 4.0}
2025-10-15 02:11:27 | {'loss': 0.6229, 'grad_norm': 2.579951763153076, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 02:11:51 | {'loss': 0.4143, 'grad_norm': 3.0185115337371826, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 02:12:15 | {'loss': 0.4364, 'grad_norm': 3.153991937637329, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 02:12:38 | {'loss': 0.431, 'grad_norm': 3.0333666801452637, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 02:13:02 | {'loss': 0.4495, 'grad_norm': 3.0814995765686035, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 02:13:26 | {'loss': 0.4545, 'grad_norm': 2.522648811340332, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 02:13:50 | {'loss': 0.4667, 'grad_norm': 2.8764383792877197, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 02:21:14 | {'eval_loss': 1.5847960710525513, 'eval_rouge1': 0.4465035191170444, 'eval_rouge2': 0.28472834816696757, 'eval_rougeL': 0.43643202253937285, 'eval_rouge_sum': 1.1676638898233849, 'eval_runtime': 440.5982, 'eval_samples_per_second': 5.656, 'eval_steps_per_second': 0.354, 'epoch': 5.0}
2025-10-15 02:21:46 | {'loss': 0.3177, 'grad_norm': 2.3683791160583496, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 02:22:25 | {'loss': 0.2996, 'grad_norm': 2.308716297149658, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 02:23:02 | {'loss': 0.3107, 'grad_norm': 3.029107093811035, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 02:23:41 | {'loss': 0.307, 'grad_norm': 3.018998146057129, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 02:24:19 | {'loss': 0.3143, 'grad_norm': 2.9203295707702637, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 02:24:57 | {'loss': 0.3204, 'grad_norm': 2.596210479736328, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 02:33:32 | {'eval_loss': 1.6546066999435425, 'eval_rouge1': 0.44050015686234745, 'eval_rouge2': 0.2818316980446988, 'eval_rougeL': 0.4310840043730296, 'eval_rouge_sum': 1.1534158592800758, 'eval_runtime': 501.5382, 'eval_samples_per_second': 4.969, 'eval_steps_per_second': 0.311, 'epoch': 6.0}
2025-10-15 02:34:01 | {'loss': 0.2488, 'grad_norm': 2.1364448070526123, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 02:34:41 | {'loss': 0.2085, 'grad_norm': 2.4540274143218994, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 02:35:21 | {'loss': 0.2197, 'grad_norm': 2.1722257137298584, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 02:36:03 | {'loss': 0.216, 'grad_norm': 2.167043685913086, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 02:36:40 | {'loss': 0.2192, 'grad_norm': 2.156496047973633, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 02:37:21 | {'loss': 0.2147, 'grad_norm': 2.2034196853637695, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 02:45:02 | {'eval_loss': 1.7391232252120972, 'eval_rouge1': 0.4641277623592565, 'eval_rouge2': 0.29199976588662435, 'eval_rougeL': 0.4520153022274265, 'eval_rouge_sum': 1.2081428304733073, 'eval_runtime': 438.5784, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 0.356, 'epoch': 7.0}
2025-10-15 02:45:17 | {'loss': 0.1901, 'grad_norm': 1.8942406177520752, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 02:45:52 | {'loss': 0.1459, 'grad_norm': 1.6815235614776611, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 02:46:28 | {'loss': 0.1527, 'grad_norm': 2.150669574737549, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 02:47:05 | {'loss': 0.151, 'grad_norm': 2.1596081256866455, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 02:47:43 | {'loss': 0.1532, 'grad_norm': 1.9382624626159668, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 02:48:18 | {'loss': 0.1524, 'grad_norm': 1.8583292961120605, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 02:54:28 | {'eval_loss': 1.797174334526062, 'eval_rouge1': 0.4562116804784507, 'eval_rouge2': 0.2912289144011507, 'eval_rougeL': 0.4462478438673826, 'eval_rouge_sum': 1.193688438746984, 'eval_runtime': 340.0409, 'eval_samples_per_second': 7.329, 'eval_steps_per_second': 0.459, 'epoch': 8.0}
2025-10-15 02:54:35 | {'loss': 0.1448, 'grad_norm': 2.099320650100708, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 02:54:58 | {'loss': 0.1026, 'grad_norm': 1.641937255859375, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 02:55:21 | {'loss': 0.1089, 'grad_norm': 1.6978543996810913, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 02:55:43 | {'loss': 0.1091, 'grad_norm': 2.010374069213867, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 02:56:05 | {'loss': 0.1064, 'grad_norm': 1.347853183746338, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 02:56:26 | {'loss': 0.1069, 'grad_norm': 1.826810359954834, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 02:56:47 | {'loss': 0.1068, 'grad_norm': 1.478243350982666, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 03:04:24 | {'eval_loss': 1.83775794506073, 'eval_rouge1': 0.4640450860927266, 'eval_rouge2': 0.29497326944917646, 'eval_rougeL': 0.45327907709513654, 'eval_rouge_sum': 1.2122974326370395, 'eval_runtime': 455.3706, 'eval_samples_per_second': 5.472, 'eval_steps_per_second': 0.343, 'epoch': 9.0}
2025-10-15 03:05:00 | {'loss': 0.081, 'grad_norm': 1.237660527229309, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 03:05:35 | {'loss': 0.0818, 'grad_norm': 1.4337315559387207, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 03:06:10 | {'loss': 0.0793, 'grad_norm': 1.6111451387405396, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 03:06:46 | {'loss': 0.08, 'grad_norm': 1.29619562625885, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 03:07:25 | {'loss': 0.0788, 'grad_norm': 1.6407006978988647, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 03:08:00 | {'loss': 0.0795, 'grad_norm': 1.1951714754104614, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 03:15:13 | {'eval_loss': 1.8662344217300415, 'eval_rouge1': 0.4606701858640066, 'eval_rouge2': 0.29561506305139945, 'eval_rougeL': 0.45053645953069216, 'eval_rouge_sum': 1.2068217084460982, 'eval_runtime': 422.3312, 'eval_samples_per_second': 5.901, 'eval_steps_per_second': 0.369, 'epoch': 10.0}
2025-10-15 03:15:16 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 03:15:16 | {'train_runtime': 5847.0504, 'train_samples_per_second': 17.043, 'train_steps_per_second': 1.065, 'train_loss': 0.5650465482310728, 'epoch': 10.0}
2025-10-15 03:15:16 | 최종 모델 저장 중...
2025-10-15 03:15:17 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_1/kfold/final_model
2025-10-15 03:15:17 | 최종 평가 중...
2025-10-15 03:21:57 | 최종 평가 결과:
2025-10-15 03:21:57 | eval_rouge1: 0.4640
2025-10-15 03:21:57 | eval_rouge2: 0.2950
2025-10-15 03:21:57 | eval_rougeL: 0.4533
2025-10-15 03:21:57 | eval_rouge_sum: 1.2123
2025-10-15 03:21:58 | wandb: updating run metadata
2025-10-15 03:21:58 | wandb: uploading history steps 73-73, summary, console lines 85-89
2025-10-15 03:21:59 | wandb: 
wandb: Run history:
wandb:               eval/loss ▂▁▁▃▄▅▆▇███
wandb:             eval/rouge1 ▁▃▄▇▆▆█▇███
wandb:             eval/rouge2 ▁▃▅▇▇▆█▇███
wandb:             eval/rougeL ▁▃▅▇▆▆█▇███
wandb:          eval/rouge_sum ▁▃▄▇▆▆█▇███
wandb:            eval/runtime ▁▃▂▃▆█▆▂▆▅▄
wandb: eval/samples_per_second █▆▆▆▃▁▃▇▂▃▄
wandb:   eval/steps_per_second █▆▆▆▃▁▃▇▂▃▄
wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:                      +3 ...
wandb: 
wandb: Run summary:
wandb:               eval/loss 1.83776
wandb:             eval/rouge1 0.46405
wandb:             eval/rouge2 0.29497
wandb:             eval/rougeL 0.45328
wandb:          eval/rouge_sum 1.2123
wandb:            eval/runtime 400.5199
wandb: eval/samples_per_second 6.222
wandb:   eval/steps_per_second 0.389
wandb:              total_flos 3.0380119031808e+16
wandb:             train/epoch 10
wandb:                      +8 ...
wandb:
2025-10-15 03:21:59 | wandb: 🚀 View run 1015-0137-kfold at: https://wandb.ai/ieyeppo/nlp-competition/runs/lpnmwho8
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 03:21:59 | wandb: Find logs at: ./wandb/wandb/run-20251015_013746-lpnmwho8/logs
2025-10-15 03:21:59 | ============================================================
2025-10-15 03:21:59 | ✅ 학습 완료!
2025-10-15 03:21:59 | ============================================================
2025-10-15 03:21:59 | ========================================
2025-10-15 03:21:59 | 📌 Fold 2/5 학습 시작
2025-10-15 03:21:59 | ========================================
2025-10-15 03:21:59 | 학습: 9965개
2025-10-15 03:21:59 | 검증: 2492개
2025-10-15 03:21:59 | 모델 타입: encoder_decoder
2025-10-15 03:21:59 | ============================================================
2025-10-15 03:21:59 | 모델 및 토크나이저 로딩 시작
2025-10-15 03:21:59 | ============================================================
2025-10-15 03:21:59 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-15 03:22:00 | 모델 로딩: digit82/kobart-summarization
2025-10-15 03:22:00 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-15 03:22:03 | → 디바이스: cuda
2025-10-15 03:22:03 | → 전체 파라미터: 123,859,968
2025-10-15 03:22:03 | → 학습 가능 파라미터: 123,859,968
2025-10-15 03:22:03 | ============================================================
2025-10-15 03:22:03 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-15 03:22:03 | ============================================================
2025-10-15 03:22:03 | ============================================================
2025-10-15 03:22:03 | 모델 학습 시작
2025-10-15 03:22:03 | ============================================================
2025-10-15 03:22:05 | WandB 로그인 상태: ieyeppo-job
2025-10-15 03:22:06 | wandb: setting up run 5nhr3yga
2025-10-15 03:22:06 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 03:22:06 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_032205-5nhr3yga
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 03:22:06 | wandb: Syncing run 1015-0322-kfold
2025-10-15 03:22:06 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-15 03:22:06 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/5nhr3yga
2025-10-15 03:22:06 | 📋 실험명: 1015-0322-kfold
2025-10-15 03:22:06 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/5nhr3yga
2025-10-15 03:22:06 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 03:22:06 | 학습 진행 중...
2025-10-15 03:22:42 | {'loss': 2.0466, 'grad_norm': 5.31545877456665, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 03:23:16 | {'loss': 1.6201, 'grad_norm': 4.796176910400391, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 03:23:51 | {'loss': 1.5231, 'grad_norm': 3.3712990283966064, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 03:24:26 | {'loss': 1.5169, 'grad_norm': 4.223225116729736, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 03:25:01 | {'loss': 1.4987, 'grad_norm': 3.088057041168213, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 03:25:36 | {'loss': 1.4882, 'grad_norm': 3.8663907051086426, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 03:32:22 | {'eval_loss': 1.414243221282959, 'eval_rouge1': 0.4035975889141587, 'eval_rouge2': 0.25635688285532315, 'eval_rougeL': 0.3968957825054073, 'eval_rouge_sum': 1.0568502542748892, 'eval_runtime': 398.1827, 'eval_samples_per_second': 6.258, 'eval_steps_per_second': 0.392, 'epoch': 1.0}
2025-10-15 03:32:51 | {'loss': 1.2667, 'grad_norm': 4.79495096206665, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 03:33:25 | {'loss': 1.2082, 'grad_norm': 3.294379949569702, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 03:34:00 | {'loss': 1.2259, 'grad_norm': 3.390331268310547, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 03:34:34 | {'loss': 1.2288, 'grad_norm': 3.0226871967315674, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 03:35:09 | {'loss': 1.241, 'grad_norm': 2.9840567111968994, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 03:35:43 | {'loss': 1.2181, 'grad_norm': 3.215672731399536, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 03:42:30 | {'eval_loss': 1.374950647354126, 'eval_rouge1': 0.41410113618067834, 'eval_rouge2': 0.26378673157176946, 'eval_rougeL': 0.4063035608960998, 'eval_rouge_sum': 1.0841914286485477, 'eval_runtime': 390.6489, 'eval_samples_per_second': 6.379, 'eval_steps_per_second': 0.399, 'epoch': 2.0}
2025-10-15 03:42:51 | {'loss': 1.0208, 'grad_norm': 3.16611909866333, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 03:43:26 | {'loss': 0.8462, 'grad_norm': 3.241356134414673, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 03:44:00 | {'loss': 0.8717, 'grad_norm': 2.915555238723755, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 03:44:34 | {'loss': 0.8731, 'grad_norm': 3.152559995651245, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 03:45:09 | {'loss': 0.9046, 'grad_norm': 4.4121294021606445, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 03:45:43 | {'loss': 0.8951, 'grad_norm': 3.074669361114502, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 03:52:39 | {'eval_loss': 1.41354501247406, 'eval_rouge1': 0.43460288694402033, 'eval_rouge2': 0.2755649074344803, 'eval_rougeL': 0.42694814214412713, 'eval_rouge_sum': 1.1371159365226278, 'eval_runtime': 391.3526, 'eval_samples_per_second': 6.368, 'eval_steps_per_second': 0.399, 'epoch': 3.0}
2025-10-15 03:52:52 | {'loss': 0.8147, 'grad_norm': 2.8222362995147705, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 03:53:26 | {'loss': 0.6074, 'grad_norm': 3.047128438949585, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 03:54:01 | {'loss': 0.617, 'grad_norm': 2.7995059490203857, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 03:54:35 | {'loss': 0.6249, 'grad_norm': 3.1235461235046387, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 03:55:11 | {'loss': 0.6434, 'grad_norm': 2.9358716011047363, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 03:55:45 | {'loss': 0.638, 'grad_norm': 3.1127986907958984, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 04:02:55 | {'eval_loss': 1.509141206741333, 'eval_rouge1': 0.4559684228539767, 'eval_rouge2': 0.29107788740064466, 'eval_rougeL': 0.4460697785341631, 'eval_rouge_sum': 1.1931160887887846, 'eval_runtime': 397.7038, 'eval_samples_per_second': 6.266, 'eval_steps_per_second': 0.392, 'epoch': 4.0}
2025-10-15 04:03:00 | {'loss': 0.6337, 'grad_norm': 2.240285873413086, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 04:03:35 | {'loss': 0.4177, 'grad_norm': 2.632636785507202, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 04:04:09 | {'loss': 0.4276, 'grad_norm': 2.7097814083099365, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 04:04:44 | {'loss': 0.4458, 'grad_norm': 2.961890459060669, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 04:05:19 | {'loss': 0.447, 'grad_norm': 2.9611117839813232, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 04:05:53 | {'loss': 0.4543, 'grad_norm': 3.1712048053741455, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 04:06:28 | {'loss': 0.4529, 'grad_norm': 2.9233756065368652, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 04:13:02 | {'eval_loss': 1.5982587337493896, 'eval_rouge1': 0.4506821840645327, 'eval_rouge2': 0.2890129878519212, 'eval_rougeL': 0.44101254005723173, 'eval_rouge_sum': 1.1807077119736857, 'eval_runtime': 389.055, 'eval_samples_per_second': 6.405, 'eval_steps_per_second': 0.401, 'epoch': 5.0}
2025-10-15 04:13:34 | {'loss': 0.3116, 'grad_norm': 2.496323347091675, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 04:14:08 | {'loss': 0.298, 'grad_norm': 2.6572048664093018, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 04:14:42 | {'loss': 0.302, 'grad_norm': 2.3496317863464355, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 04:15:17 | {'loss': 0.3102, 'grad_norm': 2.724595785140991, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 04:15:51 | {'loss': 0.3115, 'grad_norm': 3.05000901222229, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 04:16:25 | {'loss': 0.3213, 'grad_norm': 2.74334454536438, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 04:23:21 | {'eval_loss': 1.6620252132415771, 'eval_rouge1': 0.4495510145374154, 'eval_rouge2': 0.2862316052989714, 'eval_rougeL': 0.4387299896973906, 'eval_rouge_sum': 1.1745126095337775, 'eval_runtime': 402.1991, 'eval_samples_per_second': 6.196, 'eval_steps_per_second': 0.388, 'epoch': 6.0}
2025-10-15 04:23:45 | {'loss': 0.246, 'grad_norm': 2.4465787410736084, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 04:24:20 | {'loss': 0.206, 'grad_norm': 1.9984999895095825, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 04:24:55 | {'loss': 0.2101, 'grad_norm': 2.149364471435547, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 04:25:30 | {'loss': 0.2135, 'grad_norm': 2.24288010597229, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 04:26:04 | {'loss': 0.2175, 'grad_norm': 2.4222373962402344, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 04:26:39 | {'loss': 0.2178, 'grad_norm': 1.8474878072738647, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 04:33:38 | {'eval_loss': 1.7306936979293823, 'eval_rouge1': 0.45425785117289746, 'eval_rouge2': 0.29141369650124305, 'eval_rougeL': 0.443803283536838, 'eval_rouge_sum': 1.1894748312109784, 'eval_runtime': 397.6352, 'eval_samples_per_second': 6.267, 'eval_steps_per_second': 0.392, 'epoch': 7.0}
2025-10-15 04:33:41 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 04:33:41 | {'train_runtime': 4293.5193, 'train_samples_per_second': 23.209, 'train_steps_per_second': 1.451, 'train_loss': 0.7571364105144134, 'epoch': 7.0}
2025-10-15 04:33:41 | 최종 모델 저장 중...
2025-10-15 04:33:42 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_2/kfold/final_model
2025-10-15 04:33:42 | 최종 평가 중...
2025-10-15 04:40:18 | 최종 평가 결과:
2025-10-15 04:40:18 | eval_rouge1: 0.4560
2025-10-15 04:40:18 | eval_rouge2: 0.2911
2025-10-15 04:40:18 | eval_rougeL: 0.4461
2025-10-15 04:40:18 | eval_rouge_sum: 1.1931
2025-10-15 04:40:18 | wandb: updating run metadata
2025-10-15 04:40:19 | wandb: uploading history steps 51-51, summary, console lines 60-64
2025-10-15 04:40:19 | wandb: 
wandb: Run history:
wandb:               eval/loss ▂▁▂▄▅▇█▄
wandb:             eval/rouge1 ▁▂▅█▇▇██
wandb:             eval/rouge2 ▁▂▅██▇██
wandb:             eval/rougeL ▁▂▅█▇▇██
wandb:          eval/rouge_sum ▁▂▅█▇▇██
wandb:            eval/runtime ▆▂▂▆▁█▆▅
wandb: eval/samples_per_second ▃▇▇▃█▁▃▄
wandb:   eval/steps_per_second ▃▇▇▃█▁▃▄
wandb:             train/epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:                      +3 ...
wandb: 
wandb: Run summary:
wandb:               eval/loss 1.50914
wandb:             eval/rouge1 0.45597
wandb:             eval/rouge2 0.29108
wandb:             eval/rougeL 0.44607
wandb:          eval/rouge_sum 1.19312
wandb:            eval/runtime 395.8221
wandb: eval/samples_per_second 6.296
wandb:   eval/steps_per_second 0.394
wandb:              total_flos 2.12660833222656e+16
wandb:             train/epoch 7
wandb:                      +8 ...
wandb:
2025-10-15 04:40:19 | wandb: 🚀 View run 1015-0322-kfold at: https://wandb.ai/ieyeppo/nlp-competition/runs/5nhr3yga
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 04:40:19 | wandb: Find logs at: ./wandb/wandb/run-20251015_032205-5nhr3yga/logs
2025-10-15 04:40:19 | ============================================================
2025-10-15 04:40:19 | ✅ 학습 완료!
2025-10-15 04:40:19 | ============================================================
2025-10-15 04:40:19 | ========================================
2025-10-15 04:40:19 | 📌 Fold 3/5 학습 시작
2025-10-15 04:40:19 | ========================================
2025-10-15 04:40:19 | 학습: 9966개
2025-10-15 04:40:19 | 검증: 2491개
2025-10-15 04:40:19 | 모델 타입: encoder_decoder
2025-10-15 04:40:19 | ============================================================
2025-10-15 04:40:19 | 모델 및 토크나이저 로딩 시작
2025-10-15 04:40:19 | ============================================================
2025-10-15 04:40:19 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-15 04:40:20 | 모델 로딩: digit82/kobart-summarization
2025-10-15 04:40:20 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-15 04:40:22 | → 디바이스: cuda
2025-10-15 04:40:22 | → 전체 파라미터: 123,859,968
2025-10-15 04:40:22 | → 학습 가능 파라미터: 123,859,968
2025-10-15 04:40:22 | ============================================================
2025-10-15 04:40:22 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-15 04:40:22 | ============================================================
2025-10-15 04:40:23 | ============================================================
2025-10-15 04:40:23 | 모델 학습 시작
2025-10-15 04:40:23 | ============================================================
2025-10-15 04:40:24 | WandB 로그인 상태: ieyeppo-job
2025-10-15 04:40:25 | wandb: setting up run c4n143bk
2025-10-15 04:40:25 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 04:40:25 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_044024-c4n143bk
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 04:40:25 | wandb: Syncing run 1015-0440-kfold
2025-10-15 04:40:25 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-15 04:40:25 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/c4n143bk
2025-10-15 04:40:25 | 📋 실험명: 1015-0440-kfold
2025-10-15 04:40:25 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/c4n143bk
2025-10-15 04:40:25 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 04:40:25 | 학습 진행 중...
2025-10-15 04:41:01 | {'loss': 2.0478, 'grad_norm': 4.035416603088379, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 04:41:35 | {'loss': 1.599, 'grad_norm': 4.239953517913818, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 04:42:10 | {'loss': 1.5374, 'grad_norm': 3.9472270011901855, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 04:42:44 | {'loss': 1.5318, 'grad_norm': 3.790060520172119, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 04:43:19 | {'loss': 1.5175, 'grad_norm': 3.4843742847442627, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 04:43:53 | {'loss': 1.4901, 'grad_norm': 3.245954990386963, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 04:50:46 | {'eval_loss': 1.4232920408248901, 'eval_rouge1': 0.387922114132403, 'eval_rouge2': 0.24568756749645132, 'eval_rougeL': 0.3809945673182335, 'eval_rouge_sum': 1.014604248947088, 'eval_runtime': 403.862, 'eval_samples_per_second': 6.168, 'eval_steps_per_second': 0.386, 'epoch': 1.0}
2025-10-15 04:51:15 | {'loss': 1.2763, 'grad_norm': 3.3582286834716797, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 04:51:50 | {'loss': 1.2201, 'grad_norm': 4.455452919006348, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 04:52:25 | {'loss': 1.2304, 'grad_norm': 3.5214297771453857, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 04:53:00 | {'loss': 1.206, 'grad_norm': 2.934473991394043, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 04:53:35 | {'loss': 1.2196, 'grad_norm': 4.211511135101318, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 04:54:10 | {'loss': 1.2396, 'grad_norm': 3.0448882579803467, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 05:00:26 | {'eval_loss': 1.375260829925537, 'eval_rouge1': 0.41445964624834153, 'eval_rouge2': 0.260801557968608, 'eval_rougeL': 0.40639291025288576, 'eval_rouge_sum': 1.0816541144698353, 'eval_runtime': 359.5581, 'eval_samples_per_second': 6.928, 'eval_steps_per_second': 0.434, 'epoch': 2.0}
2025-10-15 05:00:40 | {'loss': 1.0158, 'grad_norm': 2.9213600158691406, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 05:01:01 | {'loss': 0.8659, 'grad_norm': 2.9645509719848633, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 05:01:22 | {'loss': 0.889, 'grad_norm': 3.4673843383789062, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 05:01:43 | {'loss': 0.8687, 'grad_norm': 3.0304222106933594, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 05:02:03 | {'loss': 0.8977, 'grad_norm': 3.1289420127868652, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 05:02:24 | {'loss': 0.8912, 'grad_norm': 2.910853624343872, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 05:08:11 | {'eval_loss': 1.4098237752914429, 'eval_rouge1': 0.42206420600100036, 'eval_rouge2': 0.26855454924750455, 'eval_rougeL': 0.41477267695378967, 'eval_rouge_sum': 1.1053914322022946, 'eval_runtime': 332.1148, 'eval_samples_per_second': 7.5, 'eval_steps_per_second': 0.47, 'epoch': 3.0}
2025-10-15 05:08:22 | {'loss': 0.7972, 'grad_norm': 2.7424421310424805, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 05:08:43 | {'loss': 0.5999, 'grad_norm': 3.509894847869873, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 05:09:03 | {'loss': 0.6158, 'grad_norm': 4.322897434234619, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 05:09:25 | {'loss': 0.6265, 'grad_norm': 3.5287036895751953, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 05:09:45 | {'loss': 0.6302, 'grad_norm': 3.2348978519439697, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 05:10:06 | {'loss': 0.6495, 'grad_norm': 3.0103983879089355, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 05:16:01 | {'eval_loss': 1.4935805797576904, 'eval_rouge1': 0.4486217368374123, 'eval_rouge2': 0.29024598270744917, 'eval_rougeL': 0.4406872614543326, 'eval_rouge_sum': 1.179554980999194, 'eval_runtime': 335.0988, 'eval_samples_per_second': 7.434, 'eval_steps_per_second': 0.466, 'epoch': 4.0}
2025-10-15 05:16:06 | {'loss': 0.6344, 'grad_norm': 2.3463149070739746, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 05:16:27 | {'loss': 0.4121, 'grad_norm': 2.801440477371216, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 05:16:48 | {'loss': 0.4308, 'grad_norm': 2.9572272300720215, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 05:17:09 | {'loss': 0.4391, 'grad_norm': 3.3030176162719727, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 05:17:29 | {'loss': 0.4479, 'grad_norm': 2.5960564613342285, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 05:17:50 | {'loss': 0.4505, 'grad_norm': 2.6499838829040527, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 05:18:12 | {'loss': 0.4565, 'grad_norm': 3.1763360500335693, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 05:23:47 | {'eval_loss': 1.6073942184448242, 'eval_rouge1': 0.44293402153236777, 'eval_rouge2': 0.28199826529079086, 'eval_rougeL': 0.43473083022748754, 'eval_rouge_sum': 1.1596631170506462, 'eval_runtime': 332.367, 'eval_samples_per_second': 7.495, 'eval_steps_per_second': 0.469, 'epoch': 5.0}
2025-10-15 05:24:07 | {'loss': 0.312, 'grad_norm': 2.199693202972412, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 05:24:28 | {'loss': 0.2968, 'grad_norm': 2.6315555572509766, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 05:24:50 | {'loss': 0.3014, 'grad_norm': 2.7056884765625, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 05:25:10 | {'loss': 0.3074, 'grad_norm': 2.511983871459961, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 05:25:31 | {'loss': 0.3152, 'grad_norm': 2.757821798324585, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 05:25:53 | {'loss': 0.318, 'grad_norm': 2.8788180351257324, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 05:32:44 | {'eval_loss': 1.6705337762832642, 'eval_rouge1': 0.45055482279404624, 'eval_rouge2': 0.29055049003283456, 'eval_rougeL': 0.44064968680799216, 'eval_rouge_sum': 1.1817549996348728, 'eval_runtime': 403.7041, 'eval_samples_per_second': 6.17, 'eval_steps_per_second': 0.386, 'epoch': 6.0}
2025-10-15 05:33:09 | {'loss': 0.2446, 'grad_norm': 2.0208845138549805, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 05:33:44 | {'loss': 0.208, 'grad_norm': 2.2084391117095947, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 05:34:19 | {'loss': 0.212, 'grad_norm': 1.8786178827285767, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 05:34:55 | {'loss': 0.2119, 'grad_norm': 2.269291400909424, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 05:35:30 | {'loss': 0.2146, 'grad_norm': 2.28810715675354, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 05:36:05 | {'loss': 0.2162, 'grad_norm': 2.432166576385498, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 05:43:09 | {'eval_loss': 1.7491995096206665, 'eval_rouge1': 0.45702157295261037, 'eval_rouge2': 0.2941804542003041, 'eval_rougeL': 0.4484521270920434, 'eval_rouge_sum': 1.199654154244958, 'eval_runtime': 402.8713, 'eval_samples_per_second': 6.183, 'eval_steps_per_second': 0.387, 'epoch': 7.0}
2025-10-15 05:43:25 | {'loss': 0.1905, 'grad_norm': 1.8230026960372925, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 05:44:01 | {'loss': 0.1438, 'grad_norm': 2.1577975749969482, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 05:44:36 | {'loss': 0.1483, 'grad_norm': 1.940536618232727, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 05:45:11 | {'loss': 0.1509, 'grad_norm': 2.2261221408843994, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 05:45:46 | {'loss': 0.1515, 'grad_norm': 1.8062381744384766, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 05:46:21 | {'loss': 0.1494, 'grad_norm': 2.52072811126709, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 05:53:37 | {'eval_loss': 1.8069485425949097, 'eval_rouge1': 0.4533288041816152, 'eval_rouge2': 0.2938997906151234, 'eval_rougeL': 0.4442749902533058, 'eval_rouge_sum': 1.1915035850500444, 'eval_runtime': 406.5459, 'eval_samples_per_second': 6.127, 'eval_steps_per_second': 0.384, 'epoch': 8.0}
2025-10-15 05:53:45 | {'loss': 0.1464, 'grad_norm': 1.660768747329712, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 05:54:20 | {'loss': 0.1041, 'grad_norm': 1.4367783069610596, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 05:54:54 | {'loss': 0.1049, 'grad_norm': 1.6490882635116577, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 05:55:29 | {'loss': 0.1088, 'grad_norm': 1.7106126546859741, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 05:56:04 | {'loss': 0.1069, 'grad_norm': 1.7130498886108398, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 05:56:41 | {'loss': 0.1045, 'grad_norm': 1.7856037616729736, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 05:57:16 | {'loss': 0.1044, 'grad_norm': 1.562135934829712, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 06:04:36 | {'eval_loss': 1.8544440269470215, 'eval_rouge1': 0.461430214454143, 'eval_rouge2': 0.29523202607302135, 'eval_rougeL': 0.4510049179679937, 'eval_rouge_sum': 1.2076671584951582, 'eval_runtime': 437.2782, 'eval_samples_per_second': 5.697, 'eval_steps_per_second': 0.357, 'epoch': 9.0}
2025-10-15 06:05:11 | {'loss': 0.0823, 'grad_norm': 1.7239741086959839, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 06:05:46 | {'loss': 0.0804, 'grad_norm': 1.9269459247589111, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 06:06:21 | {'loss': 0.0789, 'grad_norm': 1.3032562732696533, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 06:06:56 | {'loss': 0.0775, 'grad_norm': 1.5978825092315674, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 06:07:32 | {'loss': 0.0806, 'grad_norm': 1.485266923904419, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 06:08:08 | {'loss': 0.0759, 'grad_norm': 1.4631805419921875, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 06:15:33 | {'eval_loss': 1.8849741220474243, 'eval_rouge1': 0.45810598155904086, 'eval_rouge2': 0.294304728938613, 'eval_rougeL': 0.4487481285868275, 'eval_rouge_sum': 1.2011588390844814, 'eval_runtime': 434.4455, 'eval_samples_per_second': 5.734, 'eval_steps_per_second': 0.359, 'epoch': 10.0}
2025-10-15 06:15:36 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 06:15:36 | {'train_runtime': 5709.8089, 'train_samples_per_second': 17.454, 'train_steps_per_second': 1.091, 'train_loss': 0.563492120288157, 'epoch': 10.0}
2025-10-15 06:15:36 | 최종 모델 저장 중...
2025-10-15 06:15:37 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_3/kfold/final_model
2025-10-15 06:15:37 | 최종 평가 중...
2025-10-15 06:22:50 | 최종 평가 결과:
2025-10-15 06:22:50 | eval_rouge1: 0.4614
2025-10-15 06:22:50 | eval_rouge2: 0.2952
2025-10-15 06:22:50 | eval_rougeL: 0.4510
2025-10-15 06:22:50 | eval_rouge_sum: 1.2077
2025-10-15 06:22:50 | wandb: updating run metadata
2025-10-15 06:22:51 | wandb: uploading history steps 73-73, summary, console lines 82-86
2025-10-15 06:22:51 | wandb: 
wandb: Run history:
wandb:               eval/loss ▂▁▁▃▄▅▆▇███
wandb:             eval/rouge1 ▁▄▄▇▆▇█▇███
wandb:             eval/rouge2 ▁▃▄▇▆▇█████
wandb:             eval/rougeL ▁▄▄▇▆▇█▇███
wandb:          eval/rouge_sum ▁▃▄▇▆▇█▇███
wandb:            eval/runtime ▆▃▁▁▁▆▆▆███
wandb: eval/samples_per_second ▃▆███▃▃▃▁▁▁
wandb:   eval/steps_per_second ▃▆███▃▃▃▁▁▁
wandb:             train/epoch ▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████
wandb:                      +3 ...
wandb: 
wandb: Run summary:
wandb:               eval/loss 1.85444
wandb:             eval/rouge1 0.46143
wandb:             eval/rouge2 0.29523
wandb:             eval/rougeL 0.451
wandb:          eval/rouge_sum 1.20767
wandb:            eval/runtime 432.7001
wandb: eval/samples_per_second 5.757
wandb:   eval/steps_per_second 0.361
wandb:              total_flos 3.03831677140992e+16
wandb:             train/epoch 10
wandb:                      +8 ...
wandb:
2025-10-15 06:22:51 | wandb: 🚀 View run 1015-0440-kfold at: https://wandb.ai/ieyeppo/nlp-competition/runs/c4n143bk
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 06:22:51 | wandb: Find logs at: ./wandb/wandb/run-20251015_044024-c4n143bk/logs
2025-10-15 06:22:51 | ============================================================
2025-10-15 06:22:51 | ✅ 학습 완료!
2025-10-15 06:22:51 | ============================================================
2025-10-15 06:22:51 | ========================================
2025-10-15 06:22:51 | 📌 Fold 4/5 학습 시작
2025-10-15 06:22:51 | ========================================
2025-10-15 06:22:51 | 학습: 9966개
2025-10-15 06:22:51 | 검증: 2491개
2025-10-15 06:22:51 | 모델 타입: encoder_decoder
2025-10-15 06:22:51 | ============================================================
2025-10-15 06:22:51 | 모델 및 토크나이저 로딩 시작
2025-10-15 06:22:51 | ============================================================
2025-10-15 06:22:51 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-15 06:22:52 | 모델 로딩: digit82/kobart-summarization
2025-10-15 06:22:52 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-15 06:22:54 | → 디바이스: cuda
2025-10-15 06:22:54 | → 전체 파라미터: 123,859,968
2025-10-15 06:22:54 | → 학습 가능 파라미터: 123,859,968
2025-10-15 06:22:54 | ============================================================
2025-10-15 06:22:54 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-15 06:22:54 | ============================================================
2025-10-15 06:22:55 | ============================================================
2025-10-15 06:22:55 | 모델 학습 시작
2025-10-15 06:22:55 | ============================================================
2025-10-15 06:22:56 | WandB 로그인 상태: ieyeppo-job
2025-10-15 06:22:57 | wandb: setting up run auf53gld
2025-10-15 06:22:57 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 06:22:57 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_062256-auf53gld
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 06:22:57 | wandb: Syncing run 1015-0622-kfold
2025-10-15 06:22:57 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-15 06:22:57 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/auf53gld
2025-10-15 06:22:58 | 📋 실험명: 1015-0622-kfold
2025-10-15 06:22:58 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/auf53gld
2025-10-15 06:22:58 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 06:22:58 | 학습 진행 중...
2025-10-15 06:23:34 | {'loss': 2.0552, 'grad_norm': 4.045368671417236, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 06:24:09 | {'loss': 1.6244, 'grad_norm': 3.914649248123169, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 06:24:45 | {'loss': 1.5388, 'grad_norm': 3.771638870239258, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 06:25:21 | {'loss': 1.5167, 'grad_norm': 3.7084696292877197, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 06:25:56 | {'loss': 1.5057, 'grad_norm': 3.680605173110962, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 06:26:32 | {'loss': 1.5084, 'grad_norm': 3.370373487472534, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 06:33:49 | {'eval_loss': 1.4120222330093384, 'eval_rouge1': 0.40621156876806036, 'eval_rouge2': 0.2516726494962852, 'eval_rougeL': 0.3969823585800422, 'eval_rouge_sum': 1.054866576844388, 'eval_runtime': 428.2494, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 0.364, 'epoch': 1.0}
2025-10-15 06:34:20 | {'loss': 1.2643, 'grad_norm': 2.9061245918273926, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 06:34:56 | {'loss': 1.2108, 'grad_norm': 2.8720526695251465, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 06:35:32 | {'loss': 1.2259, 'grad_norm': 3.2856028079986572, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 06:36:08 | {'loss': 1.2225, 'grad_norm': 3.2307016849517822, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 06:36:43 | {'loss': 1.2274, 'grad_norm': 4.339393138885498, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 06:37:19 | {'loss': 1.2502, 'grad_norm': 3.2222089767456055, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 06:44:53 | {'eval_loss': 1.3556207418441772, 'eval_rouge1': 0.43626494593414955, 'eval_rouge2': 0.27614112774788574, 'eval_rougeL': 0.4251703220649811, 'eval_rouge_sum': 1.1375763957470164, 'eval_runtime': 437.0085, 'eval_samples_per_second': 5.7, 'eval_steps_per_second': 0.357, 'epoch': 2.0}
2025-10-15 06:45:15 | {'loss': 1.0131, 'grad_norm': 6.381958484649658, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 06:45:50 | {'loss': 0.8751, 'grad_norm': 3.0905191898345947, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 06:46:26 | {'loss': 0.8704, 'grad_norm': 3.117363214492798, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 06:47:02 | {'loss': 0.8799, 'grad_norm': 2.6971137523651123, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 06:47:37 | {'loss': 0.9047, 'grad_norm': 3.3255255222320557, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 06:48:12 | {'loss': 0.9002, 'grad_norm': 3.6186118125915527, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 06:55:51 | {'eval_loss': 1.4022278785705566, 'eval_rouge1': 0.45275010325836407, 'eval_rouge2': 0.2867816082766875, 'eval_rougeL': 0.4416817994991297, 'eval_rouge_sum': 1.1812135110341813, 'eval_runtime': 433.9151, 'eval_samples_per_second': 5.741, 'eval_steps_per_second': 0.36, 'epoch': 3.0}
2025-10-15 06:56:05 | {'loss': 0.8215, 'grad_norm': 3.435472011566162, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 06:56:41 | {'loss': 0.5981, 'grad_norm': 3.0562918186187744, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 06:57:16 | {'loss': 0.6136, 'grad_norm': 2.9181621074676514, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 06:57:52 | {'loss': 0.6383, 'grad_norm': 2.9622983932495117, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 06:58:28 | {'loss': 0.6449, 'grad_norm': 2.7642014026641846, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 06:59:05 | {'loss': 0.6594, 'grad_norm': 3.2186222076416016, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 07:06:52 | {'eval_loss': 1.496817708015442, 'eval_rouge1': 0.45161320251340087, 'eval_rouge2': 0.287436066292487, 'eval_rougeL': 0.43951931437975394, 'eval_rouge_sum': 1.1785685831856418, 'eval_runtime': 434.5901, 'eval_samples_per_second': 5.732, 'eval_steps_per_second': 0.359, 'epoch': 4.0}
2025-10-15 07:06:57 | {'loss': 0.6369, 'grad_norm': 2.4484989643096924, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 07:07:32 | {'loss': 0.4161, 'grad_norm': 2.908137559890747, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 07:08:07 | {'loss': 0.4361, 'grad_norm': 2.848815679550171, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 07:08:43 | {'loss': 0.4441, 'grad_norm': 3.0774011611938477, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 07:09:18 | {'loss': 0.4478, 'grad_norm': 2.8623311519622803, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 07:09:54 | {'loss': 0.4602, 'grad_norm': 3.054884910583496, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 07:10:30 | {'loss': 0.4569, 'grad_norm': 2.962921142578125, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 07:17:45 | {'eval_loss': 1.5977274179458618, 'eval_rouge1': 0.47036841786423983, 'eval_rouge2': 0.3006210994188213, 'eval_rougeL': 0.4578951070304753, 'eval_rouge_sum': 1.2288846243135365, 'eval_runtime': 430.186, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 0.363, 'epoch': 5.0}
2025-10-15 07:18:18 | {'loss': 0.3138, 'grad_norm': 2.41440486907959, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 07:18:58 | {'loss': 0.3, 'grad_norm': 2.7346253395080566, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 07:19:33 | {'loss': 0.3106, 'grad_norm': 2.834183692932129, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 07:20:08 | {'loss': 0.3123, 'grad_norm': 2.555375099182129, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 07:20:44 | {'loss': 0.3189, 'grad_norm': 2.9856479167938232, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 07:21:19 | {'loss': 0.314, 'grad_norm': 3.010514974594116, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 07:28:39 | {'eval_loss': 1.6716500520706177, 'eval_rouge1': 0.46510369955908515, 'eval_rouge2': 0.296242755081081, 'eval_rougeL': 0.4532575339188224, 'eval_rouge_sum': 1.2146039885589885, 'eval_runtime': 426.7274, 'eval_samples_per_second': 5.837, 'eval_steps_per_second': 0.366, 'epoch': 6.0}
2025-10-15 07:29:05 | {'loss': 0.2454, 'grad_norm': 1.986413836479187, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 07:29:41 | {'loss': 0.2072, 'grad_norm': 1.9647530317306519, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 07:30:16 | {'loss': 0.2107, 'grad_norm': 2.0606884956359863, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 07:30:54 | {'loss': 0.2152, 'grad_norm': 2.5113232135772705, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 07:31:30 | {'loss': 0.2209, 'grad_norm': 2.2772207260131836, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 07:32:05 | {'loss': 0.2198, 'grad_norm': 2.0435149669647217, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 07:39:29 | {'eval_loss': 1.7307860851287842, 'eval_rouge1': 0.4690491920460259, 'eval_rouge2': 0.3006368668432321, 'eval_rougeL': 0.45710265027608804, 'eval_rouge_sum': 1.226788709165346, 'eval_runtime': 423.3543, 'eval_samples_per_second': 5.884, 'eval_steps_per_second': 0.368, 'epoch': 7.0}
2025-10-15 07:39:45 | {'loss': 0.1885, 'grad_norm': 1.9160505533218384, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 07:40:20 | {'loss': 0.1455, 'grad_norm': 2.179718494415283, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 07:40:55 | {'loss': 0.1498, 'grad_norm': 2.08005428314209, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 07:41:29 | {'loss': 0.1525, 'grad_norm': 1.9696879386901855, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 07:42:04 | {'loss': 0.1489, 'grad_norm': 2.186988592147827, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 07:42:39 | {'loss': 0.1518, 'grad_norm': 2.114725112915039, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 07:49:51 | {'eval_loss': 1.7888641357421875, 'eval_rouge1': 0.47098083394935913, 'eval_rouge2': 0.3010231322864327, 'eval_rougeL': 0.45689377577354423, 'eval_rouge_sum': 1.228897742009336, 'eval_runtime': 402.7433, 'eval_samples_per_second': 6.185, 'eval_steps_per_second': 0.387, 'epoch': 8.0}
2025-10-15 07:49:59 | {'loss': 0.1456, 'grad_norm': 1.47307288646698, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 07:50:34 | {'loss': 0.1043, 'grad_norm': 1.5466364622116089, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 07:51:08 | {'loss': 0.1038, 'grad_norm': 2.0509796142578125, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 07:51:43 | {'loss': 0.1075, 'grad_norm': 1.8776121139526367, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 07:52:18 | {'loss': 0.1056, 'grad_norm': 1.7763476371765137, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 07:52:52 | {'loss': 0.1062, 'grad_norm': 1.9077787399291992, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 07:53:27 | {'loss': 0.1067, 'grad_norm': 1.7809641361236572, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 08:00:16 | {'eval_loss': 1.8369140625, 'eval_rouge1': 0.47515951284314806, 'eval_rouge2': 0.3018421587323041, 'eval_rougeL': 0.4611754029324552, 'eval_rouge_sum': 1.2381770745079073, 'eval_runtime': 406.4207, 'eval_samples_per_second': 6.129, 'eval_steps_per_second': 0.384, 'epoch': 9.0}
2025-10-15 08:00:50 | {'loss': 0.0791, 'grad_norm': 1.4045495986938477, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 08:01:25 | {'loss': 0.082, 'grad_norm': 1.630280613899231, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 08:02:00 | {'loss': 0.0779, 'grad_norm': 1.372799277305603, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 08:02:34 | {'loss': 0.0797, 'grad_norm': 1.29688560962677, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 08:03:09 | {'loss': 0.0795, 'grad_norm': 1.2980883121490479, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 08:03:44 | {'loss': 0.0764, 'grad_norm': 1.3247674703598022, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 08:10:39 | {'eval_loss': 1.869685411453247, 'eval_rouge1': 0.4703111559420989, 'eval_rouge2': 0.3014864439387848, 'eval_rougeL': 0.45839713588396125, 'eval_rouge_sum': 1.230194735764845, 'eval_runtime': 405.1331, 'eval_samples_per_second': 6.149, 'eval_steps_per_second': 0.385, 'epoch': 10.0}
2025-10-15 08:10:42 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 08:10:42 | {'train_runtime': 6463.8902, 'train_samples_per_second': 15.418, 'train_steps_per_second': 0.964, 'train_loss': 0.5661603761905651, 'epoch': 10.0}
2025-10-15 08:10:42 | 최종 모델 저장 중...
2025-10-15 08:10:43 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_4/kfold/final_model
2025-10-15 08:10:43 | 최종 평가 중...
2025-10-15 08:17:27 | 최종 평가 결과:
2025-10-15 08:17:27 | eval_rouge1: 0.4752
2025-10-15 08:17:27 | eval_rouge2: 0.3018
2025-10-15 08:17:27 | eval_rougeL: 0.4612
2025-10-15 08:17:27 | eval_rouge_sum: 1.2382
2025-10-15 08:17:27 | wandb: updating run metadata
2025-10-15 08:17:28 | wandb: uploading history steps 73-73, summary, console lines 82-86
2025-10-15 08:17:28 | wandb: 
wandb: Run history:
wandb:               eval/loss ▂▁▂▃▄▅▆▇███
wandb:             eval/rouge1 ▁▄▆▆█▇▇████
wandb:             eval/rouge2 ▁▄▆▆█▇█████
wandb:             eval/rougeL ▁▄▆▆█▇█████
wandb:          eval/rouge_sum ▁▄▆▆█▇█████
wandb:            eval/runtime ▆█▇█▇▆▅▁▂▁▁
wandb: eval/samples_per_second ▃▁▂▁▂▃▄█▇▇█
wandb:   eval/steps_per_second ▃▁▂▁▂▃▄█▇██
wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████
wandb:                      +3 ...
wandb: 
wandb: Run summary:
wandb:               eval/loss 1.83691
wandb:             eval/rouge1 0.47516
wandb:             eval/rouge2 0.30184
wandb:             eval/rougeL 0.46118
wandb:          eval/rouge_sum 1.23818
wandb:            eval/runtime 403.836
wandb: eval/samples_per_second 6.168
wandb:   eval/steps_per_second 0.386
wandb:              total_flos 3.03831677140992e+16
wandb:             train/epoch 10
wandb:                      +8 ...
wandb:
2025-10-15 08:17:28 | wandb: 🚀 View run 1015-0622-kfold at: https://wandb.ai/ieyeppo/nlp-competition/runs/auf53gld
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 08:17:28 | wandb: Find logs at: ./wandb/wandb/run-20251015_062256-auf53gld/logs
2025-10-15 08:17:28 | ============================================================
2025-10-15 08:17:28 | ✅ 학습 완료!
2025-10-15 08:17:28 | ============================================================
2025-10-15 08:17:28 | ========================================
2025-10-15 08:17:28 | 📌 Fold 5/5 학습 시작
2025-10-15 08:17:28 | ========================================
2025-10-15 08:17:28 | 학습: 9966개
2025-10-15 08:17:28 | 검증: 2491개
2025-10-15 08:17:28 | 모델 타입: encoder_decoder
2025-10-15 08:17:28 | ============================================================
2025-10-15 08:17:28 | 모델 및 토크나이저 로딩 시작
2025-10-15 08:17:28 | ============================================================
2025-10-15 08:17:28 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-15 08:17:29 | 모델 로딩: digit82/kobart-summarization
2025-10-15 08:17:29 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-15 08:17:31 | → 디바이스: cuda
2025-10-15 08:17:31 | → 전체 파라미터: 123,859,968
2025-10-15 08:17:31 | → 학습 가능 파라미터: 123,859,968
2025-10-15 08:17:31 | ============================================================
2025-10-15 08:17:31 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-15 08:17:31 | ============================================================
2025-10-15 08:17:31 | ============================================================
2025-10-15 08:17:31 | 모델 학습 시작
2025-10-15 08:17:31 | ============================================================
2025-10-15 08:17:33 | WandB 로그인 상태: ieyeppo-job
2025-10-15 08:17:34 | wandb: setting up run wfwrczgj
2025-10-15 08:17:34 | wandb: Tracking run with wandb version 0.22.2
2025-10-15 08:17:34 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251015_081733-wfwrczgj
wandb: Run `wandb offline` to turn off syncing.
2025-10-15 08:17:34 | wandb: Syncing run 1015-0817-kfold
2025-10-15 08:17:34 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-15 08:17:34 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/wfwrczgj
2025-10-15 08:17:34 | 📋 실험명: 1015-0817-kfold
2025-10-15 08:17:34 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/wfwrczgj
2025-10-15 08:17:34 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 08:17:34 | 학습 진행 중...
2025-10-15 08:18:09 | {'loss': 2.0263, 'grad_norm': 4.585365295410156, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 08:18:44 | {'loss': 1.5969, 'grad_norm': 3.945899724960327, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 08:19:18 | {'loss': 1.5625, 'grad_norm': 3.566885471343994, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 08:19:53 | {'loss': 1.5191, 'grad_norm': 3.5406200885772705, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 08:20:28 | {'loss': 1.4951, 'grad_norm': 5.17777156829834, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 08:21:02 | {'loss': 1.4711, 'grad_norm': 3.418663501739502, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 08:27:52 | {'eval_loss': 1.461945652961731, 'eval_rouge1': 0.3802352974167428, 'eval_rouge2': 0.23150527722084896, 'eval_rougeL': 0.3665082725373369, 'eval_rouge_sum': 0.9782488471749287, 'eval_runtime': 401.4722, 'eval_samples_per_second': 6.205, 'eval_steps_per_second': 0.389, 'epoch': 1.0}
2025-10-15 08:28:22 | {'loss': 1.2767, 'grad_norm': 3.114906072616577, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 08:28:56 | {'loss': 1.2027, 'grad_norm': 9.599915504455566, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 08:29:31 | {'loss': 1.2227, 'grad_norm': 3.5160365104675293, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 08:30:06 | {'loss': 1.233, 'grad_norm': 3.0145466327667236, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 08:30:40 | {'loss': 1.2113, 'grad_norm': 2.954576015472412, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 08:31:15 | {'loss': 1.2178, 'grad_norm': 3.00075626373291, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 08:38:18 | {'eval_loss': 1.3617980480194092, 'eval_rouge1': 0.41366312933762833, 'eval_rouge2': 0.2595388922820762, 'eval_rougeL': 0.40564432351311197, 'eval_rouge_sum': 1.0788463451328165, 'eval_runtime': 406.9416, 'eval_samples_per_second': 6.121, 'eval_steps_per_second': 0.383, 'epoch': 2.0}
2025-10-15 08:38:41 | {'loss': 1.014, 'grad_norm': 2.852675199508667, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 08:39:15 | {'loss': 0.8649, 'grad_norm': 2.8584976196289062, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 08:39:50 | {'loss': 0.8653, 'grad_norm': 3.4780771732330322, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 08:40:25 | {'loss': 0.8813, 'grad_norm': 4.451434135437012, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 08:41:00 | {'loss': 0.8924, 'grad_norm': 4.225039482116699, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 08:41:34 | {'loss': 0.8926, 'grad_norm': 3.2117528915405273, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 08:48:48 | {'eval_loss': 1.400778889656067, 'eval_rouge1': 0.444172764291807, 'eval_rouge2': 0.28327201304474336, 'eval_rougeL': 0.4351974766858461, 'eval_rouge_sum': 1.1626422540223964, 'eval_runtime': 409.7698, 'eval_samples_per_second': 6.079, 'eval_steps_per_second': 0.381, 'epoch': 3.0}
2025-10-15 08:49:02 | {'loss': 0.822, 'grad_norm': 2.997591257095337, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 08:49:36 | {'loss': 0.5986, 'grad_norm': 2.8086087703704834, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 08:50:11 | {'loss': 0.613, 'grad_norm': 2.742763042449951, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 08:50:46 | {'loss': 0.6333, 'grad_norm': 2.8307390213012695, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 08:51:21 | {'loss': 0.6351, 'grad_norm': 2.9665637016296387, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 08:51:55 | {'loss': 0.6378, 'grad_norm': 3.029090404510498, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 08:59:21 | {'eval_loss': 1.4894907474517822, 'eval_rouge1': 0.43638973720701324, 'eval_rouge2': 0.2747001547572166, 'eval_rougeL': 0.426314184947476, 'eval_rouge_sum': 1.137404076911706, 'eval_runtime': 413.815, 'eval_samples_per_second': 6.02, 'eval_steps_per_second': 0.377, 'epoch': 4.0}
2025-10-15 08:59:26 | {'loss': 0.6361, 'grad_norm': 3.076024293899536, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 09:00:01 | {'loss': 0.4228, 'grad_norm': 2.944962739944458, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 09:00:37 | {'loss': 0.4255, 'grad_norm': 2.8129031658172607, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 09:01:12 | {'loss': 0.4437, 'grad_norm': 2.460153102874756, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 09:01:47 | {'loss': 0.4427, 'grad_norm': 2.9103126525878906, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 09:02:22 | {'loss': 0.4488, 'grad_norm': 3.138399600982666, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 09:02:56 | {'loss': 0.4589, 'grad_norm': 3.1528427600860596, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 09:09:42 | {'eval_loss': 1.6024926900863647, 'eval_rouge1': 0.44834003601720895, 'eval_rouge2': 0.28535035362973676, 'eval_rougeL': 0.43748306837903067, 'eval_rouge_sum': 1.1711734580259765, 'eval_runtime': 400.8262, 'eval_samples_per_second': 6.215, 'eval_steps_per_second': 0.389, 'epoch': 5.0}
2025-10-15 09:10:15 | {'loss': 0.3149, 'grad_norm': 2.7062528133392334, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 09:10:50 | {'loss': 0.3003, 'grad_norm': 2.493779182434082, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 09:11:24 | {'loss': 0.2992, 'grad_norm': 2.2161717414855957, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 09:11:59 | {'loss': 0.3096, 'grad_norm': 2.093888521194458, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 09:12:34 | {'loss': 0.3166, 'grad_norm': 2.8410837650299072, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 09:13:08 | {'loss': 0.315, 'grad_norm': 2.665344715118408, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 09:20:13 | {'eval_loss': 1.660935401916504, 'eval_rouge1': 0.44128966623783117, 'eval_rouge2': 0.2799396234003353, 'eval_rougeL': 0.4301104477876049, 'eval_rouge_sum': 1.1513397374257712, 'eval_runtime': 411.2356, 'eval_samples_per_second': 6.057, 'eval_steps_per_second': 0.379, 'epoch': 6.0}
2025-10-15 09:20:37 | {'loss': 0.2494, 'grad_norm': 2.1023616790771484, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 09:21:12 | {'loss': 0.2072, 'grad_norm': 2.4705212116241455, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 09:21:48 | {'loss': 0.2125, 'grad_norm': 2.225376605987549, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 09:22:22 | {'loss': 0.2142, 'grad_norm': 2.724674701690674, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 09:22:57 | {'loss': 0.2162, 'grad_norm': 3.3995749950408936, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 09:23:32 | {'loss': 0.2172, 'grad_norm': 2.1786365509033203, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 09:30:38 | {'eval_loss': 1.7340869903564453, 'eval_rouge1': 0.447717806167723, 'eval_rouge2': 0.2826964494950548, 'eval_rougeL': 0.43628483993552913, 'eval_rouge_sum': 1.166699095598307, 'eval_runtime': 405.2914, 'eval_samples_per_second': 6.146, 'eval_steps_per_second': 0.385, 'epoch': 7.0}
2025-10-15 09:30:54 | {'loss': 0.1898, 'grad_norm': 1.9973840713500977, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 09:31:29 | {'loss': 0.1441, 'grad_norm': 2.1207711696624756, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 09:32:04 | {'loss': 0.1482, 'grad_norm': 2.006422758102417, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 09:32:38 | {'loss': 0.1475, 'grad_norm': 2.0903074741363525, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 09:33:13 | {'loss': 0.1506, 'grad_norm': 1.9477741718292236, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 09:33:48 | {'loss': 0.1501, 'grad_norm': 2.1785311698913574, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 09:41:05 | {'eval_loss': 1.8073689937591553, 'eval_rouge1': 0.45383190623399816, 'eval_rouge2': 0.28704865280262043, 'eval_rougeL': 0.4416267408768111, 'eval_rouge_sum': 1.1825072999134296, 'eval_runtime': 407.5316, 'eval_samples_per_second': 6.112, 'eval_steps_per_second': 0.383, 'epoch': 8.0}
2025-10-15 09:41:13 | {'loss': 0.1407, 'grad_norm': 1.6800915002822876, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 09:41:48 | {'loss': 0.106, 'grad_norm': 1.4382919073104858, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 09:42:23 | {'loss': 0.106, 'grad_norm': 1.586861491203308, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 09:42:58 | {'loss': 0.1074, 'grad_norm': 1.9099141359329224, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 09:43:32 | {'loss': 0.105, 'grad_norm': 1.8284330368041992, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 09:44:07 | {'loss': 0.1043, 'grad_norm': 1.6146959066390991, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 09:44:42 | {'loss': 0.1048, 'grad_norm': 1.8312900066375732, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 09:51:32 | {'eval_loss': 1.8449456691741943, 'eval_rouge1': 0.4632358847820775, 'eval_rouge2': 0.2954698985130164, 'eval_rougeL': 0.45103180921559993, 'eval_rouge_sum': 1.209737592510694, 'eval_runtime': 407.9498, 'eval_samples_per_second': 6.106, 'eval_steps_per_second': 0.382, 'epoch': 9.0}
2025-10-15 09:52:07 | {'loss': 0.0804, 'grad_norm': 1.5581990480422974, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 09:52:42 | {'loss': 0.0805, 'grad_norm': 2.0906929969787598, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 09:53:17 | {'loss': 0.0803, 'grad_norm': 1.7105121612548828, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 09:53:52 | {'loss': 0.0783, 'grad_norm': 1.3272407054901123, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 09:54:21 | {'loss': 0.0791, 'grad_norm': 1.532141089439392, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 09:54:42 | {'loss': 0.0762, 'grad_norm': 1.4244526624679565, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 10:02:03 | {'eval_loss': 1.8738415241241455, 'eval_rouge1': 0.45870145522481587, 'eval_rouge2': 0.29342990467761537, 'eval_rougeL': 0.44636547813740673, 'eval_rouge_sum': 1.198496838039838, 'eval_runtime': 434.3487, 'eval_samples_per_second': 5.735, 'eval_steps_per_second': 0.359, 'epoch': 10.0}
2025-10-15 10:02:07 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 10:02:07 | {'train_runtime': 6272.6737, 'train_samples_per_second': 15.888, 'train_steps_per_second': 0.993, 'train_loss': 0.5623991288113173, 'epoch': 10.0}
2025-10-15 10:02:07 | 최종 모델 저장 중...
2025-10-15 10:02:08 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_5/kfold/final_model
2025-10-15 10:02:08 | 최종 평가 중...
2025-10-15 10:08:13 | 최종 평가 결과:
2025-10-15 10:08:13 | eval_rouge1: 0.4632
2025-10-15 10:08:13 | eval_rouge2: 0.2955
2025-10-15 10:08:13 | eval_rougeL: 0.4510
2025-10-15 10:08:13 | eval_rouge_sum: 1.2097
2025-10-15 10:08:13 | wandb: updating run metadata
2025-10-15 10:08:14 | wandb: uploading history steps 73-73, summary, console lines 82-86
2025-10-15 10:08:14 | wandb: 
wandb: Run history:
wandb:               eval/loss ▂▁▂▃▄▅▆▇███
wandb:             eval/rouge1 ▁▄▆▆▇▆▇▇███
wandb:             eval/rouge2 ▁▄▇▆▇▆▇▇███
wandb:             eval/rougeL ▁▄▇▆▇▆▇▇███
wandb:          eval/rouge_sum ▁▄▇▆▇▆▇▇███
wandb:            eval/runtime ▅▅▆▆▅▆▅▅▅█▁
wandb: eval/samples_per_second ▄▃▃▃▄▃▄▃▃▁█
wandb:   eval/steps_per_second ▄▃▃▃▄▃▄▃▃▁█
wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████
wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████
wandb:                      +3 ...
wandb: 
wandb: Run summary:
wandb:               eval/loss 1.84495
wandb:             eval/rouge1 0.46324
wandb:             eval/rouge2 0.29547
wandb:             eval/rougeL 0.45103
wandb:          eval/rouge_sum 1.20974
wandb:            eval/runtime 364.4827
wandb: eval/samples_per_second 6.834
wandb:   eval/steps_per_second 0.428
wandb:              total_flos 3.03831677140992e+16
wandb:             train/epoch 10
wandb:                      +8 ...
wandb:
2025-10-15 10:08:14 | wandb: 🚀 View run 1015-0817-kfold at: https://wandb.ai/ieyeppo/nlp-competition/runs/wfwrczgj
wandb: ⭐️ View project at: https://wandb.ai/ieyeppo/nlp-competition
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-15 10:08:14 | wandb: Find logs at: ./wandb/wandb/run-20251015_081733-wfwrczgj/logs
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | ✅ 학습 완료!
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | ✅ K-FOLD 교차검증 완료!
2025-10-15 10:08:14 | 📊 평균 성능:
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | 💾 결과 저장: experiments/20251015/20251015_013734_kobart_balanced_kfold/kfold_results.json
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | ✅ 학습 완료!
2025-10-15 10:08:14 | 📁 결과 저장: experiments/20251015/20251015_013734_kobart_balanced_kfold
2025-10-15 10:08:14 | ============================================================
2025-10-15 10:08:14 | ✅ WandB 세션 종료
2025-10-15 10:08:14 | >> 로그 리디렉션 중료.
