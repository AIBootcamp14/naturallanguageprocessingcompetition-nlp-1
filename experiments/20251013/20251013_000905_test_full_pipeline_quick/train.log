2025-10-13 00:09:05 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-13 00:09:08 | 📊 FULL 모드 실행 중...
2025-10-13 00:09:08 | ============================================================
2025-10-13 00:09:08 | = FULL PIPELINE 실행 시작
2025-10-13 00:09:08 | =대상 모델: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-13 00:09:08 | =앙상블 앙상블 전략: stacking
2025-10-13 00:09:08 | = TTA 사용: True
2025-10-13 00:09:08 | ============================================================
2025-10-13 00:09:08 | [1/6] 데이터 로딩...
2025-10-13 00:09:08 | ✅ 학습 데이터: 12457개
2025-10-13 00:09:08 | ✅ 검증 데이터: 499개
2025-10-13 00:09:08 | ⚙️ max_train_samples 적용: 학습 데이터 1000개로 제한
2025-10-13 00:09:08 | [2/6] 다중 모델 학습 (6 모델)...
2025-10-13 00:09:08 | ==================================================
2025-10-13 00:09:08 | 모델 1/6: kobart
2025-10-13 00:09:08 | ==================================================
2025-10-13 00:09:08 | 모델 타입: encoder_decoder
2025-10-13 00:09:08 | ============================================================
2025-10-13 00:09:08 | 모델 및 토크나이저 로딩 시작
2025-10-13 00:09:08 | ============================================================
2025-10-13 00:09:08 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-13 00:09:09 | 모델 로딩: digit82/kobart-summarization
2025-10-13 00:09:09 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-13 00:09:10 | → 디바이스: cuda
2025-10-13 00:09:10 | → 전체 파라미터: 123,859,968
2025-10-13 00:09:10 | → 학습 가능 파라미터: 123,859,968
2025-10-13 00:09:10 | ============================================================
2025-10-13 00:09:10 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-13 00:09:10 | ============================================================
2025-10-13 00:09:10 | ============================================================
2025-10-13 00:09:10 | 모델 학습 시작
2025-10-13 00:09:10 | ============================================================
2025-10-13 00:09:10 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 00:09:10 | 학습 진행 중...
2025-10-13 00:09:10 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-13 00:09:13 | 1%|          | 1/125 [00:02<05:40,  2.75s/it]
2025-10-13 00:09:13 | 2%|▏         | 2/125 [00:02<02:33,  1.25s/it]
2025-10-13 00:09:13 | 3%|▎         | 4/125 [00:03<01:00,  2.01it/s]
2025-10-13 00:09:14 | 5%|▍         | 6/125 [00:03<00:34,  3.47it/s]
2025-10-13 00:09:17 | 6%|▋         | 8/125 [00:06<01:37,  1.19it/s]
2025-10-13 00:09:17 | 8%|▊         | 10/125 [00:06<01:03,  1.81it/s]
2025-10-13 00:09:17 | 10%|█         | 13/125 [00:06<00:36,  3.04it/s]
2025-10-13 00:09:17 | 13%|█▎        | 16/125 [00:06<00:23,  4.56it/s]
2025-10-13 00:09:17 | 15%|█▌        | 19/125 [00:06<00:16,  6.32it/s]
2025-10-13 00:09:18 | 18%|█▊        | 22/125 [00:07<00:12,  8.27it/s]
2025-10-13 00:09:18 | 20%|██        | 25/125 [00:07<00:09, 10.27it/s]
2025-10-13 00:09:20 | 22%|██▏       | 27/125 [00:09<00:31,  3.09it/s]
2025-10-13 00:09:20 | 23%|██▎       | 29/125 [00:09<00:24,  3.85it/s]
2025-10-13 00:09:20 | 26%|██▌       | 32/125 [00:09<00:17,  5.37it/s]
2025-10-13 00:09:20 | 28%|██▊       | 35/125 [00:09<00:12,  7.14it/s]
2025-10-13 00:09:20 | 30%|██▉       | 37/125 [00:09<00:10,  8.38it/s]
2025-10-13 00:09:20 | 32%|███▏      | 40/125 [00:10<00:08, 10.57it/s]
2025-10-13 00:09:21 | 34%|███▍      | 43/125 [00:10<00:06, 12.66it/s]
2025-10-13 00:09:21 | 37%|███▋      | 46/125 [00:10<00:05, 14.58it/s]
2025-10-13 00:09:21 | 39%|███▉      | 49/125 [00:10<00:04, 15.82it/s]
2025-10-13 00:09:21 | 42%|████▏     | 52/125 [00:10<00:04, 16.69it/s]
2025-10-13 00:09:21 | 43%|████▎     | 54/125 [00:10<00:04, 16.72it/s]
2025-10-13 00:09:21 | 45%|████▍     | 56/125 [00:10<00:04, 17.02it/s]
2025-10-13 00:09:21 | 46%|████▋     | 58/125 [00:10<00:03, 17.34it/s]
2025-10-13 00:09:21 | 48%|████▊     | 60/125 [00:11<00:03, 17.37it/s]
2025-10-13 00:09:22 | 50%|████▉     | 62/125 [00:11<00:03, 17.47it/s]
2025-10-13 00:09:22 | 51%|█████     | 64/125 [00:11<00:03, 17.17it/s]
2025-10-13 00:09:22 | 54%|█████▎    | 67/125 [00:11<00:03, 18.27it/s]
2025-10-13 00:09:22 | 56%|█████▌    | 70/125 [00:11<00:02, 19.76it/s]
2025-10-13 00:09:22 | 58%|█████▊    | 72/125 [00:11<00:02, 19.74it/s]
2025-10-13 00:09:22 | 59%|█████▉    | 74/125 [00:11<00:02, 19.48it/s]
2025-10-13 00:09:22 | 61%|██████    | 76/125 [00:11<00:02, 19.30it/s]
2025-10-13 00:09:22 | 63%|██████▎   | 79/125 [00:12<00:02, 20.24it/s]
2025-10-13 00:09:23 | 66%|██████▌   | 82/125 [00:12<00:02, 20.54it/s]
2025-10-13 00:09:23 | 68%|██████▊   | 85/125 [00:12<00:01, 21.20it/s]
2025-10-13 00:09:23 | 70%|███████   | 88/125 [00:12<00:01, 20.39it/s]
2025-10-13 00:09:23 | 73%|███████▎  | 91/125 [00:12<00:01, 20.07it/s]
2025-10-13 00:09:23 | 75%|███████▌  | 94/125 [00:12<00:01, 20.73it/s]
2025-10-13 00:09:23 | 78%|███████▊  | 97/125 [00:12<00:01, 20.29it/s]
2025-10-13 00:09:23 | 80%|████████  | 100/125 [00:13<00:01, 20.74it/s]
2025-10-13 00:09:23 | {'loss': 2.7223, 'grad_norm': 10.105255126953125, 'learning_rate': 9.9e-07, 'epoch': 0.8}
2025-10-13 00:09:23 | 80%|████████  | 100/125 [00:13<00:01, 20.74it/s]
2025-10-13 00:09:24 | 82%|████████▏ | 103/125 [00:13<00:01, 20.12it/s]
2025-10-13 00:09:24 | 85%|████████▍ | 106/125 [00:13<00:00, 20.11it/s]
2025-10-13 00:09:24 | 87%|████████▋ | 109/125 [00:13<00:00, 20.15it/s]
2025-10-13 00:09:24 | 90%|████████▉ | 112/125 [00:13<00:00, 21.10it/s]
2025-10-13 00:09:24 | 92%|█████████▏| 115/125 [00:13<00:00, 21.62it/s]
2025-10-13 00:09:24 | 94%|█████████▍| 118/125 [00:13<00:00, 21.79it/s]
2025-10-13 00:09:24 | 97%|█████████▋| 121/125 [00:14<00:00, 21.91it/s]
2025-10-13 00:09:25 | 99%|█████████▉| 124/125 [00:14<00:00, 22.31it/s]
2025-10-13 00:09:26 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-13 00:09:26 | [A
2025-10-13 00:09:27 | 3%|▎         | 2/63 [00:00<00:30,  2.02it/s]
2025-10-13 00:09:27 | [A
2025-10-13 00:09:28 | 5%|▍         | 3/63 [00:01<00:37,  1.62it/s]
2025-10-13 00:09:28 | [A
2025-10-13 00:09:28 | 6%|▋         | 4/63 [00:02<00:39,  1.49it/s]
2025-10-13 00:09:28 | [A
2025-10-13 00:09:29 | 8%|▊         | 5/63 [00:03<00:39,  1.45it/s]
2025-10-13 00:09:29 | [A
2025-10-13 00:09:30 | 10%|▉         | 6/63 [00:04<00:41,  1.38it/s]
2025-10-13 00:09:30 | [A
2025-10-13 00:09:31 | 11%|█         | 7/63 [00:04<00:42,  1.31it/s]
2025-10-13 00:09:31 | [A
2025-10-13 00:09:31 | 13%|█▎        | 8/63 [00:05<00:42,  1.29it/s]
2025-10-13 00:09:31 | [A
2025-10-13 00:09:32 | 14%|█▍        | 9/63 [00:06<00:42,  1.29it/s]
2025-10-13 00:09:32 | [A
2025-10-13 00:09:33 | 16%|█▌        | 10/63 [00:07<00:42,  1.26it/s]
2025-10-13 00:09:33 | [A
2025-10-13 00:09:34 | 17%|█▋        | 11/63 [00:08<00:39,  1.31it/s]
2025-10-13 00:09:34 | [A
2025-10-13 00:09:35 | 19%|█▉        | 12/63 [00:08<00:38,  1.34it/s]
2025-10-13 00:09:35 | [A
2025-10-13 00:09:35 | 21%|██        | 13/63 [00:09<00:37,  1.33it/s]
2025-10-13 00:09:35 | [A
2025-10-13 00:09:36 | 22%|██▏       | 14/63 [00:10<00:36,  1.33it/s]
2025-10-13 00:09:36 | [A
2025-10-13 00:09:37 | 24%|██▍       | 15/63 [00:11<00:36,  1.32it/s]
2025-10-13 00:09:37 | [A
2025-10-13 00:09:38 | 25%|██▌       | 16/63 [00:11<00:35,  1.33it/s]
2025-10-13 00:09:38 | [A
2025-10-13 00:09:38 | 27%|██▋       | 17/63 [00:12<00:34,  1.32it/s]
2025-10-13 00:09:38 | [A
2025-10-13 00:09:39 | 29%|██▊       | 18/63 [00:13<00:33,  1.35it/s]
2025-10-13 00:09:39 | [A
2025-10-13 00:09:40 | 30%|███       | 19/63 [00:14<00:34,  1.29it/s]
2025-10-13 00:09:40 | [A
2025-10-13 00:09:41 | 32%|███▏      | 20/63 [00:14<00:33,  1.30it/s]
2025-10-13 00:09:41 | [A
2025-10-13 00:09:41 | 100%|██████████| 125/125 [00:30<00:00, 22.31it/s]
2025-10-13 00:09:42 | 33%|███▎      | 21/63 [00:15<00:35,  1.18it/s]
2025-10-13 00:09:42 | [A
2025-10-13 00:09:42 | 35%|███▍      | 22/63 [00:16<00:34,  1.19it/s]
2025-10-13 00:09:42 | [A
2025-10-13 00:09:44 | 37%|███▋      | 23/63 [00:18<00:41,  1.05s/it]
2025-10-13 00:09:44 | [A
2025-10-13 00:09:45 | 38%|███▊      | 24/63 [00:19<00:38,  1.01it/s]
2025-10-13 00:09:45 | [A
2025-10-13 00:09:46 | 40%|███▉      | 25/63 [00:19<00:35,  1.07it/s]
2025-10-13 00:09:46 | [A
2025-10-13 00:09:47 | 41%|████▏     | 26/63 [00:20<00:36,  1.02it/s]
2025-10-13 00:09:47 | [A
2025-10-13 00:09:48 | 43%|████▎     | 27/63 [00:21<00:33,  1.09it/s]
2025-10-13 00:09:48 | [A
2025-10-13 00:09:48 | 44%|████▍     | 28/63 [00:22<00:31,  1.11it/s]
2025-10-13 00:09:48 | [A
2025-10-13 00:09:49 | 46%|████▌     | 29/63 [00:23<00:30,  1.13it/s]
2025-10-13 00:09:49 | [A
2025-10-13 00:09:50 | 48%|████▊     | 30/63 [00:24<00:27,  1.18it/s]
2025-10-13 00:09:50 | [A
2025-10-13 00:09:51 | 49%|████▉     | 31/63 [00:24<00:26,  1.22it/s]
2025-10-13 00:09:51 | [A
2025-10-13 00:09:52 | 51%|█████     | 32/63 [00:25<00:24,  1.26it/s]
2025-10-13 00:09:52 | [A
2025-10-13 00:09:52 | 52%|█████▏    | 33/63 [00:26<00:23,  1.28it/s]
2025-10-13 00:09:52 | [A
2025-10-13 00:09:53 | 54%|█████▍    | 34/63 [00:27<00:22,  1.32it/s]
2025-10-13 00:09:53 | [A
2025-10-13 00:09:54 | 56%|█████▌    | 35/63 [00:27<00:20,  1.35it/s]
2025-10-13 00:09:54 | [A
2025-10-13 00:09:54 | 57%|█████▋    | 36/63 [00:28<00:19,  1.37it/s]
2025-10-13 00:09:54 | [A
2025-10-13 00:09:55 | 59%|█████▊    | 37/63 [00:29<00:20,  1.29it/s]
2025-10-13 00:09:55 | [A
2025-10-13 00:09:56 | 60%|██████    | 38/63 [00:30<00:19,  1.28it/s]
2025-10-13 00:09:56 | [A
2025-10-13 00:09:57 | 62%|██████▏   | 39/63 [00:31<00:18,  1.29it/s]
2025-10-13 00:09:57 | [A
2025-10-13 00:09:58 | 63%|██████▎   | 40/63 [00:31<00:17,  1.28it/s]
2025-10-13 00:09:58 | [A
2025-10-13 00:09:58 | 65%|██████▌   | 41/63 [00:32<00:17,  1.29it/s]
2025-10-13 00:09:58 | [A
2025-10-13 00:09:59 | 67%|██████▋   | 42/63 [00:33<00:16,  1.28it/s]
2025-10-13 00:09:59 | [A
2025-10-13 00:10:00 | 68%|██████▊   | 43/63 [00:34<00:15,  1.27it/s]
2025-10-13 00:10:00 | [A
2025-10-13 00:10:01 | 70%|██████▉   | 44/63 [00:35<00:15,  1.23it/s]
2025-10-13 00:10:01 | [A
2025-10-13 00:10:02 | 71%|███████▏  | 45/63 [00:35<00:14,  1.25it/s]
2025-10-13 00:10:02 | [A
2025-10-13 00:10:03 | 73%|███████▎  | 46/63 [00:36<00:14,  1.18it/s]
2025-10-13 00:10:03 | [A
2025-10-13 00:10:03 | 75%|███████▍  | 47/63 [00:37<00:13,  1.20it/s]
2025-10-13 00:10:03 | [A
2025-10-13 00:10:04 | 76%|███████▌  | 48/63 [00:38<00:12,  1.20it/s]
2025-10-13 00:10:04 | [A
2025-10-13 00:10:05 | 78%|███████▊  | 49/63 [00:39<00:11,  1.21it/s]
2025-10-13 00:10:05 | [A
2025-10-13 00:10:06 | 79%|███████▉  | 50/63 [00:40<00:10,  1.22it/s]
2025-10-13 00:10:06 | [A
2025-10-13 00:10:06 | 81%|████████  | 51/63 [00:40<00:09,  1.29it/s]
2025-10-13 00:10:06 | [A
2025-10-13 00:10:07 | 83%|████████▎ | 52/63 [00:41<00:08,  1.30it/s]
2025-10-13 00:10:07 | [A
2025-10-13 00:10:08 | 84%|████████▍ | 53/63 [00:42<00:07,  1.31it/s]
2025-10-13 00:10:08 | [A
2025-10-13 00:10:09 | 86%|████████▌ | 54/63 [00:42<00:06,  1.31it/s]
2025-10-13 00:10:09 | [A
2025-10-13 00:10:09 | 87%|████████▋ | 55/63 [00:43<00:05,  1.34it/s]
2025-10-13 00:10:09 | [A
2025-10-13 00:10:10 | 89%|████████▉ | 56/63 [00:44<00:05,  1.27it/s]
2025-10-13 00:10:10 | [A
2025-10-13 00:10:11 | 90%|█████████ | 57/63 [00:45<00:04,  1.29it/s]
2025-10-13 00:10:11 | [A
2025-10-13 00:10:12 | 92%|█████████▏| 58/63 [00:46<00:03,  1.29it/s]
2025-10-13 00:10:12 | [A
2025-10-13 00:10:13 | 94%|█████████▎| 59/63 [00:46<00:03,  1.28it/s]
2025-10-13 00:10:13 | [A
2025-10-13 00:10:13 | 95%|█████████▌| 60/63 [00:47<00:02,  1.30it/s]
2025-10-13 00:10:13 | [A
2025-10-13 00:10:14 | 97%|█████████▋| 61/63 [00:48<00:01,  1.31it/s]
2025-10-13 00:10:14 | [A
2025-10-13 00:10:15 | 98%|█████████▊| 62/63 [00:49<00:00,  1.22it/s]
2025-10-13 00:10:15 | [A
2025-10-13 00:10:16 | 100%|██████████| 63/63 [00:49<00:00,  1.40it/s]
2025-10-13 00:10:16 | [A
2025-10-13 00:10:16 | ❌ kobart 학습 실패: OverflowError: out of range integral type conversion attempted
2025-10-13 00:10:16 | 오류 로그 저장: experiments/20251013/20251013_000905_test_full_pipeline_quick/errors/kobart_error.log
2025-10-13 00:10:16 | ==================================================
2025-10-13 00:10:16 | 모델 2/6: llama-3.2-korean-3b
2025-10-13 00:10:16 | ==================================================
2025-10-13 00:10:16 | 모델 타입: causal_lm
2025-10-13 00:10:16 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-13 00:10:16 | 모델 로딩 중...
2025-10-13 00:10:16 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-13 00:10:16 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-13 00:10:16 | [A[A
2025-10-13 00:10:17 | Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.50s/it]
2025-10-13 00:10:17 | [A[A
2025-10-13 00:10:18 | Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]
2025-10-13 00:10:18 | [A[A
2025-10-13 00:10:18 | Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
2025-10-13 00:10:18 | 토크나이저 로딩 중...
2025-10-13 00:10:19 | 패딩 토큰 설정: <|eot_id|>
2025-10-13 00:10:19 | LoRA 설정 적용 중...
2025-10-13 00:10:19 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-13 00:10:19 | ✅ LoRA 적용 완료
2025-10-13 00:10:19 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-13 00:10:19 | 전체 파라미터: 3,237,063,680
2025-10-13 00:10:19 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-13 00:10:19 | ✅ Gradient Checkpointing 활성화
2025-10-13 00:10:19 | ✅ Causal LM 로드 완료
2025-10-13 00:10:19 | ============================================================
2025-10-13 00:10:19 | 모델 학습 시작
2025-10-13 00:10:19 | ============================================================
2025-10-13 00:10:19 | WandB 로그인 상태: ieyeppo-job
2025-10-13 00:10:20 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-13 00:10:20 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-13 00:10:21 | wandb: setting up run 4xektfg8
2025-10-13 00:10:21 | wandb: Tracking run with wandb version 0.22.2
2025-10-13 00:10:21 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251013_001020-4xektfg8
wandb: Run `wandb offline` to turn off syncing.
2025-10-13 00:10:21 | wandb: Syncing run 1013-0010-llama_3.2_3b_qlora
2025-10-13 00:10:21 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-13 00:10:21 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/4xektfg8
2025-10-13 00:10:21 | wandb: Detected [openai] in use.
2025-10-13 00:10:21 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-13 00:10:21 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-13 00:10:21 | 📋 실험명: 1013-0010-llama_3.2_3b_qlora
2025-10-13 00:10:21 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/4xektfg8
2025-10-13 00:10:21 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 00:10:21 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 00:10:21 | 학습 진행 중...
2025-10-13 00:10:21 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-13 00:10:21 | 100%|██████████| 125/125 [01:10<00:00,  1.77it/s]
2025-10-13 00:10:21 | [A
2025-10-13 00:10:21 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-13 00:10:24 | 1%|          | 1/125 [00:03<06:34,  3.18s/it]
2025-10-13 00:10:27 | 2%|▏         | 2/125 [00:05<05:40,  2.76s/it]
2025-10-13 00:10:32 | 3%|▎         | 4/125 [00:10<05:11,  2.58s/it]
2025-10-13 00:10:34 | 4%|▍         | 5/125 [00:13<05:02,  2.52s/it]
2025-10-13 00:10:37 | 5%|▍         | 6/125 [00:15<05:10,  2.61s/it]
2025-10-13 00:10:40 | 6%|▌         | 7/125 [00:19<05:33,  2.83s/it]
2025-10-13 00:10:44 | >> 로그 리디렉션 중료.
