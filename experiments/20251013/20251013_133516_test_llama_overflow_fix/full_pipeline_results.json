{
  "mode": "full",
  "models": [
    "llama-3.2-korean-3b"
  ],
  "ensemble_strategy": "weighted_avg",
  "use_tta": true,
  "model_results": [
    {
      "model_name": "llama-3.2-korean-3b",
      "model_path": "experiments/20251013/20251013_133516_test_llama_overflow_fix/model_0_llama_3.2_korean_3b/llama_3.2_3b_qlora/final_model",
      "eval_metrics": {
        "eval_loss": 1.3466014862060547,
        "eval_rouge1": 0.11139272780492855,
        "eval_rouge2": 0.07467631836821995,
        "eval_rougeL": 0.11139272780492855,
        "eval_rouge_sum": 0.29746177397807705,
        "eval_runtime": 414.4395,
        "eval_samples_per_second": 1.204,
        "eval_steps_per_second": 0.152,
        "epoch": 1.0
      },
      "status": "success"
    }
  ],
  "ensemble_results": {},
  "solar_results": {
    "solar_rouge_1_f1": 0.22721529000252147,
    "solar_rouge_2_f1": 0.07650273711618417,
    "solar_rouge_l_f1": 0.21771327705012092,
    "n_samples": 50
  },
  "tta_results": {
    "tta_applied": false,
    "strategies": [
      "paraphrase"
    ],
    "num_aug": 2
  },
  "inference_results": {
    "submission_path": null,
    "num_predictions": 0,
    "error": "Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, GraniteSpeechConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, T5GemmaConfig, UMT5Config, VoxtralConfig, XLMProphetNetConfig."
  }
}