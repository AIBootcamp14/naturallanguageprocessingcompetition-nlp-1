{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment #2: 후처리 개선 (Post-processing v2)\n\n**날짜**: 2025-10-13  \n**베이스**: Baseline Modular (46.9526)  \n**변경사항**: `postprocess_summaries_v2` 사용 (공백 정규화 + 중복 문장 제거)\n\n**목표**: +0.5~1.2점  \n**예상 점수**: 47.5~48.2\n\n**주요 변경**:\n- `scripts/inference_utils.py`에 `postprocess_summaries_v2()` 추가\n- 학습 생략 (기존 checkpoint-1750 사용)\n- 추론만 실행\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# scripts 디렉토리를 Python path에 추가\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 모듈 import\n",
    "from utils import load_config, get_device, set_seed\n",
    "from data_loader import Preprocess, load_data\n",
    "from tokenizer_utils import load_tokenizer\n",
    "from model_utils import load_model_for_train, get_model_info\n",
    "from dataset import prepare_train_dataset, prepare_test_dataset\n",
    "from trainer_utils import get_trainer\n",
    "from inference_utils import run_inference\n",
    "\n",
    "print(\"✅ 모든 모듈 import 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config 로드 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 로드\n",
    "config = load_config('./config.yaml')\n",
    "\n",
    "# Device 설정\n",
    "device = get_device()\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "# 시드 설정 (재현성)\n",
    "set_seed(config['training']['seed'])\n",
    "print(f\"시드 설정 완료: {config['training']['seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wandb 설정 (선택적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb 사용 시 주석 해제\n",
    "# import wandb\n",
    "# from dotenv import load_dotenv\n",
    "# \n",
    "# load_dotenv()\n",
    "# wandb.login()\n",
    "# wandb.init(\n",
    "#     project=config['wandb']['project'],\n",
    "#     entity=config['wandb']['entity'],\n",
    "#     name=config['wandb']['name'] + \"-modular\"\n",
    "# )\n",
    "\n",
    "# Wandb 비활성화 (기본값)\n",
    "config['training']['report_to'] = 'none'\n",
    "print(\"Wandb 비활성화 (report_to='none')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenizer 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 로드 (special tokens 추가)\n",
    "model_name = config['general']['model_name']\n",
    "special_tokens = config['tokenizer']['special_tokens']\n",
    "\n",
    "tokenizer = load_tokenizer(model_name, special_tokens)\n",
    "\n",
    "print(f\"✅ Tokenizer 로드 완료\")\n",
    "print(f\"모델: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Special tokens: {special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor 생성\n",
    "preprocessor = Preprocess(\n",
    "    bos_token=config['tokenizer']['bos_token'],\n",
    "    eos_token=config['tokenizer']['eos_token']\n",
    ")\n",
    "\n",
    "# Train/Val 데이터셋 준비\n",
    "data_path = config['general']['data_path']\n",
    "train_dataset, val_dataset = prepare_train_dataset(\n",
    "    config, preprocessor, data_path, tokenizer\n",
    ")\n",
    "\n",
    "print(f\"✅ 데이터셋 준비 완료\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"\\n샘플 데이터:\")\n",
    "sample = train_dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 모델 로드 (checkpoint에서 직접)\nfrom model_utils import load_model_for_inference\n\ncheckpoint_path = '../submission/checkpoint-1750'\nprint(f\"Checkpoint 로드 중: {checkpoint_path}\")\n\nmodel = load_model_for_inference(checkpoint_path, tokenizer, device)\n\nprint(f\"✅ 모델 로드 완료 (checkpoint-1750)\")\n\n# 모델 정보 출력\nmodel_info = get_model_info(model)\nprint(\"\\n모델 정보:\")\nfor key, value in model_info.items():\n    if 'parameters' in key:\n        print(f\"  {key}: {value:,}\")\n    else:\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성\n",
    "trainer = get_trainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer 설정 완료\")\n",
    "print(f\"학습 에폭: {config['training']['num_train_epochs']}\")\n",
    "print(f\"학습률: {config['training']['learning_rate']}\")\n",
    "print(f\"배치 크기: {config['training']['per_device_train_batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 실행\n",
    "print(\"\\n🔮 추론 시작...\\n\")\n",
    "\n",
    "result_df = run_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataloader=test_dataloader,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    save_path='./prediction/output_modular.csv'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 추론 완료!\")\n",
    "print(f\"결과 파일: ./prediction/output_modular.csv\")\n",
    "print(f\"샘플 수: {len(result_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 5개 샘플 출력\n",
    "print(\"\\n샘플 결과 (처음 5개):\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(result_df))):\n",
    "    print(f\"\\n[{i}] {result_df.iloc[i]['fname']}\")\n",
    "    print(f\"요약: {result_df.iloc[i]['summary'][:100]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 추론 실행 (postprocess_summaries_v2 사용)\nprint(\"\\n🔮 추론 시작...\\n\")\n\n# 1. 요약 생성\nfrom inference_utils import generate_summaries, postprocess_summaries_v2, save_predictions\n\nconfig['tokenizer'] = tokenizer\nfnames, raw_summaries = generate_summaries(model, test_dataloader, config, device)\n\nprint(f\"✅ {len(fnames)}개의 요약문 생성 완료\")\nprint(f\"   - 원본 요약 예시: {raw_summaries[0][:100]}...\")\n\n# 2. 후처리 v2 (Exp #2의 핵심!)\nprint(\"\\n🔥 후처리 v2 적용 중...\")\nprint(\"   - 특수 토큰 제거\")\nprint(\"   - 공백 정규화\")\nprint(\"   - 중복 문장 제거\")\n\nremove_tokens = config['inference']['remove_tokens']\ncleaned_summaries = postprocess_summaries_v2(raw_summaries, remove_tokens)\n\nprint(f\"✅ 후처리 v2 완료\")\nprint(f\"   - 후처리 요약 예시: {cleaned_summaries[0][:100]}...\")\n\n# 3. CSV 저장\noutput_path = save_predictions(\n    fnames, cleaned_summaries,\n    output_dir='./prediction',\n    filename='output_modular_v2.csv'\n)\n\nprint(f\"\\n✅ 추론 완료!\")\nprint(f\"결과 파일: {output_path}\")\nprint(f\"샘플 수: {len(fnames)}\")\n\n# DataFrame 생성 (확인용)\nimport pandas as pd\nresult_df = pd.DataFrame({\n    'fname': fnames,\n    'summary': cleaned_summaries\n})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 원본 Baseline과 비교 (선택적)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 baseline.ipynb의 결과와 비교\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    baseline_output = pd.read_csv('./prediction/output.csv')\n",
    "    modular_output = pd.read_csv('./prediction/output_modular.csv')\n",
    "    \n",
    "    # 동일한 샘플 수인지 확인\n",
    "    print(f\"\\nBaseline 샘플 수: {len(baseline_output)}\")\n",
    "    print(f\"Modular 샘플 수: {len(modular_output)}\")\n",
    "    \n",
    "    # fname 순서가 동일한지 확인\n",
    "    if baseline_output['fname'].equals(modular_output['fname']):\n",
    "        print(\"✅ fname 순서 일치\")\n",
    "    else:\n",
    "        print(\"⚠️ fname 순서 불일치\")\n",
    "    \n",
    "    # 일치하는 샘플 수 계산\n",
    "    identical_count = (baseline_output['summary'] == modular_output['summary']).sum()\n",
    "    print(f\"\\n동일한 요약문 수: {identical_count} / {len(baseline_output)}\")\n",
    "    print(f\"일치율: {identical_count / len(baseline_output) * 100:.2f}%\")\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"⚠️ 원본 baseline 결과 파일(output.csv)을 찾을 수 없습니다.\")\n",
    "    print(\"baseline.ipynb를 먼저 실행해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CSV 검증\nfrom utils import validate_csv\n\nvalidation_result = validate_csv('./prediction/output_modular_v2.csv')\n\nprint(\"\\nCSV 검증 결과:\")\nprint(f\"유효성: {'✅ 통과' if validation_result['valid'] else '❌ 실패'}\")\nprint(f\"샘플 수: {validation_result['num_samples']}\")\nprint(f\"컬럼: {validation_result['columns']}\")\n\nif validation_result['errors']:\n    print(\"\\n⚠️ 오류:\")\n    for error in validation_result['errors']:\n        print(f\"  - {error}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Baseline Modular과 비교",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}