#!/usr/bin/env python3
"""
Dev Set í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (Baseline íŒ¨í„´ ì¤€ìˆ˜)

checkpointë¡œ Dev set ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
baseline_modular.ipynbì˜ ì •í™•í•œ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤.
"""

import sys
sys.path.append('../scripts')

import os
import torch
from torch.utils.data import DataLoader
from rouge import Rouge
from tqdm import tqdm

from utils import load_config, get_device, set_seed
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from model_utils import load_model_for_inference
from dataset import DatasetForVal
from inference_utils import postprocess_summaries


def evaluate_dev_set(checkpoint_path, config_path='./config.yaml'):
    """
    Dev setìœ¼ë¡œ ëª¨ë¸ í‰ê°€ (baseline_modular íŒ¨í„´ ì¤€ìˆ˜)
    """
    print("="*80)
    print("ğŸ“Š Baseline Dev Set í‰ê°€")
    print("="*80)

    # 1. Config & Device
    config = load_config(config_path)
    device = get_device()
    set_seed(config['training']['seed'])
    print(f"âœ… Config ë¡œë“œ ì™„ë£Œ")
    print(f"   Device: {device}")

    # 2. Tokenizer
    tokenizer = load_tokenizer(
        config['general']['model_name'],
        config['tokenizer']['special_tokens']
    )
    print(f"âœ… Tokenizer ë¡œë“œ ì™„ë£Œ")
    print(f"   Vocab size: {len(tokenizer)}")

    # 3. Preprocessor
    preprocessor = Preprocess(
        bos_token=config['tokenizer']['bos_token'],
        eos_token=config['tokenizer']['eos_token']
    )

    # 4. Dev ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (prepare_train_dataset íŒ¨í„´)
    dev_file_path = os.path.join(config['general']['data_path'], 'dev.csv')
    print(f"\nğŸ“‚ Dev set ë¡œë“œ: {dev_file_path}")

    # CSV ë¡œë“œ ë° ì „ì²˜ë¦¬
    dev_data = preprocessor.make_set_as_df(dev_file_path, is_train=True)
    print(f"âœ… Dev ë°ì´í„°: {len(dev_data)} samples")

    # BART ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
    encoder_input_dev, decoder_input_dev, decoder_output_dev = \
        preprocessor.make_input(dev_data, is_test=False)

    # 5. Tokenization (prepare_train_dataset íŒ¨í„´)
    print(f"\nğŸ”¤ Tokenization ì‹œì‘...")

    # Encoder input
    tokenized_encoder_inputs = tokenizer(
        encoder_input_dev,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['encoder_max_len'],
        return_token_type_ids=False
    )

    # Decoder input
    tokenized_decoder_inputs = tokenizer(
        decoder_input_dev,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['decoder_max_len'],
        return_token_type_ids=False
    )

    # Decoder output (labels)
    tokenized_decoder_outputs = tokenizer(
        decoder_output_dev,
        return_tensors="pt",
        padding=True,
        add_special_tokens=True,
        truncation=True,
        max_length=config['tokenizer']['decoder_max_len'],
        return_token_type_ids=False
    )

    print(f"âœ… Tokenization ì™„ë£Œ")

    # 6. Dataset ìƒì„± (prepare_train_dataset íŒ¨í„´)
    dev_dataset = DatasetForVal(
        tokenized_encoder_inputs,
        tokenized_decoder_inputs,
        tokenized_decoder_outputs,
        len(encoder_input_dev)
    )

    dev_dataloader = DataLoader(
        dev_dataset,
        batch_size=config['inference']['batch_size'],
        shuffle=False
    )

    print(f"âœ… Dataset ìƒì„± ì™„ë£Œ ({len(dev_dataset)} samples)")

    # 7. ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ: {checkpoint_path}")
    model = load_model_for_inference(checkpoint_path, tokenizer, device)
    model.eval()
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 8. ì¶”ë¡  ì‹¤í–‰
    print(f"\nğŸ”® Dev set ì¶”ë¡  ì‹œì‘...")
    predictions = []
    references = []

    with torch.no_grad():
        for batch in tqdm(dev_dataloader, desc="Inference"):
            # Encoder input
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # Generate
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=config['inference']['generate_max_length'],
                num_beams=config['inference']['num_beams'],
                early_stopping=config['inference']['early_stopping'],
                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size']
            )

            # Decode predictions
            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)
            predictions.extend(decoded)

            # Decode references (labels)
            labels = batch['labels']
            # -100ì„ pad_token_idë¡œ ë³€ê²½
            labels = labels.masked_fill(labels == -100, tokenizer.pad_token_id)
            refs = tokenizer.batch_decode(labels, skip_special_tokens=False)
            references.extend(refs)

    print(f"âœ… ì¶”ë¡  ì™„ë£Œ ({len(predictions)} samples)")

    # 9. í›„ì²˜ë¦¬ (baseline íŒ¨í„´)
    print(f"\nğŸ§¹ í›„ì²˜ë¦¬ ì ìš©...")
    predictions = postprocess_summaries(
        predictions,
        config['inference']['remove_tokens']
    )
    references = postprocess_summaries(
        references,
        config['inference']['remove_tokens']
    )

    # 10. ROUGE ê³„ì‚°
    print(f"\nğŸ“ˆ ROUGE ì ìˆ˜ ê³„ì‚°...")
    rouge = Rouge()
    scores = rouge.get_scores(predictions, references, avg=True)

    # 11. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ“Š Baseline Dev Set ROUGE ì ìˆ˜")
    print("="*80)
    print(f"ROUGE-1 F1:  {scores['rouge-1']['f']*100:.2f}%")
    print(f"ROUGE-2 F1:  {scores['rouge-2']['f']*100:.2f}%")
    print(f"ROUGE-L F1:  {scores['rouge-l']['f']*100:.2f}%")
    print("-"*80)
    avg_score = (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) / 3 * 100
    print(f"í‰ê·  ì ìˆ˜:    {avg_score:.4f}")
    print("="*80)

    return {
        'rouge-1': scores['rouge-1']['f'] * 100,
        'rouge-2': scores['rouge-2']['f'] * 100,
        'rouge-l': scores['rouge-l']['f'] * 100,
        'avg': avg_score
    }


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', required=True, help='ëª¨ë¸ checkpoint ê²½ë¡œ')
    parser.add_argument('--config', default='./config.yaml', help='config.yaml ê²½ë¡œ')
    args = parser.parse_args()

    scores = evaluate_dev_set(args.checkpoint, args.config)
