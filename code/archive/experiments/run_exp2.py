#!/usr/bin/env python3
"""
Experiment #2: í›„ì²˜ë¦¬ ê°œì„  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

checkpoint-1750ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³ ,
postprocess_summaries_v2ë¥¼ ì ìš©í•˜ì—¬ output_modular_v2.csvë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
    python run_exp2.py
"""

import sys
sys.path.append('../scripts')

import torch
from torch.utils.data import DataLoader

from utils import load_config, get_device, set_seed, validate_csv
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from model_utils import load_model_for_inference
from dataset import prepare_test_dataset
from inference_utils import generate_summaries, postprocess_summaries_v2, save_predictions


def main():
    print("=" * 80)
    print("ğŸš€ Experiment #2: í›„ì²˜ë¦¬ ê°œì„  (Post-processing v2)")
    print("=" * 80)
    print()
    print("ë³€ê²½ì‚¬í•­:")
    print("  - postprocess_summaries_v2 ì‚¬ìš©")
    print("  - ê³µë°± ì •ê·œí™” ì¶”ê°€")
    print("  - ì¤‘ë³µ ë¬¸ì¥ ì œê±° ì¶”ê°€")
    print()
    print("ëª©í‘œ: +0.5~1.2ì ")
    print("ì˜ˆìƒ ì ìˆ˜: 47.5~48.2")
    print("=" * 80)
    print()

    # 1. Config ë¡œë“œ
    print("Step 1: Config ë¡œë“œ")
    config = load_config('./config.yaml')
    device = get_device()
    set_seed(config['training']['seed'])
    print(f"  âœ… Device: {device}")
    print(f"  âœ… Seed: {config['training']['seed']}")
    print()

    # 2. Tokenizer ë¡œë“œ
    print("Step 2: Tokenizer ë¡œë“œ")
    tokenizer = load_tokenizer(
        config['general']['model_name'],
        config['tokenizer']['special_tokens']
    )
    print(f"  âœ… Model: {config['general']['model_name']}")
    print(f"  âœ… Vocab size: {len(tokenizer)}")
    print()

    # 3. ëª¨ë¸ ë¡œë“œ (checkpoint-1750)
    print("Step 3: ëª¨ë¸ ë¡œë“œ (checkpoint-1750)")
    checkpoint_path = '../submission/checkpoint-1750'
    print(f"  Checkpoint: {checkpoint_path}")
    model = load_model_for_inference(checkpoint_path, tokenizer, device)
    print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print()

    # 4. Test ë°ì´í„° ì¤€ë¹„
    print("Step 4: Test ë°ì´í„° ì¤€ë¹„")
    preprocessor = Preprocess(
        bos_token=config['tokenizer']['bos_token'],
        eos_token=config['tokenizer']['eos_token']
    )
    test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=config['inference']['batch_size']
    )
    print(f"  âœ… Test samples: {len(test_dataset)}")
    print()

    # 5. ì¶”ë¡  ì‹¤í–‰
    print("Step 5: ì¶”ë¡  ì‹¤í–‰")
    print("-" * 80)
    config['tokenizer'] = tokenizer
    fnames, raw_summaries = generate_summaries(model, test_dataloader, config, device)
    print()
    print(f"  âœ… {len(fnames)}ê°œì˜ ìš”ì•½ë¬¸ ìƒì„± ì™„ë£Œ")
    print(f"  ì›ë³¸ ìš”ì•½ ì˜ˆì‹œ: {raw_summaries[0][:80]}...")
    print()

    # 6. í›„ì²˜ë¦¬ v2 ì ìš© (Exp #2ì˜ í•µì‹¬!)
    print("Step 6: í›„ì²˜ë¦¬ v2 ì ìš© ğŸ”¥")
    print("  ì²˜ë¦¬ ë‹¨ê³„:")
    print("    1. íŠ¹ìˆ˜ í† í° ì œê±°")
    print("    2. ê³µë°± ì •ê·œí™”")
    print("    3. ì¤‘ë³µ ë¬¸ì¥ ì œê±°")

    remove_tokens = config['inference']['remove_tokens']
    cleaned_summaries = postprocess_summaries_v2(raw_summaries, remove_tokens)

    print(f"  âœ… í›„ì²˜ë¦¬ v2 ì™„ë£Œ")
    print(f"  í›„ì²˜ë¦¬ ìš”ì•½ ì˜ˆì‹œ: {cleaned_summaries[0][:80]}...")
    print()

    # 7. CSV ì €ì¥
    print("Step 7: CSV ì €ì¥")
    output_path = save_predictions(
        fnames, cleaned_summaries,
        output_dir='./prediction',
        filename='output_modular_v2.csv'
    )
    print(f"  âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
    print()

    # 8. CSV ê²€ì¦
    print("Step 8: CSV ê²€ì¦")
    validation_result = validate_csv(output_path)
    print(f"  ìœ íš¨ì„±: {'âœ… í†µê³¼' if validation_result['valid'] else 'âŒ ì‹¤íŒ¨'}")
    print(f"  ìƒ˜í”Œ ìˆ˜: {validation_result['num_samples']}")
    print(f"  ì»¬ëŸ¼: {validation_result['columns']}")

    if validation_result['errors']:
        print("\n  âš ï¸ ì˜¤ë¥˜:")
        for error in validation_result['errors']:
            print(f"    - {error}")
        return False
    print()

    # 9. ìƒ˜í”Œ í™•ì¸
    print("Step 9: ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 5ê°œ)")
    print("-" * 80)
    import pandas as pd
    result_df = pd.read_csv(output_path)
    for i in range(min(5, len(result_df))):
        print(f"\n[{i}] {result_df.iloc[i]['fname']}")
        print(f"    {result_df.iloc[i]['summary'][:100]}...")
    print("-" * 80)
    print()

    # 10. Baselineê³¼ ë¹„êµ
    print("Step 10: Baseline Modularê³¼ ë¹„êµ")
    try:
        baseline_modular = pd.read_csv('./prediction/output_modular.csv')
        exp2_output = result_df

        identical_count = (baseline_modular['summary'] == exp2_output['summary']).sum()
        print(f"  Baseline Modular ìƒ˜í”Œ ìˆ˜: {len(baseline_modular)}")
        print(f"  Exp #2 ìƒ˜í”Œ ìˆ˜: {len(exp2_output)}")
        print(f"  ë™ì¼í•œ ìš”ì•½ë¬¸ ìˆ˜: {identical_count} / {len(baseline_modular)}")
        print(f"  ì¼ì¹˜ìœ¨: {identical_count / len(baseline_modular) * 100:.2f}%")

        # ì°¨ì´ë‚˜ëŠ” ìƒ˜í”Œ í™•ì¸
        different_mask = baseline_modular['summary'] != exp2_output['summary']
        different_count = different_mask.sum()
        print(f"  ë³€ê²½ëœ ìƒ˜í”Œ ìˆ˜: {different_count}")

        if different_count > 0:
            print("\n  ë³€ê²½ëœ ìƒ˜í”Œ ì˜ˆì‹œ (ì²˜ìŒ 3ê°œ):")
            different_samples = baseline_modular[different_mask].head(3)
            for idx in different_samples.index:
                print(f"\n    [{idx}] {baseline_modular.iloc[idx]['fname']}")
                print(f"      Before: {baseline_modular.iloc[idx]['summary'][:60]}...")
                print(f"      After:  {exp2_output.iloc[idx]['summary'][:60]}...")
    except FileNotFoundError:
        print("  âš ï¸ Baseline Modular ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print()

    # 11. ì™„ë£Œ
    print("=" * 80)
    print("âœ… Experiment #2 ì¶”ë¡  ì™„ë£Œ!")
    print("=" * 80)
    print()
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. output_modular_v2.csvë¥¼ ëŒ€íšŒ í”Œë«í¼ì— ì œì¶œ")
    print("  2. ì ìˆ˜ í™•ì¸ (47.5~48.2 ê¸°ëŒ€)")
    print("  3. ê²°ê³¼ë¥¼ experiment_logs.mdì— ê¸°ë¡")
    print("=" * 80)

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)