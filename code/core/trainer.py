#!/usr/bin/env python3
"""
í•™ìŠµ ëª¨ë“ˆ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'scripts'))

import torch
from torch.utils.data import DataLoader, WeightedRandomSampler
from typing import Dict, Optional
from transformers import Seq2SeqTrainer, BartForConditionalGeneration, PreTrainedTokenizerFast
from trainer_utils import get_trainer
from dataset import DatasetForTrain, DatasetForVal


class WeightedSeq2SeqTrainer(Seq2SeqTrainer):
    """
    WeightedRandomSamplerë¥¼ ì§€ì›í•˜ëŠ” Custom Trainer (train_exp7f.py ì°¸ì¡°)
    """

    def __init__(self, *args, train_sampler: Optional[WeightedRandomSampler] = None, **kwargs):
        """
        WeightedSeq2SeqTrainer ì´ˆê¸°í™”

        Args:
            train_sampler: WeightedRandomSampler (Noneì´ë©´ ê¸°ë³¸ ìƒ˜í”ŒëŸ¬ ì‚¬ìš©)
        """
        super().__init__(*args, **kwargs)
        self.train_sampler = train_sampler

    def get_train_dataloader(self) -> DataLoader:
        """
        í•™ìŠµ ë°ì´í„°ë¡œë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (WeightedRandomSampler ì ìš©)

        Returns:
            DataLoader
        """
        if self.train_sampler is None:
            return super().get_train_dataloader()

        return DataLoader(
            self.train_dataset,
            batch_size=self.args.per_device_train_batch_size,
            sampler=self.train_sampler,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
        )


class Trainer:
    """
    í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤
    """

    def __init__(self, config: Dict, experiment_name: str):
        """
        Trainer ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            experiment_name: ì‹¤í—˜ ì´ë¦„
        """
        self.config = config
        self.experiment_name = experiment_name
        self.device = self._get_device()

        # scripts ê²½ë¡œ ì¶”ê°€
        scripts_path = os.path.join(os.path.dirname(__file__), '..', '..', 'scripts')
        if scripts_path not in sys.path:
            sys.path.append(scripts_path)

    def _get_device(self) -> torch.device:
        """
        ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

        Returns:
            torch.device
        """
        from utils import get_device
        return get_device()

    def train(self, model: BartForConditionalGeneration, tokenizer: PreTrainedTokenizerFast,
             train_dataset: DatasetForTrain, val_dataset: DatasetForVal,
             sampler: Optional[WeightedRandomSampler] = None):
        """
        ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤

        Args:
            model: í•™ìŠµí•  ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
            val_dataset: ê²€ì¦ ë°ì´í„°ì…‹
            sampler: WeightedRandomSampler (ì„ íƒì )
        """
        print("\n" + "=" * 80)
        print("ğŸš€ Trainer ì„¤ì • ì¤‘...")
        print("=" * 80)

        # scripts/trainer_utils.pyì˜ get_trainer í™œìš©
        base_trainer = get_trainer(
            config=self.config,
            model=model,
            tokenizer=tokenizer,
            train_dataset=train_dataset,
            val_dataset=val_dataset
        )

        # WeightedSeq2SeqTrainerë¡œ ë³€í™˜ (ê°€ì¤‘ì¹˜ ì‚¬ìš© ì‹œ)
        if sampler is not None:
            print("\nâœ… WeightedSeq2SeqTrainer ì ìš© (ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§)")
            trainer = WeightedSeq2SeqTrainer(
                model=base_trainer.model,
                args=base_trainer.args,
                data_collator=base_trainer.data_collator,
                train_dataset=base_trainer.train_dataset,
                eval_dataset=base_trainer.eval_dataset,
                tokenizer=base_trainer.tokenizer,
                compute_metrics=base_trainer.compute_metrics if hasattr(base_trainer, 'compute_metrics') else None,
                callbacks=base_trainer.callback_handler.callbacks if hasattr(base_trainer, 'callback_handler') else None,
                train_sampler=sampler
            )
        else:
            print("\nâœ… Seq2SeqTrainer ì ìš© (ìì—° ë¶„í¬)")
            trainer = base_trainer

        print("=" * 80)

        # í•™ìŠµ ì‹œì‘
        print("\n" + "=" * 80)
        print("ğŸš€ í•™ìŠµ ì‹œì‘...")
        print("=" * 80)

        try:
            trainer.train()

            print("\n" + "=" * 80)
            print(f"âœ… {self.experiment_name} í•™ìŠµ ì™„ë£Œ!")
            print("=" * 80)
            print(f"\nğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {self.config['general']['output_dir']}")
            print(f"ğŸ“Š Best checkpointë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ì§„í–‰í•˜ì„¸ìš”")
            print("=" * 80)

            return trainer

        except Exception as e:
            print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
