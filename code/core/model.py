#!/usr/bin/env python3
"""
λ¨λΈ λ΅λ”© λ° κ΄€λ¦¬ λ¨λ“
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'scripts'))

import torch
from typing import Dict
from model_utils import load_model_for_train, load_model_for_inference
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast


class ModelManager:
    """
    λ¨λΈ λ΅λ”© λ° κ΄€λ¦¬ ν΄λμ¤
    """

    def __init__(self, config: Dict, tokenizer: PreTrainedTokenizerFast, device: torch.device):
        """
        ModelManager μ΄κΈ°ν™”

        Args:
            config: μ„¤μ • λ”•μ…”λ„λ¦¬
            tokenizer: ν† ν¬λ‚μ΄μ €
            device: μ‚¬μ©ν•  λ””λ°”μ΄μ¤
        """
        self.config = config
        self.tokenizer = tokenizer
        self.device = device

    def load_model_for_training(self) -> BartForConditionalGeneration:
        """
        ν•™μµμ© λ¨λΈμ„ λ΅λ“ν•©λ‹λ‹¤ (scripts/model_utils.py ν™μ©)

        Returns:
            ν•™μµμ© BART λ¨λΈ
        """
        print("\n" + "=" * 80)
        print("π“¦ λ¨λΈ λ΅λ”© μ¤‘...")
        print("=" * 80)

        model = load_model_for_train(self.config, self.tokenizer, self.device)

        print(f"β… λ¨λΈ λ΅λ“ μ™„λ£")
        print(f"   λ¨λΈ: {self.config['general']['model_name']}")
        print(f"   Vocab size: {len(self.tokenizer)}")
        print(f"   Device: {self.device}")
        print("=" * 80)

        return model

    def load_model_for_inference(self, checkpoint_path: str) -> BartForConditionalGeneration:
        """
        μ¶”λ΅ μ© λ¨λΈμ„ μ²΄ν¬ν¬μΈνΈμ—μ„ λ΅λ“ν•©λ‹λ‹¤ (scripts/model_utils.py ν™μ©)

        Args:
            checkpoint_path: μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ κ²½λ΅

        Returns:
            μ¶”λ΅ μ© BART λ¨λΈ
        """
        print("\n" + "=" * 80)
        print("π“¦ μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ¤‘...")
        print("=" * 80)

        model = load_model_for_inference(checkpoint_path, self.tokenizer, self.device)

        print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£")
        print(f"   κ²½λ΅: {checkpoint_path}")
        print(f"   Vocab size: {len(self.tokenizer)}")
        print(f"   Device: {self.device}")
        print("=" * 80)

        return model
