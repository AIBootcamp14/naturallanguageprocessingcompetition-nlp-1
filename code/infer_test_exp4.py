#!/usr/bin/env python3
"""
Exp #4 Test Set ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

ê¸¸ì´ ì •ê·œí™” + Max Length 768ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
Test setì— ëŒ€í•œ ìš”ì•½ë¬¸ì„ ìƒì„±í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ë³€ê²½ì‚¬í•­:
1. encoder_max_len: 768 (ê¸´ ëŒ€í™” ì²˜ë¦¬)
2. length_penalty: 0.6 (GNMT ê¸¸ì´ ì •ê·œí™”)

ì‚¬ìš© ë°©ë²•:
    python infer_test_exp4.py

ì¶œë ¥:
    - prediction/exp4_output.csv
"""

import sys
import os
import glob

# scripts ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.append('../scripts')

import torch
from torch.utils.data import DataLoader
from transformers import BartForConditionalGeneration

# ëª¨ë“ˆ import
from utils import load_config, get_device, set_seed
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from dataset import prepare_test_dataset
from inference_utils import run_inference

def main():
    print("=" * 80)
    print("ğŸš€ Exp #4 Test Set ì¶”ë¡ ")
    print("   (ê¸¸ì´ ì •ê·œí™” + Max Length 768)")
    print("=" * 80)

    # 1. Config ë¡œë“œ - config_exp4.yaml ì‚¬ìš©
    print("\nâœ… Config ë¡œë“œ...")
    config = load_config('./config_exp4.yaml')

    # Device ì„¤ì •
    device = get_device()
    print(f"   Device: {device}")

    # ì‹œë“œ ì„¤ì •
    set_seed(config['training']['seed'])
    print(f"   Seed: {config['training']['seed']}")

    # Config í™•ì¸
    print(f"   Encoder Max Length: {config['tokenizer']['encoder_max_len']}")
    print(f"   Length Penalty: {config['inference'].get('length_penalty', 1.0)}")

    # 2. Checkpoint ìë™ íƒìƒ‰ (ìµœì‹  checkpoint ì‚¬ìš©)
    output_dir = config['general']['output_dir']
    checkpoints = glob.glob(f"{output_dir}/checkpoint-*")

    if not checkpoints:
        print(f"\nâŒ Checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {output_dir}")
        print(f"   ë¨¼ì € python train_exp4.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        return

    # checkpoint ë²ˆí˜¸ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    checkpoints.sort(key=lambda x: int(x.split('-')[-1]), reverse=True)
    checkpoint_path = checkpoints[0]

    print(f"\nâœ… Checkpoint: {checkpoint_path}")

    # 3. Tokenizer ë¡œë“œ
    print("\nâœ… Tokenizer ë¡œë“œ...")
    model_name = config['general']['model_name']
    special_tokens = config['tokenizer']['special_tokens']
    tokenizer = load_tokenizer(model_name, special_tokens)
    print(f"   Vocab size: {len(tokenizer)}")

    # 4. ëª¨ë¸ ë¡œë“œ (checkpointì—ì„œ)
    print("\nâœ… ëª¨ë¸ ë¡œë“œ...")
    model = BartForConditionalGeneration.from_pretrained(checkpoint_path)
    model.to(device)
    print(f"   ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")

    # 5. Test ë°ì´í„°ì…‹ ì¤€ë¹„
    print("\nâœ… Test ë°ì´í„°ì…‹ ì¤€ë¹„...")
    preprocessor = Preprocess(
        bos_token=config['tokenizer']['bos_token'],
        eos_token=config['tokenizer']['eos_token']
    )

    test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)
    print(f"   Test samples: {len(test_dataset)}")

    # DataLoader ìƒì„±
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=config['inference']['batch_size']
    )

    # 6. ì¶”ë¡  ì‹¤í–‰
    print("\n" + "=" * 80)
    print("ğŸ”® ì¶”ë¡  ì‹œì‘...")
    print("=" * 80)

    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    save_path = './prediction/exp4_output.csv'

    result_df = run_inference(
        model=model,
        tokenizer=tokenizer,
        test_dataloader=test_dataloader,
        config=config,
        device=device,
        save_path=save_path
    )

    # 7. ê²°ê³¼ í™•ì¸
    print("\n" + "=" * 80)
    print("ğŸ“Š ê²°ê³¼ í™•ì¸")
    print("=" * 80)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"   ìƒ˜í”Œ ìˆ˜: {len(result_df)}")

    # ì²˜ìŒ 3ê°œ ìƒ˜í”Œ ì¶œë ¥
    print("\nìƒ˜í”Œ ìš”ì•½ (ì²˜ìŒ 3ê°œ):")
    print("-" * 80)
    for i in range(min(3, len(result_df))):
        print(f"\n[{i}] {result_df.iloc[i]['fname']}")
        summary = result_df.iloc[i]['summary']
        if len(summary) > 100:
            print(f"    {summary[:100]}...")
        else:
            print(f"    {summary}")
    print("-" * 80)

    # CSV í˜•ì‹ ê²€ì¦
    print("\nâœ… CSV í˜•ì‹ ê²€ì¦...")
    from utils import validate_csv
    validation = validate_csv(save_path)

    if validation['valid']:
        print("   âœ… ê²€ì¦ í†µê³¼")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {validation['num_samples']}")
        print(f"   - ì»¬ëŸ¼: {validation['columns']}")
    else:
        print("   âŒ ê²€ì¦ ì‹¤íŒ¨")
        for error in validation['errors']:
            print(f"   - {error}")

    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ!")
    print("=" * 80)
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"1. íŒŒì¼ í™•ì¸: {save_path}")
    print(f"2. ëŒ€íšŒ í”Œë«í¼ ì œì¶œ")
    print(f"3. ì ìˆ˜ ê¸°ë¡")
    print(f"   - Baseline: 46.95ì ")
    print(f"   - Exp #4: ???ì ")
    print(f"   - ë¸íƒ€: ???ì ")
    print("=" * 80)

if __name__ == '__main__':
    main()
