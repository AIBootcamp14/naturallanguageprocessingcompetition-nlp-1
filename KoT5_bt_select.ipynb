{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b880380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "전체 데이터셋 노이즈 분석\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Train 분석 (12457 samples)\n",
      "================================================================================\n",
      "1. \\n 포함: 1개 (0.0%)\n",
      "   샘플: #Person1#: 저, 불만이 있어요. 열 분 동안 테이블에서 기다렸는데, 웨이터가 드디어 와서 주문을 받았어요. 그런데 나온 음식이 제가 주문한 게 아니더라고요.\\n#Perso...\n",
      "2. <br> 태그 포함: 1개 (0.0%)\n",
      "   샘플: #Person1#: 요즘 잘 지내고 있어요?<br>#Person2#: 제 코치가 제 혈압을 체크해 달라고 부탁했어요.<br>#Person1#: 전에 고혈압 있다고 들은 적 있나요?...\n",
      "3. 연속 공백(3개+): 0개 (0.0%)\n",
      "4. 연속 개행(3개+): 0개 (0.0%)\n",
      "\n",
      "5. Person 태그 형태:\n",
      "   - #Person1# 형태: 12457개\n",
      "   - # Person 1 # 형태: 0개\n",
      "   - #person1# 형태: 0개\n",
      "   - 총 Person 태그 포함: 12457개 (100.0%)\n",
      "\n",
      "6. 앞쪽 공백 포함: 0개 (0.0%)\n",
      "   뒤쪽 공백 포함: 0개 (0.0%)\n",
      "\n",
      "7. 특수문자 포함: 642개 (5.2%)\n",
      "\n",
      "8. 대화 길이 통계:\n",
      "   - 최소: 84자\n",
      "   - 최대: 2165자\n",
      "   - 평균: 406.1자\n",
      "   - 중앙값: 369.0자\n",
      "\n",
      "9. 실제 샘플 (첫 3개):\n",
      "\n",
      "   [1] fname: train_0\n",
      "       dialogue (첫 150자): '#Person1#: 안녕하세요, Mr. Smith. 저는 Dr. Hawkins입니다. 오늘 무슨 일로 오셨어요? \\n#Person2#: 건강검진을 받으려고 왔어요. \\n#Person1#: 네, 5년 동안 검진을 안 받으셨네요. 매년 한 번씩 받으셔야 해요. \\n#Person'...\n",
      "       summary: Mr. Smith는 Dr. Hawkins에게 건강검진을 받으러 와서, 매년 검진 필요성을 안내받고 흡연 습관 개선을 위한 도움을 제안받았습니다.\n",
      "\n",
      "   [2] fname: train_1\n",
      "       dialogue (첫 150자): '#Person1#: 안녕하세요, Mrs. Parker. 잘 지내셨나요?\\n#Person2#: 안녕하세요, Dr. Peters. 잘 지내고 있어요. Ricky랑 저희 둘 다 백신 맞으러 왔어요.\\n#Person1#: 알겠습니다. 백신 기록을 보니 Ricky는 소아마비, 파상'...\n",
      "       summary: Mrs. Parker가 Ricky와 함께 백신 접종을 위해 방문하였고, Dr. Peters는 Ricky에게 적절한 백신을 접종하도록 안내합니다.\n",
      "\n",
      "   [3] fname: train_2\n",
      "       dialogue (첫 150자): '#Person1#: 저기요, 열쇠 세트 본 적 있어요?\\n#Person2#: 어떤 종류의 열쇠요?\\n#Person1#: 열쇠 다섯 개랑 작은 발 장식이 달려 있어요.\\n#Person2#: 아, 안타깝네요! 못 봤어요.\\n#Person1#: 그럼, 같이 좀 찾아주실 수 있어요?'...\n",
      "       summary: #Person1#은 열쇠 세트를 잃어버리고 #Person2#에게 찾는 것을 도와달라고 요청합니다.\n",
      "\n",
      "================================================================================\n",
      "Dev 분석 (499 samples)\n",
      "================================================================================\n",
      "1. \\n 포함: 0개 (0.0%)\n",
      "2. <br> 태그 포함: 0개 (0.0%)\n",
      "3. 연속 공백(3개+): 0개 (0.0%)\n",
      "4. 연속 개행(3개+): 0개 (0.0%)\n",
      "\n",
      "5. Person 태그 형태:\n",
      "   - #Person1# 형태: 499개\n",
      "   - # Person 1 # 형태: 0개\n",
      "   - #person1# 형태: 0개\n",
      "   - 총 Person 태그 포함: 499개 (100.0%)\n",
      "\n",
      "6. 앞쪽 공백 포함: 0개 (0.0%)\n",
      "   뒤쪽 공백 포함: 0개 (0.0%)\n",
      "\n",
      "7. 특수문자 포함: 27개 (5.4%)\n",
      "\n",
      "8. 대화 길이 통계:\n",
      "   - 최소: 114자\n",
      "   - 최대: 1269자\n",
      "   - 평균: 400.1자\n",
      "   - 중앙값: 367.0자\n",
      "\n",
      "9. 실제 샘플 (첫 3개):\n",
      "\n",
      "   [1] fname: dev_0\n",
      "       dialogue (첫 150자): '#Person1#: 안녕하세요, 오늘 기분이 어떠세요?\\n#Person2#: 요즘 숨쉬기가 힘들어요.\\n#Person1#: 최근에 감기에 걸렸나요?\\n#Person2#: 아니요, 감기는 안 걸렸어요. 숨쉴 때 가슴이 답답해요.\\n#Person1#: 혹시 알고 있는 알레르기 있'...\n",
      "       summary: #Person2#는 숨쉬기 어려워합니다. 의사는 #Person2#에게 증상을 확인하고, 천식 검사를 위해 폐 전문의에게 가볼 것을 권합니다.\n",
      "\n",
      "   [2] fname: dev_1\n",
      "       dialogue (첫 150자): '#Person1#: 야 Jimmy, 오늘 좀 이따 운동하러 가자.\\n#Person2#: 그래, 몇 시에 갈래?\\n#Person1#: 3시 30분 어때?\\n#Person2#: 좋아. 오늘은 다리랑 팔 운동하는 날이야.\\n#Person1#: 근데 나 아까 농구해서 다리가 좀 아파'...\n",
      "       summary: #Person1#는 Jimmy를 운동하러 초대하고 팔과 복근 운동을 하도록 설득합니다.\n",
      "\n",
      "   [3] fname: dev_2\n",
      "       dialogue (첫 150자): '#Person1#: 나 진짜 건강에 안 좋은 음식 좀 그만 먹어야겠어. \\n#Person2#: 맞아, 무슨 말인지 알아. 나도 요즘 건강하게 먹으려고 하거든. \\n#Person1#: 요즘은 뭐 먹어? \\n#Person2#: 주로 과일이랑 채소, 닭고기 먹지. \\n#Person'...\n",
      "       summary: #Person1#은 건강에 안 좋은 음식을 그만 먹기로 결심하고, #Person2#는 자신의 건강한 식단을 #Person1#에게 공유합니다.\n",
      "\n",
      "================================================================================\n",
      "Test 분석 (499 samples)\n",
      "================================================================================\n",
      "1. \\n 포함: 0개 (0.0%)\n",
      "2. <br> 태그 포함: 0개 (0.0%)\n",
      "3. 연속 공백(3개+): 0개 (0.0%)\n",
      "4. 연속 개행(3개+): 0개 (0.0%)\n",
      "\n",
      "5. Person 태그 형태:\n",
      "   - #Person1# 형태: 499개\n",
      "   - # Person 1 # 형태: 0개\n",
      "   - #person1# 형태: 0개\n",
      "   - 총 Person 태그 포함: 499개 (100.0%)\n",
      "\n",
      "6. 앞쪽 공백 포함: 0개 (0.0%)\n",
      "   뒤쪽 공백 포함: 0개 (0.0%)\n",
      "\n",
      "7. 특수문자 포함: 26개 (5.2%)\n",
      "\n",
      "8. 대화 길이 통계:\n",
      "   - 최소: 111자\n",
      "   - 최대: 2275자\n",
      "   - 평균: 422.1자\n",
      "   - 중앙값: 386.0자\n",
      "\n",
      "9. 실제 샘플 (첫 3개):\n",
      "\n",
      "   [1] fname: test_0\n",
      "       dialogue (첫 150자): '#Person1#: Ms. Dawson, 받아쓰기 좀 부탁드려야겠어요. \\n#Person2#: 네, 말씀하세요... \\n#Person1#: 이걸 오늘 오후까지 모든 직원들에게 사내 메모로 보내야 해요. 준비됐나요? \\n#Person2#: 네, 말씀하세요. \\n#Person1#'...\n",
      "\n",
      "   [2] fname: test_1\n",
      "       dialogue (첫 150자): '#Person1#: 드디어 왔네! 뭐가 이렇게 오래 걸렸어?\\n#Person2#: 차가 또 막혔어. Carrefour 교차로 근처에서 교통체증이 엄청 심했거든.\\n#Person1#: 거긴 출퇴근 시간에 항상 혼잡하잖아. 집에 갈 때 다른 길 좀 찾아보는 게 어때?\\n#Per'...\n",
      "\n",
      "   [3] fname: test_2\n",
      "       dialogue (첫 150자): '#Person1#: Kate, 여기서 일어난 일을 믿기 힘들 거야.\\n#Person2#: 무슨 일이야?\\n#Person1#: Masha랑 Hero가 이혼한대.\\n#Person2#: 설마, 무슨 일 있었던 거야?\\n#Person1#: 글쎄, 자세히는 모르겠는데 두 달 동안 별거'...\n",
      "\n",
      "================================================================================\n",
      "Augmented (고품질) 분석 (3476 samples)\n",
      "================================================================================\n",
      "1. \\n 포함: 0개 (0.0%)\n",
      "2. <br> 태그 포함: 0개 (0.0%)\n",
      "3. 연속 공백(3개+): 0개 (0.0%)\n",
      "4. 연속 개행(3개+): 0개 (0.0%)\n",
      "\n",
      "5. Person 태그 형태:\n",
      "   - #Person1# 형태: 3476개\n",
      "   - # Person 1 # 형태: 0개\n",
      "   - #person1# 형태: 0개\n",
      "   - 총 Person 태그 포함: 3476개 (100.0%)\n",
      "\n",
      "6. 앞쪽 공백 포함: 0개 (0.0%)\n",
      "   뒤쪽 공백 포함: 0개 (0.0%)\n",
      "\n",
      "7. 특수문자 포함: 2947개 (84.8%)\n",
      "\n",
      "8. 대화 길이 통계:\n",
      "   - 최소: 171자\n",
      "   - 최대: 2058자\n",
      "   - 평균: 559.6자\n",
      "   - 중앙값: 522.0자\n",
      "\n",
      "9. 실제 샘플 (첫 3개):\n",
      "\n",
      "   [1] fname: train_9687_aug\n",
      "       dialogue (첫 150자): '#Person1#: 보상 요청을 받고 놀랐습니다. 무슨 일이 있었나요?  \\n#Person2#: 품질에 관한 문제입니다.  \\n#Person1#: 자세히 설명해 주시겠어요?  \\n#Person2#: 프리미엄 등급 제품을 요구했던 것, 기억나지 않나요? 품질, 사양, 가격은 '...\n",
      "       summary: #Person2#는 상품의 3분의 1이 기준에 미달하여 #Person1#에게 보상을 요청합니다.\n",
      "\n",
      "   [2] fname: train_10704_aug\n",
      "       dialogue (첫 150자): '#Person1#: 지금까지 얼마를 썼고 어디에 썼는지 파악해야 해.  \\n#Person2#: 왜?  \\n#Person1#: 음, 더 절약해야 할 것 같아서. 돈을 더 모으면 일찍 은퇴해서 삶을 더 즐길 수 있잖아.  \\n#Person2#: 정말? 좋아, 영수증을 보자.  '...\n",
      "       summary: #Person1#과 #Person2#는 지출을 검토하고 절약할 방법을 의논하여, 외식 대신 집에서 식사를 더 자주 하기로 합니다.\n",
      "\n",
      "   [3] fname: train_2233_aug\n",
      "       dialogue (첫 150자): '#Person1#: 아빠, 저 사랑하시죠?  \\n#Person2#: 물론이지. 왜? 무슨 일이야?  \\n#Person1#: 신문에서 공짜 휴대폰 프로모션을 봤어요…  \\n#Person2#: 공짜라고? 세상에 공짜는 없어.  \\n#Person1#: 아니에요, 휴대폰은 공짜인데…'...\n",
      "       summary: #Person1#은 아빠에게 새 휴대폰을 사달라고 부탁하며 친구들이 모두 휴대폰을 갖고 있다는 이유와 차 고장 시 연락을 위해 필요하다고 설명합니다. #Person1#은 이후 새 차도 함께 요구하지만, 아빠는 이 요구가 터무니없다고 느낍니다.\n",
      "\n",
      "================================================================================\n",
      "종합 요약\n",
      "================================================================================\n",
      "           total  backslash_n  br_tag  multi_space  multi_newline  person_tag  \\\n",
      "Train      12457            1       1            0              0       12457   \n",
      "Dev          499            0       0            0              0         499   \n",
      "Test         499            0       0            0              0         499   \n",
      "Augmented   3476            0       0            0              0        3476   \n",
      "\n",
      "           leading_space  trailing_space  special_chars  \n",
      "Train                  0               0            642  \n",
      "Dev                    0               0             27  \n",
      "Test                   0               0             26  \n",
      "Augmented              0               0           2947  \n",
      "\n",
      "================================================================================\n",
      "전처리 필요 여부 판단\n",
      "================================================================================\n",
      "\n",
      "전체 샘플: 13455개\n",
      "\n",
      "노이즈 비율:\n",
      "  - \\n: 1개 (0.01%)\n",
      "  - <br>: 1개 (0.01%)\n",
      "  - 연속 공백: 0개 (0.00%)\n",
      "\n",
      "================================================================================\n",
      "권장 사항:\n",
      "================================================================================\n",
      "❌ \\n 전처리 불필요 (1% 미만)\n",
      "❌ <br> 전처리 불필요 (1% 미만)\n",
      "❌ 연속 공백 전처리 불필요 (1% 미만)\n",
      "\n",
      "================================================================================\n",
      "✨ 데이터가 매우 깨끗합니다! 전처리 불필요\n",
      "   → 바로 모델 학습 진행 가능\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def analyze_noise(df, df_name):\n",
    "    \"\"\"실제 데이터에 어떤 노이즈가 있는지 확인\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{df_name} 분석 ({len(df)} samples)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    dialogues = df['dialogue'].astype(str)\n",
    "    \n",
    "    # 1. \\\\n 체크\n",
    "    backslash_n_count = dialogues.str.contains(r'\\\\n', regex=True).sum()\n",
    "    print(f\"1. \\\\n 포함: {backslash_n_count}개 ({backslash_n_count/len(df)*100:.1f}%)\")\n",
    "    if backslash_n_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'\\\\n', regex=True)].index[0]\n",
    "        print(f\"   샘플: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 2. <br> 태그 체크\n",
    "    br_count = dialogues.str.contains(r'<br', regex=True, flags=re.IGNORECASE).sum()\n",
    "    print(f\"2. <br> 태그 포함: {br_count}개 ({br_count/len(df)*100:.1f}%)\")\n",
    "    if br_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'<br', regex=True, flags=re.IGNORECASE)].index[0]\n",
    "        print(f\"   샘플: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 3. 연속 공백 체크 (3개 이상)\n",
    "    multi_space_count = dialogues.str.contains(r'   +', regex=True).sum()\n",
    "    print(f\"3. 연속 공백(3개+): {multi_space_count}개 ({multi_space_count/len(df)*100:.1f}%)\")\n",
    "    if multi_space_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'   +', regex=True)].index[0]\n",
    "        print(f\"   샘플: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 4. 연속 개행 체크 (3개 이상)\n",
    "    multi_newline_count = dialogues.str.contains(r'\\n\\n\\n+', regex=True).sum()\n",
    "    print(f\"4. 연속 개행(3개+): {multi_newline_count}개 ({multi_newline_count/len(df)*100:.1f}%)\")\n",
    "    if multi_newline_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'\\n\\n\\n+', regex=True)].index[0]\n",
    "        print(f\"   샘플: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 5. Person 태그 형태 분석\n",
    "    print(f\"\\n5. Person 태그 형태:\")\n",
    "    \n",
    "    # 정규 형태: #Person1#\n",
    "    regular_pattern = dialogues.str.contains(r'#Person\\d+#', regex=True).sum()\n",
    "    print(f\"   - #Person1# 형태: {regular_pattern}개\")\n",
    "    \n",
    "    # 공백 있는 형태: # Person 1 #\n",
    "    space_pattern = dialogues.str.contains(r'#\\s+Person\\s+\\d+\\s+#', regex=True).sum()\n",
    "    print(f\"   - # Person 1 # 형태: {space_pattern}개\")\n",
    "    \n",
    "    # 소문자 형태: #person1#\n",
    "    lower_pattern = dialogues.str.contains(r'#person\\d+#', regex=True).sum()\n",
    "    print(f\"   - #person1# 형태: {lower_pattern}개\")\n",
    "    \n",
    "    # 불규칙 형태 (전체)\n",
    "    irregular_pattern = dialogues.str.contains(r'#\\s*[Pp]erson\\s*\\d+\\s*#', regex=True).sum()\n",
    "    print(f\"   - 총 Person 태그 포함: {irregular_pattern}개 ({irregular_pattern/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 6. 앞뒤 공백 체크\n",
    "    leading_space = dialogues.str.match(r'^\\s+').sum()\n",
    "    trailing_space = dialogues.str.match(r'.*\\s+$').sum()\n",
    "    print(f\"\\n6. 앞쪽 공백 포함: {leading_space}개 ({leading_space/len(df)*100:.1f}%)\")\n",
    "    print(f\"   뒤쪽 공백 포함: {trailing_space}개 ({trailing_space/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 7. 특수문자 체크\n",
    "    special_chars = dialogues.str.contains(r'[^\\w\\s가-힣#:\\n\\-.,!?()\\'\\\"@]', regex=True).sum()\n",
    "    print(f\"\\n7. 특수문자 포함: {special_chars}개 ({special_chars/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 8. 길이 통계\n",
    "    print(f\"\\n8. 대화 길이 통계:\")\n",
    "    lengths = dialogues.str.len()\n",
    "    print(f\"   - 최소: {lengths.min()}자\")\n",
    "    print(f\"   - 최대: {lengths.max()}자\")\n",
    "    print(f\"   - 평균: {lengths.mean():.1f}자\")\n",
    "    print(f\"   - 중앙값: {lengths.median():.1f}자\")\n",
    "    \n",
    "    # 9. 샘플 출력\n",
    "    print(f\"\\n9. 실제 샘플 (첫 3개):\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\n   [{i+1}] fname: {df.iloc[i]['fname']}\")\n",
    "        print(f\"       dialogue (첫 150자): {repr(dialogues.iloc[i][:150])}...\")\n",
    "        if 'summary' in df.columns:\n",
    "            print(f\"       summary: {df.iloc[i]['summary']}\")\n",
    "    \n",
    "    return {\n",
    "        'total': len(df),\n",
    "        'backslash_n': backslash_n_count,\n",
    "        'br_tag': br_count,\n",
    "        'multi_space': multi_space_count,\n",
    "        'multi_newline': multi_newline_count,\n",
    "        'person_tag': irregular_pattern,\n",
    "        'leading_space': leading_space,\n",
    "        'trailing_space': trailing_space,\n",
    "        'special_chars': special_chars\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 실행 코드\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"전체 데이터셋 노이즈 분석\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/train.csv')\n",
    "dev_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/dev.csv')\n",
    "test_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/test.csv')\n",
    "\n",
    "# 각 데이터셋 분석\n",
    "train_stats = analyze_noise(train_df, \"Train\")\n",
    "dev_stats = analyze_noise(dev_df, \"Dev\")\n",
    "test_stats = analyze_noise(test_df, \"Test\")\n",
    "\n",
    "# 증강 데이터도 확인 (있으면)\n",
    "try:\n",
    "    augmented_df = pd.read_csv('/home/NLP_contest/NPL_contest/augmented_data/high_quality_augmented.csv')\n",
    "    aug_stats = analyze_noise(augmented_df, \"Augmented (고품질)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n⚠️ 증강 데이터 파일이 없습니다.\")\n",
    "    aug_stats = None\n",
    "\n",
    "# ============================================================\n",
    "# 종합 요약\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"종합 요약\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    'Train': train_stats,\n",
    "    'Dev': dev_stats,\n",
    "    'Test': test_stats\n",
    "})\n",
    "\n",
    "if aug_stats:\n",
    "    summary_table['Augmented'] = aug_stats\n",
    "\n",
    "print(summary_table.T)\n",
    "\n",
    "# 전처리 필요 여부 판단\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"전처리 필요 여부 판단\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_samples = train_stats['total'] + dev_stats['total'] + test_stats['total']\n",
    "total_backslash_n = train_stats['backslash_n'] + dev_stats['backslash_n'] + test_stats['backslash_n']\n",
    "total_br = train_stats['br_tag'] + dev_stats['br_tag'] + test_stats['br_tag']\n",
    "total_multi_space = train_stats['multi_space'] + dev_stats['multi_space'] + test_stats['multi_space']\n",
    "\n",
    "print(f\"\\n전체 샘플: {total_samples}개\")\n",
    "print(f\"\\n노이즈 비율:\")\n",
    "print(f\"  - \\\\n: {total_backslash_n}개 ({total_backslash_n/total_samples*100:.2f}%)\")\n",
    "print(f\"  - <br>: {total_br}개 ({total_br/total_samples*100:.2f}%)\")\n",
    "print(f\"  - 연속 공백: {total_multi_space}개 ({total_multi_space/total_samples*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"권장 사항:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 판단 기준: 1% 이상이면 전처리 권장\n",
    "needs_preprocessing = []\n",
    "\n",
    "if total_backslash_n / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"\\\\n → \\\\n 변환\")\n",
    "    print(\"✅ \\\\n 전처리 필요 (1% 이상)\")\n",
    "else:\n",
    "    print(\"❌ \\\\n 전처리 불필요 (1% 미만)\")\n",
    "\n",
    "if total_br / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"<br> → \\\\n 변환\")\n",
    "    print(\"✅ <br> 전처리 필요 (1% 이상)\")\n",
    "else:\n",
    "    print(\"❌ <br> 전처리 불필요 (1% 미만)\")\n",
    "\n",
    "if total_multi_space / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"연속 공백 제거\")\n",
    "    print(\"✅ 연속 공백 전처리 필요 (1% 이상)\")\n",
    "else:\n",
    "    print(\"❌ 연속 공백 전처리 불필요 (1% 미만)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if needs_preprocessing:\n",
    "    print(\"🔧 전처리 필요 항목:\")\n",
    "    for item in needs_preprocessing:\n",
    "        print(f\"   - {item}\")\n",
    "else:\n",
    "    print(\"✨ 데이터가 매우 깨끗합니다! 전처리 불필요\")\n",
    "    print(\"   → 바로 모델 학습 진행 가능\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13105d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "데이터셋 노이즈 요약\n",
      "================================================================================\n",
      "\n",
      "Train:\n",
      "  총 샘플: 12457\n",
      "  \\n: 1개 (0.01%)\n",
      "  <br>: 1개 (0.01%)\n",
      "  연속 공백: 0개 (0.00%)\n",
      "  Person 태그: 12457개 (100.00%)\n",
      "\n",
      "Dev:\n",
      "  총 샘플: 499\n",
      "  \\n: 0개 (0.00%)\n",
      "  <br>: 0개 (0.00%)\n",
      "  연속 공백: 0개 (0.00%)\n",
      "  Person 태그: 499개 (100.00%)\n",
      "\n",
      "Test:\n",
      "  총 샘플: 499\n",
      "  \\n: 0개 (0.00%)\n",
      "  <br>: 0개 (0.00%)\n",
      "  연속 공백: 0개 (0.00%)\n",
      "  Person 태그: 499개 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "종합 표\n",
      "================================================================================\n",
      "Dataset  Total  \\n  <br>  연속공백  Person태그\n",
      "  Train  12457   1     1     0     12457\n",
      "    Dev    499   0     0     0       499\n",
      "   Test    499   0     0     0       499\n",
      "\n",
      "================================================================================\n",
      "전체 통계\n",
      "================================================================================\n",
      "전체 샘플: 13455개\n",
      "\\n 총: 1개 (0.007%)\n",
      "<br> 총: 1개 (0.007%)\n",
      "\n",
      "================================================================================\n",
      "결론\n",
      "================================================================================\n",
      "✨ 모든 데이터셋이 매우 깨끗합니다!\n",
      "   전처리 불필요 → 바로 학습 가능\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "train = pd.read_csv('/home/NLP_contest/NPL_contest/data/train.csv')\n",
    "dev = pd.read_csv('/home/NLP_contest/NPL_contest/data/dev.csv')\n",
    "test = pd.read_csv('/home/NLP_contest/NPL_contest/data/test.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"데이터셋 노이즈 요약\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {\n",
    "    'Train': train,\n",
    "    'Dev': dev,\n",
    "    'Test': test\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    backslash_n = df['dialogue'].str.contains(r'\\\\n', regex=True).sum()\n",
    "    br_tag = df['dialogue'].str.contains(r'<br', regex=True, case=False).sum()\n",
    "    multi_space = df['dialogue'].str.contains(r'   +', regex=True).sum()\n",
    "    person_tag = df['dialogue'].str.contains(r'#Person\\d+#', regex=True).sum()\n",
    "    \n",
    "    summary.append({\n",
    "        'Dataset': name,\n",
    "        'Total': len(df),\n",
    "        '\\\\n': backslash_n,\n",
    "        '<br>': br_tag,\n",
    "        '연속공백': multi_space,\n",
    "        'Person태그': person_tag\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  총 샘플: {len(df)}\")\n",
    "    print(f\"  \\\\n: {backslash_n}개 ({backslash_n/len(df)*100:.2f}%)\")\n",
    "    print(f\"  <br>: {br_tag}개 ({br_tag/len(df)*100:.2f}%)\")\n",
    "    print(f\"  연속 공백: {multi_space}개 ({multi_space/len(df)*100:.2f}%)\")\n",
    "    print(f\"  Person 태그: {person_tag}개 ({person_tag/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 표로 정리\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"종합 표\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# 전체 통계\n",
    "total_samples = sum([len(df) for df in datasets.values()])\n",
    "total_backslash_n = sum([df['dialogue'].str.contains(r'\\\\n', regex=True).sum() for df in datasets.values()])\n",
    "total_br = sum([df['dialogue'].str.contains(r'<br', regex=True, case=False).sum() for df in datasets.values()])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"전체 통계\")\n",
    "print(\"=\"*80)\n",
    "print(f\"전체 샘플: {total_samples}개\")\n",
    "print(f\"\\\\n 총: {total_backslash_n}개 ({total_backslash_n/total_samples*100:.3f}%)\")\n",
    "print(f\"<br> 총: {total_br}개 ({total_br/total_samples*100:.3f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"결론\")\n",
    "print(\"=\"*80)\n",
    "if total_backslash_n / total_samples < 0.01 and total_br / total_samples < 0.01:\n",
    "    print(\"✨ 모든 데이터셋이 매우 깨끗합니다!\")\n",
    "    print(\"   전처리 불필요 → 바로 학습 가능\")\n",
    "else:\n",
    "    print(\"⚠️ 일부 전처리 필요\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b11ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 23.69 GB\n",
      "\n",
      "================================================================================\n",
      "데이터 로드 및 결합\n",
      "================================================================================\n",
      "원본 Train 데이터: 12457 samples\n",
      "고품질 증강 데이터: 3476 samples\n",
      "최종 Train 데이터: 15933 samples\n",
      "  - 원본: 12457 samples\n",
      "  - 증강: 3476 samples\n",
      "\n",
      "Dev: 499 samples\n",
      "Test: 499 samples\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "KoT5 모델 및 토크나이저 로드\n",
      "================================================================================\n",
      "Tokenizer loaded: KETI-AIR/ke-t5-base\n",
      "Vocab size: 64100\n",
      "Added 7 special tokens: ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#']\n",
      "Total parameters: 247,463,424\n",
      "Trainable parameters: 247,463,424\n",
      "\n",
      "================================================================================\n",
      "데이터셋 생성\n",
      "================================================================================\n",
      "Train batches: 1992\n",
      "Dev batches: 63\n",
      "Test batches: 63\n",
      "\n",
      "================================================================================\n",
      "학습 설정\n",
      "================================================================================\n",
      "Total training steps: 11952\n",
      "Warmup steps: 1195\n",
      "Learning rate: 0.0001\n",
      "\n",
      "================================================================================\n",
      "KoT5 학습 시작 (고품질 증강 데이터 활용)\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:51<00:00,  2.80it/s, loss=4.4603, lr=9.87e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.3613\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 38.78\n",
      "ROUGE-2 F1: 15.92\n",
      "ROUGE-L F1: 35.26\n",
      "================================================================================\n",
      "Final Score: 89.96\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 89.96)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 2/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:52<00:00,  2.79it/s, loss=3.2594, lr=8.43e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0283\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:07<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 45.91\n",
      "ROUGE-2 F1: 21.61\n",
      "ROUGE-L F1: 40.71\n",
      "================================================================================\n",
      "Final Score: 108.23\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 108.23)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 3/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:51<00:00,  2.80it/s, loss=3.1098, lr=5.87e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.4254\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:02<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 46.37\n",
      "ROUGE-2 F1: 22.32\n",
      "ROUGE-L F1: 41.11\n",
      "================================================================================\n",
      "Final Score: 109.80\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 109.80)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 4/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:52<00:00,  2.80it/s, loss=3.2330, lr=3.02e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1495\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 47.24\n",
      "ROUGE-2 F1: 22.98\n",
      "ROUGE-L F1: 41.61\n",
      "================================================================================\n",
      "Final Score: 111.84\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 111.84)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 5/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:51<00:00,  2.80it/s, loss=3.0736, lr=8.23e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0039\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 48.11\n",
      "ROUGE-2 F1: 23.81\n",
      "ROUGE-L F1: 42.31\n",
      "================================================================================\n",
      "Final Score: 114.23\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 114.23)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 6/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1992/1992 [11:52<00:00,  2.80it/s, loss=2.5434, lr=0.00e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9503\n",
      "\n",
      "Dev 데이터 평가 (ROUGE 계산)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|██████████| 63/63 [01:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 및 ROUGE 계산 중...\n",
      "\n",
      "ROUGE-1 F1: 48.38\n",
      "ROUGE-2 F1: 23.81\n",
      "ROUGE-L F1: 42.60\n",
      "================================================================================\n",
      "Final Score: 114.79\n",
      "================================================================================\n",
      "✓ New best model! (ROUGE Final: 114.79)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "================================================================================\n",
      "학습 완료!\n",
      "================================================================================\n",
      "Best ROUGE Final: 114.79\n",
      "\n",
      "================================================================================\n",
      "테스트 데이터 추론\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating: 100%|██████████| 63/63 [01:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "제출 파일 저장: /home/NLP_contest/NPL_contest/outputs/submission_kot5_filtered_aug_20251013_034209.csv\n",
      "생성된 요약 개수: 499\n",
      "\n",
      "================================================================================\n",
      "생성된 요약 샘플\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "Fname: test_0\n",
      "Summary (29 words): 아이언 Dawson은 #Person1# 에게 사내 메모를 보내달라고 요청합니다. Ms. Dawson은 #Person2# 에게 메시지 프로그램 사용은 제한되지만, #Person1# 은 메시지 프로그램 사용이 제한된다고 설명합니다. #Person2# 는 #Person1# 의 제안에 동의합니다.\n",
      "\n",
      "[Sample 2]\n",
      "Fname: test_1\n",
      "Summary (17 words): 재상은 #Person1# 에게 지하철으로 출퇴근할 것을 제안하지만, #Person1# 은 차를 피하고 싶어합니다. #Person2# 는 대중교통으로 출근하기로 결정합니다.\n",
      "\n",
      "[Sample 3]\n",
      "Fname: test_2\n",
      "Summary (17 words): 아이언은 Kate에게 Masha와 Hero가 이혼했다고 말합니다. Kate는 #Person1# 에게 Masha가 양육권을 가지고, 아이들을 양육할 수 있다고 말합니다.\n",
      "\n",
      "[Sample 4]\n",
      "Fname: test_3\n",
      "Summary (12 words): 재상은 #Person1# 의 생일 파티에 초대하고, #Person1# 은 #Person2# 에게 춤추기로 합니다.\n",
      "\n",
      "[Sample 5]\n",
      "Fname: test_4\n",
      "Summary (20 words): 아이언 공원은 #Person1# 에게 올림픽 스타디움에 대해 이야기합니다. #Person1# 은 #Person2# 에게 좌석이 5000석이라고 설명합니다. #Person2# 는 등산 금기라고 설명합니다.\n",
      "\n",
      "================================================================================\n",
      "요약 길이 통계\n",
      "================================================================================\n",
      "평균: 18.0 단어\n",
      "중간값: 18.0 단어\n",
      "최소: 6 단어\n",
      "최대: 34 단어\n",
      "표준편차: 5.6 단어\n",
      "\n",
      "================================================================================\n",
      "KoT5 + 고품질 증강 데이터 파이프라인 완료!\n",
      "================================================================================\n",
      "\n",
      "저장된 파일:\n",
      "1. 모델: /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "2. 제출 파일: /home/NLP_contest/NPL_contest/outputs/submission_kot5_filtered_aug_20251013_034209.csv\n",
      "\n",
      "기대 성능:\n",
      "- 고품질 증강 데이터로 일반화 성능 향상\n",
      "- 노이즈가 제거되어 더 안정적인 학습\n",
      "- 예상 점수: 50~55점 (필터링된 증강 데이터 효과)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KoT5 학습 - 고품질 증강 데이터 활용\n",
    "필터링된 증강 데이터와 원본 데이터를 결합하여 학습\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Config 설정\n",
    "# ========================================\n",
    "class Config:\n",
    "    # 경로\n",
    "    train_path = '/home/NLP_contest/NPL_contest/data/train.csv'  # 원본 train 데이터\n",
    "    augmented_path = '/home/NLP_contest/NPL_contest/augmented_data/high_quality_augmented.csv'  # 필터링된 증강 데이터\n",
    "    dev_path = '/home/NLP_contest/NPL_contest/data/dev.csv'  # dev 경로 확인 필요\n",
    "    test_path = '/home/NLP_contest/NPL_contest/data/test.csv'  # test 경로 확인 필요\n",
    "    output_dir = '/home/NLP_contest/NPL_contest/outputs'\n",
    "    model_save_path = '/home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation'\n",
    "    \n",
    "    # 모델 설정\n",
    "    model_name = 'KETI-AIR/ke-t5-base'  # KoT5 base 모델\n",
    "    max_input_length = 512  # 입력 최대 길이\n",
    "    max_target_length = 128  # 출력 최대 길이\n",
    "    \n",
    "    # 학습 설정\n",
    "    batch_size = 8  # 배치 크기 (GPU 메모리에 따라 조정)\n",
    "    num_epochs = 6  # 학습 에폭 수\n",
    "    learning_rate = 1e-4  # 학습률\n",
    "    warmup_ratio = 0.1  # Warmup 비율\n",
    "    weight_decay = 0.01  # Weight decay\n",
    "    max_grad_norm = 1.0  # Gradient clipping\n",
    "    label_smoothing = 0.1  # Label smoothing\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 3  # Early stopping patience\n",
    "    \n",
    "    # 생성 설정\n",
    "    num_beams = 4  # Beam search 크기\n",
    "    length_penalty = 1.0  # 길이 페널티\n",
    "    no_repeat_ngram_size = 4  # N-gram 반복 방지\n",
    "    min_length = 10  # 최소 생성 길이\n",
    "    max_gen_length = 64  # 최대 생성 길이\n",
    "    \n",
    "    # 증강 데이터 사용 설정\n",
    "    use_augmentation = True  # 증강 데이터 사용 여부\n",
    "    augmentation_ratio = 1.0  # 증강 데이터 사용 비율 (1.0 = 전체 사용)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(config.output_dir, exist_ok=True)  # 출력 디렉토리 생성\n",
    "os.makedirs(config.model_save_path, exist_ok=True)  # 모델 저장 디렉토리 생성\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 데이터 로드 및 결합\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"데이터 로드 및 결합\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 원본 train 데이터 로드\n",
    "train_df = pd.read_csv(config.train_path)\n",
    "print(f\"원본 Train 데이터: {len(train_df)} samples\")\n",
    "\n",
    "# 증강 데이터 로드 및 결합\n",
    "if config.use_augmentation:\n",
    "    try:\n",
    "        # 필터링된 고품질 증강 데이터 로드\n",
    "        augmented_df = pd.read_csv(config.augmented_path)\n",
    "        print(f\"고품질 증강 데이터: {len(augmented_df)} samples\")\n",
    "        \n",
    "        # 증강 데이터 비율 적용 (필요시)\n",
    "        if config.augmentation_ratio < 1.0:\n",
    "            sample_size = int(len(augmented_df) * config.augmentation_ratio)\n",
    "            augmented_df = augmented_df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"증강 데이터 샘플링: {len(augmented_df)} samples ({config.augmentation_ratio*100:.0f}%)\")\n",
    "        \n",
    "        # 증강 데이터 표시 컬럼 추가 (디버깅용)\n",
    "        train_df['is_augmented'] = False\n",
    "        augmented_df['is_augmented'] = True\n",
    "        \n",
    "        # 원본 + 증강 데이터 결합\n",
    "        train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "        \n",
    "        # 데이터 셔플\n",
    "        train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"최종 Train 데이터: {len(train_df)} samples\")\n",
    "        print(f\"  - 원본: {(~train_df['is_augmented']).sum()} samples\")\n",
    "        print(f\"  - 증강: {train_df['is_augmented'].sum()} samples\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ 증강 데이터 파일을 찾을 수 없습니다: {config.augmented_path}\")\n",
    "        print(\"원본 데이터만 사용합니다.\")\n",
    "        train_df['is_augmented'] = False\n",
    "else:\n",
    "    print(\"증강 데이터를 사용하지 않습니다.\")\n",
    "    train_df['is_augmented'] = False\n",
    "\n",
    "# Dev, Test 데이터 로드\n",
    "dev_df = pd.read_csv(config.dev_path)\n",
    "test_df = pd.read_csv(config.test_path)\n",
    "\n",
    "print(f\"\\nDev: {len(dev_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 데이터셋 클래스\n",
    "# ========================================\n",
    "class DialogueSummarizationDataset(Dataset):\n",
    "    \"\"\"대화 요약 데이터셋\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_input_length, max_target_length, is_test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: 데이터프레임\n",
    "            tokenizer: T5 토크나이저\n",
    "            max_input_length: 입력 최대 길이\n",
    "            max_target_length: 출력 최대 길이\n",
    "            is_test: 테스트 모드 여부\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 대화 텍스트 가져오기\n",
    "        dialogue = str(row['dialogue'])\n",
    "        \n",
    "        # T5는 task prefix를 사용 (예: \"summarize: \")\n",
    "        input_text = f\"summarize: {dialogue}\"\n",
    "        \n",
    "        # 입력 토큰화\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',  # 최대 길이로 패딩\n",
    "            truncation=True,  # 최대 길이 초과시 자르기\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = input_encoding['input_ids'].squeeze()  # (seq_len,)\n",
    "        attention_mask = input_encoding['attention_mask'].squeeze()  # (seq_len,)\n",
    "        \n",
    "        # 테스트 모드면 fname도 반환\n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'fname': row['fname']\n",
    "            }\n",
    "        \n",
    "        # 요약문 토큰화 (학습용)\n",
    "        summary = str(row['summary'])\n",
    "        \n",
    "        target_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding['input_ids'].squeeze()  # (seq_len,)\n",
    "        \n",
    "        # 패딩 토큰은 -100으로 변경 (loss 계산에서 제외)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 모델 및 토크나이저 로드\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 모델 및 토크나이저 로드\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = T5TokenizerFast.from_pretrained(config.model_name)\n",
    "print(f\"Tokenizer loaded: {config.model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Person 태그 추가\n",
    "special_tokens = [f'#Person{i}#' for i in range(1, 8)]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(f\"Added {num_added} special tokens: {special_tokens}\")\n",
    "\n",
    "# 모델 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))  # 토크나이저 크기에 맞게 임베딩 조정\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 파라미터 정보\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"데이터셋 생성\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = DialogueSummarizationDataset(\n",
    "    train_df, tokenizer, \n",
    "    config.max_input_length, \n",
    "    config.max_target_length\n",
    ")\n",
    "\n",
    "dev_dataset = DialogueSummarizationDataset(\n",
    "    dev_df, tokenizer,\n",
    "    config.max_input_length,\n",
    "    config.max_target_length\n",
    ")\n",
    "\n",
    "test_dataset = DialogueSummarizationDataset(\n",
    "    test_df, tokenizer,\n",
    "    config.max_input_length,\n",
    "    config.max_target_length,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,  # 학습 데이터는 셔플\n",
    "    num_workers=2  # 병렬 데이터 로딩\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,  # Dev 데이터는 셔플 안 함\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,  # Test 데이터는 셔플 안 함\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Dev batches: {len(dev_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Optimizer 및 Scheduler\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"학습 설정\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Weight decay를 적용하지 않을 파라미터 지정\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config.weight_decay\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Optimizer 생성\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=config.learning_rate,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Scheduler 생성 (Cosine annealing with warmup)\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Early Stopping 클래스\n",
    "# ========================================\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping을 위한 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=3, min_delta=0.0, mode='max'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: 개선이 없어도 기다릴 에폭 수\n",
    "            min_delta: 개선으로 인정할 최소 변화량\n",
    "            mode: 'max' (높을수록 좋음) or 'min' (낮을수록 좋음)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0  # 개선 없는 에폭 카운터\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, score):\n",
    "        \"\"\"\n",
    "        현재 점수를 체크하고 개선 여부 반환\n",
    "        \n",
    "        Args:\n",
    "            score: 현재 에폭의 점수\n",
    "            \n",
    "        Returns:\n",
    "            bool: 개선되었으면 True\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        \n",
    "        # 개선 여부 체크\n",
    "        if self.mode == 'max':\n",
    "            improved = score > self.best_score + self.min_delta\n",
    "        else:\n",
    "            improved = score < self.best_score - self.min_delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 학습 함수\n",
    "# ========================================\n",
    "def train_epoch(model, loader, optimizer, scheduler, device, config):\n",
    "    \"\"\"\n",
    "    한 에폭 학습\n",
    "    \n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        loader: 학습 데이터로더\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: 디바이스 (cuda/cpu)\n",
    "        config: 설정\n",
    "        \n",
    "    Returns:\n",
    "        float: 평균 loss\n",
    "    \"\"\"\n",
    "    model.train()  # 학습 모드\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # 데이터를 디바이스로 이동\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 진행상황 업데이트\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ROUGE 평가 함수\n",
    "# ========================================\n",
    "def evaluate_model_on_dev(model, dev_loader, tokenizer, device, config):\n",
    "    \"\"\"\n",
    "    Dev 데이터로 모델 평가 (ROUGE 계산)\n",
    "    \n",
    "    Args:\n",
    "        model: 평가할 모델\n",
    "        dev_loader: Dev 데이터로더\n",
    "        tokenizer: 토크나이저\n",
    "        device: 디바이스\n",
    "        config: 설정\n",
    "        \n",
    "    Returns:\n",
    "        dict: ROUGE 점수들\n",
    "    \"\"\"\n",
    "    from rouge import Rouge\n",
    "    \n",
    "    # 한국어 형태소 분석기 (kiwipiepy)\n",
    "    try:\n",
    "        from kiwipiepy import Kiwi\n",
    "        kiwi = Kiwi()\n",
    "        use_morpheme = True\n",
    "    except:\n",
    "        use_morpheme = False\n",
    "    \n",
    "    def tokenize_for_rouge(text):\n",
    "        \"\"\"ROUGE 계산을 위한 토큰화\"\"\"\n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        if use_morpheme:\n",
    "            # 형태소 분석\n",
    "            tokens = kiwi.tokenize(text)\n",
    "            morphemes = [token.form for token in tokens]\n",
    "            return ' '.join(morphemes)\n",
    "        else:\n",
    "            # 단순 공백 분리\n",
    "            return ' '.join(text.split())\n",
    "    \n",
    "    print(\"\\nDev 데이터 평가 (ROUGE 계산)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    model.eval()  # 평가 모드\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # 요약문 생성\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Generating summaries\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Beam search로 요약문 생성\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_gen_length,\n",
    "                min_length=config.min_length,\n",
    "                num_beams=config.num_beams,\n",
    "                length_penalty=config.length_penalty,\n",
    "                no_repeat_ngram_size=config.no_repeat_ngram_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # 생성된 텍스트 디코딩\n",
    "            for gen_ids, label_ids in zip(generated_ids, batch['labels']):\n",
    "                pred = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                label_ids = label_ids[label_ids != -100]\n",
    "                ref = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                references.append(ref)\n",
    "    \n",
    "    # ROUGE 계산\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    print(\"토큰화 및 ROUGE 계산 중...\")\n",
    "    pred_tokenized = [tokenize_for_rouge(p) for p in predictions]\n",
    "    ref_tokenized = [tokenize_for_rouge(r) for r in references]\n",
    "    \n",
    "    # 빈 문자열 제거\n",
    "    valid_pairs = [(p, r) for p, r in zip(pred_tokenized, ref_tokenized) if p and r]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'final_score': 0.0}\n",
    "    \n",
    "    pred_valid, ref_valid = zip(*valid_pairs)\n",
    "    \n",
    "    try:\n",
    "        # ROUGE 점수 계산\n",
    "        scores = rouge.get_scores(list(pred_valid), list(ref_valid))\n",
    "        \n",
    "        # F1 점수 평균\n",
    "        rouge1 = np.mean([s['rouge-1']['f'] for s in scores]) * 100\n",
    "        rouge2 = np.mean([s['rouge-2']['f'] for s in scores]) * 100\n",
    "        rougel = np.mean([s['rouge-l']['f'] for s in scores]) * 100\n",
    "        \n",
    "        results = {\n",
    "            'rouge-1': rouge1,\n",
    "            'rouge-2': rouge2,\n",
    "            'rouge-l': rougel,\n",
    "            'final_score': rouge1 + rouge2 + rougel\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nROUGE-1 F1: {rouge1:.2f}\")\n",
    "        print(f\"ROUGE-2 F1: {rouge2:.2f}\")\n",
    "        print(f\"ROUGE-L F1: {rougel:.2f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Final Score: {results['final_score']:.2f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE 계산 중 오류: {e}\")\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'final_score': 0.0}\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 학습 루프\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 학습 시작 (고품질 증강 데이터 활용)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_rouge_score = 0.0  # 최고 ROUGE 점수 저장\n",
    "early_stopping = EarlyStopping(patience=config.patience, mode='max')\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 학습\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, device, config\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Dev 평가\n",
    "    rouge_scores = evaluate_model_on_dev(\n",
    "        model, dev_loader, tokenizer, device, config\n",
    "    )\n",
    "    \n",
    "    current_score = rouge_scores['final_score']\n",
    "    \n",
    "    # Best model 저장\n",
    "    if current_score > best_rouge_score:\n",
    "        best_rouge_score = current_score\n",
    "        print(f\"✓ New best model! (ROUGE Final: {current_score:.2f})\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        model.save_pretrained(config.model_save_path)\n",
    "        tokenizer.save_pretrained(config.model_save_path)\n",
    "        print(f\"Model saved to {config.model_save_path}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    improved = early_stopping(current_score)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n⚠ Early stopping triggered at epoch {epoch + 1}\")\n",
    "        print(f\"Best ROUGE score: {best_rouge_score:.2f}\")\n",
    "        break\n",
    "    \n",
    "    if not improved:\n",
    "        print(f\"⚠ No improvement for {early_stopping.counter} epoch(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"학습 완료!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best ROUGE Final: {best_rouge_score:.2f}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 후처리 함수\n",
    "# ========================================\n",
    "def post_process_summary(summary, max_words=25):\n",
    "    \"\"\"\n",
    "    생성된 요약문 후처리\n",
    "    \n",
    "    Args:\n",
    "        summary: 생성된 요약문\n",
    "        max_words: 최대 단어 수\n",
    "        \n",
    "    Returns:\n",
    "        str: 후처리된 요약문\n",
    "    \"\"\"\n",
    "    if not summary or not summary.strip():\n",
    "        return \"#Person1#과 #Person2#가 대화합니다.\"\n",
    "    \n",
    "    summary = summary.strip()\n",
    "    \n",
    "    # 마침표 추가 (한국어 어미로 끝나면)\n",
    "    if not summary.endswith('.'):\n",
    "        if summary.split()[-1][-1] in ['다', '요', '음', '지', '나']:\n",
    "            summary += '.'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 추론 함수\n",
    "# ========================================\n",
    "def generate_summaries(model, loader, tokenizer, device, config):\n",
    "    \"\"\"\n",
    "    테스트 데이터에 대한 요약문 생성\n",
    "    \n",
    "    Args:\n",
    "        model: 학습된 모델\n",
    "        loader: 테스트 데이터로더\n",
    "        tokenizer: 토크나이저\n",
    "        device: 디바이스\n",
    "        config: 설정\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fnames, summaries)\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드\n",
    "    \n",
    "    fnames = []\n",
    "    summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Beam search로 요약문 생성\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_gen_length,\n",
    "                min_length=config.min_length,\n",
    "                num_beams=config.num_beams,\n",
    "                length_penalty=config.length_penalty,\n",
    "                no_repeat_ngram_size=config.no_repeat_ngram_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # 디코딩 및 후처리\n",
    "            for fname, gen_ids in zip(batch['fname'], generated_ids):\n",
    "                summary = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                summary = post_process_summary(summary, max_words=25)\n",
    "                \n",
    "                fnames.append(fname)\n",
    "                summaries.append(summary)\n",
    "    \n",
    "    return fnames, summaries\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 테스트 데이터 추론\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"테스트 데이터 추론\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.model_save_path)\n",
    "model = model.to(device)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(config.model_save_path)\n",
    "\n",
    "# 요약문 생성\n",
    "fnames, summaries = generate_summaries(\n",
    "    model, test_loader, tokenizer, device, config\n",
    ")\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission_df = pd.DataFrame({\n",
    "    'fname': fnames,\n",
    "    'summary': summaries\n",
    "})\n",
    "\n",
    "# 제출 파일 저장\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f'submission_kot5_filtered_aug_{timestamp}.csv'\n",
    "submission_path = os.path.join(config.output_dir, submission_filename)\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n제출 파일 저장: {submission_path}\")\n",
    "print(f\"생성된 요약 개수: {len(submission_df)}\")\n",
    "\n",
    "# 샘플 출력\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"생성된 요약 샘플\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(submission_df))):\n",
    "    summary = submission_df.iloc[i]['summary']\n",
    "    word_count = len(summary.split())\n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"Fname: {submission_df.iloc[i]['fname']}\")\n",
    "    print(f\"Summary ({word_count} words): {summary}\")\n",
    "\n",
    "# 요약 길이 통계\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"요약 길이 통계\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_lengths = submission_df['summary'].apply(lambda x: len(x.split()))\n",
    "print(f\"평균: {summary_lengths.mean():.1f} 단어\")\n",
    "print(f\"중간값: {summary_lengths.median():.1f} 단어\")\n",
    "print(f\"최소: {summary_lengths.min()} 단어\")\n",
    "print(f\"최대: {summary_lengths.max()} 단어\")\n",
    "print(f\"표준편차: {summary_lengths.std():.1f} 단어\")\n",
    "\n",
    "# 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 + 고품질 증강 데이터 파이프라인 완료!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n저장된 파일:\")\n",
    "print(f\"1. 모델: {config.model_save_path}\")\n",
    "print(f\"2. 제출 파일: {submission_path}\")\n",
    "print(f\"\\n기대 성능:\")\n",
    "print(f\"- 고품질 증강 데이터로 일반화 성능 향상\")\n",
    "print(f\"- 노이즈가 제거되어 더 안정적인 학습\")\n",
    "print(f\"- 예상 점수: 50~55점 (필터링된 증강 데이터 효과)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94499f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
