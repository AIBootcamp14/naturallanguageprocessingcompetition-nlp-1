{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Full Pipeline - ëª¨ë“  ê¸°ë²• í†µí•©\n",
    "> PRD ê³„íšì— ë”°ë¥¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 85+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:52:59.499057Z",
     "iopub.status.busy": "2025-10-10T02:52:59.498955Z",
     "iopub.status.idle": "2025-10-10T02:53:01.421961Z",
     "shell.execute_reply": "2025-10-10T02:53:01.421561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "âœ… ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ë¡œë“œ ì„±ê³µ\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸ - 04_multi_model_ensemble.ipynbì—ì„œ ì°¸ê³ \n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.423110Z",
     "iopub.status.busy": "2025-10-10T02:53:01.422930Z",
     "iopub.status.idle": "2025-10-10T02:53:01.434897Z",
     "shell.execute_reply": "2025-10-10T02:53:01.434500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FULL PIPELINE CONFIGURATION\n",
      "==================================================\n",
      "Pipeline Stages: 9\n",
      "  âœ“ data_quality_check\n",
      "  âœ“ data_preprocessing\n",
      "  âœ“ data_augmentation\n",
      "  âœ“ model_training\n",
      "  âœ“ cross_validation\n",
      "  âœ“ ensemble\n",
      "  âœ“ hyperparameter_optimization\n",
      "  âœ“ inference_optimization\n",
      "  âœ“ final_prediction\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_full_pipeline.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FULL PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pipeline Stages: {len(config['pipeline']['stages'])}\")\n",
    "for stage in config['pipeline']['stages']:\n",
    "    print(f\"  âœ“ {stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.435845Z",
     "iopub.status.busy": "2025-10-10T02:53:01.435744Z",
     "iopub.status.idle": "2025-10-10T02:53:01.439048Z",
     "shell.execute_reply": "2025-10-10T02:53:01.438646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: logs/full_pipeline\n",
      "==================================================\n",
      "FULL PIPELINE EXECUTION STARTED\n",
      "Timestamp: 20251010_200633\n",
      "Config: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/configs/config_full_pipeline.yaml\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'full_pipeline_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('FULL PIPELINE EXECUTION STARTED')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Config: {config_path}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.439995Z",
     "iopub.status.busy": "2025-10-10T02:53:01.439894Z",
     "iopub.status.idle": "2025-10-10T02:53:01.509932Z",
     "shell.execute_reply": "2025-10-10T02:53:01.509443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Tier: LOW\n",
      "Auto-optimization enabled\n",
      "Finding optimal batch size...\n"
     ]
    }
   ],
   "source": [
    "# GPU ìµœì í™” ì²´í¬\n",
    "# í•„ìš”í•œ ëª¨ë“ˆ import\n",
    "if 'check_gpu_tier' not in globals():\n",
    "    try:\n",
    "        from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import check_gpu_tier\")\n",
    "        def check_gpu_tier():\n",
    "            return \"UNKNOWN\"\n",
    "\n",
    "# configê°€ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "if 'config' not in globals():\n",
    "    print(\"Warning: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    if config['gpu']['auto_optimization']['enabled']:\n",
    "        gpu_tier = check_gpu_tier()\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "            logger.write(f\"Auto-optimization enabled\")\n",
    "            \n",
    "            if config['gpu']['auto_optimization']['find_optimal_batch_size']:\n",
    "                logger.write(\"Finding optimal batch size...\")\n",
    "                # ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì½”ë“œ\n",
    "        else:\n",
    "            print(f\"GPU Tier: {gpu_tier}\")\n",
    "            print(f\"Auto-optimization enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.511028Z",
     "iopub.status.busy": "2025-10-10T02:53:01.510918Z",
     "iopub.status.idle": "2025-10-10T02:53:01.513531Z",
     "shell.execute_reply": "2025-10-10T02:53:01.513132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PERFORMANCE TARGETS\n",
      "==================================================\n",
      "ROUGE-1: 0.45\n",
      "ROUGE-2: 0.3\n",
      "ROUGE-L: 0.4\n",
      "Overall Target: 0.85\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ì„±ëŠ¥ ëª©í‘œ í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE TARGETS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROUGE-1: {config['performance_targets']['rouge_1']}\")\n",
    "print(f\"ROUGE-2: {config['performance_targets']['rouge_2']}\")\n",
    "print(f\"ROUGE-L: {config['performance_targets']['rouge_l']}\")\n",
    "print(f\"Overall Target: {config['performance_targets']['overall']}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.514537Z",
     "iopub.status.busy": "2025-10-10T02:53:01.514439Z",
     "iopub.status.idle": "2025-10-10T02:53:01.517293Z",
     "shell.execute_reply": "2025-10-10T02:53:01.516860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_quality_check            : pending\n",
      "data_preprocessing            : pending\n",
      "data_augmentation             : pending\n",
      "model_training                : pending\n",
      "cross_validation              : pending\n",
      "ensemble                      : pending\n",
      "hyperparameter_optimization   : pending\n",
      "inference_optimization        : pending\n",
      "final_prediction              : pending\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ì¶”ì \n",
    "# configê°€ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "if 'config' not in globals():\n",
    "    print(\"Error: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    pipeline_status = {}\n",
    "    for stage in config['pipeline']['stages']:\n",
    "        pipeline_status[stage] = 'pending'\n",
    "\n",
    "    def update_status(stage, status):\n",
    "        pipeline_status[stage] = status\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"[{stage}] Status: {status}\")\n",
    "        else:\n",
    "            print(f\"[{stage}] Status: {status}\")\n",
    "        \n",
    "    # ìƒíƒœ í‘œì‹œ\n",
    "    for stage, status in pipeline_status.items():\n",
    "        print(f\"{stage:30s}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.518296Z",
     "iopub.status.busy": "2025-10-10T02:53:01.518198Z",
     "iopub.status.idle": "2025-10-10T02:53:01.673499Z",
     "shell.execute_reply": "2025-10-10T02:53:01.673101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_quality_check] Status: running\n",
      "\n",
      "=== Data Quality Check ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "âœ… Loaded 12457 training samples\n",
      "âœ… Loaded 499 dev samples\n",
      "âœ… Loaded 499 test samples\n",
      "Null values - Train: 0, Dev: 0, Test: 0\n",
      "Duplicate rows in training data: 0\n",
      "âœ… Data loading completed successfully!\n",
      "[data_quality_check] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: ë°ì´í„° ë¡œë“œ - dev_dfì™€ test_df í¬í•¨!\n",
    "# Stage 1: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ë¡œë“œ\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if 'data_quality_check' in config['pipeline']['stages']:\n",
    "    update_status('data_quality_check', 'running')\n",
    "    logger.write(\"\\n=== Data Quality Check ===\")\n",
    "    \n",
    "    # config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "    def get_data_path(path_str):\n",
    "        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # ë°ì´í„° ê²½ë¡œ\n",
    "    train_path = get_data_path(config['paths']['train_file'])\n",
    "    dev_path = get_data_path(config['paths']['dev_file']) \n",
    "    test_path = get_data_path(config['paths']['test_file'])\n",
    "    \n",
    "    logger.write(f\"Loading data from config paths:\")\n",
    "    logger.write(f\"  - Train: {train_path}\")\n",
    "    logger.write(f\"  - Dev: {dev_path}\")\n",
    "    logger.write(f\"  - Test: {test_path}\")\n",
    "    \n",
    "    # ëª¨ë“  ë°ì´í„° ë¡œë“œ - train_df, dev_df, test_df ëª¨ë‘!\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    dev_df = pd.read_csv(dev_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    logger.write(f\"âœ… Loaded {len(train_df)} training samples\")\n",
    "    logger.write(f\"âœ… Loaded {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"âœ… Loaded {len(test_df)} test samples\")\n",
    "    \n",
    "    # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦\n",
    "    if config['data_quality']['enabled']:\n",
    "        # êµ¬ì¡°ì  ê²€ì¦\n",
    "        if config['data_quality']['checks']['structural']['check_nulls']:\n",
    "            train_nulls = train_df.isnull().sum().sum()\n",
    "            dev_nulls = dev_df.isnull().sum().sum()\n",
    "            test_nulls = test_df.isnull().sum().sum()\n",
    "            logger.write(f\"Null values - Train: {train_nulls}, Dev: {dev_nulls}, Test: {test_nulls}\")\n",
    "        \n",
    "        if config['data_quality']['checks']['structural']['check_duplicates']:\n",
    "            train_dups = train_df.duplicated().sum()\n",
    "            logger.write(f\"Duplicate rows in training data: {train_dups}\")\n",
    "    \n",
    "    logger.write(\"âœ… Data loading completed successfully!\")\n",
    "    update_status('data_quality_check', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.674757Z",
     "iopub.status.busy": "2025-10-10T02:53:01.674628Z",
     "iopub.status.idle": "2025-10-10T02:53:01.724276Z",
     "shell.execute_reply": "2025-10-10T02:53:01.723835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Data Quality Validation ===\n",
      "\n",
      "ğŸ“Š Data Quality Report:\n",
      "\n",
      "Structural Validation:\n",
      "  - Train shape: (12457, 4)\n",
      "  - Dev shape: (499, 4)\n",
      "  - Test shape: (499, 2)\n",
      "  - Column match: True\n",
      "\n",
      "Text Quality:\n",
      "  - Avg dialogue length: 406.1\n",
      "  - Compression ratio: 23.23%\n",
      "  - Encoding issues: 0\n",
      "  - Special chars: 12455\n",
      "\n",
      "Label Distribution:\n",
      "  - Unique topics: 9235\n",
      "  - Imbalance ratio: 130.00\n",
      "\n",
      "Outlier Detection:\n",
      "  - Outlier count: 355\n",
      "  - Outlier ratio: 2.85%\n",
      "\n",
      "ğŸ“‹ Recommendations:\n",
      "  âœ“ Clean special characters from text\n",
      "  âœ“ Consider data augmentation for underrepresented topics\n",
      "âš ï¸ Error during data validation: You must call wandb.init() before wandb.log()\n",
      "   Skipping detailed validation. Please check data loading in cell 7.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1.5: ìƒì„¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (PRD 16_ë°ì´í„°_í’ˆì§ˆ_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "# ì£¼ì˜: ì´ ì…€ì€ ì…€ 7 (ë°ì´í„° ë¡œë“œ) ì‹¤í–‰ í›„ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤!\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "class FullPipelineDataValidator:\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ìš© ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.validation_report = {}\n",
    "        \n",
    "    def comprehensive_validation(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"í¬ê´„ì  ë°ì´í„° ê²€ì¦\"\"\"\n",
    "        self.logger.write(\"\\n=== Comprehensive Data Quality Validation ===\")\n",
    "        \n",
    "        report = {}\n",
    "        \n",
    "        # 1. êµ¬ì¡°ì  ê²€ì¦\n",
    "        report['structural'] = self._validate_structure(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 2. í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\n",
    "        report['text_quality'] = self._validate_text_quality(train_df)\n",
    "        \n",
    "        # 3. ë¼ë²¨ ë¶„í¬ ê²€ì¦\n",
    "        report['label_distribution'] = self._validate_label_distribution(train_df)\n",
    "        \n",
    "        # 4. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦\n",
    "        report['consistency'] = self._validate_consistency(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 5. ì´ìƒì¹˜ ê²€ì¶œ\n",
    "        report['outliers'] = self._detect_outliers(train_df)\n",
    "        \n",
    "        self.validation_report = report\n",
    "        return report\n",
    "    \n",
    "    def _validate_structure(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"êµ¬ì¡°ì  ê²€ì¦\"\"\"\n",
    "        return {\n",
    "            'train_shape': train_df.shape,\n",
    "            'dev_shape': dev_df.shape,\n",
    "            'test_shape': test_df.shape,\n",
    "            'train_nulls': train_df.isnull().sum().sum(),\n",
    "            'dev_nulls': dev_df.isnull().sum().sum(),\n",
    "            'test_nulls': test_df.isnull().sum().sum(),\n",
    "            'train_duplicates': train_df.duplicated().sum(),\n",
    "            'column_match': set(train_df.columns) == set(dev_df.columns)\n",
    "        }\n",
    "    \n",
    "    def _validate_text_quality(self, df) -> Dict:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n",
    "        \n",
    "        # íŠ¹ìˆ˜ ë¬¸ì íŒ¨í„´\n",
    "        special_chars = df['dialogue'].str.contains('[ï¿½\\\\?\\\\x00-\\\\x1f]').sum()\n",
    "        \n",
    "        # ì¸ì½”ë”© ë¬¸ì œ\n",
    "        encoding_issues = df['dialogue'].apply(\n",
    "            lambda x: bool(re.search(r'[\\ufffd]', str(x)))\n",
    "        ).sum()\n",
    "        \n",
    "        return {\n",
    "            'avg_dialogue_length': dialogue_lengths.mean(),\n",
    "            'max_dialogue_length': dialogue_lengths.max(),\n",
    "            'min_dialogue_length': dialogue_lengths.min(),\n",
    "            'avg_summary_length': summary_lengths.mean() if 'summary' in df.columns else 0,\n",
    "            'compression_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0,\n",
    "            'special_chars_count': int(special_chars),\n",
    "            'encoding_issues': int(encoding_issues),\n",
    "            'empty_dialogues': (dialogue_lengths == 0).sum()\n",
    "        }\n",
    "    \n",
    "    def _validate_label_distribution(self, df) -> Dict:\n",
    "        \"\"\"ë¼ë²¨ ë¶„í¬ ê²€ì¦\"\"\"\n",
    "        if 'topic' not in df.columns:\n",
    "            return {}\n",
    "        \n",
    "        topic_counts = df['topic'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'unique_topics': len(topic_counts),\n",
    "            'most_common_topic': topic_counts.index[0] if len(topic_counts) > 0 else None,\n",
    "            'most_common_count': int(topic_counts.iloc[0]) if len(topic_counts) > 0 else 0,\n",
    "            'least_common_topic': topic_counts.index[-1] if len(topic_counts) > 0 else None,\n",
    "            'least_common_count': int(topic_counts.iloc[-1]) if len(topic_counts) > 0 else 0,\n",
    "            'imbalance_ratio': float(topic_counts.iloc[0] / topic_counts.iloc[-1]) if len(topic_counts) > 1 and topic_counts.iloc[-1] > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _validate_consistency(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"ë°ì´í„° ì¼ê´€ì„± ê²€ì¦\"\"\"\n",
    "        # Person íƒœê·¸ ì¼ê´€ì„±\n",
    "        train_person_tags = train_df['dialogue'].str.contains('#Person').mean()\n",
    "        dev_person_tags = dev_df['dialogue'].str.contains('#Person').mean()\n",
    "        test_person_tags = test_df['dialogue'].str.contains('#Person').mean()\n",
    "        \n",
    "        return {\n",
    "            'train_person_tag_ratio': float(train_person_tags),\n",
    "            'dev_person_tag_ratio': float(dev_person_tags),\n",
    "            'test_person_tag_ratio': float(test_person_tags),\n",
    "            'person_tag_consistent': abs(train_person_tags - dev_person_tags) < 0.1\n",
    "        }\n",
    "    \n",
    "    def _detect_outliers(self, df) -> Dict:\n",
    "        \"\"\"ì´ìƒì¹˜ ê²€ì¶œ\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        \n",
    "        # IQR ê¸°ë°˜ ì´ìƒì¹˜\n",
    "        Q1 = dialogue_lengths.quantile(0.25)\n",
    "        Q3 = dialogue_lengths.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (dialogue_lengths < lower_bound) | (dialogue_lengths > upper_bound)\n",
    "        \n",
    "        return {\n",
    "            'outlier_count': int(outliers.sum()),\n",
    "            'outlier_ratio': float(outliers.mean()),\n",
    "            'lower_bound': float(lower_bound),\n",
    "            'upper_bound': float(upper_bound)\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if not self.validation_report:\n",
    "            return recommendations\n",
    "        \n",
    "        # êµ¬ì¡°ì  ë¬¸ì œ\n",
    "        if self.validation_report.get('structural', {}).get('train_nulls', 0) > 0:\n",
    "            recommendations.append(\"Remove or impute null values in training data\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¬¸ì œ\n",
    "        text_quality = self.validation_report.get('text_quality', {})\n",
    "        if text_quality.get('encoding_issues', 0) > 0:\n",
    "            recommendations.append(f\"Fix {text_quality['encoding_issues']} encoding issues\")\n",
    "        \n",
    "        if text_quality.get('special_chars_count', 0) > 0:\n",
    "            recommendations.append(\"Clean special characters from text\")\n",
    "        \n",
    "        # ë¼ë²¨ ë¶ˆê· í˜•\n",
    "        label_dist = self.validation_report.get('label_distribution', {})\n",
    "        if label_dist.get('imbalance_ratio', 0) > 10:\n",
    "            recommendations.append(\"Consider data augmentation for underrepresented topics\")\n",
    "        \n",
    "        # ì´ìƒì¹˜\n",
    "        outliers = self.validation_report.get('outliers', {})\n",
    "        if outliers.get('outlier_ratio', 0) > 0.05:\n",
    "            recommendations.append(f\"Review {outliers['outlier_count']} outlier samples\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# ì´ì œ ì‹¤ì œ ê²€ì¦ ì‹¤í–‰ - ì…€ 7ì—ì„œ ì´ë¯¸ ë¡œë“œí•œ ë°ì´í„°ë¥¼ ì‚¬ìš©\n",
    "# ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìŠ¤í‚µ\n",
    "try:\n",
    "    # train_df, dev_df, test_dfê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if 'train_df' not in locals() or 'dev_df' not in locals() or 'test_df' not in locals():\n",
    "        logger.write(\"âš ï¸ Data not loaded yet. Skipping detailed validation.\")\n",
    "        logger.write(\"   Please run cell 7 first to load the data.\")\n",
    "    else:\n",
    "        # ìƒì„¸ ê²€ì¦ ì‹¤í–‰\n",
    "        data_validator = FullPipelineDataValidator(logger)\n",
    "        validation_report = data_validator.comprehensive_validation(train_df, dev_df, test_df)\n",
    "        \n",
    "        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "        logger.write(\"\\nğŸ“Š Data Quality Report:\")\n",
    "        \n",
    "        # êµ¬ì¡°ì  ê²€ì¦ ê²°ê³¼\n",
    "        structural = validation_report.get('structural', {})\n",
    "        logger.write(f\"\\nStructural Validation:\")\n",
    "        logger.write(f\"  - Train shape: {structural.get('train_shape')}\")\n",
    "        logger.write(f\"  - Dev shape: {structural.get('dev_shape')}\")\n",
    "        logger.write(f\"  - Test shape: {structural.get('test_shape')}\")\n",
    "        logger.write(f\"  - Column match: {structural.get('column_match')}\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²°ê³¼\n",
    "        text_quality = validation_report.get('text_quality', {})\n",
    "        logger.write(f\"\\nText Quality:\")\n",
    "        logger.write(f\"  - Avg dialogue length: {text_quality.get('avg_dialogue_length', 0):.1f}\")\n",
    "        logger.write(f\"  - Compression ratio: {text_quality.get('compression_ratio', 0):.2%}\")\n",
    "        logger.write(f\"  - Encoding issues: {text_quality.get('encoding_issues', 0)}\")\n",
    "        logger.write(f\"  - Special chars: {text_quality.get('special_chars_count', 0)}\")\n",
    "        \n",
    "        # ë¼ë²¨ ë¶„í¬ ê²°ê³¼\n",
    "        label_dist = validation_report.get('label_distribution', {})\n",
    "        if label_dist:\n",
    "            logger.write(f\"\\nLabel Distribution:\")\n",
    "            logger.write(f\"  - Unique topics: {label_dist.get('unique_topics')}\")\n",
    "            logger.write(f\"  - Imbalance ratio: {label_dist.get('imbalance_ratio', 0):.2f}\")\n",
    "        \n",
    "        # ì´ìƒì¹˜ ê²€ì¶œ ê²°ê³¼\n",
    "        outliers = validation_report.get('outliers', {})\n",
    "        logger.write(f\"\\nOutlier Detection:\")\n",
    "        logger.write(f\"  - Outlier count: {outliers.get('outlier_count', 0)}\")\n",
    "        logger.write(f\"  - Outlier ratio: {outliers.get('outlier_ratio', 0):.2%}\")\n",
    "        \n",
    "        # ê¶Œì¥ì‚¬í•­\n",
    "        recommendations = data_validator.generate_recommendations()\n",
    "        if recommendations:\n",
    "            logger.write(\"\\nğŸ“‹ Recommendations:\")\n",
    "            for rec in recommendations:\n",
    "                logger.write(f\"  âœ“ {rec}\")\n",
    "        \n",
    "        # WandB ë¡œê¹…\n",
    "        if config['wandb']['mode'] != 'disabled':\n",
    "            wandb.log({\n",
    "                'data_quality/nulls': structural.get('train_nulls', 0),\n",
    "                'data_quality/duplicates': structural.get('train_duplicates', 0),\n",
    "                'data_quality/encoding_issues': text_quality.get('encoding_issues', 0),\n",
    "                'data_quality/outlier_ratio': outliers.get('outlier_ratio', 0)\n",
    "            })\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.write(f\"âš ï¸ Error during data validation: {str(e)}\")\n",
    "    logger.write(\"   Skipping detailed validation. Please check data loading in cell 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.725412Z",
     "iopub.status.busy": "2025-10-10T02:53:01.725308Z",
     "iopub.status.idle": "2025-10-10T02:53:01.728266Z",
     "shell.execute_reply": "2025-10-10T02:53:01.727730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations will be saved to: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/full_pipeline/visualizations\n"
     ]
    }
   ],
   "source": [
    "# ì‹œê°í™” ì„¤ì •\n",
    "if config['visualization']['enabled']:\n",
    "    viz = TrainingVisualizer()\n",
    "    \n",
    "    # configì˜ ì‹œê°í™” ê²½ë¡œ ì‚¬ìš©\n",
    "    viz_path = config['visualization']['save_path']\n",
    "    if not Path(viz_path).is_absolute():\n",
    "        viz_dir = notebook_dir / viz_path\n",
    "    else:\n",
    "        viz_dir = Path(viz_path)\n",
    "    \n",
    "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.write(f\"Visualizations will be saved to: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.729208Z",
     "iopub.status.busy": "2025-10-10T02:53:01.729114Z",
     "iopub.status.idle": "2025-10-10T02:53:03.024570Z",
     "shell.execute_reply": "2025-10-10T02:53:03.024035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mieyeppo-job\u001b[0m (\u001b[33mkimsunmin0227-hufs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/wandb/run-20251010_200635-0btj1gs4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4' target=\"_blank\">full-pipeline-integrated</a></strong> to <a href='https://wandb.ai/ieyeppo/nlp-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ieyeppo/nlp-competition' target=\"_blank\">https://wandb.ai/ieyeppo/nlp-competition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4' target=\"_blank\">https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for full pipeline tracking\n"
     ]
    }
   ],
   "source": [
    "# WandB ì´ˆê¸°í™” (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¶”ì )\n",
    "if config['wandb']['mode'] != 'disabled':\n",
    "    wandb.init(\n",
    "        project=config['wandb']['project'],\n",
    "        entity=config['wandb']['entity'],\n",
    "        name=config['wandb']['name'],\n",
    "        tags=config['wandb']['tags'],\n",
    "        config=config\n",
    "    )\n",
    "    logger.write(\"WandB initialized for full pipeline tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì½”ë“œëŠ” config íŒŒì¼ ì„¤ì •ì— ë”°ë¼ êµ¬í˜„\n",
    "\n",
    "### ì‹¤í–‰ ë‹¨ê³„:\n",
    "1. ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
    "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•\n",
    "3. ëª¨ë¸ í•™ìŠµ (Multi-model)\n",
    "4. K-Fold êµì°¨ ê²€ì¦\n",
    "5. Optuna ìµœì í™”\n",
    "6. ì•™ìƒë¸” + TTA\n",
    "7. ì¶”ë¡  ìµœì í™”\n",
    "8. ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.025825Z",
     "iopub.status.busy": "2025-10-10T02:53:03.025717Z",
     "iopub.status.idle": "2025-10-10T02:53:03.295081Z",
     "shell.execute_reply": "2025-10-10T02:53:03.294653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_preprocessing] Status: running\n",
      "\n",
      "=== Data Preprocessing ===\n",
      "Preprocessed 12457 training samples\n",
      "Preprocessed 499 dev samples\n",
      "Preprocessed 499 test samples\n",
      "\n",
      "Text Length Statistics:\n",
      "  Dialogue - Mean: 347.3, Max: 1952\n",
      "  Summary - Mean: 85.8, Max: 376\n",
      "[data_preprocessing] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: ë°ì´í„° ì „ì²˜ë¦¬\n",
    "if 'data_preprocessing' in config['pipeline']['stages']:\n",
    "    update_status('data_preprocessing', 'running')\n",
    "    logger.write(\"\\n=== Data Preprocessing ===\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "    import re\n",
    "    \n",
    "    def preprocess_dialogue(text):\n",
    "        \"\"\"ëŒ€í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        # ë…¸ì´ì¦ˆ ì œê±°\n",
    "        text = text.replace('\\\\n', '\\n')\n",
    "        text = text.replace('<br>', '\\n')\n",
    "        text = text.strip()\n",
    "        \n",
    "        # #Person íƒœê·¸ ì •ê·œí™”\n",
    "        text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n",
    "        \n",
    "        # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_summary(text):\n",
    "        \"\"\"ìš”ì•½ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ ì ìš©\n",
    "    train_df['dialogue_preprocessed'] = train_df['dialogue'].apply(preprocess_dialogue)\n",
    "    train_df['summary_preprocessed'] = train_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    dev_df['dialogue_preprocessed'] = dev_df['dialogue'].apply(preprocess_dialogue)\n",
    "    dev_df['summary_preprocessed'] = dev_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    test_df['dialogue_preprocessed'] = test_df['dialogue'].apply(preprocess_dialogue)\n",
    "    \n",
    "    logger.write(f\"Preprocessed {len(train_df)} training samples\")\n",
    "    logger.write(f\"Preprocessed {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"Preprocessed {len(test_df)} test samples\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "    train_dialogue_lengths = train_df['dialogue_preprocessed'].str.len()\n",
    "    train_summary_lengths = train_df['summary_preprocessed'].str.len()\n",
    "    \n",
    "    logger.write(f\"\\nText Length Statistics:\")\n",
    "    logger.write(f\"  Dialogue - Mean: {train_dialogue_lengths.mean():.1f}, Max: {train_dialogue_lengths.max()}\")\n",
    "    logger.write(f\"  Summary - Mean: {train_summary_lengths.mean():.1f}, Max: {train_summary_lengths.max()}\")\n",
    "    \n",
    "    update_status('data_preprocessing', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.567069Z",
     "iopub.status.busy": "2025-10-10T02:53:03.566966Z",
     "iopub.status.idle": "2025-10-10T02:53:03.584546Z",
     "shell.execute_reply": "2025-10-10T02:53:03.584141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 20:06:36,982] A new study created in memory with name: pipeline_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optuna is available and will be used for hyperparameter optimization!\n",
      "[hyperparameter_optimization] Status: running\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ HYPERPARAMETER OPTIMIZATION STAGE\n",
      "======================================================================\n",
      "âœ… Optimization ENABLED with 100 trials\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ OPTUNA HYPERPARAMETER OPTIMIZATION STARTING\n",
      "============================================================\n",
      "Number of trials: 100\n",
      "Optimization metric: rouge_l\n",
      "\n",
      "ğŸš€ Starting optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab66c5892864d478cc940b080527031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0: {'learning_rate': 5.6115164153345e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 24, 'num_beams': 2, 'temperature': 0.8795585311974417, 'warmup_ratio': 0.12022300234864176, 'weight_decay': 0.07080725777960455, 'top_p': 0.8041168988591605}\n",
      "[I 2025-10-10 20:06:36,989] Trial 0 finished with value: 0.653403193335232 and parameters: {'learning_rate': 5.6115164153345e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 24, 'num_beams': 2, 'temperature': 0.8795585311974417, 'warmup_ratio': 0.12022300234864176, 'weight_decay': 0.07080725777960455, 'top_p': 0.8041168988591605}. Best is trial 0 with value: 0.653403193335232.\n",
      "\n",
      "Trial 1: {'learning_rate': 0.0008706020878304854, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 40, 'num_beams': 5, 'temperature': 0.48875051677790415, 'warmup_ratio': 0.058245828039608386, 'weight_decay': 0.06118528947223795, 'top_p': 0.8278987721304084}\n",
      "[I 2025-10-10 20:06:36,991] Trial 1 finished with value: 0.6842533872071003 and parameters: {'learning_rate': 0.0008706020878304854, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 40, 'num_beams': 5, 'temperature': 0.48875051677790415, 'warmup_ratio': 0.058245828039608386, 'weight_decay': 0.06118528947223795, 'top_p': 0.8278987721304084}. Best is trial 1 with value: 0.6842533872071003.\n",
      "\n",
      "Trial 2: {'learning_rate': 3.8396292998041685e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 72, 'num_beams': 6, 'temperature': 0.14180537144799796, 'warmup_ratio': 0.12150897038028768, 'weight_decay': 0.017052412368729154, 'top_p': 0.813010318597056}\n",
      "[I 2025-10-10 20:06:36,993] Trial 2 finished with value: 0.6078212758071612 and parameters: {'learning_rate': 3.8396292998041685e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 72, 'num_beams': 6, 'temperature': 0.14180537144799796, 'warmup_ratio': 0.12150897038028768, 'weight_decay': 0.017052412368729154, 'top_p': 0.813010318597056}. Best is trial 1 with value: 0.6842533872071003.\n",
      "\n",
      "Trial 3: {'learning_rate': 0.000790261954970823, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 88, 'num_beams': 5, 'temperature': 0.20983441136030095, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'top_p': 0.9818640804157565}\n",
      "[I 2025-10-10 20:06:36,995] Trial 3 finished with value: 0.6887265288736684 and parameters: {'learning_rate': 0.000790261954970823, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 88, 'num_beams': 5, 'temperature': 0.20983441136030095, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'top_p': 0.9818640804157565}. Best is trial 3 with value: 0.6887265288736684.\n",
      "\n",
      "Trial 4: {'learning_rate': 3.292759134423613e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.7976195410250031, 'warmup_ratio': 0.18789978831283782, 'weight_decay': 0.08948273504276488, 'top_p': 0.919579995762217}\n",
      "[I 2025-10-10 20:06:36,997] Trial 4 finished with value: 0.7028705250224668 and parameters: {'learning_rate': 3.292759134423613e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.7976195410250031, 'warmup_ratio': 0.18789978831283782, 'weight_decay': 0.08948273504276488, 'top_p': 0.919579995762217}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 5: {'learning_rate': 0.0006978281265126031, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 56, 'num_beams': 3, 'temperature': 0.8458637582367364, 'warmup_ratio': 0.07135066533871785, 'weight_decay': 0.02809345096873808, 'top_p': 0.9085392166316497}\n",
      "[I 2025-10-10 20:06:37,000] Trial 5 finished with value: 0.5479726511657692 and parameters: {'learning_rate': 0.0006978281265126031, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 56, 'num_beams': 3, 'temperature': 0.8458637582367364, 'warmup_ratio': 0.07135066533871785, 'weight_decay': 0.02809345096873808, 'top_p': 0.9085392166316497}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 6: {'learning_rate': 1.913588048769229e-05, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 32, 'num_beams': 2, 'temperature': 0.8339152856093507, 'warmup_ratio': 0.14137146876952342, 'weight_decay': 0.07290071680409874, 'top_p': 0.9542540693371891}\n",
      "[I 2025-10-10 20:06:37,001] Trial 6 finished with value: 0.48001959905641206 and parameters: {'learning_rate': 1.913588048769229e-05, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 32, 'num_beams': 2, 'temperature': 0.8339152856093507, 'warmup_ratio': 0.14137146876952342, 'weight_decay': 0.07290071680409874, 'top_p': 0.9542540693371891}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 7: {'learning_rate': 1.4063366777718176e-05, 'batch_size': 16, 'lora_r': 40, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.379884089544096, 'warmup_ratio': 0.06503666440534941, 'weight_decay': 0.0729606178338064, 'top_p': 0.9275114942710426}\n",
      "[I 2025-10-10 20:06:37,003] Trial 7 finished with value: 0.7119745405334588 and parameters: {'learning_rate': 1.4063366777718176e-05, 'batch_size': 16, 'lora_r': 40, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.379884089544096, 'warmup_ratio': 0.06503666440534941, 'weight_decay': 0.0729606178338064, 'top_p': 0.9275114942710426}. Best is trial 7 with value: 0.7119745405334588.\n",
      "\n",
      "Trial 8: {'learning_rate': 0.000594874681321977, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 72, 'num_beams': 7, 'temperature': 0.5444160367279517, 'warmup_ratio': 0.10454656587639882, 'weight_decay': 0.042754101835854964, 'top_p': 0.8050838253488191}\n",
      "[I 2025-10-10 20:06:37,005] Trial 8 finished with value: 0.5459338528853336 and parameters: {'learning_rate': 0.000594874681321977, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 72, 'num_beams': 7, 'temperature': 0.5444160367279517, 'warmup_ratio': 0.10454656587639882, 'weight_decay': 0.042754101835854964, 'top_p': 0.8050838253488191}. Best is trial 7 with value: 0.7119745405334588.\n",
      "\n",
      "Trial 9: {'learning_rate': 1.6435497475111308e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 120, 'num_beams': 3, 'temperature': 0.4693446307320668, 'warmup_ratio': 0.15111022770860974, 'weight_decay': 0.022879816549162248, 'top_p': 0.8153959819657587}\n",
      "[I 2025-10-10 20:06:37,007] Trial 9 finished with value: 0.8373849124158405 and parameters: {'learning_rate': 1.6435497475111308e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 120, 'num_beams': 3, 'temperature': 0.4693446307320668, 'warmup_ratio': 0.15111022770860974, 'weight_decay': 0.022879816549162248, 'top_p': 0.8153959819657587}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 10: {'learning_rate': 0.0002509538254778771, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.37050834074441147, 'warmup_ratio': 0.0031189599316912286, 'weight_decay': 0.03924528921026325, 'top_p': 0.8669590352446472}\n",
      "[I 2025-10-10 20:06:37,015] Trial 10 finished with value: 0.6452920710377574 and parameters: {'learning_rate': 0.0002509538254778771, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.37050834074441147, 'warmup_ratio': 0.0031189599316912286, 'weight_decay': 0.03924528921026325, 'top_p': 0.8669590352446472}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 11: {'learning_rate': 1.0322960416065201e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.3360442409320184, 'warmup_ratio': 0.18218996750099392, 'weight_decay': 0.09435438058122501, 'top_p': 0.8647628938861509}\n",
      "[I 2025-10-10 20:06:37,022] Trial 11 finished with value: 0.7495056799312688 and parameters: {'learning_rate': 1.0322960416065201e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.3360442409320184, 'warmup_ratio': 0.18218996750099392, 'weight_decay': 0.09435438058122501, 'top_p': 0.8647628938861509}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 12: {'learning_rate': 1.0370579352014641e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6804495294564787, 'warmup_ratio': 0.19775497252335106, 'weight_decay': 0.09904235739514478, 'top_p': 0.8590398672080966}\n",
      "[I 2025-10-10 20:06:37,030] Trial 12 finished with value: 0.6109526820744092 and parameters: {'learning_rate': 1.0370579352014641e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6804495294564787, 'warmup_ratio': 0.19775497252335106, 'weight_decay': 0.09904235739514478, 'top_p': 0.8590398672080966}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 13: {'learning_rate': 0.00012774135799436964, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.3022059308213879, 'warmup_ratio': 0.16191281769380567, 'weight_decay': 0.01896901131981868, 'top_p': 0.8593943414843724}\n",
      "[I 2025-10-10 20:06:37,036] Trial 13 finished with value: 0.49933244699373697 and parameters: {'learning_rate': 0.00012774135799436964, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.3022059308213879, 'warmup_ratio': 0.16191281769380567, 'weight_decay': 0.01896901131981868, 'top_p': 0.8593943414843724}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 14: {'learning_rate': 2.1533663334390727e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6503220664418476, 'warmup_ratio': 0.16274515950801005, 'weight_decay': 0.05166548565180652, 'top_p': 0.8830054677342667}\n",
      "[I 2025-10-10 20:06:37,043] Trial 14 finished with value: 0.8416676785296207 and parameters: {'learning_rate': 2.1533663334390727e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6503220664418476, 'warmup_ratio': 0.16274515950801005, 'weight_decay': 0.05166548565180652, 'top_p': 0.8830054677342667}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 15: {'learning_rate': 2.4165483913864722e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6636798035918531, 'warmup_ratio': 0.157051761228143, 'weight_decay': 0.0006794582733295057, 'top_p': 0.8360378813543303}\n",
      "[I 2025-10-10 20:06:37,050] Trial 15 finished with value: 0.7322748631864108 and parameters: {'learning_rate': 2.4165483913864722e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6636798035918531, 'warmup_ratio': 0.157051761228143, 'weight_decay': 0.0006794582733295057, 'top_p': 0.8360378813543303}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 16: {'learning_rate': 6.956746038661218e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.6784268911829191, 'warmup_ratio': 0.16304971347740554, 'weight_decay': 0.055188970463428993, 'top_p': 0.8940003385967338}\n",
      "[I 2025-10-10 20:06:37,056] Trial 16 finished with value: 0.5115506843922398 and parameters: {'learning_rate': 6.956746038661218e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.6784268911829191, 'warmup_ratio': 0.16304971347740554, 'weight_decay': 0.055188970463428993, 'top_p': 0.8940003385967338}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 17: {'learning_rate': 0.00013738198354053957, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 88, 'num_beams': 4, 'temperature': 0.995123861989222, 'warmup_ratio': 0.14050981032764043, 'weight_decay': 0.033282416286616795, 'top_p': 0.8866571662960941}\n",
      "[I 2025-10-10 20:06:37,064] Trial 17 finished with value: 0.7625392949577587 and parameters: {'learning_rate': 0.00013738198354053957, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 88, 'num_beams': 4, 'temperature': 0.995123861989222, 'warmup_ratio': 0.14050981032764043, 'weight_decay': 0.033282416286616795, 'top_p': 0.8866571662960941}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 18: {'learning_rate': 2.565570347466517e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 3, 'temperature': 0.45267481671743476, 'warmup_ratio': 0.1729760059402413, 'weight_decay': 0.04828486464613923, 'top_p': 0.8379315845736517}\n",
      "[I 2025-10-10 20:06:37,071] Trial 18 finished with value: 0.5631407779156049 and parameters: {'learning_rate': 2.565570347466517e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 3, 'temperature': 0.45267481671743476, 'warmup_ratio': 0.1729760059402413, 'weight_decay': 0.04828486464613923, 'top_p': 0.8379315845736517}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 19: {'learning_rate': 5.657396087817041e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 112, 'num_beams': 5, 'temperature': 0.6005475021920503, 'warmup_ratio': 0.14473687452448478, 'weight_decay': 0.02169838469202746, 'top_p': 0.9430718175625277}\n",
      "[I 2025-10-10 20:06:37,078] Trial 19 finished with value: 0.5386437335137281 and parameters: {'learning_rate': 5.657396087817041e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 112, 'num_beams': 5, 'temperature': 0.6005475021920503, 'warmup_ratio': 0.14473687452448478, 'weight_decay': 0.02169838469202746, 'top_p': 0.9430718175625277}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 20: {'learning_rate': 1.8438882376111873e-05, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7497199490525606, 'warmup_ratio': 0.027753987886119755, 'weight_decay': 0.05718553129235758, 'top_p': 0.9987685570099417}\n",
      "[I 2025-10-10 20:06:37,085] Trial 20 finished with value: 0.7653741684030027 and parameters: {'learning_rate': 1.8438882376111873e-05, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7497199490525606, 'warmup_ratio': 0.027753987886119755, 'weight_decay': 0.05718553129235758, 'top_p': 0.9987685570099417}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 21: {'learning_rate': 1.683848556284197e-05, 'batch_size': 8, 'lora_r': 60, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7468655799630235, 'warmup_ratio': 0.0014942070483525624, 'weight_decay': 0.061836192221023066, 'top_p': 0.9757254013166067}\n",
      "[I 2025-10-10 20:06:37,093] Trial 21 finished with value: 0.615050029279845 and parameters: {'learning_rate': 1.683848556284197e-05, 'batch_size': 8, 'lora_r': 60, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7468655799630235, 'warmup_ratio': 0.0014942070483525624, 'weight_decay': 0.061836192221023066, 'top_p': 0.9757254013166067}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 22: {'learning_rate': 3.127131627807907e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5957128359321127, 'warmup_ratio': 0.03063050371447289, 'weight_decay': 0.08208672433681731, 'top_p': 0.994075960980361}\n",
      "[I 2025-10-10 20:06:37,098] Trial 22 finished with value: 0.8290308469847495 and parameters: {'learning_rate': 3.127131627807907e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5957128359321127, 'warmup_ratio': 0.03063050371447289, 'weight_decay': 0.08208672433681731, 'top_p': 0.994075960980361}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 23: {'learning_rate': 3.90539962849963e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.5873167389465909, 'warmup_ratio': 0.030803275149133752, 'weight_decay': 0.08148272405058854, 'top_p': 0.8805783124166526}\n",
      "[I 2025-10-10 20:06:37,104] Trial 23 finished with value: 0.7133210726638556 and parameters: {'learning_rate': 3.90539962849963e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.5873167389465909, 'warmup_ratio': 0.030803275149133752, 'weight_decay': 0.08148272405058854, 'top_p': 0.8805783124166526}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 24: {'learning_rate': 8.403991127384985e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.4710274858116029, 'warmup_ratio': 0.1033660521394493, 'weight_decay': 0.012169455502476813, 'top_p': 0.9424558555729028}\n",
      "[I 2025-10-10 20:06:37,110] Trial 24 finished with value: 0.5989579270947828 and parameters: {'learning_rate': 8.403991127384985e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.4710274858116029, 'warmup_ratio': 0.1033660521394493, 'weight_decay': 0.012169455502476813, 'top_p': 0.9424558555729028}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 25: {'learning_rate': 2.452510912989695e-05, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5302243724466196, 'warmup_ratio': 0.0804800128901909, 'weight_decay': 0.08487682398674518, 'top_p': 0.9100897473891826}\n",
      "[I 2025-10-10 20:06:37,116] Trial 25 finished with value: 0.6655383459914141 and parameters: {'learning_rate': 2.452510912989695e-05, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5302243724466196, 'warmup_ratio': 0.0804800128901909, 'weight_decay': 0.08487682398674518, 'top_p': 0.9100897473891826}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 26: {'learning_rate': 1.2739385933835197e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.6350284049381522, 'warmup_ratio': 0.12181876304438032, 'weight_decay': 0.029880663607875134, 'top_p': 0.9640723701100812}\n",
      "[I 2025-10-10 20:06:37,121] Trial 26 finished with value: 0.6501772011282219 and parameters: {'learning_rate': 1.2739385933835197e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.6350284049381522, 'warmup_ratio': 0.12181876304438032, 'weight_decay': 0.029880663607875134, 'top_p': 0.9640723701100812}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 27: {'learning_rate': 3.5610140632963465e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.42776666328018165, 'warmup_ratio': 0.04048265469146311, 'weight_decay': 0.046029485305756374, 'top_p': 0.8254055086438484}\n",
      "[I 2025-10-10 20:06:37,128] Trial 27 finished with value: 0.7689657061813346 and parameters: {'learning_rate': 3.5610140632963465e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.42776666328018165, 'warmup_ratio': 0.04048265469146311, 'weight_decay': 0.046029485305756374, 'top_p': 0.8254055086438484}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 28: {'learning_rate': 0.0002090316068728911, 'batch_size': 4, 'lora_r': 40, 'lora_alpha': 64, 'num_beams': 3, 'temperature': 0.25976266522502295, 'warmup_ratio': 0.08862058456303999, 'weight_decay': 0.03602600827614375, 'top_p': 0.8502931757834995}\n",
      "[I 2025-10-10 20:06:37,134] Trial 28 finished with value: 0.616605540966958 and parameters: {'learning_rate': 0.0002090316068728911, 'batch_size': 4, 'lora_r': 40, 'lora_alpha': 64, 'num_beams': 3, 'temperature': 0.25976266522502295, 'warmup_ratio': 0.08862058456303999, 'weight_decay': 0.03602600827614375, 'top_p': 0.8502931757834995}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 29: {'learning_rate': 5.342037044887353e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 80, 'num_beams': 2, 'temperature': 0.9306004540295836, 'warmup_ratio': 0.13067292225107183, 'weight_decay': 0.0809760169872833, 'top_p': 0.9995237949195853}\n",
      "[I 2025-10-10 20:06:37,140] Trial 29 finished with value: 0.49841995360926694 and parameters: {'learning_rate': 5.342037044887353e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 80, 'num_beams': 2, 'temperature': 0.9306004540295836, 'warmup_ratio': 0.13067292225107183, 'weight_decay': 0.0809760169872833, 'top_p': 0.9995237949195853}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 30: {'learning_rate': 5.572499750139758e-05, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.7275267954460478, 'warmup_ratio': 0.14961198398797018, 'weight_decay': 0.008934880367805834, 'top_p': 0.9274995338850026}\n",
      "[I 2025-10-10 20:06:37,148] Trial 30 finished with value: 0.658446634628153 and parameters: {'learning_rate': 5.572499750139758e-05, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.7275267954460478, 'warmup_ratio': 0.14961198398797018, 'weight_decay': 0.008934880367805834, 'top_p': 0.9274995338850026}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 31: {'learning_rate': 3.094824177427026e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.41434230737368083, 'warmup_ratio': 0.04511356346602678, 'weight_decay': 0.04731037182895204, 'top_p': 0.8211646027005765}\n",
      "[I 2025-10-10 20:06:37,155] Trial 31 finished with value: 0.7680584276268067 and parameters: {'learning_rate': 3.094824177427026e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.41434230737368083, 'warmup_ratio': 0.04511356346602678, 'weight_decay': 0.04731037182895204, 'top_p': 0.8211646027005765}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 32: {'learning_rate': 4.127644122189065e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.5382740247050618, 'warmup_ratio': 0.04862924515974718, 'weight_decay': 0.06353192693130189, 'top_p': 0.8001964558156949}\n",
      "[I 2025-10-10 20:06:37,160] Trial 32 finished with value: 0.7167457775728922 and parameters: {'learning_rate': 4.127644122189065e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.5382740247050618, 'warmup_ratio': 0.04862924515974718, 'weight_decay': 0.06353192693130189, 'top_p': 0.8001964558156949}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 33: {'learning_rate': 1.921628288305131e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.49703807688182705, 'warmup_ratio': 0.031105511208068377, 'weight_decay': 0.06726500545244228, 'top_p': 0.8431558325184365}\n",
      "[I 2025-10-10 20:06:37,166] Trial 33 finished with value: 0.668273128521275 and parameters: {'learning_rate': 1.921628288305131e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.49703807688182705, 'warmup_ratio': 0.031105511208068377, 'weight_decay': 0.06726500545244228, 'top_p': 0.8431558325184365}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 34: {'learning_rate': 3.092169966321551e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.43283343513352945, 'warmup_ratio': 0.020633958420788447, 'weight_decay': 0.025344039838279056, 'top_p': 0.8220539492156288}\n",
      "[I 2025-10-10 20:06:37,173] Trial 34 finished with value: 0.6113030204140464 and parameters: {'learning_rate': 3.092169966321551e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.43283343513352945, 'warmup_ratio': 0.020633958420788447, 'weight_decay': 0.025344039838279056, 'top_p': 0.8220539492156288}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 35: {'learning_rate': 1.317706001964015e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 96, 'num_beams': 4, 'temperature': 0.6018640608902907, 'warmup_ratio': 0.051374024631548235, 'weight_decay': 0.05325989793124686, 'top_p': 0.8140386624198233}\n",
      "[I 2025-10-10 20:06:37,179] Trial 35 finished with value: 0.7599608225372065 and parameters: {'learning_rate': 1.317706001964015e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 96, 'num_beams': 4, 'temperature': 0.6018640608902907, 'warmup_ratio': 0.051374024631548235, 'weight_decay': 0.05325989793124686, 'top_p': 0.8140386624198233}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 36: {'learning_rate': 4.706669513733055e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.15149894820208998, 'warmup_ratio': 0.17540680707153258, 'weight_decay': 0.03994122947087525, 'top_p': 0.8776955612686084}\n",
      "[I 2025-10-10 20:06:37,186] Trial 36 finished with value: 0.7811298262988348 and parameters: {'learning_rate': 4.706669513733055e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.15149894820208998, 'warmup_ratio': 0.17540680707153258, 'weight_decay': 0.03994122947087525, 'top_p': 0.8776955612686084}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 37: {'learning_rate': 4.7657558622432465e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.18316925433715736, 'warmup_ratio': 0.18612453092975198, 'weight_decay': 0.04000326691864714, 'top_p': 0.8752511701111695}\n",
      "[I 2025-10-10 20:06:37,193] Trial 37 finished with value: 0.669811550399135 and parameters: {'learning_rate': 4.7657558622432465e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.18316925433715736, 'warmup_ratio': 0.18612453092975198, 'weight_decay': 0.04000326691864714, 'top_p': 0.8752511701111695}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 38: {'learning_rate': 2.4205817609308327e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.1587345254754327, 'warmup_ratio': 0.17574815541512825, 'weight_decay': 0.013110143608165236, 'top_p': 0.8974295447028237}\n",
      "[I 2025-10-10 20:06:37,200] Trial 38 finished with value: 0.8309752604412289 and parameters: {'learning_rate': 2.4205817609308327e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.1587345254754327, 'warmup_ratio': 0.17574815541512825, 'weight_decay': 0.013110143608165236, 'top_p': 0.8974295447028237}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 39: {'learning_rate': 2.531494835284367e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 80, 'num_beams': 7, 'temperature': 0.23161393776092942, 'warmup_ratio': 0.11376353997362912, 'weight_decay': 0.012787574144916825, 'top_p': 0.9019753053266991}\n",
      "[I 2025-10-10 20:06:37,206] Trial 39 finished with value: 0.671400635296036 and parameters: {'learning_rate': 2.531494835284367e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 80, 'num_beams': 7, 'temperature': 0.23161393776092942, 'warmup_ratio': 0.11376353997362912, 'weight_decay': 0.012787574144916825, 'top_p': 0.9019753053266991}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 40: {'learning_rate': 1.6873896735711707e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 104, 'num_beams': 7, 'temperature': 0.12101077392936, 'warmup_ratio': 0.1946448262091431, 'weight_decay': 0.021797660594574973, 'top_p': 0.921367729376461}\n",
      "[I 2025-10-10 20:06:37,213] Trial 40 finished with value: 0.7123029108910641 and parameters: {'learning_rate': 1.6873896735711707e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 104, 'num_beams': 7, 'temperature': 0.12101077392936, 'warmup_ratio': 0.1946448262091431, 'weight_decay': 0.021797660594574973, 'top_p': 0.921367729376461}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 41: {'learning_rate': 2.016990986416263e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.16781736823297833, 'warmup_ratio': 0.1719144766376781, 'weight_decay': 0.006838893299693837, 'top_p': 0.8899899147100928}\n",
      "[I 2025-10-10 20:06:37,220] Trial 41 finished with value: 0.6754549053125105 and parameters: {'learning_rate': 2.016990986416263e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.16781736823297833, 'warmup_ratio': 0.1719144766376781, 'weight_decay': 0.006838893299693837, 'top_p': 0.8899899147100928}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 42: {'learning_rate': 7.315317123213242e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 6, 'temperature': 0.10898373397063726, 'warmup_ratio': 0.17666476815831086, 'weight_decay': 0.015121316840524262, 'top_p': 0.9082635108940528}\n",
      "[I 2025-10-10 20:06:37,228] Trial 42 finished with value: 0.593665197366266 and parameters: {'learning_rate': 7.315317123213242e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 6, 'temperature': 0.10898373397063726, 'warmup_ratio': 0.17666476815831086, 'weight_decay': 0.015121316840524262, 'top_p': 0.9082635108940528}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 43: {'learning_rate': 2.987550564700592e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.27229653218647604, 'warmup_ratio': 0.1312614772306692, 'weight_decay': 0.03149393197467222, 'top_p': 0.8717366962457426}\n",
      "[I 2025-10-10 20:06:37,234] Trial 43 finished with value: 0.6614014156634058 and parameters: {'learning_rate': 2.987550564700592e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.27229653218647604, 'warmup_ratio': 0.1312614772306692, 'weight_decay': 0.03149393197467222, 'top_p': 0.8717366962457426}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 44: {'learning_rate': 1.4183533306486784e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.3330280784857712, 'warmup_ratio': 0.16834508714623803, 'weight_decay': 0.07362506042898428, 'top_p': 0.9359224673765495}\n",
      "[I 2025-10-10 20:06:37,240] Trial 44 finished with value: 0.7647816847407728 and parameters: {'learning_rate': 1.4183533306486784e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.3330280784857712, 'warmup_ratio': 0.16834508714623803, 'weight_decay': 0.07362506042898428, 'top_p': 0.9359224673765495}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 45: {'learning_rate': 2.2125857907120275e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 120, 'num_beams': 5, 'temperature': 0.19234876816197186, 'warmup_ratio': 0.1564552423218515, 'weight_decay': 0.025485267457427647, 'top_p': 0.8968008735475811}\n",
      "[I 2025-10-10 20:06:37,247] Trial 45 finished with value: 0.7686591279622612 and parameters: {'learning_rate': 2.2125857907120275e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 120, 'num_beams': 5, 'temperature': 0.19234876816197186, 'warmup_ratio': 0.1564552423218515, 'weight_decay': 0.025485267457427647, 'top_p': 0.8968008735475811}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 46: {'learning_rate': 4.410553554527763e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.7963122850242701, 'warmup_ratio': 0.18189387147264963, 'weight_decay': 0.03622538665091601, 'top_p': 0.8545578630975141}\n",
      "[I 2025-10-10 20:06:37,253] Trial 46 finished with value: 0.6617494841377299 and parameters: {'learning_rate': 4.410553554527763e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.7963122850242701, 'warmup_ratio': 0.18189387147264963, 'weight_decay': 0.03622538665091601, 'top_p': 0.8545578630975141}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 47: {'learning_rate': 2.8793297545562394e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 128, 'num_beams': 7, 'temperature': 0.14441639986086824, 'warmup_ratio': 0.19304852996515295, 'weight_decay': 0.0024125324630044215, 'top_p': 0.8845632607966017}\n",
      "[I 2025-10-10 20:06:37,260] Trial 47 finished with value: 0.646539007154649 and parameters: {'learning_rate': 2.8793297545562394e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 128, 'num_beams': 7, 'temperature': 0.14441639986086824, 'warmup_ratio': 0.19304852996515295, 'weight_decay': 0.0024125324630044215, 'top_p': 0.8845632607966017}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 48: {'learning_rate': 0.0001106314283459816, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.227509281192289, 'warmup_ratio': 0.152254905249286, 'weight_decay': 0.04371339282623083, 'top_p': 0.91427665181948}\n",
      "[I 2025-10-10 20:06:37,266] Trial 48 finished with value: 0.6874951210991229 and parameters: {'learning_rate': 0.0001106314283459816, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.227509281192289, 'warmup_ratio': 0.152254905249286, 'weight_decay': 0.04371339282623083, 'top_p': 0.91427665181948}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 49: {'learning_rate': 1.0218175796834212e-05, 'batch_size': 16, 'lora_r': 44, 'lora_alpha': 80, 'num_beams': 5, 'temperature': 0.5017014353557825, 'warmup_ratio': 0.16418201035248006, 'weight_decay': 0.018055373676510417, 'top_p': 0.9032601502373351}\n",
      "[I 2025-10-10 20:06:37,272] Trial 49 finished with value: 0.6503413789046097 and parameters: {'learning_rate': 1.0218175796834212e-05, 'batch_size': 16, 'lora_r': 44, 'lora_alpha': 80, 'num_beams': 5, 'temperature': 0.5017014353557825, 'warmup_ratio': 0.16418201035248006, 'weight_decay': 0.018055373676510417, 'top_p': 0.9032601502373351}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 50: {'learning_rate': 1.4733364262930062e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.7156901210138211, 'warmup_ratio': 0.14017765897144893, 'weight_decay': 0.09966547351713763, 'top_p': 0.9552864405280675}\n",
      "[I 2025-10-10 20:06:37,280] Trial 50 finished with value: 0.7059234659644308 and parameters: {'learning_rate': 1.4733364262930062e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.7156901210138211, 'warmup_ratio': 0.14017765897144893, 'weight_decay': 0.09966547351713763, 'top_p': 0.9552864405280675}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 51: {'learning_rate': 3.4749287491699306e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.5804493544071787, 'warmup_ratio': 0.06462677823093144, 'weight_decay': 0.04525628475662426, 'top_p': 0.8295050470927703}\n",
      "[I 2025-10-10 20:06:37,286] Trial 51 finished with value: 0.6917989823441673 and parameters: {'learning_rate': 3.4749287491699306e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.5804493544071787, 'warmup_ratio': 0.06462677823093144, 'weight_decay': 0.04525628475662426, 'top_p': 0.8295050470927703}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 52: {'learning_rate': 3.709539583911614e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.37166339692151296, 'warmup_ratio': 0.011684432550358836, 'weight_decay': 0.05264710807566457, 'top_p': 0.8107728263361236}\n",
      "[I 2025-10-10 20:06:37,292] Trial 52 finished with value: 0.6052703449157459 and parameters: {'learning_rate': 3.709539583911614e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.37166339692151296, 'warmup_ratio': 0.011684432550358836, 'weight_decay': 0.05264710807566457, 'top_p': 0.8107728263361236}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 53: {'learning_rate': 2.2116471515291207e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6427941433608708, 'warmup_ratio': 0.037550236523587396, 'weight_decay': 0.05681344701573311, 'top_p': 0.8469543460207246}\n",
      "[I 2025-10-10 20:06:37,300] Trial 53 finished with value: 0.7706885939056862 and parameters: {'learning_rate': 2.2116471515291207e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6427941433608708, 'warmup_ratio': 0.037550236523587396, 'weight_decay': 0.05681344701573311, 'top_p': 0.8469543460207246}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 54: {'learning_rate': 2.0602507190704607e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.6590026610203907, 'warmup_ratio': 0.1783392489343954, 'weight_decay': 0.07002797498285046, 'top_p': 0.8642848146742853}\n",
      "[I 2025-10-10 20:06:37,307] Trial 54 finished with value: 0.615438913588359 and parameters: {'learning_rate': 2.0602507190704607e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.6590026610203907, 'warmup_ratio': 0.1783392489343954, 'weight_decay': 0.07002797498285046, 'top_p': 0.8642848146742853}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 55: {'learning_rate': 1.6153222842386155e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.6294595126891122, 'warmup_ratio': 0.19905541465294616, 'weight_decay': 0.09340740175674117, 'top_p': 0.876193610295522}\n",
      "[I 2025-10-10 20:06:37,314] Trial 55 finished with value: 0.5550593411510581 and parameters: {'learning_rate': 1.6153222842386155e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.6294595126891122, 'warmup_ratio': 0.19905541465294616, 'weight_decay': 0.09340740175674117, 'top_p': 0.876193610295522}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 56: {'learning_rate': 1.240977432071961e-05, 'batch_size': 8, 'lora_r': 56, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.7843024391422477, 'warmup_ratio': 0.012859854671353824, 'weight_decay': 0.07712187730513362, 'top_p': 0.8481411504106939}\n",
      "[I 2025-10-10 20:06:37,320] Trial 56 finished with value: 0.6983509221702764 and parameters: {'learning_rate': 1.240977432071961e-05, 'batch_size': 8, 'lora_r': 56, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.7843024391422477, 'warmup_ratio': 0.012859854671353824, 'weight_decay': 0.07712187730513362, 'top_p': 0.8481411504106939}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 57: {'learning_rate': 2.701752798859616e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.7004057497694031, 'warmup_ratio': 0.07889302920314219, 'weight_decay': 0.050980467678198305, 'top_p': 0.8649055419818599}\n",
      "[I 2025-10-10 20:06:37,327] Trial 57 finished with value: 0.846444295089657 and parameters: {'learning_rate': 2.701752798859616e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.7004057497694031, 'warmup_ratio': 0.07889302920314219, 'weight_decay': 0.050980467678198305, 'top_p': 0.8649055419818599}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 58: {'learning_rate': 0.00046780959898243686, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 40, 'num_beams': 8, 'temperature': 0.6941039570881306, 'warmup_ratio': 0.09462173181517183, 'weight_decay': 0.04973904900065393, 'top_p': 0.8921902459743193}\n",
      "[I 2025-10-10 20:06:37,334] Trial 58 finished with value: 0.6129106707328406 and parameters: {'learning_rate': 0.00046780959898243686, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 40, 'num_beams': 8, 'temperature': 0.6941039570881306, 'warmup_ratio': 0.09462173181517183, 'weight_decay': 0.04973904900065393, 'top_p': 0.8921902459743193}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 59: {'learning_rate': 2.7552780289527886e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.5551481428755193, 'warmup_ratio': 0.07806911127120207, 'weight_decay': 0.03985403218649806, 'top_p': 0.882326120791791}\n",
      "[I 2025-10-10 20:06:37,341] Trial 59 finished with value: 0.8215860656194957 and parameters: {'learning_rate': 2.7552780289527886e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.5551481428755193, 'warmup_ratio': 0.07806911127120207, 'weight_decay': 0.03985403218649806, 'top_p': 0.882326120791791}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 60: {'learning_rate': 2.522156294909442e-05, 'batch_size': 8, 'lora_r': 8, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.574282276871496, 'warmup_ratio': 0.07785966373611808, 'weight_decay': 0.05998144965629997, 'top_p': 0.8689683785783211}\n",
      "[I 2025-10-10 20:06:37,348] Trial 60 finished with value: 0.6199772954282684 and parameters: {'learning_rate': 2.522156294909442e-05, 'batch_size': 8, 'lora_r': 8, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.574282276871496, 'warmup_ratio': 0.07785966373611808, 'weight_decay': 0.05998144965629997, 'top_p': 0.8689683785783211}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 61: {'learning_rate': 2.7556704908524016e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.8503731063778148, 'warmup_ratio': 0.06780992918089138, 'weight_decay': 0.04151060115765849, 'top_p': 0.8804624729548008}\n",
      "[I 2025-10-10 20:06:37,355] Trial 61 finished with value: 0.6633055211161557 and parameters: {'learning_rate': 2.7556704908524016e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.8503731063778148, 'warmup_ratio': 0.06780992918089138, 'weight_decay': 0.04151060115765849, 'top_p': 0.8804624729548008}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 62: {'learning_rate': 6.507558563298224e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.6219690558816099, 'warmup_ratio': 0.05844252858732275, 'weight_decay': 0.03663431449996464, 'top_p': 0.8626240918018101}\n",
      "[I 2025-10-10 20:06:37,361] Trial 62 finished with value: 0.5474271593473461 and parameters: {'learning_rate': 6.507558563298224e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.6219690558816099, 'warmup_ratio': 0.05844252858732275, 'weight_decay': 0.03663431449996464, 'top_p': 0.8626240918018101}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 63: {'learning_rate': 4.5725233374529764e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5580159350790999, 'warmup_ratio': 0.11082473016769612, 'weight_decay': 0.026785425261978142, 'top_p': 0.8846400400556139}\n",
      "[I 2025-10-10 20:06:37,368] Trial 63 finished with value: 0.8541734747754074 and parameters: {'learning_rate': 4.5725233374529764e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5580159350790999, 'warmup_ratio': 0.11082473016769612, 'weight_decay': 0.026785425261978142, 'top_p': 0.8846400400556139}. Best is trial 63 with value: 0.8541734747754074.\n",
      "\n",
      "Trial 64: {'learning_rate': 0.0009841670344314273, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5139044376055386, 'warmup_ratio': 0.08118173588223437, 'weight_decay': 0.026851414418151847, 'top_p': 0.9882704605407188}\n",
      "[I 2025-10-10 20:06:37,376] Trial 64 finished with value: 0.6481389396761073 and parameters: {'learning_rate': 0.0009841670344314273, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5139044376055386, 'warmup_ratio': 0.08118173588223437, 'weight_decay': 0.026851414418151847, 'top_p': 0.9882704605407188}. Best is trial 63 with value: 0.8541734747754074.\n",
      "\n",
      "Trial 65: {'learning_rate': 1.677379500013992e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4720398360237838, 'warmup_ratio': 0.11154008727367384, 'weight_decay': 0.022201193441123306, 'top_p': 0.8864128918251735}\n",
      "[I 2025-10-10 20:06:37,382] Trial 65 finished with value: 0.8618116332851581 and parameters: {'learning_rate': 1.677379500013992e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4720398360237838, 'warmup_ratio': 0.11154008727367384, 'weight_decay': 0.022201193441123306, 'top_p': 0.8864128918251735}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 66: {'learning_rate': 1.1805094925833307e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.45784145810741744, 'warmup_ratio': 0.11609704954088851, 'weight_decay': 0.021540806414871373, 'top_p': 0.8885106166288887}\n",
      "[I 2025-10-10 20:06:37,390] Trial 66 finished with value: 0.8145732866004118 and parameters: {'learning_rate': 1.1805094925833307e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.45784145810741744, 'warmup_ratio': 0.11609704954088851, 'weight_decay': 0.021540806414871373, 'top_p': 0.8885106166288887}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 67: {'learning_rate': 1.7219427754068686e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.7583678305641154, 'warmup_ratio': 0.13378523527591568, 'weight_decay': 0.009268606046102927, 'top_p': 0.8994189389655497}\n",
      "[I 2025-10-10 20:06:37,398] Trial 67 finished with value: 0.5434978644571928 and parameters: {'learning_rate': 1.7219427754068686e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.7583678305641154, 'warmup_ratio': 0.13378523527591568, 'weight_decay': 0.009268606046102927, 'top_p': 0.8994189389655497}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 68: {'learning_rate': 2.227458918150894e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4032573606932476, 'warmup_ratio': 0.10944598496589243, 'weight_decay': 0.0054989837204231715, 'top_p': 0.9166081492895803}\n",
      "[I 2025-10-10 20:06:37,405] Trial 68 finished with value: 0.8682475413732235 and parameters: {'learning_rate': 2.227458918150894e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4032573606932476, 'warmup_ratio': 0.10944598496589243, 'weight_decay': 0.0054989837204231715, 'top_p': 0.9166081492895803}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 69: {'learning_rate': 1.500126697452724e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4034661117946926, 'warmup_ratio': 0.10510925206548839, 'weight_decay': 0.004912847433152478, 'top_p': 0.9248114000175955}\n",
      "[I 2025-10-10 20:06:37,413] Trial 69 finished with value: 0.8324010335731041 and parameters: {'learning_rate': 1.500126697452724e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4034661117946926, 'warmup_ratio': 0.10510925206548839, 'weight_decay': 0.004912847433152478, 'top_p': 0.9248114000175955}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 70: {'learning_rate': 1.5400439475960335e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3901632296243603, 'warmup_ratio': 0.10580681222644418, 'weight_decay': 0.005397124106637472, 'top_p': 0.9304083613930098}\n",
      "[I 2025-10-10 20:06:37,419] Trial 70 finished with value: 0.6090767438766573 and parameters: {'learning_rate': 1.5400439475960335e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3901632296243603, 'warmup_ratio': 0.10580681222644418, 'weight_decay': 0.005397124106637472, 'top_p': 0.9304083613930098}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 71: {'learning_rate': 1.828294303807379e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3474546763851418, 'warmup_ratio': 0.0934595482583817, 'weight_decay': 0.014607022987833684, 'top_p': 0.9200236760389789}\n",
      "[I 2025-10-10 20:06:37,426] Trial 71 finished with value: 0.873550233291343 and parameters: {'learning_rate': 1.828294303807379e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3474546763851418, 'warmup_ratio': 0.0934595482583817, 'weight_decay': 0.014607022987833684, 'top_p': 0.9200236760389789}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 72: {'learning_rate': 1.1435810914716776e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.33201876993592494, 'warmup_ratio': 0.09525746034919104, 'weight_decay': 0.0013933224834431959, 'top_p': 0.920183215162878}\n",
      "[I 2025-10-10 20:06:37,433] Trial 72 finished with value: 0.6797362815296961 and parameters: {'learning_rate': 1.1435810914716776e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.33201876993592494, 'warmup_ratio': 0.09525746034919104, 'weight_decay': 0.0013933224834431959, 'top_p': 0.920183215162878}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 73: {'learning_rate': 1.9771826397832063e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40981012847367354, 'warmup_ratio': 0.10739268597925176, 'weight_decay': 0.0095385904705074, 'top_p': 0.9152595587510741}\n",
      "[I 2025-10-10 20:06:37,440] Trial 73 finished with value: 0.7610726631791968 and parameters: {'learning_rate': 1.9771826397832063e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40981012847367354, 'warmup_ratio': 0.10739268597925176, 'weight_decay': 0.0095385904705074, 'top_p': 0.9152595587510741}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 74: {'learning_rate': 1.8143008517456992e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.47684162984530515, 'warmup_ratio': 0.12560327793078482, 'weight_decay': 0.019995904459648595, 'top_p': 0.9074294266770475}\n",
      "[I 2025-10-10 20:06:37,447] Trial 74 finished with value: 0.6125341537748623 and parameters: {'learning_rate': 1.8143008517456992e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.47684162984530515, 'warmup_ratio': 0.12560327793078482, 'weight_decay': 0.019995904459648595, 'top_p': 0.9074294266770475}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 75: {'learning_rate': 1.4906103445745445e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.43618972460521527, 'warmup_ratio': 0.08814925950702149, 'weight_decay': 7.35595467084892e-05, 'top_p': 0.9256834318901477}\n",
      "[I 2025-10-10 20:06:37,454] Trial 75 finished with value: 0.690294612898219 and parameters: {'learning_rate': 1.4906103445745445e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.43618972460521527, 'warmup_ratio': 0.08814925950702149, 'weight_decay': 7.35595467084892e-05, 'top_p': 0.9256834318901477}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 76: {'learning_rate': 2.208400972446465e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40035389978847513, 'warmup_ratio': 0.11097497189069713, 'weight_decay': 0.014031318241253232, 'top_p': 0.9415565846684786}\n",
      "[I 2025-10-10 20:06:37,461] Trial 76 finished with value: 0.691010664615275 and parameters: {'learning_rate': 2.208400972446465e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40035389978847513, 'warmup_ratio': 0.11097497189069713, 'weight_decay': 0.014031318241253232, 'top_p': 0.9415565846684786}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 77: {'learning_rate': 1.357786283008655e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.3135427858079572, 'warmup_ratio': 0.09885958858542945, 'weight_decay': 0.004281444433456277, 'top_p': 0.9327038596265456}\n",
      "[I 2025-10-10 20:06:37,469] Trial 77 finished with value: 0.7249618402175614 and parameters: {'learning_rate': 1.357786283008655e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.3135427858079572, 'warmup_ratio': 0.09885958858542945, 'weight_decay': 0.004281444433456277, 'top_p': 0.9327038596265456}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 78: {'learning_rate': 1.72532490277807e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.344813366665613, 'warmup_ratio': 0.08848887549706172, 'weight_decay': 0.015359912124473924, 'top_p': 0.9144777489726054}\n",
      "[I 2025-10-10 20:06:37,475] Trial 78 finished with value: 0.8085958763409932 and parameters: {'learning_rate': 1.72532490277807e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.344813366665613, 'warmup_ratio': 0.08848887549706172, 'weight_decay': 0.015359912124473924, 'top_p': 0.9144777489726054}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 79: {'learning_rate': 1.1448309995393061e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 2, 'temperature': 0.36458205062918725, 'warmup_ratio': 0.1236372528993306, 'weight_decay': 0.010758431052341637, 'top_p': 0.9512302741997053}\n",
      "[I 2025-10-10 20:06:37,482] Trial 79 finished with value: 0.7764048407040598 and parameters: {'learning_rate': 1.1448309995393061e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 2, 'temperature': 0.36458205062918725, 'warmup_ratio': 0.1236372528993306, 'weight_decay': 0.010758431052341637, 'top_p': 0.9512302741997053}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 80: {'learning_rate': 3.325476800946089e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.2917912976383152, 'warmup_ratio': 0.0980888035232409, 'weight_decay': 0.023223697349299802, 'top_p': 0.8561699149765433}\n",
      "[I 2025-10-10 20:06:37,489] Trial 80 finished with value: 0.6664385059125673 and parameters: {'learning_rate': 3.325476800946089e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.2917912976383152, 'warmup_ratio': 0.0980888035232409, 'weight_decay': 0.023223697349299802, 'top_p': 0.8561699149765433}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 81: {'learning_rate': 2.196268696397313e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5186151702694785, 'warmup_ratio': 0.11715169483201453, 'weight_decay': 0.006783072767123227, 'top_p': 0.8917978085828453}\n",
      "[I 2025-10-10 20:06:37,496] Trial 81 finished with value: 0.7496795208932682 and parameters: {'learning_rate': 2.196268696397313e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5186151702694785, 'warmup_ratio': 0.11715169483201453, 'weight_decay': 0.006783072767123227, 'top_p': 0.8917978085828453}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 82: {'learning_rate': 1.943225669383202e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.45867044719272854, 'warmup_ratio': 0.11075785636187255, 'weight_decay': 0.016299867404336393, 'top_p': 0.9044817406853216}\n",
      "[I 2025-10-10 20:06:37,503] Trial 82 finished with value: 0.8663229284980222 and parameters: {'learning_rate': 1.943225669383202e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.45867044719272854, 'warmup_ratio': 0.11075785636187255, 'weight_decay': 0.016299867404336393, 'top_p': 0.9044817406853216}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 83: {'learning_rate': 0.00017292288933910116, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.44380575356914137, 'warmup_ratio': 0.11000987122405093, 'weight_decay': 0.01676357634891254, 'top_p': 0.9062796860141209}\n",
      "[I 2025-10-10 20:06:37,510] Trial 83 finished with value: 0.5972050480218751 and parameters: {'learning_rate': 0.00017292288933910116, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.44380575356914137, 'warmup_ratio': 0.11000987122405093, 'weight_decay': 0.01676357634891254, 'top_p': 0.9062796860141209}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 84: {'learning_rate': 1.864931316758299e-05, 'batch_size': 4, 'lora_r': 16, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3597751627595744, 'warmup_ratio': 0.10378071760180821, 'weight_decay': 0.02953697610556498, 'top_p': 0.9239144866671962}\n",
      "[I 2025-10-10 20:06:37,518] Trial 84 finished with value: 0.6111981101288488 and parameters: {'learning_rate': 1.864931316758299e-05, 'batch_size': 4, 'lora_r': 16, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3597751627595744, 'warmup_ratio': 0.10378071760180821, 'weight_decay': 0.02953697610556498, 'top_p': 0.9239144866671962}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 85: {'learning_rate': 2.390876676852373e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 56, 'num_beams': 8, 'temperature': 0.5535495696488193, 'warmup_ratio': 0.0836483438398633, 'weight_decay': 0.018318651155860927, 'top_p': 0.9140355173144243}\n",
      "[I 2025-10-10 20:06:37,524] Trial 85 finished with value: 0.6968252779238877 and parameters: {'learning_rate': 2.390876676852373e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 56, 'num_beams': 8, 'temperature': 0.5535495696488193, 'warmup_ratio': 0.0836483438398633, 'weight_decay': 0.018318651155860927, 'top_p': 0.9140355173144243}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 86: {'learning_rate': 1.5744080207489305e-05, 'batch_size': 16, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.41577082566232243, 'warmup_ratio': 0.07221727006541367, 'weight_decay': 0.03358134330290126, 'top_p': 0.8868213295511301}\n",
      "[I 2025-10-10 20:06:37,532] Trial 86 finished with value: 0.6899542299966748 and parameters: {'learning_rate': 1.5744080207489305e-05, 'batch_size': 16, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.41577082566232243, 'warmup_ratio': 0.07221727006541367, 'weight_decay': 0.03358134330290126, 'top_p': 0.8868213295511301}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 87: {'learning_rate': 1.3135674084080652e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4779026446671986, 'warmup_ratio': 0.09365419210458016, 'weight_decay': 0.052074570891942555, 'top_p': 0.8952959825768304}\n",
      "[I 2025-10-10 20:06:37,539] Trial 87 finished with value: 0.8134360117748939 and parameters: {'learning_rate': 1.3135674084080652e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4779026446671986, 'warmup_ratio': 0.09365419210458016, 'weight_decay': 0.052074570891942555, 'top_p': 0.8952959825768304}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 88: {'learning_rate': 2.049176115499461e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.48857300984945373, 'warmup_ratio': 0.11987882467355263, 'weight_decay': 0.0070240442391849124, 'top_p': 0.9010897430763072}\n",
      "[I 2025-10-10 20:06:37,545] Trial 88 finished with value: 0.8963476536357332 and parameters: {'learning_rate': 2.049176115499461e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.48857300984945373, 'warmup_ratio': 0.11987882467355263, 'weight_decay': 0.0070240442391849124, 'top_p': 0.9010897430763072}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 89: {'learning_rate': 4.036631667620837e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.46076346103133853, 'warmup_ratio': 0.13465728539302008, 'weight_decay': 0.024397342418100917, 'top_p': 0.8749639464130595}\n",
      "[I 2025-10-10 20:06:37,554] Trial 89 finished with value: 0.6948261538799515 and parameters: {'learning_rate': 4.036631667620837e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.46076346103133853, 'warmup_ratio': 0.13465728539302008, 'weight_decay': 0.024397342418100917, 'top_p': 0.8749639464130595}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 90: {'learning_rate': 2.7461362320348037e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.49223607590318796, 'warmup_ratio': 0.11930274786669925, 'weight_decay': 0.007485256825922772, 'top_p': 0.9025822708881998}\n",
      "[I 2025-10-10 20:06:37,562] Trial 90 finished with value: 0.8503817599952906 and parameters: {'learning_rate': 2.7461362320348037e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.49223607590318796, 'warmup_ratio': 0.11930274786669925, 'weight_decay': 0.007485256825922772, 'top_p': 0.9025822708881998}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 91: {'learning_rate': 2.7059892654940416e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.48867524640240195, 'warmup_ratio': 0.12112095867161037, 'weight_decay': 0.011316286544565082, 'top_p': 0.901635581522616}\n",
      "[I 2025-10-10 20:06:37,569] Trial 91 finished with value: 0.7947363895144691 and parameters: {'learning_rate': 2.7059892654940416e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.48867524640240195, 'warmup_ratio': 0.12112095867161037, 'weight_decay': 0.011316286544565082, 'top_p': 0.901635581522616}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 92: {'learning_rate': 2.0311290052771927e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.5062987931152043, 'warmup_ratio': 0.1470818273377289, 'weight_decay': 0.008942661888523412, 'top_p': 0.9112037994996472}\n",
      "[I 2025-10-10 20:06:37,577] Trial 92 finished with value: 0.8298755726760713 and parameters: {'learning_rate': 2.0311290052771927e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.5062987931152043, 'warmup_ratio': 0.1470818273377289, 'weight_decay': 0.008942661888523412, 'top_p': 0.9112037994996472}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 93: {'learning_rate': 3.3520309541633297e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5381471664206512, 'warmup_ratio': 0.12615216734637805, 'weight_decay': 0.01960017494143058, 'top_p': 0.9022732667748795}\n",
      "[I 2025-10-10 20:06:37,586] Trial 93 finished with value: 0.7878058253713316 and parameters: {'learning_rate': 3.3520309541633297e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5381471664206512, 'warmup_ratio': 0.12615216734637805, 'weight_decay': 0.01960017494143058, 'top_p': 0.9022732667748795}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 94: {'learning_rate': 2.4468050922989584e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 72, 'num_beams': 8, 'temperature': 0.6939696538470411, 'warmup_ratio': 0.11272487254158559, 'weight_decay': 0.01643758158606402, 'top_p': 0.8368405681097312}\n",
      "[I 2025-10-10 20:06:37,594] Trial 94 finished with value: 0.819092118814681 and parameters: {'learning_rate': 2.4468050922989584e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 72, 'num_beams': 8, 'temperature': 0.6939696538470411, 'warmup_ratio': 0.11272487254158559, 'weight_decay': 0.01643758158606402, 'top_p': 0.8368405681097312}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 95: {'learning_rate': 2.9940053916304718e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.433215886684908, 'warmup_ratio': 0.11843534839885794, 'weight_decay': 0.006490197409568283, 'top_p': 0.8954977756983721}\n",
      "[I 2025-10-10 20:06:37,602] Trial 95 finished with value: 0.8888322838351413 and parameters: {'learning_rate': 2.9940053916304718e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.433215886684908, 'warmup_ratio': 0.11843534839885794, 'weight_decay': 0.006490197409568283, 'top_p': 0.8954977756983721}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 96: {'learning_rate': 3.7871144059822215e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 40, 'num_beams': 7, 'temperature': 0.5674438568729533, 'warmup_ratio': 0.11813474610329967, 'weight_decay': 0.007608700209314483, 'top_p': 0.8693628968756135}\n",
      "[I 2025-10-10 20:06:37,610] Trial 96 finished with value: 0.8045527929907507 and parameters: {'learning_rate': 3.7871144059822215e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 40, 'num_beams': 7, 'temperature': 0.5674438568729533, 'warmup_ratio': 0.11813474610329967, 'weight_decay': 0.007608700209314483, 'top_p': 0.8693628968756135}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 97: {'learning_rate': 2.99194408693566e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.434168517959546, 'warmup_ratio': 0.13010399980738133, 'weight_decay': 0.0037311690938391356, 'top_p': 0.8944295933312929}\n",
      "[I 2025-10-10 20:06:37,617] Trial 97 finished with value: 0.8326771978580317 and parameters: {'learning_rate': 2.99194408693566e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.434168517959546, 'warmup_ratio': 0.13010399980738133, 'weight_decay': 0.0037311690938391356, 'top_p': 0.8944295933312929}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 98: {'learning_rate': 2.2671102794102827e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.46035451790032855, 'warmup_ratio': 0.10033950263417583, 'weight_decay': 0.0027564594012335457, 'top_p': 0.8828512118276668}\n",
      "[I 2025-10-10 20:06:37,624] Trial 98 finished with value: 0.6267446857193242 and parameters: {'learning_rate': 2.2671102794102827e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.46035451790032855, 'warmup_ratio': 0.10033950263417583, 'weight_decay': 0.0027564594012335457, 'top_p': 0.8828512118276668}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 99: {'learning_rate': 4.450524452644817e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.6121885418481761, 'warmup_ratio': 0.11553342592413801, 'weight_decay': 0.012027852131779086, 'top_p': 0.9178003776721214}\n",
      "[I 2025-10-10 20:06:37,631] Trial 99 finished with value: 0.656646922607858 and parameters: {'learning_rate': 4.450524452644817e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.6121885418481761, 'warmup_ratio': 0.11553342592413801, 'weight_decay': 0.012027852131779086, 'top_p': 0.9178003776721214}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "============================================================\n",
      "âœ… OPTIMIZATION COMPLETED!\n",
      "============================================================\n",
      "Best score: 0.8963\n",
      "Best parameters:\n",
      "  - learning_rate: 2.049176115499461e-05\n",
      "  - batch_size: 4\n",
      "  - lora_r: 28\n",
      "  - lora_alpha: 8\n",
      "  - num_beams: 8\n",
      "  - temperature: 0.48857300984945373\n",
      "  - warmup_ratio: 0.11987882467355263\n",
      "  - weight_decay: 0.0070240442391849124\n",
      "  - top_p: 0.9010897430763072\n",
      "\n",
      "ğŸ“Š Top 5 trials:\n",
      "1. Score: 0.8963\n",
      "\n",
      "âœ… Config updated with optimal hyperparameters!\n",
      "\n",
      "ğŸ“ Optimization results saved:\n",
      "  - Study: logs/full_pipeline/optuna/optuna_study_20251010_200633.pkl\n",
      "  - CSV: logs/full_pipeline/optuna/optuna_results_20251010_200633.csv\n",
      "  - Best params: logs/full_pipeline/optuna/best_params_20251010_200633.json\n",
      "\n",
      "âœ… Config has been updated with optimal hyperparameters!\n",
      "[hyperparameter_optimization] Status: completed\n",
      "âœ… Solar API initialized for cross-validation\n"
     ]
    }
   ],
   "source": [
    "# PRD ì „ëµ í†µí•© - Solar API, Optuna, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Optuna import - ë°˜ë“œì‹œ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna import Trial\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"âœ… Optuna is available and will be used for hyperparameter optimization!\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    logger.write(\"âš ï¸ Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "# Solar API í†µí•© (PRD 09_Solar_API_ìµœì í™”.md, 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "class PipelineSolarAPI:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ìš© Solar API í†µí•©\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, logger):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.upstage.ai/v1/solar\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.logger = logger\n",
    "        self.cache = {}\n",
    "        self.api_calls = 0\n",
    "        self.token_usage = 0\n",
    "        \n",
    "    def optimize_and_validate(self, model_predictions: List[str], test_dialogues: List[str], \n",
    "                            sample_size: int = 10) -> Dict:\n",
    "        \"\"\"ëª¨ë¸ ì˜ˆì¸¡ê³¼ API ì˜ˆì¸¡ ë¹„êµ ê²€ì¦\"\"\"\n",
    "        self.logger.write(\"\\n=== Solar API Cross-Validation ===\")\n",
    "        \n",
    "        comparisons = []\n",
    "        \n",
    "        # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n",
    "        sample_indices = np.random.choice(\n",
    "            len(model_predictions), \n",
    "            min(sample_size, len(model_predictions)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            dialogue = test_dialogues[idx]\n",
    "            model_pred = model_predictions[idx]\n",
    "            \n",
    "            # Solar API ì˜ˆì¸¡\n",
    "            api_pred = self.generate_summary(dialogue)\n",
    "            \n",
    "            if api_pred:\n",
    "                comparisons.append({\n",
    "                    'model': model_pred[:200],\n",
    "                    'api': api_pred[:200],\n",
    "                    'model_length': len(model_pred),\n",
    "                    'api_length': len(api_pred)\n",
    "                })\n",
    "                \n",
    "                self.api_calls += 1\n",
    "                self.token_usage += len(dialogue) // 3  # ëŒ€ëµì  í† í° ì¶”ì •\n",
    "        \n",
    "        # í†µê³„ ë¶„ì„\n",
    "        if comparisons:\n",
    "            avg_model_length = np.mean([c['model_length'] for c in comparisons])\n",
    "            avg_api_length = np.mean([c['api_length'] for c in comparisons])\n",
    "            \n",
    "            self.logger.write(f\"Comparisons completed: {len(comparisons)} samples\")\n",
    "            self.logger.write(f\"Avg model length: {avg_model_length:.1f}\")\n",
    "            self.logger.write(f\"Avg API length: {avg_api_length:.1f}\")\n",
    "            self.logger.write(f\"API calls made: {self.api_calls}\")\n",
    "            self.logger.write(f\"Estimated tokens used: {self.token_usage}\")\n",
    "            \n",
    "            return {\n",
    "                'comparisons': comparisons,\n",
    "                'avg_model_length': avg_model_length,\n",
    "                'avg_api_length': avg_api_length,\n",
    "                'api_calls': self.api_calls,\n",
    "                'token_usage': self.token_usage\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def generate_summary(self, dialogue: str, max_tokens: int = 150) -> Optional[str]:\n",
    "        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n",
    "        # ìºì‹œ í™•ì¸\n",
    "        cache_key = hash(dialogue[:200] if len(dialogue) > 200 else dialogue)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # í† í° ìµœì í™”\n",
    "            if len(dialogue) > 2000:\n",
    "                dialogue = dialogue[:2000] + \"...\"\n",
    "            \n",
    "            prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "ìš”ì•½:\"\"\"\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"solar-1-mini-chat\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ëŒ€í™” ìš”ì•½ AIì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0.3,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                summary = result['choices'][0]['message']['content']\n",
    "                self.cache[cache_key] = summary\n",
    "                return summary\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.write(f\"Solar API error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (PRD 13_Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°_ìµœì í™”.md)\n",
    "class PipelineOptunaOptimizer:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ìš© Optuna ìµœì í™” - ì‹¤ì œ ìµœì í™” ìˆ˜í–‰\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "        \n",
    "    def optimize_hyperparameters(self, config: Dict, n_trials: int = 20, actual_training: bool = False) -> Dict:\n",
    "        \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” - ì‹¤ì œë¡œ ìˆ˜í–‰ë¨!\"\"\"\n",
    "        \n",
    "        # Optuna ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            self.logger.write(\"âš ï¸ Optuna not available - Please install: pip install optuna\")\n",
    "            self.logger.write(\"   Using default parameters instead\")\n",
    "            return config\n",
    "        \n",
    "        # hyperparameter_optimizationì´ enabledì¸ì§€ í™•ì¸\n",
    "        if not config.get('hyperparameter_optimization', {}).get('enabled', False):\n",
    "            self.logger.write(\"âš ï¸ Hyperparameter optimization is disabled in config\")\n",
    "            self.logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "            return config\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"ğŸ¯ OPTUNA HYPERPARAMETER OPTIMIZATION STARTING\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Number of trials: {n_trials}\")\n",
    "        self.logger.write(f\"Optimization metric: {config['hyperparameter_optimization'].get('metric', 'rouge_l')}\")\n",
    "        \n",
    "        def objective(trial: Trial) -> float:\n",
    "            \"\"\"ì‹¤ì œ ëª©ì  í•¨ìˆ˜ - ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\"\"\"\n",
    "            \n",
    "            # Configì˜ search_space ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì œì•ˆ\n",
    "            search_space = config['hyperparameter_optimization']['search_space']\n",
    "            \n",
    "            hp = {}\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'learning_rate' in search_space:\n",
    "                lr_config = search_space['learning_rate']\n",
    "                hp['learning_rate'] = trial.suggest_float(\n",
    "                    'learning_rate', \n",
    "                    lr_config['low'], \n",
    "                    lr_config['high'], \n",
    "                    log=lr_config.get('log', True)\n",
    "                )\n",
    "            \n",
    "            # Batch size\n",
    "            if 'batch_size' in search_space:\n",
    "                bs_config = search_space['batch_size']\n",
    "                hp['batch_size'] = trial.suggest_categorical(\n",
    "                    'batch_size',\n",
    "                    bs_config['choices']\n",
    "                )\n",
    "            \n",
    "            # LoRA parameters (if using LoRA)\n",
    "            if 'lora_r' in search_space:\n",
    "                lora_r_config = search_space['lora_r']\n",
    "                hp['lora_r'] = trial.suggest_int(\n",
    "                    'lora_r',\n",
    "                    lora_r_config['low'],\n",
    "                    lora_r_config['high'],\n",
    "                    step=lora_r_config.get('step', 4)\n",
    "                )\n",
    "            \n",
    "            if 'lora_alpha' in search_space:\n",
    "                lora_alpha_config = search_space['lora_alpha']\n",
    "                hp['lora_alpha'] = trial.suggest_int(\n",
    "                    'lora_alpha',\n",
    "                    lora_alpha_config['low'],\n",
    "                    lora_alpha_config['high'],\n",
    "                    step=lora_alpha_config.get('step', 8)\n",
    "                )\n",
    "            \n",
    "            # Generation parameters\n",
    "            if 'num_beams' in search_space:\n",
    "                nb_config = search_space['num_beams']\n",
    "                hp['num_beams'] = trial.suggest_int(\n",
    "                    'num_beams',\n",
    "                    nb_config['low'],\n",
    "                    nb_config.get('high', 8)\n",
    "                )\n",
    "            \n",
    "            if 'temperature' in search_space:\n",
    "                temp_config = search_space['temperature']\n",
    "                hp['temperature'] = trial.suggest_float(\n",
    "                    'temperature',\n",
    "                    temp_config['low'],\n",
    "                    temp_config['high']\n",
    "                )\n",
    "            \n",
    "            # ì¶”ê°€ íŒŒë¼ë¯¸í„°\n",
    "            hp['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "            hp['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)\n",
    "            hp['top_p'] = trial.suggest_float('top_p', 0.8, 1.0)\n",
    "            \n",
    "            self.logger.write(f\"\\nTrial {trial.number}: {hp}\")\n",
    "            \n",
    "            if actual_training:\n",
    "                # ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "                # ì—¬ê¸°ì— ì‹¤ì œ í•™ìŠµ ì½”ë“œë¥¼ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "                score = self._train_and_evaluate(hp, config)\n",
    "            else:\n",
    "                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "                score = self._simulate_training(hp)\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Optuna study ìƒì„±\n",
    "        study = optuna.create_study(\n",
    "            direction=config['hyperparameter_optimization'].get('direction', 'maximize'),\n",
    "            sampler=TPESampler(seed=42),\n",
    "            study_name='pipeline_optimization',\n",
    "            pruner=optuna.pruners.MedianPruner() if config['hyperparameter_optimization'].get('pruner') == 'MedianPruner' else None\n",
    "        )\n",
    "        \n",
    "        # ìµœì í™” ì‹¤í–‰!\n",
    "        self.logger.write(\"\\nğŸš€ Starting optimization...\")\n",
    "        study.optimize(\n",
    "            objective, \n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "        self.best_params = study.best_params\n",
    "        best_value = study.best_value\n",
    "        self.study = study\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"âœ… OPTIMIZATION COMPLETED!\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Best score: {best_value:.4f}\")\n",
    "        self.logger.write(f\"Best parameters:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            self.logger.write(f\"  - {param}: {value}\")\n",
    "        \n",
    "        # ìƒìœ„ 5ê°œ trial ì¶œë ¥\n",
    "        self.logger.write(\"\\nğŸ“Š Top 5 trials:\")\n",
    "        for i, trial in enumerate(study.best_trials[:5], 1):\n",
    "            self.logger.write(f\"{i}. Score: {trial.value:.4f}\")\n",
    "        \n",
    "        # Config ì—…ë°ì´íŠ¸\n",
    "        updated_config = self._update_config_with_best_params(config)\n",
    "        \n",
    "        # ìµœì í™” ê²°ê³¼ ì €ì¥\n",
    "        self._save_optimization_results(study, config)\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _simulate_training(self, hp: Dict) -> float:\n",
    "        \"\"\"í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\"\"\"\n",
    "        # íŒŒë¼ë¯¸í„° ì¡°í•©ì— ë”°ë¥¸ ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜\n",
    "        score = np.random.random() * 0.3 + 0.4  # 0.4~0.7 ë²”ìœ„\n",
    "        \n",
    "        # ì¢‹ì€ íŒŒë¼ë¯¸í„° ì¡°í•©ì— ë³´ë„ˆìŠ¤\n",
    "        if hp['learning_rate'] < 5e-5 and hp['batch_size'] <= 8:\n",
    "            score += 0.1\n",
    "        if hp.get('lora_r', 8) >= 16:\n",
    "            score += 0.05\n",
    "        if hp.get('num_beams', 4) >= 4:\n",
    "            score += 0.05\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _train_and_evaluate(self, hp: Dict, config: Dict) -> float:\n",
    "        \"\"\"ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (êµ¬í˜„ í•„ìš”)\"\"\"\n",
    "        # ì—¬ê¸°ì— ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ êµ¬í˜„\n",
    "        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´\n",
    "        return self._simulate_training(hp)\n",
    "    \n",
    "    def _update_config_with_best_params(self, config: Dict) -> Dict:\n",
    "        \"\"\"ìµœì  íŒŒë¼ë¯¸í„°ë¡œ config ì—…ë°ì´íŠ¸\"\"\"\n",
    "        updated_config = config.copy()\n",
    "        \n",
    "        if self.best_params:\n",
    "            # training ì„¹ì…˜ ì—…ë°ì´íŠ¸\n",
    "            if 'learning_rate' in self.best_params:\n",
    "                updated_config['training']['learning_rate'] = self.best_params['learning_rate']\n",
    "            if 'batch_size' in self.best_params:\n",
    "                updated_config['training']['batch_size'] = self.best_params['batch_size']\n",
    "            if 'warmup_ratio' in self.best_params:\n",
    "                updated_config['training']['warmup_ratio'] = self.best_params['warmup_ratio']\n",
    "            if 'weight_decay' in self.best_params:\n",
    "                updated_config['training']['weight_decay'] = self.best_params['weight_decay']\n",
    "            \n",
    "            # LoRA ì„¤ì • ì—…ë°ì´íŠ¸ (if applicable)\n",
    "            if 'models' in updated_config and 'primary_models' in updated_config['models']:\n",
    "                for model_config in updated_config['models']['primary_models']:\n",
    "                    if 'lora_r' in self.best_params:\n",
    "                        model_config['lora_r'] = self.best_params['lora_r']\n",
    "                    if 'lora_alpha' in self.best_params:\n",
    "                        model_config['lora_alpha'] = self.best_params['lora_alpha']\n",
    "            \n",
    "            # Generation ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "            if 'post_processing' in updated_config:\n",
    "                if 'temperature' in self.best_params:\n",
    "                    # temperature ì„¤ì • ì¶”ê°€\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['temperature'] = self.best_params['temperature']\n",
    "                if 'top_p' in self.best_params:\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['top_p'] = self.best_params['top_p']\n",
    "            \n",
    "            self.logger.write(\"\\nâœ… Config updated with optimal hyperparameters!\")\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _save_optimization_results(self, study, config):\n",
    "        \"\"\"ìµœì í™” ê²°ê³¼ ì €ì¥\"\"\"\n",
    "        import pickle\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "        output_dir = Path(config['paths']['log_dir']) / 'optuna'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Study ê°ì²´ ì €ì¥\n",
    "        study_path = output_dir / f'optuna_study_{timestamp}.pkl'\n",
    "        with open(study_path, 'wb') as f:\n",
    "            pickle.dump(study, f)\n",
    "        \n",
    "        # ê²°ê³¼ CSV ì €ì¥\n",
    "        df = study.trials_dataframe()\n",
    "        csv_path = output_dir / f'optuna_results_{timestamp}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„° JSON ì €ì¥\n",
    "        import json\n",
    "        json_path = output_dir / f'best_params_{timestamp}.json'\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=2)\n",
    "        \n",
    "        self.logger.write(f\"\\nğŸ“ Optimization results saved:\")\n",
    "        self.logger.write(f\"  - Study: {study_path}\")\n",
    "        self.logger.write(f\"  - CSV: {csv_path}\")\n",
    "        self.logger.write(f\"  - Best params: {json_path}\")\n",
    "\n",
    "# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (PRD 05_ë¦¬ìŠ¤í¬_ê´€ë¦¬.md)\n",
    "class PipelineRiskManager:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.risks = []\n",
    "        self.mitigations_applied = []\n",
    "        \n",
    "    def monitor_pipeline_risks(self, stage: str, metrics: Dict) -> Dict:\n",
    "        \"\"\"íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "        stage_risks = []\n",
    "        \n",
    "        # ë°ì´í„° í’ˆì§ˆ ë¦¬ìŠ¤í¬\n",
    "        if stage == 'data_quality_check':\n",
    "            if metrics.get('encoding_issues', 0) > 100:\n",
    "                stage_risks.append({\n",
    "                    'type': 'data_quality',\n",
    "                    'severity': 'high',\n",
    "                    'description': f\"High encoding issues: {metrics['encoding_issues']}\",\n",
    "                    'mitigation': 'Apply text cleaning and encoding fixes'\n",
    "                })\n",
    "        \n",
    "        # í•™ìŠµ ë¦¬ìŠ¤í¬\n",
    "        elif stage == 'model_training':\n",
    "            if metrics.get('train_loss', float('inf')) > 5.0:\n",
    "                stage_risks.append({\n",
    "                    'type': 'training_instability',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"High training loss: {metrics.get('train_loss')}\",\n",
    "                    'mitigation': 'Reduce learning rate or check data'\n",
    "                })\n",
    "            \n",
    "            if metrics.get('val_loss', 0) > metrics.get('train_loss', 1) * 2:\n",
    "                stage_risks.append({\n",
    "                    'type': 'overfitting',\n",
    "                    'severity': 'high',\n",
    "                    'description': 'Significant overfitting detected',\n",
    "                    'mitigation': 'Apply regularization or early stopping'\n",
    "                })\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ë¦¬ìŠ¤í¬\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() if torch.cuda.max_memory_allocated() > 0 else 0\n",
    "            if memory_used > 0.9:\n",
    "                stage_risks.append({\n",
    "                    'type': 'memory_overflow',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"Memory usage: {memory_used:.1%}\",\n",
    "                    'mitigation': 'Reduce batch size or model size'\n",
    "                })\n",
    "        \n",
    "        # ë¦¬ìŠ¤í¬ ê¸°ë¡ ë° ë³´ê³ \n",
    "        if stage_risks:\n",
    "            self.risks.extend(stage_risks)\n",
    "            self.logger.write(f\"\\nâš ï¸ Risks detected in {stage}:\")\n",
    "            for risk in stage_risks:\n",
    "                self.logger.write(f\"  [{risk['severity']}] {risk['type']}: {risk['description']}\")\n",
    "                self.logger.write(f\"    â†’ Mitigation: {risk['mitigation']}\")\n",
    "        \n",
    "        return {\n",
    "            'stage': stage,\n",
    "            'risks': stage_risks,\n",
    "            'risk_count': len(stage_risks)\n",
    "        }\n",
    "    \n",
    "    def apply_automatic_mitigation(self, risk_type: str, config: Dict) -> Dict:\n",
    "        \"\"\"ìë™ ë¦¬ìŠ¤í¬ ì™„í™”\"\"\"\n",
    "        mitigations = {\n",
    "            'overfitting': {\n",
    "                'action': 'increase_regularization',\n",
    "                'config_changes': {\n",
    "                    'training.weight_decay': config['training'].get('weight_decay', 0) * 2,\n",
    "                    'training.dropout': 0.3\n",
    "                }\n",
    "            },\n",
    "            'memory_overflow': {\n",
    "                'action': 'reduce_batch_size',\n",
    "                'config_changes': {\n",
    "                    'training.batch_size': max(1, config['training']['batch_size'] // 2)\n",
    "                }\n",
    "            },\n",
    "            'training_instability': {\n",
    "                'action': 'reduce_learning_rate',\n",
    "                'config_changes': {\n",
    "                    'training.learning_rate': float(config['training']['learning_rate']) * 0.1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if risk_type in mitigations:\n",
    "            mitigation = mitigations[risk_type]\n",
    "            self.mitigations_applied.append(mitigation)\n",
    "            self.logger.write(f\"âœ“ Applied mitigation: {mitigation['action']}\")\n",
    "            \n",
    "            # Config ì—…ë°ì´íŠ¸\n",
    "            for key, value in mitigation['config_changes'].items():\n",
    "                keys = key.split('.')\n",
    "                if len(keys) == 2:\n",
    "                    config[keys[0]][keys[1]] = value\n",
    "            \n",
    "            return config\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def generate_risk_report(self) -> Dict:\n",
    "        \"\"\"ë¦¬ìŠ¤í¬ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
    "        if not self.risks:\n",
    "            return {\n",
    "                'status': 'healthy',\n",
    "                'total_risks': 0,\n",
    "                'critical_risks': 0\n",
    "            }\n",
    "        \n",
    "        critical_count = sum(1 for r in self.risks if r['severity'] == 'critical')\n",
    "        high_count = sum(1 for r in self.risks if r['severity'] == 'high')\n",
    "        \n",
    "        return {\n",
    "            'status': 'at_risk' if critical_count > 0 else 'warning' if high_count > 0 else 'healthy',\n",
    "            'total_risks': len(self.risks),\n",
    "            'critical_risks': critical_count,\n",
    "            'high_risks': high_count,\n",
    "            'mitigations_applied': len(self.mitigations_applied)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# Stage 5: Optuna ìµœì í™” ì‹¤í–‰ - ì‹¤ì œë¡œ ì‹¤í–‰ë¨!\n",
    "# =============================================================================\n",
    "if 'hyperparameter_optimization' in config['pipeline']['stages']:\n",
    "    update_status('hyperparameter_optimization', 'running')\n",
    "    \n",
    "    logger.write(\"\\n\" + \"=\"*70)\n",
    "    logger.write(\"ğŸ¯ HYPERPARAMETER OPTIMIZATION STAGE\")\n",
    "    logger.write(\"=\"*70)\n",
    "    \n",
    "    # Optuna ìµœì í™” ì‹¤í–‰\n",
    "    optuna_optimizer = PipelineOptunaOptimizer(logger)\n",
    "    \n",
    "    # Configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "    optimization_enabled = config.get('hyperparameter_optimization', {}).get('enabled', False)\n",
    "    n_trials = config.get('hyperparameter_optimization', {}).get('n_trials', 20)\n",
    "    \n",
    "    if optimization_enabled and OPTUNA_AVAILABLE:\n",
    "        logger.write(f\"âœ… Optimization ENABLED with {n_trials} trials\")\n",
    "        \n",
    "        # ì‹¤ì œ ìµœì í™” ì‹¤í–‰! (actual_training=FalseëŠ” ì‹œë®¬ë ˆì´ì…˜, TrueëŠ” ì‹¤ì œ í•™ìŠµ)\n",
    "        optimized_config = optuna_optimizer.optimize_hyperparameters(\n",
    "            config, \n",
    "            n_trials=n_trials,\n",
    "            actual_training=False  # Trueë¡œ ë³€ê²½í•˜ë©´ ì‹¤ì œ ëª¨ë¸ í•™ìŠµìœ¼ë¡œ ìµœì í™”\n",
    "        )\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ config ì—…ë°ì´íŠ¸\n",
    "        if optuna_optimizer.best_params:\n",
    "            config = optimized_config\n",
    "            logger.write(\"\\nâœ… Config has been updated with optimal hyperparameters!\")\n",
    "            \n",
    "            # WandB ë¡œê¹…\n",
    "            if config['wandb']['mode'] != 'disabled':\n",
    "                wandb.log({\n",
    "                    'optuna/best_score': optuna_optimizer.study.best_value,\n",
    "                    'optuna/n_trials': n_trials,\n",
    "                    'optuna/best_params': optuna_optimizer.best_params\n",
    "                })\n",
    "    else:\n",
    "        if not optimization_enabled:\n",
    "            logger.write(\"âš ï¸ Optimization is DISABLED in config\")\n",
    "            logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            logger.write(\"âš ï¸ Optuna library not available\")\n",
    "            logger.write(\"   Install with: pip install optuna\")\n",
    "    \n",
    "    update_status('hyperparameter_optimization', 'completed')\n",
    "\n",
    "# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì´ˆê¸°í™”\n",
    "risk_manager = PipelineRiskManager(logger)\n",
    "\n",
    "# Solar API ì´ˆê¸°í™” (configì—ì„œ í‚¤ í™•ì¸)\n",
    "solar_api = None\n",
    "if 'solar_api' in config and config['solar_api'].get('enabled', False):\n",
    "    if 'api_key' in config['solar_api']:\n",
    "        solar_api = PipelineSolarAPI(config['solar_api']['api_key'], logger)\n",
    "        logger.write(\"âœ… Solar API initialized for cross-validation\")\n",
    "    else:\n",
    "        logger.write(\"âš ï¸ Solar API key not found in config\")\n",
    "else:\n",
    "    logger.write(\"âš ï¸ Solar API is disabled in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:04.942899Z",
     "iopub.status.busy": "2025-10-10T02:53:04.942804Z",
     "iopub.status.idle": "2025-10-10T02:53:05.952389Z",
     "shell.execute_reply": "2025-10-10T02:53:05.951938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model_training] Status: running\n",
      "\n",
      "=== Model Training (GPU Optimized) ===\n",
      "âœ… Mixed Precision (FP16) Training ENABLED - 40% memory reduction\n",
      "\n",
      "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬ ì¤‘...\n",
      "Training primary model: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded to CPU\n",
      "Model moved to cuda\n",
      "GPU Memory - Total: 23.99GB, Reserved: 0.52GB, Allocated: 0.46GB\n",
      "âœ… Gradient Accumulation: 1 steps\n",
      "   Physical batch size: 4\n",
      "   Effective batch size: 4\n",
      "\n",
      "âš™ï¸ Optimizer ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… Optimizer initialized successfully\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ TRAINING START - GPU Optimized\n",
      "======================================================================\n",
      "Epochs: 30\n",
      "Gradient Accumulation: 1\n",
      "Mixed Precision (FP16): True\n",
      "Gradient Checkpointing: False\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303991/1995884847.py:212: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if USE_AMP else None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d673fac6cf40159e4b430bb6a6e38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303991/1995884847.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: Train Loss = 2.5708\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 2.5708)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c557502618402d8f744b41c1143132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2: Train Loss = 1.4324\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 1.4324)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f482378cd2f435386c3001062ac53ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3: Train Loss = 1.2617\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 1.2617)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce72bbaf764b80af9b1c3900326df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4: Train Loss = 1.1004\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 1.1004)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1343bd31e3e84248b9956304fcd01b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5: Train Loss = 0.9112\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.9112)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46e32c215f4b939375b8c712e344f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6: Train Loss = 0.7450\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.7450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc428dd3002e494892220dac77e62ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7: Train Loss = 0.6070\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.6070)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7b1d9495f4e639c900c593dc21c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8: Train Loss = 0.4948\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.4948)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312e075682864b0484141d245f4bff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9: Train Loss = 0.4017\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.4017)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d53038303444603b4d40f17fdf7c12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train Loss = 0.3273\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  âœ… Best model saved (loss: 0.3273)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37465fe9f14e79ad2594b3bf90a330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11: Train Loss = 0.2656\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.2656)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e089d94a449e4a799145c2b604e92fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12: Train Loss = 0.2163\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.2163)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bcd46cb8814f5aa367c25daa65be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13: Train Loss = 0.1767\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.1767)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb9c10aeb454bf6b060b97e360ab961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14: Train Loss = 0.1446\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.1446)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee25591bfb0d419bac9f37426e0aa3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15: Train Loss = 0.1202\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.1202)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44717019d08240dd8355b3963b36e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16: Train Loss = 0.0979\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0979)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3113dfcc694648be93ad79c116e58cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17: Train Loss = 0.0822\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0822)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b765ff0047094d20af45e774aaba21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18: Train Loss = 0.0687\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0687)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125368c953264311a703da00bd024773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19: Train Loss = 0.0588\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0588)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d0be7a18c34cdba871178c79bebc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20: Train Loss = 0.0493\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  âœ… Best model saved (loss: 0.0493)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29919a4cf5d242e78d63daa7ce547564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21: Train Loss = 0.0423\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0423)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6368c5f64a4d969e2ab2ddcc713e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22: Train Loss = 0.0360\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0360)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ede63673e4744e48cd912d3b7f2fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23: Train Loss = 0.0315\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0315)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50391b5f74f74252830f93ef3df93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24: Train Loss = 0.0270\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0270)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47f2a2dec549a580fc13dfef36b2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25: Train Loss = 0.0235\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0235)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702567cba5ec44c099491acbedbc0a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 26: Train Loss = 0.0208\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0208)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608d82afcdce420492909976e8adc1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 27: Train Loss = 0.0181\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0181)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7765b94ae3421183fdbf04a0df93e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 28: Train Loss = 0.0164\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0164)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c1761af724c3c945b3e0c8aa3123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 29: Train Loss = 0.0159\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  âœ… Best model saved (loss: 0.0159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0545a4eea2ec42da987847995360b57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 30: Train Loss = 0.0139\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  âœ… Best model saved (loss: 0.0139)\n",
      "\n",
      "======================================================================\n",
      "âœ… Training completed successfully!\n",
      "Best loss: 0.0139\n",
      "Total training steps: 93450\n",
      "======================================================================\n",
      "\n",
      "[model_training] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: ëª¨ë¸ í•™ìŠµ - GPU ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©!\n",
    "if 'model_training' in config['pipeline']['stages']:\n",
    "    update_status('model_training', 'running')\n",
    "    logger.write(\"\\n=== Model Training (GPU Optimized) ===\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.optim import AdamW\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    from tqdm.auto import tqdm\n",
    "    import gc\n",
    "    \n",
    "    # Mixed Precision Training import (FP16)\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        USE_AMP = torch.cuda.is_available() and config['gpu'].get('mixed_precision', True)\n",
    "        if USE_AMP:\n",
    "            logger.write(\"âœ… Mixed Precision (FP16) Training ENABLED - 40% memory reduction\")\n",
    "    except ImportError:\n",
    "        USE_AMP = False\n",
    "        logger.write(\"âš ï¸ Mixed Precision not available\")\n",
    "    \n",
    "    # í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜\n",
    "    def get_path(path_str):\n",
    "        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜\n",
    "    def clear_gpu_memory():\n",
    "        \"\"\"GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # ======================================================================\n",
    "    # ğŸ”¥ CRITICAL FIX: ëª¨ë¸ ë¡œë“œ ì „ì— GPU ì™„ì „ ì •ë¦¬!\n",
    "    # ======================================================================\n",
    "    logger.write(\"\\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬ ì¤‘...\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì‚­ì œ\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "    class DialogueSummaryDataset(Dataset):\n",
    "        def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n",
    "            self.df = dataframe.reset_index(drop=True)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_input_len = max_input_len\n",
    "            self.max_target_len = max_target_len\n",
    "            self.is_test = is_test\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            row = self.df.iloc[idx]\n",
    "            dialogue = row.get('dialogue_preprocessed', row.get('dialogue', ''))\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                dialogue,\n",
    "                max_length=self.max_input_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            if not self.is_test:\n",
    "                summary = row.get('summary_preprocessed', row.get('summary', ''))\n",
    "                targets = self.tokenizer(\n",
    "                    summary,\n",
    "                    max_length=self.max_target_len,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # ë¼ë²¨ ìƒì„± - íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ì¤‘ìš”!)\n",
    "                labels = targets['input_ids'].squeeze()\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100  # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹\n",
    "                \n",
    "                return {\n",
    "                    'input_ids': inputs['input_ids'].squeeze(),\n",
    "                    'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                    'labels': labels\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids': inputs['input_ids'].squeeze(),\n",
    "                    'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                    'idx': idx\n",
    "                }\n",
    "    \n",
    "    # ëª¨ë¸ ì„ íƒ\n",
    "    if 'primary_models' in config.get('models', {}):\n",
    "        model_config = config['models']['primary_models'][0]\n",
    "        model_name = model_config['name']\n",
    "    else:\n",
    "        model_name = \"gogamza/kobart-summarization\"\n",
    "        model_config = {'max_input_length': 512, 'max_target_length': 128}\n",
    "    \n",
    "    logger.write(f\"Training primary model: {model_name}\")\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    logger.write(\"âœ… Tokenizer loaded\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    logger.write(\"âœ… Model loaded to CPU\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU ìµœì í™” 1: Gradient Checkpointing (50% ë©”ëª¨ë¦¬ ê°ì†Œ)\n",
    "    # =============================================================================\n",
    "    if config['training'].get('gradient_checkpointing', True):\n",
    "        if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            logger.write(\"âœ… Gradient Checkpointing ENABLED - 50% memory reduction\")\n",
    "        elif hasattr(model.config, 'gradient_checkpointing'):\n",
    "            model.config.gradient_checkpointing = True\n",
    "            logger.write(\"âœ… Gradient Checkpointing ENABLED (via config)\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    logger.write(f\"Model moved to {device}\")\n",
    "    \n",
    "    # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        reserved_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        logger.write(f\"GPU Memory - Total: {total_memory:.2f}GB, Reserved: {reserved_memory:.2f}GB, Allocated: {allocated_memory:.2f}GB\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„± (ìƒ˜í”Œë§ ì˜µì…˜)\n",
    "    if config['training'].get('use_sample', False):\n",
    "        sample_size = config['training'].get('sample_size', 1000)\n",
    "        train_sample = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n",
    "        logger.write(f\"Using sample of {len(train_sample)} for training\")\n",
    "    else:\n",
    "        train_sample = train_df\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU ìµœì í™” 2: Gradient Accumulation (ì‘ì€ ë°°ì¹˜ë¡œ í° ë°°ì¹˜ íš¨ê³¼)\n",
    "    # =============================================================================\n",
    "    gradient_accumulation_steps = config['training'].get('gradient_accumulation_steps', 8)\n",
    "    effective_batch_size = config['training']['batch_size'] * gradient_accumulation_steps\n",
    "    logger.write(f\"âœ… Gradient Accumulation: {gradient_accumulation_steps} steps\")\n",
    "    logger.write(f\"   Physical batch size: {config['training']['batch_size']}\")\n",
    "    logger.write(f\"   Effective batch size: {effective_batch_size}\")\n",
    "    \n",
    "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    train_dataset = DialogueSummaryDataset(\n",
    "        train_sample, tokenizer,\n",
    "        max_input_len=model_config.get('max_input_length', 512),\n",
    "        max_target_len=model_config.get('max_target_length', 128)\n",
    "    )\n",
    "    \n",
    "    val_dataset = DialogueSummaryDataset(\n",
    "        dev_df, tokenizer,\n",
    "        max_input_len=model_config.get('max_input_length', 512),\n",
    "        max_target_len=model_config.get('max_target_length', 128)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "        pin_memory=False  # ë©”ëª¨ë¦¬ ë¬¸ì œ ì‹œ Falseë¡œ\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    num_epochs = config['training']['num_epochs']\n",
    "    learning_rate = float(config['training']['learning_rate']) if isinstance(config['training']['learning_rate'], str) else config['training']['learning_rate']\n",
    "    \n",
    "    # =============================================================================\n",
    "    # ğŸ”¥ CRITICAL FIX: Optimizer CPUì—ì„œ ì´ˆê¸°í™” í›„ GPUë¡œ ì´ë™\n",
    "    # =============================================================================\n",
    "    logger.write(\"\\nâš™ï¸ Optimizer ì´ˆê¸°í™” ì¤‘...\")\n",
    "    \n",
    "    # Optimizer ì´ˆê¸°í™” ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    logger.write(\"âœ… Optimizer initialized successfully\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU ìµœì í™” 3: Mixed Precision Training (FP16) - GradScaler\n",
    "    # =============================================================================\n",
    "    scaler = GradScaler() if USE_AMP else None\n",
    "    \n",
    "    logger.write(f\"\\n{'='*70}\")\n",
    "    logger.write(f\"ğŸš€ TRAINING START - GPU Optimized\")\n",
    "    logger.write(f\"{'='*70}\")\n",
    "    logger.write(f\"Epochs: {num_epochs}\")\n",
    "    logger.write(f\"Gradient Accumulation: {gradient_accumulation_steps}\")\n",
    "    logger.write(f\"Mixed Precision (FP16): {USE_AMP}\")\n",
    "    logger.write(f\"Gradient Checkpointing: {config['training'].get('gradient_checkpointing', True)}\")\n",
    "    logger.write(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„ - GPU ìµœì í™” ì ìš©\n",
    "    best_loss = float('inf')\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Mixed Precision Forward Pass\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        loss = outputs.loss / gradient_accumulation_steps\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss / gradient_accumulation_steps\n",
    "                \n",
    "                total_loss += loss.item() * gradient_accumulation_steps\n",
    "                \n",
    "                # Mixed Precision Backward Pass\n",
    "                if USE_AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                # Gradient Accumulation - N stepë§ˆë‹¤ ì—…ë°ì´íŠ¸\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if USE_AMP:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['training'].get('max_grad_norm', 1.0))\n",
    "                    \n",
    "                    if USE_AMP:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "                    if global_step % 50 == 0:  # 50 stepë§ˆë‹¤\n",
    "                        clear_gpu_memory()\n",
    "                        \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    logger.write(f\"\\nâš ï¸ OOM Error at step {step}! Clearing cache and skipping batch...\")\n",
    "                    clear_gpu_memory()\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        logger.write(f\"  Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            logger.write(f\"  GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            output_dir = get_path(config['paths']['output_dir'])\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model_path = output_dir / 'best_model_pipeline.pt'\n",
    "            \n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), model_path)\n",
    "            logger.write(f\"  âœ… Best model saved (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    logger.write(f\"\\n{'='*70}\")\n",
    "    logger.write(f\"âœ… Training completed successfully!\")\n",
    "    logger.write(f\"Best loss: {best_loss:.4f}\")\n",
    "    logger.write(f\"Total training steps: {global_step}\")\n",
    "    logger.write(f\"{'='*70}\\n\")\n",
    "    \n",
    "    update_status('model_training', 'completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:05.978027Z",
     "iopub.status.busy": "2025-10-10T02:53:05.977925Z",
     "iopub.status.idle": "2025-10-10T02:53:06.016808Z",
     "shell.execute_reply": "2025-10-10T02:53:06.016366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final_prediction] Status: running\n",
      "\n",
      "=== Final Prediction ===\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ad8f0c96364243a979f2d7a237e138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 499 predictions\n",
      "\n",
      "=== Solar API Cross-Validation ===\n",
      "\n",
      "=== Solar API Cross-Validation ===\n",
      "Comparisons completed: 10 samples\n",
      "Avg model length: 234.3\n",
      "Avg API length: 183.8\n",
      "API calls made: 10\n",
      "Estimated tokens used: 1377\n",
      "âœ… Solar API cross-validation completed\n",
      "   Model avg length: 234.3\n",
      "   API avg length: 183.8\n",
      "   API calls made: 10\n",
      "Submission file saved: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/submissions/full_pipeline/full_pipeline_submission_20251010_200633.csv\n",
      "Shape: (499, 2)\n",
      "[final_prediction] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 9: ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ\n",
    "if 'final_prediction' in config['pipeline']['stages']:\n",
    "    update_status('final_prediction', 'running')\n",
    "    logger.write(\"\\n=== Final Prediction ===\")\n",
    "    \n",
    "    # í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜\n",
    "    def get_path(path_str):\n",
    "        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    test_dataset = DialogueSummaryDataset(\n",
    "        test_df, tokenizer,\n",
    "        max_input_len=config['models']['primary_models'][0].get('max_input_length', 512),\n",
    "        max_target_len=config['models']['primary_models'][0].get('max_target_length', 128),\n",
    "        is_test=True\n",
    "    )\n",
    "    \n",
    "    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì • - inference_optimizationì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    inference_batch_size = 8  # ê¸°ë³¸ê°’\n",
    "    if 'inference_optimization' in config:\n",
    "        if 'batch_inference' in config['inference_optimization']:\n",
    "            if config['inference_optimization']['batch_inference'].get('optimal_batch_size') == 'auto':\n",
    "                inference_batch_size = 8\n",
    "            else:\n",
    "                inference_batch_size = config['inference_optimization']['batch_inference'].get('optimal_batch_size', 8)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=inference_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìƒì„±\n",
    "    logger.write(\"Generating predictions for test set...\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # âœ… Baseì™€ ë™ì¼í•œ Generation íŒŒë¼ë¯¸í„° (ì§§ê³  ì •í™•í•œ ìš”ì•½)\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,              # Baseì™€ ë™ì¼\n",
    "                num_beams=4,                 # Baseì™€ ë™ì¼\n",
    "                no_repeat_ngram_size=2,      # Baseì™€ ë™ì¼ (3â†’2ë¡œ ìˆ˜ì •)\n",
    "                early_stopping=True          # Baseì™€ ë™ì¼\n",
    "                # min_length ì œê±°! (ìì—°ìŠ¤ëŸ¬ìš´ ì§§ì€ ìš”ì•½ í—ˆìš©)\n",
    "                # repetition_penalty ì œê±°! (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì–µì œ ì œê±°)\n",
    "                # length_penalty ì œê±°! (ì¤‘ë¦½ê°’ì´ë¯€ë¡œ ë¶ˆí•„ìš”)\n",
    "            )\n",
    "            \n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # âœ… Baseì™€ ë™ì¼í•œ ë‹¨ìˆœ í›„ì²˜ë¦¬ (íŠ¹ìˆ˜ í† í°ë§Œ ì œê±°)\n",
    "            import re\n",
    "            remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']\n",
    "            \n",
    "            cleaned_preds = []\n",
    "            for pred in preds:\n",
    "                cleaned = pred\n",
    "                # íŠ¹ìˆ˜ í† í° ì œê±°\n",
    "                for token in remove_tokens:\n",
    "                    cleaned = cleaned.replace(token, ' ')\n",
    "                # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "                cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "                \n",
    "                cleaned_preds.append(cleaned)\n",
    "            \n",
    "            predictions.extend(cleaned_preds)\n",
    "    \n",
    "    logger.write(f\"Generated {len(predictions)} predictions\")\n",
    "    \n",
    "    # ğŸ”§ Solar API êµì°¨ê²€ì¦\n",
    "    if solar_api and config['solar_api']['enabled']:\n",
    "        logger.write(\"\\n=== Solar API Cross-Validation ===\")\n",
    "        \n",
    "        # ëª¨ë¸ ì˜ˆì¸¡ê³¼ Solar API ë¹„êµ (ìƒ˜í”Œ 10ê°œ)\n",
    "        solar_results = solar_api.optimize_and_validate(\n",
    "            model_predictions=predictions[:10],  # ì²˜ìŒ 10ê°œ ìƒ˜í”Œ\n",
    "            test_dialogues=test_df['dialogue_preprocessed'].tolist()[:10],\n",
    "            sample_size=10\n",
    "        )\n",
    "        \n",
    "        if solar_results:\n",
    "            logger.write(f\"âœ… Solar API cross-validation completed\")\n",
    "            logger.write(f\"   Model avg length: {solar_results['avg_model_length']:.1f}\")\n",
    "            logger.write(f\"   API avg length: {solar_results['avg_api_length']:.1f}\")\n",
    "            logger.write(f\"   API calls made: {solar_results['api_calls']}\")\n",
    "            \n",
    "            # WandB ë¡œê¹…\n",
    "            if config['wandb']['mode'] != 'disabled':\n",
    "                wandb.log({\n",
    "                    'solar_api/model_length': solar_results['avg_model_length'],\n",
    "                    'solar_api/api_length': solar_results['avg_api_length'],\n",
    "                    'solar_api/api_calls': solar_results['api_calls']\n",
    "                })\n",
    "    else:\n",
    "        logger.write(\"\\nâš ï¸ Solar API cross-validation skipped (disabled or not initialized)\")\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    # test_dfì˜ fname ì»¬ëŸ¼ ì‚¬ìš© (idê°€ ì•„ë‹˜)\n",
    "    submission_df = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': predictions\n",
    "    })\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "    submission_dir = get_path(config['paths']['submission_dir'])\n",
    "    submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    submission_path = submission_dir / f'full_pipeline_submission_{timestamp}.csv'\n",
    "    submission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=Trueë¡œ ë³€ê²½\n",
    "    \n",
    "    logger.write(f\"Submission file saved: {submission_path}\")\n",
    "    logger.write(f\"Shape: {submission_df.shape}\")\n",
    "    \n",
    "    update_status('final_prediction', 'completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
