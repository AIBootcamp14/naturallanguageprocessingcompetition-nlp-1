{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 Full Pipeline - 모든 기법 통합\n",
    "> PRD 계획에 따른 전체 파이프라인 통합 실행\n",
    "\n",
    "**목표 성능**: ROUGE-F1 85+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:52:59.499057Z",
     "iopub.status.busy": "2025-10-10T02:52:59.498955Z",
     "iopub.status.idle": "2025-10-10T02:53:01.421961Z",
     "shell.execute_reply": "2025-10-10T02:53:01.421561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "✅ 나눔고딕 폰트 로드 성공\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 프로젝트 루트 경로 추가\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3번만 parent 사용!\n",
    "\n",
    "# 다른 프로젝트 경로 제거하고 현재 프로젝트 경로만 추가\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# 커스텀 모듈 임포트 - 04_multi_model_ensemble.ipynb에서 참고\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.423110Z",
     "iopub.status.busy": "2025-10-10T02:53:01.422930Z",
     "iopub.status.idle": "2025-10-10T02:53:01.434897Z",
     "shell.execute_reply": "2025-10-10T02:53:01.434500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FULL PIPELINE CONFIGURATION\n",
      "==================================================\n",
      "Pipeline Stages: 9\n",
      "  ✓ data_quality_check\n",
      "  ✓ data_preprocessing\n",
      "  ✓ data_augmentation\n",
      "  ✓ model_training\n",
      "  ✓ cross_validation\n",
      "  ✓ ensemble\n",
      "  ✓ hyperparameter_optimization\n",
      "  ✓ inference_optimization\n",
      "  ✓ final_prediction\n"
     ]
    }
   ],
   "source": [
    "# 설정 파일 로드\n",
    "config_path = notebook_dir / 'configs' / 'config_full_pipeline.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FULL PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pipeline Stages: {len(config['pipeline']['stages'])}\")\n",
    "for stage in config['pipeline']['stages']:\n",
    "    print(f\"  ✓ {stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.435845Z",
     "iopub.status.busy": "2025-10-10T02:53:01.435744Z",
     "iopub.status.idle": "2025-10-10T02:53:01.439048Z",
     "shell.execute_reply": "2025-10-10T02:53:01.438646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: logs/full_pipeline\n",
      "==================================================\n",
      "FULL PIPELINE EXECUTION STARTED\n",
      "Timestamp: 20251010_200633\n",
      "Config: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/configs/config_full_pipeline.yaml\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 로그 디렉토리 생성\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 타임스탬프 생성\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 로거 초기화\n",
    "log_file = log_dir / f'full_pipeline_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('FULL PIPELINE EXECUTION STARTED')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Config: {config_path}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.439995Z",
     "iopub.status.busy": "2025-10-10T02:53:01.439894Z",
     "iopub.status.idle": "2025-10-10T02:53:01.509932Z",
     "shell.execute_reply": "2025-10-10T02:53:01.509443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Tier: LOW\n",
      "Auto-optimization enabled\n",
      "Finding optimal batch size...\n"
     ]
    }
   ],
   "source": [
    "# GPU 최적화 체크\n",
    "# 필요한 모듈 import\n",
    "if 'check_gpu_tier' not in globals():\n",
    "    try:\n",
    "        from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import check_gpu_tier\")\n",
    "        def check_gpu_tier():\n",
    "            return \"UNKNOWN\"\n",
    "\n",
    "# config가 로드되어 있는지 확인\n",
    "if 'config' not in globals():\n",
    "    print(\"Warning: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    if config['gpu']['auto_optimization']['enabled']:\n",
    "        gpu_tier = check_gpu_tier()\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "            logger.write(f\"Auto-optimization enabled\")\n",
    "            \n",
    "            if config['gpu']['auto_optimization']['find_optimal_batch_size']:\n",
    "                logger.write(\"Finding optimal batch size...\")\n",
    "                # 최적 배치 크기 탐색 코드\n",
    "        else:\n",
    "            print(f\"GPU Tier: {gpu_tier}\")\n",
    "            print(f\"Auto-optimization enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.511028Z",
     "iopub.status.busy": "2025-10-10T02:53:01.510918Z",
     "iopub.status.idle": "2025-10-10T02:53:01.513531Z",
     "shell.execute_reply": "2025-10-10T02:53:01.513132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PERFORMANCE TARGETS\n",
      "==================================================\n",
      "ROUGE-1: 0.45\n",
      "ROUGE-2: 0.3\n",
      "ROUGE-L: 0.4\n",
      "Overall Target: 0.85\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 성능 목표 확인\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE TARGETS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROUGE-1: {config['performance_targets']['rouge_1']}\")\n",
    "print(f\"ROUGE-2: {config['performance_targets']['rouge_2']}\")\n",
    "print(f\"ROUGE-L: {config['performance_targets']['rouge_l']}\")\n",
    "print(f\"Overall Target: {config['performance_targets']['overall']}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.514537Z",
     "iopub.status.busy": "2025-10-10T02:53:01.514439Z",
     "iopub.status.idle": "2025-10-10T02:53:01.517293Z",
     "shell.execute_reply": "2025-10-10T02:53:01.516860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_quality_check            : pending\n",
      "data_preprocessing            : pending\n",
      "data_augmentation             : pending\n",
      "model_training                : pending\n",
      "cross_validation              : pending\n",
      "ensemble                      : pending\n",
      "hyperparameter_optimization   : pending\n",
      "inference_optimization        : pending\n",
      "final_prediction              : pending\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 실행 상태 추적\n",
    "# config가 로드되어 있는지 확인\n",
    "if 'config' not in globals():\n",
    "    print(\"Error: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    pipeline_status = {}\n",
    "    for stage in config['pipeline']['stages']:\n",
    "        pipeline_status[stage] = 'pending'\n",
    "\n",
    "    def update_status(stage, status):\n",
    "        pipeline_status[stage] = status\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"[{stage}] Status: {status}\")\n",
    "        else:\n",
    "            print(f\"[{stage}] Status: {status}\")\n",
    "        \n",
    "    # 상태 표시\n",
    "    for stage, status in pipeline_status.items():\n",
    "        print(f\"{stage:30s}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.518296Z",
     "iopub.status.busy": "2025-10-10T02:53:01.518198Z",
     "iopub.status.idle": "2025-10-10T02:53:01.673499Z",
     "shell.execute_reply": "2025-10-10T02:53:01.673101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_quality_check] Status: running\n",
      "\n",
      "=== Data Quality Check ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "✅ Loaded 12457 training samples\n",
      "✅ Loaded 499 dev samples\n",
      "✅ Loaded 499 test samples\n",
      "Null values - Train: 0, Dev: 0, Test: 0\n",
      "Duplicate rows in training data: 0\n",
      "✅ Data loading completed successfully!\n",
      "[data_quality_check] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 데이터 로드 - dev_df와 test_df 포함!\n",
    "# Stage 1: 데이터 품질 검증 및 로드\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if 'data_quality_check' in config['pipeline']['stages']:\n",
    "    update_status('data_quality_check', 'running')\n",
    "    logger.write(\"\\n=== Data Quality Check ===\")\n",
    "    \n",
    "    # config 파일의 경로 사용\n",
    "    def get_data_path(path_str):\n",
    "        \"\"\"config의 상대 경로를 절대 경로로 변환\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # 데이터 경로\n",
    "    train_path = get_data_path(config['paths']['train_file'])\n",
    "    dev_path = get_data_path(config['paths']['dev_file']) \n",
    "    test_path = get_data_path(config['paths']['test_file'])\n",
    "    \n",
    "    logger.write(f\"Loading data from config paths:\")\n",
    "    logger.write(f\"  - Train: {train_path}\")\n",
    "    logger.write(f\"  - Dev: {dev_path}\")\n",
    "    logger.write(f\"  - Test: {test_path}\")\n",
    "    \n",
    "    # 모든 데이터 로드 - train_df, dev_df, test_df 모두!\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    dev_df = pd.read_csv(dev_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    logger.write(f\"✅ Loaded {len(train_df)} training samples\")\n",
    "    logger.write(f\"✅ Loaded {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"✅ Loaded {len(test_df)} test samples\")\n",
    "    \n",
    "    # 기본 품질 검증\n",
    "    if config['data_quality']['enabled']:\n",
    "        # 구조적 검증\n",
    "        if config['data_quality']['checks']['structural']['check_nulls']:\n",
    "            train_nulls = train_df.isnull().sum().sum()\n",
    "            dev_nulls = dev_df.isnull().sum().sum()\n",
    "            test_nulls = test_df.isnull().sum().sum()\n",
    "            logger.write(f\"Null values - Train: {train_nulls}, Dev: {dev_nulls}, Test: {test_nulls}\")\n",
    "        \n",
    "        if config['data_quality']['checks']['structural']['check_duplicates']:\n",
    "            train_dups = train_df.duplicated().sum()\n",
    "            logger.write(f\"Duplicate rows in training data: {train_dups}\")\n",
    "    \n",
    "    logger.write(\"✅ Data loading completed successfully!\")\n",
    "    update_status('data_quality_check', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.674757Z",
     "iopub.status.busy": "2025-10-10T02:53:01.674628Z",
     "iopub.status.idle": "2025-10-10T02:53:01.724276Z",
     "shell.execute_reply": "2025-10-10T02:53:01.723835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Data Quality Validation ===\n",
      "\n",
      "📊 Data Quality Report:\n",
      "\n",
      "Structural Validation:\n",
      "  - Train shape: (12457, 4)\n",
      "  - Dev shape: (499, 4)\n",
      "  - Test shape: (499, 2)\n",
      "  - Column match: True\n",
      "\n",
      "Text Quality:\n",
      "  - Avg dialogue length: 406.1\n",
      "  - Compression ratio: 23.23%\n",
      "  - Encoding issues: 0\n",
      "  - Special chars: 12455\n",
      "\n",
      "Label Distribution:\n",
      "  - Unique topics: 9235\n",
      "  - Imbalance ratio: 130.00\n",
      "\n",
      "Outlier Detection:\n",
      "  - Outlier count: 355\n",
      "  - Outlier ratio: 2.85%\n",
      "\n",
      "📋 Recommendations:\n",
      "  ✓ Clean special characters from text\n",
      "  ✓ Consider data augmentation for underrepresented topics\n",
      "⚠️ Error during data validation: You must call wandb.init() before wandb.log()\n",
      "   Skipping detailed validation. Please check data loading in cell 7.\n"
     ]
    }
   ],
   "source": [
    "# Stage 1.5: 상세 데이터 품질 검증 (PRD 16_데이터_품질_검증_시스템.md)\n",
    "# 주의: 이 셀은 셀 7 (데이터 로드) 실행 후에 실행해야 합니다!\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "class FullPipelineDataValidator:\n",
    "    \"\"\"전체 파이프라인용 데이터 품질 검증\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.validation_report = {}\n",
    "        \n",
    "    def comprehensive_validation(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"포괄적 데이터 검증\"\"\"\n",
    "        self.logger.write(\"\\n=== Comprehensive Data Quality Validation ===\")\n",
    "        \n",
    "        report = {}\n",
    "        \n",
    "        # 1. 구조적 검증\n",
    "        report['structural'] = self._validate_structure(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 2. 텍스트 품질 검증\n",
    "        report['text_quality'] = self._validate_text_quality(train_df)\n",
    "        \n",
    "        # 3. 라벨 분포 검증\n",
    "        report['label_distribution'] = self._validate_label_distribution(train_df)\n",
    "        \n",
    "        # 4. 데이터 일관성 검증\n",
    "        report['consistency'] = self._validate_consistency(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 5. 이상치 검출\n",
    "        report['outliers'] = self._detect_outliers(train_df)\n",
    "        \n",
    "        self.validation_report = report\n",
    "        return report\n",
    "    \n",
    "    def _validate_structure(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"구조적 검증\"\"\"\n",
    "        return {\n",
    "            'train_shape': train_df.shape,\n",
    "            'dev_shape': dev_df.shape,\n",
    "            'test_shape': test_df.shape,\n",
    "            'train_nulls': train_df.isnull().sum().sum(),\n",
    "            'dev_nulls': dev_df.isnull().sum().sum(),\n",
    "            'test_nulls': test_df.isnull().sum().sum(),\n",
    "            'train_duplicates': train_df.duplicated().sum(),\n",
    "            'column_match': set(train_df.columns) == set(dev_df.columns)\n",
    "        }\n",
    "    \n",
    "    def _validate_text_quality(self, df) -> Dict:\n",
    "        \"\"\"텍스트 품질 검증\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n",
    "        \n",
    "        # 특수 문자 패턴\n",
    "        special_chars = df['dialogue'].str.contains('[�\\\\?\\\\x00-\\\\x1f]').sum()\n",
    "        \n",
    "        # 인코딩 문제\n",
    "        encoding_issues = df['dialogue'].apply(\n",
    "            lambda x: bool(re.search(r'[\\ufffd]', str(x)))\n",
    "        ).sum()\n",
    "        \n",
    "        return {\n",
    "            'avg_dialogue_length': dialogue_lengths.mean(),\n",
    "            'max_dialogue_length': dialogue_lengths.max(),\n",
    "            'min_dialogue_length': dialogue_lengths.min(),\n",
    "            'avg_summary_length': summary_lengths.mean() if 'summary' in df.columns else 0,\n",
    "            'compression_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0,\n",
    "            'special_chars_count': int(special_chars),\n",
    "            'encoding_issues': int(encoding_issues),\n",
    "            'empty_dialogues': (dialogue_lengths == 0).sum()\n",
    "        }\n",
    "    \n",
    "    def _validate_label_distribution(self, df) -> Dict:\n",
    "        \"\"\"라벨 분포 검증\"\"\"\n",
    "        if 'topic' not in df.columns:\n",
    "            return {}\n",
    "        \n",
    "        topic_counts = df['topic'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'unique_topics': len(topic_counts),\n",
    "            'most_common_topic': topic_counts.index[0] if len(topic_counts) > 0 else None,\n",
    "            'most_common_count': int(topic_counts.iloc[0]) if len(topic_counts) > 0 else 0,\n",
    "            'least_common_topic': topic_counts.index[-1] if len(topic_counts) > 0 else None,\n",
    "            'least_common_count': int(topic_counts.iloc[-1]) if len(topic_counts) > 0 else 0,\n",
    "            'imbalance_ratio': float(topic_counts.iloc[0] / topic_counts.iloc[-1]) if len(topic_counts) > 1 and topic_counts.iloc[-1] > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _validate_consistency(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"데이터 일관성 검증\"\"\"\n",
    "        # Person 태그 일관성\n",
    "        train_person_tags = train_df['dialogue'].str.contains('#Person').mean()\n",
    "        dev_person_tags = dev_df['dialogue'].str.contains('#Person').mean()\n",
    "        test_person_tags = test_df['dialogue'].str.contains('#Person').mean()\n",
    "        \n",
    "        return {\n",
    "            'train_person_tag_ratio': float(train_person_tags),\n",
    "            'dev_person_tag_ratio': float(dev_person_tags),\n",
    "            'test_person_tag_ratio': float(test_person_tags),\n",
    "            'person_tag_consistent': abs(train_person_tags - dev_person_tags) < 0.1\n",
    "        }\n",
    "    \n",
    "    def _detect_outliers(self, df) -> Dict:\n",
    "        \"\"\"이상치 검출\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        \n",
    "        # IQR 기반 이상치\n",
    "        Q1 = dialogue_lengths.quantile(0.25)\n",
    "        Q3 = dialogue_lengths.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (dialogue_lengths < lower_bound) | (dialogue_lengths > upper_bound)\n",
    "        \n",
    "        return {\n",
    "            'outlier_count': int(outliers.sum()),\n",
    "            'outlier_ratio': float(outliers.mean()),\n",
    "            'lower_bound': float(lower_bound),\n",
    "            'upper_bound': float(upper_bound)\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"개선 권장사항 생성\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if not self.validation_report:\n",
    "            return recommendations\n",
    "        \n",
    "        # 구조적 문제\n",
    "        if self.validation_report.get('structural', {}).get('train_nulls', 0) > 0:\n",
    "            recommendations.append(\"Remove or impute null values in training data\")\n",
    "        \n",
    "        # 텍스트 품질 문제\n",
    "        text_quality = self.validation_report.get('text_quality', {})\n",
    "        if text_quality.get('encoding_issues', 0) > 0:\n",
    "            recommendations.append(f\"Fix {text_quality['encoding_issues']} encoding issues\")\n",
    "        \n",
    "        if text_quality.get('special_chars_count', 0) > 0:\n",
    "            recommendations.append(\"Clean special characters from text\")\n",
    "        \n",
    "        # 라벨 불균형\n",
    "        label_dist = self.validation_report.get('label_distribution', {})\n",
    "        if label_dist.get('imbalance_ratio', 0) > 10:\n",
    "            recommendations.append(\"Consider data augmentation for underrepresented topics\")\n",
    "        \n",
    "        # 이상치\n",
    "        outliers = self.validation_report.get('outliers', {})\n",
    "        if outliers.get('outlier_ratio', 0) > 0.05:\n",
    "            recommendations.append(f\"Review {outliers['outlier_count']} outlier samples\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# 이제 실제 검증 실행 - 셀 7에서 이미 로드한 데이터를 사용\n",
    "# 데이터가 로드되지 않았다면 스킵\n",
    "try:\n",
    "    # train_df, dev_df, test_df가 정의되어 있는지 확인\n",
    "    if 'train_df' not in locals() or 'dev_df' not in locals() or 'test_df' not in locals():\n",
    "        logger.write(\"⚠️ Data not loaded yet. Skipping detailed validation.\")\n",
    "        logger.write(\"   Please run cell 7 first to load the data.\")\n",
    "    else:\n",
    "        # 상세 검증 실행\n",
    "        data_validator = FullPipelineDataValidator(logger)\n",
    "        validation_report = data_validator.comprehensive_validation(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 검증 결과 출력\n",
    "        logger.write(\"\\n📊 Data Quality Report:\")\n",
    "        \n",
    "        # 구조적 검증 결과\n",
    "        structural = validation_report.get('structural', {})\n",
    "        logger.write(f\"\\nStructural Validation:\")\n",
    "        logger.write(f\"  - Train shape: {structural.get('train_shape')}\")\n",
    "        logger.write(f\"  - Dev shape: {structural.get('dev_shape')}\")\n",
    "        logger.write(f\"  - Test shape: {structural.get('test_shape')}\")\n",
    "        logger.write(f\"  - Column match: {structural.get('column_match')}\")\n",
    "        \n",
    "        # 텍스트 품질 결과\n",
    "        text_quality = validation_report.get('text_quality', {})\n",
    "        logger.write(f\"\\nText Quality:\")\n",
    "        logger.write(f\"  - Avg dialogue length: {text_quality.get('avg_dialogue_length', 0):.1f}\")\n",
    "        logger.write(f\"  - Compression ratio: {text_quality.get('compression_ratio', 0):.2%}\")\n",
    "        logger.write(f\"  - Encoding issues: {text_quality.get('encoding_issues', 0)}\")\n",
    "        logger.write(f\"  - Special chars: {text_quality.get('special_chars_count', 0)}\")\n",
    "        \n",
    "        # 라벨 분포 결과\n",
    "        label_dist = validation_report.get('label_distribution', {})\n",
    "        if label_dist:\n",
    "            logger.write(f\"\\nLabel Distribution:\")\n",
    "            logger.write(f\"  - Unique topics: {label_dist.get('unique_topics')}\")\n",
    "            logger.write(f\"  - Imbalance ratio: {label_dist.get('imbalance_ratio', 0):.2f}\")\n",
    "        \n",
    "        # 이상치 검출 결과\n",
    "        outliers = validation_report.get('outliers', {})\n",
    "        logger.write(f\"\\nOutlier Detection:\")\n",
    "        logger.write(f\"  - Outlier count: {outliers.get('outlier_count', 0)}\")\n",
    "        logger.write(f\"  - Outlier ratio: {outliers.get('outlier_ratio', 0):.2%}\")\n",
    "        \n",
    "        # 권장사항\n",
    "        recommendations = data_validator.generate_recommendations()\n",
    "        if recommendations:\n",
    "            logger.write(\"\\n📋 Recommendations:\")\n",
    "            for rec in recommendations:\n",
    "                logger.write(f\"  ✓ {rec}\")\n",
    "        \n",
    "        # WandB 로깅\n",
    "        if config['wandb']['mode'] != 'disabled':\n",
    "            wandb.log({\n",
    "                'data_quality/nulls': structural.get('train_nulls', 0),\n",
    "                'data_quality/duplicates': structural.get('train_duplicates', 0),\n",
    "                'data_quality/encoding_issues': text_quality.get('encoding_issues', 0),\n",
    "                'data_quality/outlier_ratio': outliers.get('outlier_ratio', 0)\n",
    "            })\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.write(f\"⚠️ Error during data validation: {str(e)}\")\n",
    "    logger.write(\"   Skipping detailed validation. Please check data loading in cell 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.725412Z",
     "iopub.status.busy": "2025-10-10T02:53:01.725308Z",
     "iopub.status.idle": "2025-10-10T02:53:01.728266Z",
     "shell.execute_reply": "2025-10-10T02:53:01.727730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations will be saved to: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/full_pipeline/visualizations\n"
     ]
    }
   ],
   "source": [
    "# 시각화 설정\n",
    "if config['visualization']['enabled']:\n",
    "    viz = TrainingVisualizer()\n",
    "    \n",
    "    # config의 시각화 경로 사용\n",
    "    viz_path = config['visualization']['save_path']\n",
    "    if not Path(viz_path).is_absolute():\n",
    "        viz_dir = notebook_dir / viz_path\n",
    "    else:\n",
    "        viz_dir = Path(viz_path)\n",
    "    \n",
    "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.write(f\"Visualizations will be saved to: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.729208Z",
     "iopub.status.busy": "2025-10-10T02:53:01.729114Z",
     "iopub.status.idle": "2025-10-10T02:53:03.024570Z",
     "shell.execute_reply": "2025-10-10T02:53:03.024035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mieyeppo-job\u001b[0m (\u001b[33mkimsunmin0227-hufs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/wandb/run-20251010_200635-0btj1gs4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4' target=\"_blank\">full-pipeline-integrated</a></strong> to <a href='https://wandb.ai/ieyeppo/nlp-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ieyeppo/nlp-competition' target=\"_blank\">https://wandb.ai/ieyeppo/nlp-competition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4' target=\"_blank\">https://wandb.ai/ieyeppo/nlp-competition/runs/0btj1gs4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for full pipeline tracking\n"
     ]
    }
   ],
   "source": [
    "# WandB 초기화 (전체 파이프라인 추적)\n",
    "if config['wandb']['mode'] != 'disabled':\n",
    "    wandb.init(\n",
    "        project=config['wandb']['project'],\n",
    "        entity=config['wandb']['entity'],\n",
    "        name=config['wandb']['name'],\n",
    "        tags=config['wandb']['tags'],\n",
    "        config=config\n",
    "    )\n",
    "    logger.write(\"WandB initialized for full pipeline tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 파이프라인 실행 코드는 config 파일 설정에 따라 구현\n",
    "\n",
    "### 실행 단계:\n",
    "1. 데이터 품질 검증\n",
    "2. 데이터 전처리 및 증강\n",
    "3. 모델 학습 (Multi-model)\n",
    "4. K-Fold 교차 검증\n",
    "5. Optuna 최적화\n",
    "6. 앙상블 + TTA\n",
    "7. 추론 최적화\n",
    "8. 최종 예측 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.025825Z",
     "iopub.status.busy": "2025-10-10T02:53:03.025717Z",
     "iopub.status.idle": "2025-10-10T02:53:03.295081Z",
     "shell.execute_reply": "2025-10-10T02:53:03.294653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_preprocessing] Status: running\n",
      "\n",
      "=== Data Preprocessing ===\n",
      "Preprocessed 12457 training samples\n",
      "Preprocessed 499 dev samples\n",
      "Preprocessed 499 test samples\n",
      "\n",
      "Text Length Statistics:\n",
      "  Dialogue - Mean: 347.3, Max: 1952\n",
      "  Summary - Mean: 85.8, Max: 376\n",
      "[data_preprocessing] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: 데이터 전처리\n",
    "if 'data_preprocessing' in config['pipeline']['stages']:\n",
    "    update_status('data_preprocessing', 'running')\n",
    "    logger.write(\"\\n=== Data Preprocessing ===\")\n",
    "    \n",
    "    # 전처리 함수 정의\n",
    "    import re\n",
    "    \n",
    "    def preprocess_dialogue(text):\n",
    "        \"\"\"대화 텍스트 전처리\"\"\"\n",
    "        # 노이즈 제거\n",
    "        text = text.replace('\\\\n', '\\n')\n",
    "        text = text.replace('<br>', '\\n')\n",
    "        text = text.strip()\n",
    "        \n",
    "        # #Person 태그 정규화\n",
    "        text = re.sub(r'#Person(\\d+)#:', r'화자\\1:', text)\n",
    "        \n",
    "        # 중복 공백 제거\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_summary(text):\n",
    "        \"\"\"요약 텍스트 전처리\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    # 전처리 적용\n",
    "    train_df['dialogue_preprocessed'] = train_df['dialogue'].apply(preprocess_dialogue)\n",
    "    train_df['summary_preprocessed'] = train_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    dev_df['dialogue_preprocessed'] = dev_df['dialogue'].apply(preprocess_dialogue)\n",
    "    dev_df['summary_preprocessed'] = dev_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    test_df['dialogue_preprocessed'] = test_df['dialogue'].apply(preprocess_dialogue)\n",
    "    \n",
    "    logger.write(f\"Preprocessed {len(train_df)} training samples\")\n",
    "    logger.write(f\"Preprocessed {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"Preprocessed {len(test_df)} test samples\")\n",
    "    \n",
    "    # 전처리 후 텍스트 길이 분석\n",
    "    train_dialogue_lengths = train_df['dialogue_preprocessed'].str.len()\n",
    "    train_summary_lengths = train_df['summary_preprocessed'].str.len()\n",
    "    \n",
    "    logger.write(f\"\\nText Length Statistics:\")\n",
    "    logger.write(f\"  Dialogue - Mean: {train_dialogue_lengths.mean():.1f}, Max: {train_dialogue_lengths.max()}\")\n",
    "    logger.write(f\"  Summary - Mean: {train_summary_lengths.mean():.1f}, Max: {train_summary_lengths.max()}\")\n",
    "    \n",
    "    update_status('data_preprocessing', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.567069Z",
     "iopub.status.busy": "2025-10-10T02:53:03.566966Z",
     "iopub.status.idle": "2025-10-10T02:53:03.584546Z",
     "shell.execute_reply": "2025-10-10T02:53:03.584141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 20:06:36,982] A new study created in memory with name: pipeline_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optuna is available and will be used for hyperparameter optimization!\n",
      "[hyperparameter_optimization] Status: running\n",
      "\n",
      "======================================================================\n",
      "🎯 HYPERPARAMETER OPTIMIZATION STAGE\n",
      "======================================================================\n",
      "✅ Optimization ENABLED with 100 trials\n",
      "\n",
      "============================================================\n",
      "🎯 OPTUNA HYPERPARAMETER OPTIMIZATION STARTING\n",
      "============================================================\n",
      "Number of trials: 100\n",
      "Optimization metric: rouge_l\n",
      "\n",
      "🚀 Starting optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab66c5892864d478cc940b080527031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 0: {'learning_rate': 5.6115164153345e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 24, 'num_beams': 2, 'temperature': 0.8795585311974417, 'warmup_ratio': 0.12022300234864176, 'weight_decay': 0.07080725777960455, 'top_p': 0.8041168988591605}\n",
      "[I 2025-10-10 20:06:36,989] Trial 0 finished with value: 0.653403193335232 and parameters: {'learning_rate': 5.6115164153345e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 24, 'num_beams': 2, 'temperature': 0.8795585311974417, 'warmup_ratio': 0.12022300234864176, 'weight_decay': 0.07080725777960455, 'top_p': 0.8041168988591605}. Best is trial 0 with value: 0.653403193335232.\n",
      "\n",
      "Trial 1: {'learning_rate': 0.0008706020878304854, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 40, 'num_beams': 5, 'temperature': 0.48875051677790415, 'warmup_ratio': 0.058245828039608386, 'weight_decay': 0.06118528947223795, 'top_p': 0.8278987721304084}\n",
      "[I 2025-10-10 20:06:36,991] Trial 1 finished with value: 0.6842533872071003 and parameters: {'learning_rate': 0.0008706020878304854, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 40, 'num_beams': 5, 'temperature': 0.48875051677790415, 'warmup_ratio': 0.058245828039608386, 'weight_decay': 0.06118528947223795, 'top_p': 0.8278987721304084}. Best is trial 1 with value: 0.6842533872071003.\n",
      "\n",
      "Trial 2: {'learning_rate': 3.8396292998041685e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 72, 'num_beams': 6, 'temperature': 0.14180537144799796, 'warmup_ratio': 0.12150897038028768, 'weight_decay': 0.017052412368729154, 'top_p': 0.813010318597056}\n",
      "[I 2025-10-10 20:06:36,993] Trial 2 finished with value: 0.6078212758071612 and parameters: {'learning_rate': 3.8396292998041685e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 72, 'num_beams': 6, 'temperature': 0.14180537144799796, 'warmup_ratio': 0.12150897038028768, 'weight_decay': 0.017052412368729154, 'top_p': 0.813010318597056}. Best is trial 1 with value: 0.6842533872071003.\n",
      "\n",
      "Trial 3: {'learning_rate': 0.000790261954970823, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 88, 'num_beams': 5, 'temperature': 0.20983441136030095, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'top_p': 0.9818640804157565}\n",
      "[I 2025-10-10 20:06:36,995] Trial 3 finished with value: 0.6887265288736684 and parameters: {'learning_rate': 0.000790261954970823, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 88, 'num_beams': 5, 'temperature': 0.20983441136030095, 'warmup_ratio': 0.09903538202225404, 'weight_decay': 0.0034388521115218396, 'top_p': 0.9818640804157565}. Best is trial 3 with value: 0.6887265288736684.\n",
      "\n",
      "Trial 4: {'learning_rate': 3.292759134423613e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.7976195410250031, 'warmup_ratio': 0.18789978831283782, 'weight_decay': 0.08948273504276488, 'top_p': 0.919579995762217}\n",
      "[I 2025-10-10 20:06:36,997] Trial 4 finished with value: 0.7028705250224668 and parameters: {'learning_rate': 3.292759134423613e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.7976195410250031, 'warmup_ratio': 0.18789978831283782, 'weight_decay': 0.08948273504276488, 'top_p': 0.919579995762217}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 5: {'learning_rate': 0.0006978281265126031, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 56, 'num_beams': 3, 'temperature': 0.8458637582367364, 'warmup_ratio': 0.07135066533871785, 'weight_decay': 0.02809345096873808, 'top_p': 0.9085392166316497}\n",
      "[I 2025-10-10 20:06:37,000] Trial 5 finished with value: 0.5479726511657692 and parameters: {'learning_rate': 0.0006978281265126031, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 56, 'num_beams': 3, 'temperature': 0.8458637582367364, 'warmup_ratio': 0.07135066533871785, 'weight_decay': 0.02809345096873808, 'top_p': 0.9085392166316497}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 6: {'learning_rate': 1.913588048769229e-05, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 32, 'num_beams': 2, 'temperature': 0.8339152856093507, 'warmup_ratio': 0.14137146876952342, 'weight_decay': 0.07290071680409874, 'top_p': 0.9542540693371891}\n",
      "[I 2025-10-10 20:06:37,001] Trial 6 finished with value: 0.48001959905641206 and parameters: {'learning_rate': 1.913588048769229e-05, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 32, 'num_beams': 2, 'temperature': 0.8339152856093507, 'warmup_ratio': 0.14137146876952342, 'weight_decay': 0.07290071680409874, 'top_p': 0.9542540693371891}. Best is trial 4 with value: 0.7028705250224668.\n",
      "\n",
      "Trial 7: {'learning_rate': 1.4063366777718176e-05, 'batch_size': 16, 'lora_r': 40, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.379884089544096, 'warmup_ratio': 0.06503666440534941, 'weight_decay': 0.0729606178338064, 'top_p': 0.9275114942710426}\n",
      "[I 2025-10-10 20:06:37,003] Trial 7 finished with value: 0.7119745405334588 and parameters: {'learning_rate': 1.4063366777718176e-05, 'batch_size': 16, 'lora_r': 40, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.379884089544096, 'warmup_ratio': 0.06503666440534941, 'weight_decay': 0.0729606178338064, 'top_p': 0.9275114942710426}. Best is trial 7 with value: 0.7119745405334588.\n",
      "\n",
      "Trial 8: {'learning_rate': 0.000594874681321977, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 72, 'num_beams': 7, 'temperature': 0.5444160367279517, 'warmup_ratio': 0.10454656587639882, 'weight_decay': 0.042754101835854964, 'top_p': 0.8050838253488191}\n",
      "[I 2025-10-10 20:06:37,005] Trial 8 finished with value: 0.5459338528853336 and parameters: {'learning_rate': 0.000594874681321977, 'batch_size': 16, 'lora_r': 52, 'lora_alpha': 72, 'num_beams': 7, 'temperature': 0.5444160367279517, 'warmup_ratio': 0.10454656587639882, 'weight_decay': 0.042754101835854964, 'top_p': 0.8050838253488191}. Best is trial 7 with value: 0.7119745405334588.\n",
      "\n",
      "Trial 9: {'learning_rate': 1.6435497475111308e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 120, 'num_beams': 3, 'temperature': 0.4693446307320668, 'warmup_ratio': 0.15111022770860974, 'weight_decay': 0.022879816549162248, 'top_p': 0.8153959819657587}\n",
      "[I 2025-10-10 20:06:37,007] Trial 9 finished with value: 0.8373849124158405 and parameters: {'learning_rate': 1.6435497475111308e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 120, 'num_beams': 3, 'temperature': 0.4693446307320668, 'warmup_ratio': 0.15111022770860974, 'weight_decay': 0.022879816549162248, 'top_p': 0.8153959819657587}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 10: {'learning_rate': 0.0002509538254778771, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.37050834074441147, 'warmup_ratio': 0.0031189599316912286, 'weight_decay': 0.03924528921026325, 'top_p': 0.8669590352446472}\n",
      "[I 2025-10-10 20:06:37,015] Trial 10 finished with value: 0.6452920710377574 and parameters: {'learning_rate': 0.0002509538254778771, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.37050834074441147, 'warmup_ratio': 0.0031189599316912286, 'weight_decay': 0.03924528921026325, 'top_p': 0.8669590352446472}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 11: {'learning_rate': 1.0322960416065201e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.3360442409320184, 'warmup_ratio': 0.18218996750099392, 'weight_decay': 0.09435438058122501, 'top_p': 0.8647628938861509}\n",
      "[I 2025-10-10 20:06:37,022] Trial 11 finished with value: 0.7495056799312688 and parameters: {'learning_rate': 1.0322960416065201e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.3360442409320184, 'warmup_ratio': 0.18218996750099392, 'weight_decay': 0.09435438058122501, 'top_p': 0.8647628938861509}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 12: {'learning_rate': 1.0370579352014641e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6804495294564787, 'warmup_ratio': 0.19775497252335106, 'weight_decay': 0.09904235739514478, 'top_p': 0.8590398672080966}\n",
      "[I 2025-10-10 20:06:37,030] Trial 12 finished with value: 0.6109526820744092 and parameters: {'learning_rate': 1.0370579352014641e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6804495294564787, 'warmup_ratio': 0.19775497252335106, 'weight_decay': 0.09904235739514478, 'top_p': 0.8590398672080966}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 13: {'learning_rate': 0.00012774135799436964, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.3022059308213879, 'warmup_ratio': 0.16191281769380567, 'weight_decay': 0.01896901131981868, 'top_p': 0.8593943414843724}\n",
      "[I 2025-10-10 20:06:37,036] Trial 13 finished with value: 0.49933244699373697 and parameters: {'learning_rate': 0.00012774135799436964, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.3022059308213879, 'warmup_ratio': 0.16191281769380567, 'weight_decay': 0.01896901131981868, 'top_p': 0.8593943414843724}. Best is trial 9 with value: 0.8373849124158405.\n",
      "\n",
      "Trial 14: {'learning_rate': 2.1533663334390727e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6503220664418476, 'warmup_ratio': 0.16274515950801005, 'weight_decay': 0.05166548565180652, 'top_p': 0.8830054677342667}\n",
      "[I 2025-10-10 20:06:37,043] Trial 14 finished with value: 0.8416676785296207 and parameters: {'learning_rate': 2.1533663334390727e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6503220664418476, 'warmup_ratio': 0.16274515950801005, 'weight_decay': 0.05166548565180652, 'top_p': 0.8830054677342667}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 15: {'learning_rate': 2.4165483913864722e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6636798035918531, 'warmup_ratio': 0.157051761228143, 'weight_decay': 0.0006794582733295057, 'top_p': 0.8360378813543303}\n",
      "[I 2025-10-10 20:06:37,050] Trial 15 finished with value: 0.7322748631864108 and parameters: {'learning_rate': 2.4165483913864722e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 4, 'temperature': 0.6636798035918531, 'warmup_ratio': 0.157051761228143, 'weight_decay': 0.0006794582733295057, 'top_p': 0.8360378813543303}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 16: {'learning_rate': 6.956746038661218e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.6784268911829191, 'warmup_ratio': 0.16304971347740554, 'weight_decay': 0.055188970463428993, 'top_p': 0.8940003385967338}\n",
      "[I 2025-10-10 20:06:37,056] Trial 16 finished with value: 0.5115506843922398 and parameters: {'learning_rate': 6.956746038661218e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.6784268911829191, 'warmup_ratio': 0.16304971347740554, 'weight_decay': 0.055188970463428993, 'top_p': 0.8940003385967338}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 17: {'learning_rate': 0.00013738198354053957, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 88, 'num_beams': 4, 'temperature': 0.995123861989222, 'warmup_ratio': 0.14050981032764043, 'weight_decay': 0.033282416286616795, 'top_p': 0.8866571662960941}\n",
      "[I 2025-10-10 20:06:37,064] Trial 17 finished with value: 0.7625392949577587 and parameters: {'learning_rate': 0.00013738198354053957, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 88, 'num_beams': 4, 'temperature': 0.995123861989222, 'warmup_ratio': 0.14050981032764043, 'weight_decay': 0.033282416286616795, 'top_p': 0.8866571662960941}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 18: {'learning_rate': 2.565570347466517e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 3, 'temperature': 0.45267481671743476, 'warmup_ratio': 0.1729760059402413, 'weight_decay': 0.04828486464613923, 'top_p': 0.8379315845736517}\n",
      "[I 2025-10-10 20:06:37,071] Trial 18 finished with value: 0.5631407779156049 and parameters: {'learning_rate': 2.565570347466517e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 3, 'temperature': 0.45267481671743476, 'warmup_ratio': 0.1729760059402413, 'weight_decay': 0.04828486464613923, 'top_p': 0.8379315845736517}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 19: {'learning_rate': 5.657396087817041e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 112, 'num_beams': 5, 'temperature': 0.6005475021920503, 'warmup_ratio': 0.14473687452448478, 'weight_decay': 0.02169838469202746, 'top_p': 0.9430718175625277}\n",
      "[I 2025-10-10 20:06:37,078] Trial 19 finished with value: 0.5386437335137281 and parameters: {'learning_rate': 5.657396087817041e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 112, 'num_beams': 5, 'temperature': 0.6005475021920503, 'warmup_ratio': 0.14473687452448478, 'weight_decay': 0.02169838469202746, 'top_p': 0.9430718175625277}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 20: {'learning_rate': 1.8438882376111873e-05, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7497199490525606, 'warmup_ratio': 0.027753987886119755, 'weight_decay': 0.05718553129235758, 'top_p': 0.9987685570099417}\n",
      "[I 2025-10-10 20:06:37,085] Trial 20 finished with value: 0.7653741684030027 and parameters: {'learning_rate': 1.8438882376111873e-05, 'batch_size': 8, 'lora_r': 64, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7497199490525606, 'warmup_ratio': 0.027753987886119755, 'weight_decay': 0.05718553129235758, 'top_p': 0.9987685570099417}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 21: {'learning_rate': 1.683848556284197e-05, 'batch_size': 8, 'lora_r': 60, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7468655799630235, 'warmup_ratio': 0.0014942070483525624, 'weight_decay': 0.061836192221023066, 'top_p': 0.9757254013166067}\n",
      "[I 2025-10-10 20:06:37,093] Trial 21 finished with value: 0.615050029279845 and parameters: {'learning_rate': 1.683848556284197e-05, 'batch_size': 8, 'lora_r': 60, 'lora_alpha': 88, 'num_beams': 6, 'temperature': 0.7468655799630235, 'warmup_ratio': 0.0014942070483525624, 'weight_decay': 0.061836192221023066, 'top_p': 0.9757254013166067}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 22: {'learning_rate': 3.127131627807907e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5957128359321127, 'warmup_ratio': 0.03063050371447289, 'weight_decay': 0.08208672433681731, 'top_p': 0.994075960980361}\n",
      "[I 2025-10-10 20:06:37,098] Trial 22 finished with value: 0.8290308469847495 and parameters: {'learning_rate': 3.127131627807907e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5957128359321127, 'warmup_ratio': 0.03063050371447289, 'weight_decay': 0.08208672433681731, 'top_p': 0.994075960980361}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 23: {'learning_rate': 3.90539962849963e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.5873167389465909, 'warmup_ratio': 0.030803275149133752, 'weight_decay': 0.08148272405058854, 'top_p': 0.8805783124166526}\n",
      "[I 2025-10-10 20:06:37,104] Trial 23 finished with value: 0.7133210726638556 and parameters: {'learning_rate': 3.90539962849963e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.5873167389465909, 'warmup_ratio': 0.030803275149133752, 'weight_decay': 0.08148272405058854, 'top_p': 0.8805783124166526}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 24: {'learning_rate': 8.403991127384985e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.4710274858116029, 'warmup_ratio': 0.1033660521394493, 'weight_decay': 0.012169455502476813, 'top_p': 0.9424558555729028}\n",
      "[I 2025-10-10 20:06:37,110] Trial 24 finished with value: 0.5989579270947828 and parameters: {'learning_rate': 8.403991127384985e-05, 'batch_size': 8, 'lora_r': 4, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.4710274858116029, 'warmup_ratio': 0.1033660521394493, 'weight_decay': 0.012169455502476813, 'top_p': 0.9424558555729028}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 25: {'learning_rate': 2.452510912989695e-05, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5302243724466196, 'warmup_ratio': 0.0804800128901909, 'weight_decay': 0.08487682398674518, 'top_p': 0.9100897473891826}\n",
      "[I 2025-10-10 20:06:37,116] Trial 25 finished with value: 0.6655383459914141 and parameters: {'learning_rate': 2.452510912989695e-05, 'batch_size': 8, 'lora_r': 44, 'lora_alpha': 112, 'num_beams': 7, 'temperature': 0.5302243724466196, 'warmup_ratio': 0.0804800128901909, 'weight_decay': 0.08487682398674518, 'top_p': 0.9100897473891826}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 26: {'learning_rate': 1.2739385933835197e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.6350284049381522, 'warmup_ratio': 0.12181876304438032, 'weight_decay': 0.029880663607875134, 'top_p': 0.9640723701100812}\n",
      "[I 2025-10-10 20:06:37,121] Trial 26 finished with value: 0.6501772011282219 and parameters: {'learning_rate': 1.2739385933835197e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.6350284049381522, 'warmup_ratio': 0.12181876304438032, 'weight_decay': 0.029880663607875134, 'top_p': 0.9640723701100812}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 27: {'learning_rate': 3.5610140632963465e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.42776666328018165, 'warmup_ratio': 0.04048265469146311, 'weight_decay': 0.046029485305756374, 'top_p': 0.8254055086438484}\n",
      "[I 2025-10-10 20:06:37,128] Trial 27 finished with value: 0.7689657061813346 and parameters: {'learning_rate': 3.5610140632963465e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.42776666328018165, 'warmup_ratio': 0.04048265469146311, 'weight_decay': 0.046029485305756374, 'top_p': 0.8254055086438484}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 28: {'learning_rate': 0.0002090316068728911, 'batch_size': 4, 'lora_r': 40, 'lora_alpha': 64, 'num_beams': 3, 'temperature': 0.25976266522502295, 'warmup_ratio': 0.08862058456303999, 'weight_decay': 0.03602600827614375, 'top_p': 0.8502931757834995}\n",
      "[I 2025-10-10 20:06:37,134] Trial 28 finished with value: 0.616605540966958 and parameters: {'learning_rate': 0.0002090316068728911, 'batch_size': 4, 'lora_r': 40, 'lora_alpha': 64, 'num_beams': 3, 'temperature': 0.25976266522502295, 'warmup_ratio': 0.08862058456303999, 'weight_decay': 0.03602600827614375, 'top_p': 0.8502931757834995}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 29: {'learning_rate': 5.342037044887353e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 80, 'num_beams': 2, 'temperature': 0.9306004540295836, 'warmup_ratio': 0.13067292225107183, 'weight_decay': 0.0809760169872833, 'top_p': 0.9995237949195853}\n",
      "[I 2025-10-10 20:06:37,140] Trial 29 finished with value: 0.49841995360926694 and parameters: {'learning_rate': 5.342037044887353e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 80, 'num_beams': 2, 'temperature': 0.9306004540295836, 'warmup_ratio': 0.13067292225107183, 'weight_decay': 0.0809760169872833, 'top_p': 0.9995237949195853}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 30: {'learning_rate': 5.572499750139758e-05, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.7275267954460478, 'warmup_ratio': 0.14961198398797018, 'weight_decay': 0.008934880367805834, 'top_p': 0.9274995338850026}\n",
      "[I 2025-10-10 20:06:37,148] Trial 30 finished with value: 0.658446634628153 and parameters: {'learning_rate': 5.572499750139758e-05, 'batch_size': 4, 'lora_r': 8, 'lora_alpha': 120, 'num_beams': 8, 'temperature': 0.7275267954460478, 'warmup_ratio': 0.14961198398797018, 'weight_decay': 0.008934880367805834, 'top_p': 0.9274995338850026}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 31: {'learning_rate': 3.094824177427026e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.41434230737368083, 'warmup_ratio': 0.04511356346602678, 'weight_decay': 0.04731037182895204, 'top_p': 0.8211646027005765}\n",
      "[I 2025-10-10 20:06:37,155] Trial 31 finished with value: 0.7680584276268067 and parameters: {'learning_rate': 3.094824177427026e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.41434230737368083, 'warmup_ratio': 0.04511356346602678, 'weight_decay': 0.04731037182895204, 'top_p': 0.8211646027005765}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 32: {'learning_rate': 4.127644122189065e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.5382740247050618, 'warmup_ratio': 0.04862924515974718, 'weight_decay': 0.06353192693130189, 'top_p': 0.8001964558156949}\n",
      "[I 2025-10-10 20:06:37,160] Trial 32 finished with value: 0.7167457775728922 and parameters: {'learning_rate': 4.127644122189065e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.5382740247050618, 'warmup_ratio': 0.04862924515974718, 'weight_decay': 0.06353192693130189, 'top_p': 0.8001964558156949}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 33: {'learning_rate': 1.921628288305131e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.49703807688182705, 'warmup_ratio': 0.031105511208068377, 'weight_decay': 0.06726500545244228, 'top_p': 0.8431558325184365}\n",
      "[I 2025-10-10 20:06:37,166] Trial 33 finished with value: 0.668273128521275 and parameters: {'learning_rate': 1.921628288305131e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.49703807688182705, 'warmup_ratio': 0.031105511208068377, 'weight_decay': 0.06726500545244228, 'top_p': 0.8431558325184365}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 34: {'learning_rate': 3.092169966321551e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.43283343513352945, 'warmup_ratio': 0.020633958420788447, 'weight_decay': 0.025344039838279056, 'top_p': 0.8220539492156288}\n",
      "[I 2025-10-10 20:06:37,173] Trial 34 finished with value: 0.6113030204140464 and parameters: {'learning_rate': 3.092169966321551e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.43283343513352945, 'warmup_ratio': 0.020633958420788447, 'weight_decay': 0.025344039838279056, 'top_p': 0.8220539492156288}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 35: {'learning_rate': 1.317706001964015e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 96, 'num_beams': 4, 'temperature': 0.6018640608902907, 'warmup_ratio': 0.051374024631548235, 'weight_decay': 0.05325989793124686, 'top_p': 0.8140386624198233}\n",
      "[I 2025-10-10 20:06:37,179] Trial 35 finished with value: 0.7599608225372065 and parameters: {'learning_rate': 1.317706001964015e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 96, 'num_beams': 4, 'temperature': 0.6018640608902907, 'warmup_ratio': 0.051374024631548235, 'weight_decay': 0.05325989793124686, 'top_p': 0.8140386624198233}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 36: {'learning_rate': 4.706669513733055e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.15149894820208998, 'warmup_ratio': 0.17540680707153258, 'weight_decay': 0.03994122947087525, 'top_p': 0.8776955612686084}\n",
      "[I 2025-10-10 20:06:37,186] Trial 36 finished with value: 0.7811298262988348 and parameters: {'learning_rate': 4.706669513733055e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.15149894820208998, 'warmup_ratio': 0.17540680707153258, 'weight_decay': 0.03994122947087525, 'top_p': 0.8776955612686084}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 37: {'learning_rate': 4.7657558622432465e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.18316925433715736, 'warmup_ratio': 0.18612453092975198, 'weight_decay': 0.04000326691864714, 'top_p': 0.8752511701111695}\n",
      "[I 2025-10-10 20:06:37,193] Trial 37 finished with value: 0.669811550399135 and parameters: {'learning_rate': 4.7657558622432465e-05, 'batch_size': 16, 'lora_r': 16, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.18316925433715736, 'warmup_ratio': 0.18612453092975198, 'weight_decay': 0.04000326691864714, 'top_p': 0.8752511701111695}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 38: {'learning_rate': 2.4205817609308327e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.1587345254754327, 'warmup_ratio': 0.17574815541512825, 'weight_decay': 0.013110143608165236, 'top_p': 0.8974295447028237}\n",
      "[I 2025-10-10 20:06:37,200] Trial 38 finished with value: 0.8309752604412289 and parameters: {'learning_rate': 2.4205817609308327e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.1587345254754327, 'warmup_ratio': 0.17574815541512825, 'weight_decay': 0.013110143608165236, 'top_p': 0.8974295447028237}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 39: {'learning_rate': 2.531494835284367e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 80, 'num_beams': 7, 'temperature': 0.23161393776092942, 'warmup_ratio': 0.11376353997362912, 'weight_decay': 0.012787574144916825, 'top_p': 0.9019753053266991}\n",
      "[I 2025-10-10 20:06:37,206] Trial 39 finished with value: 0.671400635296036 and parameters: {'learning_rate': 2.531494835284367e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 80, 'num_beams': 7, 'temperature': 0.23161393776092942, 'warmup_ratio': 0.11376353997362912, 'weight_decay': 0.012787574144916825, 'top_p': 0.9019753053266991}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 40: {'learning_rate': 1.6873896735711707e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 104, 'num_beams': 7, 'temperature': 0.12101077392936, 'warmup_ratio': 0.1946448262091431, 'weight_decay': 0.021797660594574973, 'top_p': 0.921367729376461}\n",
      "[I 2025-10-10 20:06:37,213] Trial 40 finished with value: 0.7123029108910641 and parameters: {'learning_rate': 1.6873896735711707e-05, 'batch_size': 4, 'lora_r': 12, 'lora_alpha': 104, 'num_beams': 7, 'temperature': 0.12101077392936, 'warmup_ratio': 0.1946448262091431, 'weight_decay': 0.021797660594574973, 'top_p': 0.921367729376461}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 41: {'learning_rate': 2.016990986416263e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.16781736823297833, 'warmup_ratio': 0.1719144766376781, 'weight_decay': 0.006838893299693837, 'top_p': 0.8899899147100928}\n",
      "[I 2025-10-10 20:06:37,220] Trial 41 finished with value: 0.6754549053125105 and parameters: {'learning_rate': 2.016990986416263e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.16781736823297833, 'warmup_ratio': 0.1719144766376781, 'weight_decay': 0.006838893299693837, 'top_p': 0.8899899147100928}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 42: {'learning_rate': 7.315317123213242e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 6, 'temperature': 0.10898373397063726, 'warmup_ratio': 0.17666476815831086, 'weight_decay': 0.015121316840524262, 'top_p': 0.9082635108940528}\n",
      "[I 2025-10-10 20:06:37,228] Trial 42 finished with value: 0.593665197366266 and parameters: {'learning_rate': 7.315317123213242e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 128, 'num_beams': 6, 'temperature': 0.10898373397063726, 'warmup_ratio': 0.17666476815831086, 'weight_decay': 0.015121316840524262, 'top_p': 0.9082635108940528}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 43: {'learning_rate': 2.987550564700592e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.27229653218647604, 'warmup_ratio': 0.1312614772306692, 'weight_decay': 0.03149393197467222, 'top_p': 0.8717366962457426}\n",
      "[I 2025-10-10 20:06:37,234] Trial 43 finished with value: 0.6614014156634058 and parameters: {'learning_rate': 2.987550564700592e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 96, 'num_beams': 5, 'temperature': 0.27229653218647604, 'warmup_ratio': 0.1312614772306692, 'weight_decay': 0.03149393197467222, 'top_p': 0.8717366962457426}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 44: {'learning_rate': 1.4183533306486784e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.3330280784857712, 'warmup_ratio': 0.16834508714623803, 'weight_decay': 0.07362506042898428, 'top_p': 0.9359224673765495}\n",
      "[I 2025-10-10 20:06:37,240] Trial 44 finished with value: 0.7647816847407728 and parameters: {'learning_rate': 1.4183533306486784e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 104, 'num_beams': 6, 'temperature': 0.3330280784857712, 'warmup_ratio': 0.16834508714623803, 'weight_decay': 0.07362506042898428, 'top_p': 0.9359224673765495}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 45: {'learning_rate': 2.2125857907120275e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 120, 'num_beams': 5, 'temperature': 0.19234876816197186, 'warmup_ratio': 0.1564552423218515, 'weight_decay': 0.025485267457427647, 'top_p': 0.8968008735475811}\n",
      "[I 2025-10-10 20:06:37,247] Trial 45 finished with value: 0.7686591279622612 and parameters: {'learning_rate': 2.2125857907120275e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 120, 'num_beams': 5, 'temperature': 0.19234876816197186, 'warmup_ratio': 0.1564552423218515, 'weight_decay': 0.025485267457427647, 'top_p': 0.8968008735475811}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 46: {'learning_rate': 4.410553554527763e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.7963122850242701, 'warmup_ratio': 0.18189387147264963, 'weight_decay': 0.03622538665091601, 'top_p': 0.8545578630975141}\n",
      "[I 2025-10-10 20:06:37,253] Trial 46 finished with value: 0.6617494841377299 and parameters: {'learning_rate': 4.410553554527763e-05, 'batch_size': 8, 'lora_r': 12, 'lora_alpha': 48, 'num_beams': 2, 'temperature': 0.7963122850242701, 'warmup_ratio': 0.18189387147264963, 'weight_decay': 0.03622538665091601, 'top_p': 0.8545578630975141}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 47: {'learning_rate': 2.8793297545562394e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 128, 'num_beams': 7, 'temperature': 0.14441639986086824, 'warmup_ratio': 0.19304852996515295, 'weight_decay': 0.0024125324630044215, 'top_p': 0.8845632607966017}\n",
      "[I 2025-10-10 20:06:37,260] Trial 47 finished with value: 0.646539007154649 and parameters: {'learning_rate': 2.8793297545562394e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 128, 'num_beams': 7, 'temperature': 0.14441639986086824, 'warmup_ratio': 0.19304852996515295, 'weight_decay': 0.0024125324630044215, 'top_p': 0.8845632607966017}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 48: {'learning_rate': 0.0001106314283459816, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.227509281192289, 'warmup_ratio': 0.152254905249286, 'weight_decay': 0.04371339282623083, 'top_p': 0.91427665181948}\n",
      "[I 2025-10-10 20:06:37,266] Trial 48 finished with value: 0.6874951210991229 and parameters: {'learning_rate': 0.0001106314283459816, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 112, 'num_beams': 6, 'temperature': 0.227509281192289, 'warmup_ratio': 0.152254905249286, 'weight_decay': 0.04371339282623083, 'top_p': 0.91427665181948}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 49: {'learning_rate': 1.0218175796834212e-05, 'batch_size': 16, 'lora_r': 44, 'lora_alpha': 80, 'num_beams': 5, 'temperature': 0.5017014353557825, 'warmup_ratio': 0.16418201035248006, 'weight_decay': 0.018055373676510417, 'top_p': 0.9032601502373351}\n",
      "[I 2025-10-10 20:06:37,272] Trial 49 finished with value: 0.6503413789046097 and parameters: {'learning_rate': 1.0218175796834212e-05, 'batch_size': 16, 'lora_r': 44, 'lora_alpha': 80, 'num_beams': 5, 'temperature': 0.5017014353557825, 'warmup_ratio': 0.16418201035248006, 'weight_decay': 0.018055373676510417, 'top_p': 0.9032601502373351}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 50: {'learning_rate': 1.4733364262930062e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.7156901210138211, 'warmup_ratio': 0.14017765897144893, 'weight_decay': 0.09966547351713763, 'top_p': 0.9552864405280675}\n",
      "[I 2025-10-10 20:06:37,280] Trial 50 finished with value: 0.7059234659644308 and parameters: {'learning_rate': 1.4733364262930062e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 104, 'num_beams': 3, 'temperature': 0.7156901210138211, 'warmup_ratio': 0.14017765897144893, 'weight_decay': 0.09966547351713763, 'top_p': 0.9552864405280675}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 51: {'learning_rate': 3.4749287491699306e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.5804493544071787, 'warmup_ratio': 0.06462677823093144, 'weight_decay': 0.04525628475662426, 'top_p': 0.8295050470927703}\n",
      "[I 2025-10-10 20:06:37,286] Trial 51 finished with value: 0.6917989823441673 and parameters: {'learning_rate': 3.4749287491699306e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.5804493544071787, 'warmup_ratio': 0.06462677823093144, 'weight_decay': 0.04525628475662426, 'top_p': 0.8295050470927703}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 52: {'learning_rate': 3.709539583911614e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.37166339692151296, 'warmup_ratio': 0.011684432550358836, 'weight_decay': 0.05264710807566457, 'top_p': 0.8107728263361236}\n",
      "[I 2025-10-10 20:06:37,292] Trial 52 finished with value: 0.6052703449157459 and parameters: {'learning_rate': 3.709539583911614e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 120, 'num_beams': 4, 'temperature': 0.37166339692151296, 'warmup_ratio': 0.011684432550358836, 'weight_decay': 0.05264710807566457, 'top_p': 0.8107728263361236}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 53: {'learning_rate': 2.2116471515291207e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6427941433608708, 'warmup_ratio': 0.037550236523587396, 'weight_decay': 0.05681344701573311, 'top_p': 0.8469543460207246}\n",
      "[I 2025-10-10 20:06:37,300] Trial 53 finished with value: 0.7706885939056862 and parameters: {'learning_rate': 2.2116471515291207e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 4, 'temperature': 0.6427941433608708, 'warmup_ratio': 0.037550236523587396, 'weight_decay': 0.05681344701573311, 'top_p': 0.8469543460207246}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 54: {'learning_rate': 2.0602507190704607e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.6590026610203907, 'warmup_ratio': 0.1783392489343954, 'weight_decay': 0.07002797498285046, 'top_p': 0.8642848146742853}\n",
      "[I 2025-10-10 20:06:37,307] Trial 54 finished with value: 0.615438913588359 and parameters: {'learning_rate': 2.0602507190704607e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 128, 'num_beams': 5, 'temperature': 0.6590026610203907, 'warmup_ratio': 0.1783392489343954, 'weight_decay': 0.07002797498285046, 'top_p': 0.8642848146742853}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 55: {'learning_rate': 1.6153222842386155e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.6294595126891122, 'warmup_ratio': 0.19905541465294616, 'weight_decay': 0.09340740175674117, 'top_p': 0.876193610295522}\n",
      "[I 2025-10-10 20:06:37,314] Trial 55 finished with value: 0.5550593411510581 and parameters: {'learning_rate': 1.6153222842386155e-05, 'batch_size': 8, 'lora_r': 48, 'lora_alpha': 112, 'num_beams': 3, 'temperature': 0.6294595126891122, 'warmup_ratio': 0.19905541465294616, 'weight_decay': 0.09340740175674117, 'top_p': 0.876193610295522}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 56: {'learning_rate': 1.240977432071961e-05, 'batch_size': 8, 'lora_r': 56, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.7843024391422477, 'warmup_ratio': 0.012859854671353824, 'weight_decay': 0.07712187730513362, 'top_p': 0.8481411504106939}\n",
      "[I 2025-10-10 20:06:37,320] Trial 56 finished with value: 0.6983509221702764 and parameters: {'learning_rate': 1.240977432071961e-05, 'batch_size': 8, 'lora_r': 56, 'lora_alpha': 128, 'num_beams': 3, 'temperature': 0.7843024391422477, 'warmup_ratio': 0.012859854671353824, 'weight_decay': 0.07712187730513362, 'top_p': 0.8481411504106939}. Best is trial 14 with value: 0.8416676785296207.\n",
      "\n",
      "Trial 57: {'learning_rate': 2.701752798859616e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.7004057497694031, 'warmup_ratio': 0.07889302920314219, 'weight_decay': 0.050980467678198305, 'top_p': 0.8649055419818599}\n",
      "[I 2025-10-10 20:06:37,327] Trial 57 finished with value: 0.846444295089657 and parameters: {'learning_rate': 2.701752798859616e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.7004057497694031, 'warmup_ratio': 0.07889302920314219, 'weight_decay': 0.050980467678198305, 'top_p': 0.8649055419818599}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 58: {'learning_rate': 0.00046780959898243686, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 40, 'num_beams': 8, 'temperature': 0.6941039570881306, 'warmup_ratio': 0.09462173181517183, 'weight_decay': 0.04973904900065393, 'top_p': 0.8921902459743193}\n",
      "[I 2025-10-10 20:06:37,334] Trial 58 finished with value: 0.6129106707328406 and parameters: {'learning_rate': 0.00046780959898243686, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 40, 'num_beams': 8, 'temperature': 0.6941039570881306, 'warmup_ratio': 0.09462173181517183, 'weight_decay': 0.04973904900065393, 'top_p': 0.8921902459743193}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 59: {'learning_rate': 2.7552780289527886e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.5551481428755193, 'warmup_ratio': 0.07806911127120207, 'weight_decay': 0.03985403218649806, 'top_p': 0.882326120791791}\n",
      "[I 2025-10-10 20:06:37,341] Trial 59 finished with value: 0.8215860656194957 and parameters: {'learning_rate': 2.7552780289527886e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.5551481428755193, 'warmup_ratio': 0.07806911127120207, 'weight_decay': 0.03985403218649806, 'top_p': 0.882326120791791}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 60: {'learning_rate': 2.522156294909442e-05, 'batch_size': 8, 'lora_r': 8, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.574282276871496, 'warmup_ratio': 0.07785966373611808, 'weight_decay': 0.05998144965629997, 'top_p': 0.8689683785783211}\n",
      "[I 2025-10-10 20:06:37,348] Trial 60 finished with value: 0.6199772954282684 and parameters: {'learning_rate': 2.522156294909442e-05, 'batch_size': 8, 'lora_r': 8, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.574282276871496, 'warmup_ratio': 0.07785966373611808, 'weight_decay': 0.05998144965629997, 'top_p': 0.8689683785783211}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 61: {'learning_rate': 2.7556704908524016e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.8503731063778148, 'warmup_ratio': 0.06780992918089138, 'weight_decay': 0.04151060115765849, 'top_p': 0.8804624729548008}\n",
      "[I 2025-10-10 20:06:37,355] Trial 61 finished with value: 0.6633055211161557 and parameters: {'learning_rate': 2.7556704908524016e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.8503731063778148, 'warmup_ratio': 0.06780992918089138, 'weight_decay': 0.04151060115765849, 'top_p': 0.8804624729548008}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 62: {'learning_rate': 6.507558563298224e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.6219690558816099, 'warmup_ratio': 0.05844252858732275, 'weight_decay': 0.03663431449996464, 'top_p': 0.8626240918018101}\n",
      "[I 2025-10-10 20:06:37,361] Trial 62 finished with value: 0.5474271593473461 and parameters: {'learning_rate': 6.507558563298224e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.6219690558816099, 'warmup_ratio': 0.05844252858732275, 'weight_decay': 0.03663431449996464, 'top_p': 0.8626240918018101}. Best is trial 57 with value: 0.846444295089657.\n",
      "\n",
      "Trial 63: {'learning_rate': 4.5725233374529764e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5580159350790999, 'warmup_ratio': 0.11082473016769612, 'weight_decay': 0.026785425261978142, 'top_p': 0.8846400400556139}\n",
      "[I 2025-10-10 20:06:37,368] Trial 63 finished with value: 0.8541734747754074 and parameters: {'learning_rate': 4.5725233374529764e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5580159350790999, 'warmup_ratio': 0.11082473016769612, 'weight_decay': 0.026785425261978142, 'top_p': 0.8846400400556139}. Best is trial 63 with value: 0.8541734747754074.\n",
      "\n",
      "Trial 64: {'learning_rate': 0.0009841670344314273, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5139044376055386, 'warmup_ratio': 0.08118173588223437, 'weight_decay': 0.026851414418151847, 'top_p': 0.9882704605407188}\n",
      "[I 2025-10-10 20:06:37,376] Trial 64 finished with value: 0.6481389396761073 and parameters: {'learning_rate': 0.0009841670344314273, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.5139044376055386, 'warmup_ratio': 0.08118173588223437, 'weight_decay': 0.026851414418151847, 'top_p': 0.9882704605407188}. Best is trial 63 with value: 0.8541734747754074.\n",
      "\n",
      "Trial 65: {'learning_rate': 1.677379500013992e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4720398360237838, 'warmup_ratio': 0.11154008727367384, 'weight_decay': 0.022201193441123306, 'top_p': 0.8864128918251735}\n",
      "[I 2025-10-10 20:06:37,382] Trial 65 finished with value: 0.8618116332851581 and parameters: {'learning_rate': 1.677379500013992e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4720398360237838, 'warmup_ratio': 0.11154008727367384, 'weight_decay': 0.022201193441123306, 'top_p': 0.8864128918251735}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 66: {'learning_rate': 1.1805094925833307e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.45784145810741744, 'warmup_ratio': 0.11609704954088851, 'weight_decay': 0.021540806414871373, 'top_p': 0.8885106166288887}\n",
      "[I 2025-10-10 20:06:37,390] Trial 66 finished with value: 0.8145732866004118 and parameters: {'learning_rate': 1.1805094925833307e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.45784145810741744, 'warmup_ratio': 0.11609704954088851, 'weight_decay': 0.021540806414871373, 'top_p': 0.8885106166288887}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 67: {'learning_rate': 1.7219427754068686e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.7583678305641154, 'warmup_ratio': 0.13378523527591568, 'weight_decay': 0.009268606046102927, 'top_p': 0.8994189389655497}\n",
      "[I 2025-10-10 20:06:37,398] Trial 67 finished with value: 0.5434978644571928 and parameters: {'learning_rate': 1.7219427754068686e-05, 'batch_size': 16, 'lora_r': 36, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.7583678305641154, 'warmup_ratio': 0.13378523527591568, 'weight_decay': 0.009268606046102927, 'top_p': 0.8994189389655497}. Best is trial 65 with value: 0.8618116332851581.\n",
      "\n",
      "Trial 68: {'learning_rate': 2.227458918150894e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4032573606932476, 'warmup_ratio': 0.10944598496589243, 'weight_decay': 0.0054989837204231715, 'top_p': 0.9166081492895803}\n",
      "[I 2025-10-10 20:06:37,405] Trial 68 finished with value: 0.8682475413732235 and parameters: {'learning_rate': 2.227458918150894e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4032573606932476, 'warmup_ratio': 0.10944598496589243, 'weight_decay': 0.0054989837204231715, 'top_p': 0.9166081492895803}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 69: {'learning_rate': 1.500126697452724e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4034661117946926, 'warmup_ratio': 0.10510925206548839, 'weight_decay': 0.004912847433152478, 'top_p': 0.9248114000175955}\n",
      "[I 2025-10-10 20:06:37,413] Trial 69 finished with value: 0.8324010335731041 and parameters: {'learning_rate': 1.500126697452724e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.4034661117946926, 'warmup_ratio': 0.10510925206548839, 'weight_decay': 0.004912847433152478, 'top_p': 0.9248114000175955}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 70: {'learning_rate': 1.5400439475960335e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3901632296243603, 'warmup_ratio': 0.10580681222644418, 'weight_decay': 0.005397124106637472, 'top_p': 0.9304083613930098}\n",
      "[I 2025-10-10 20:06:37,419] Trial 70 finished with value: 0.6090767438766573 and parameters: {'learning_rate': 1.5400439475960335e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3901632296243603, 'warmup_ratio': 0.10580681222644418, 'weight_decay': 0.005397124106637472, 'top_p': 0.9304083613930098}. Best is trial 68 with value: 0.8682475413732235.\n",
      "\n",
      "Trial 71: {'learning_rate': 1.828294303807379e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3474546763851418, 'warmup_ratio': 0.0934595482583817, 'weight_decay': 0.014607022987833684, 'top_p': 0.9200236760389789}\n",
      "[I 2025-10-10 20:06:37,426] Trial 71 finished with value: 0.873550233291343 and parameters: {'learning_rate': 1.828294303807379e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3474546763851418, 'warmup_ratio': 0.0934595482583817, 'weight_decay': 0.014607022987833684, 'top_p': 0.9200236760389789}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 72: {'learning_rate': 1.1435810914716776e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.33201876993592494, 'warmup_ratio': 0.09525746034919104, 'weight_decay': 0.0013933224834431959, 'top_p': 0.920183215162878}\n",
      "[I 2025-10-10 20:06:37,433] Trial 72 finished with value: 0.6797362815296961 and parameters: {'learning_rate': 1.1435810914716776e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.33201876993592494, 'warmup_ratio': 0.09525746034919104, 'weight_decay': 0.0013933224834431959, 'top_p': 0.920183215162878}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 73: {'learning_rate': 1.9771826397832063e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40981012847367354, 'warmup_ratio': 0.10739268597925176, 'weight_decay': 0.0095385904705074, 'top_p': 0.9152595587510741}\n",
      "[I 2025-10-10 20:06:37,440] Trial 73 finished with value: 0.7610726631791968 and parameters: {'learning_rate': 1.9771826397832063e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40981012847367354, 'warmup_ratio': 0.10739268597925176, 'weight_decay': 0.0095385904705074, 'top_p': 0.9152595587510741}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 74: {'learning_rate': 1.8143008517456992e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.47684162984530515, 'warmup_ratio': 0.12560327793078482, 'weight_decay': 0.019995904459648595, 'top_p': 0.9074294266770475}\n",
      "[I 2025-10-10 20:06:37,447] Trial 74 finished with value: 0.6125341537748623 and parameters: {'learning_rate': 1.8143008517456992e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.47684162984530515, 'warmup_ratio': 0.12560327793078482, 'weight_decay': 0.019995904459648595, 'top_p': 0.9074294266770475}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 75: {'learning_rate': 1.4906103445745445e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.43618972460521527, 'warmup_ratio': 0.08814925950702149, 'weight_decay': 7.35595467084892e-05, 'top_p': 0.9256834318901477}\n",
      "[I 2025-10-10 20:06:37,454] Trial 75 finished with value: 0.690294612898219 and parameters: {'learning_rate': 1.4906103445745445e-05, 'batch_size': 4, 'lora_r': 32, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.43618972460521527, 'warmup_ratio': 0.08814925950702149, 'weight_decay': 7.35595467084892e-05, 'top_p': 0.9256834318901477}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 76: {'learning_rate': 2.208400972446465e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40035389978847513, 'warmup_ratio': 0.11097497189069713, 'weight_decay': 0.014031318241253232, 'top_p': 0.9415565846684786}\n",
      "[I 2025-10-10 20:06:37,461] Trial 76 finished with value: 0.691010664615275 and parameters: {'learning_rate': 2.208400972446465e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 8, 'temperature': 0.40035389978847513, 'warmup_ratio': 0.11097497189069713, 'weight_decay': 0.014031318241253232, 'top_p': 0.9415565846684786}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 77: {'learning_rate': 1.357786283008655e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.3135427858079572, 'warmup_ratio': 0.09885958858542945, 'weight_decay': 0.004281444433456277, 'top_p': 0.9327038596265456}\n",
      "[I 2025-10-10 20:06:37,469] Trial 77 finished with value: 0.7249618402175614 and parameters: {'learning_rate': 1.357786283008655e-05, 'batch_size': 4, 'lora_r': 36, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.3135427858079572, 'warmup_ratio': 0.09885958858542945, 'weight_decay': 0.004281444433456277, 'top_p': 0.9327038596265456}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 78: {'learning_rate': 1.72532490277807e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.344813366665613, 'warmup_ratio': 0.08848887549706172, 'weight_decay': 0.015359912124473924, 'top_p': 0.9144777489726054}\n",
      "[I 2025-10-10 20:06:37,475] Trial 78 finished with value: 0.8085958763409932 and parameters: {'learning_rate': 1.72532490277807e-05, 'batch_size': 8, 'lora_r': 40, 'lora_alpha': 8, 'num_beams': 7, 'temperature': 0.344813366665613, 'warmup_ratio': 0.08848887549706172, 'weight_decay': 0.015359912124473924, 'top_p': 0.9144777489726054}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 79: {'learning_rate': 1.1448309995393061e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 2, 'temperature': 0.36458205062918725, 'warmup_ratio': 0.1236372528993306, 'weight_decay': 0.010758431052341637, 'top_p': 0.9512302741997053}\n",
      "[I 2025-10-10 20:06:37,482] Trial 79 finished with value: 0.7764048407040598 and parameters: {'learning_rate': 1.1448309995393061e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 2, 'temperature': 0.36458205062918725, 'warmup_ratio': 0.1236372528993306, 'weight_decay': 0.010758431052341637, 'top_p': 0.9512302741997053}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 80: {'learning_rate': 3.325476800946089e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.2917912976383152, 'warmup_ratio': 0.0980888035232409, 'weight_decay': 0.023223697349299802, 'top_p': 0.8561699149765433}\n",
      "[I 2025-10-10 20:06:37,489] Trial 80 finished with value: 0.6664385059125673 and parameters: {'learning_rate': 3.325476800946089e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.2917912976383152, 'warmup_ratio': 0.0980888035232409, 'weight_decay': 0.023223697349299802, 'top_p': 0.8561699149765433}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 81: {'learning_rate': 2.196268696397313e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5186151702694785, 'warmup_ratio': 0.11715169483201453, 'weight_decay': 0.006783072767123227, 'top_p': 0.8917978085828453}\n",
      "[I 2025-10-10 20:06:37,496] Trial 81 finished with value: 0.7496795208932682 and parameters: {'learning_rate': 2.196268696397313e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5186151702694785, 'warmup_ratio': 0.11715169483201453, 'weight_decay': 0.006783072767123227, 'top_p': 0.8917978085828453}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 82: {'learning_rate': 1.943225669383202e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.45867044719272854, 'warmup_ratio': 0.11075785636187255, 'weight_decay': 0.016299867404336393, 'top_p': 0.9044817406853216}\n",
      "[I 2025-10-10 20:06:37,503] Trial 82 finished with value: 0.8663229284980222 and parameters: {'learning_rate': 1.943225669383202e-05, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.45867044719272854, 'warmup_ratio': 0.11075785636187255, 'weight_decay': 0.016299867404336393, 'top_p': 0.9044817406853216}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 83: {'learning_rate': 0.00017292288933910116, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.44380575356914137, 'warmup_ratio': 0.11000987122405093, 'weight_decay': 0.01676357634891254, 'top_p': 0.9062796860141209}\n",
      "[I 2025-10-10 20:06:37,510] Trial 83 finished with value: 0.5972050480218751 and parameters: {'learning_rate': 0.00017292288933910116, 'batch_size': 4, 'lora_r': 24, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.44380575356914137, 'warmup_ratio': 0.11000987122405093, 'weight_decay': 0.01676357634891254, 'top_p': 0.9062796860141209}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 84: {'learning_rate': 1.864931316758299e-05, 'batch_size': 4, 'lora_r': 16, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3597751627595744, 'warmup_ratio': 0.10378071760180821, 'weight_decay': 0.02953697610556498, 'top_p': 0.9239144866671962}\n",
      "[I 2025-10-10 20:06:37,518] Trial 84 finished with value: 0.6111981101288488 and parameters: {'learning_rate': 1.864931316758299e-05, 'batch_size': 4, 'lora_r': 16, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.3597751627595744, 'warmup_ratio': 0.10378071760180821, 'weight_decay': 0.02953697610556498, 'top_p': 0.9239144866671962}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 85: {'learning_rate': 2.390876676852373e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 56, 'num_beams': 8, 'temperature': 0.5535495696488193, 'warmup_ratio': 0.0836483438398633, 'weight_decay': 0.018318651155860927, 'top_p': 0.9140355173144243}\n",
      "[I 2025-10-10 20:06:37,524] Trial 85 finished with value: 0.6968252779238877 and parameters: {'learning_rate': 2.390876676852373e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 56, 'num_beams': 8, 'temperature': 0.5535495696488193, 'warmup_ratio': 0.0836483438398633, 'weight_decay': 0.018318651155860927, 'top_p': 0.9140355173144243}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 86: {'learning_rate': 1.5744080207489305e-05, 'batch_size': 16, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.41577082566232243, 'warmup_ratio': 0.07221727006541367, 'weight_decay': 0.03358134330290126, 'top_p': 0.8868213295511301}\n",
      "[I 2025-10-10 20:06:37,532] Trial 86 finished with value: 0.6899542299966748 and parameters: {'learning_rate': 1.5744080207489305e-05, 'batch_size': 16, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.41577082566232243, 'warmup_ratio': 0.07221727006541367, 'weight_decay': 0.03358134330290126, 'top_p': 0.8868213295511301}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 87: {'learning_rate': 1.3135674084080652e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4779026446671986, 'warmup_ratio': 0.09365419210458016, 'weight_decay': 0.052074570891942555, 'top_p': 0.8952959825768304}\n",
      "[I 2025-10-10 20:06:37,539] Trial 87 finished with value: 0.8134360117748939 and parameters: {'learning_rate': 1.3135674084080652e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.4779026446671986, 'warmup_ratio': 0.09365419210458016, 'weight_decay': 0.052074570891942555, 'top_p': 0.8952959825768304}. Best is trial 71 with value: 0.873550233291343.\n",
      "\n",
      "Trial 88: {'learning_rate': 2.049176115499461e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.48857300984945373, 'warmup_ratio': 0.11987882467355263, 'weight_decay': 0.0070240442391849124, 'top_p': 0.9010897430763072}\n",
      "[I 2025-10-10 20:06:37,545] Trial 88 finished with value: 0.8963476536357332 and parameters: {'learning_rate': 2.049176115499461e-05, 'batch_size': 4, 'lora_r': 28, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.48857300984945373, 'warmup_ratio': 0.11987882467355263, 'weight_decay': 0.0070240442391849124, 'top_p': 0.9010897430763072}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 89: {'learning_rate': 4.036631667620837e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.46076346103133853, 'warmup_ratio': 0.13465728539302008, 'weight_decay': 0.024397342418100917, 'top_p': 0.8749639464130595}\n",
      "[I 2025-10-10 20:06:37,554] Trial 89 finished with value: 0.6948261538799515 and parameters: {'learning_rate': 4.036631667620837e-05, 'batch_size': 8, 'lora_r': 36, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.46076346103133853, 'warmup_ratio': 0.13465728539302008, 'weight_decay': 0.024397342418100917, 'top_p': 0.8749639464130595}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 90: {'learning_rate': 2.7461362320348037e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.49223607590318796, 'warmup_ratio': 0.11930274786669925, 'weight_decay': 0.007485256825922772, 'top_p': 0.9025822708881998}\n",
      "[I 2025-10-10 20:06:37,562] Trial 90 finished with value: 0.8503817599952906 and parameters: {'learning_rate': 2.7461362320348037e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.49223607590318796, 'warmup_ratio': 0.11930274786669925, 'weight_decay': 0.007485256825922772, 'top_p': 0.9025822708881998}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 91: {'learning_rate': 2.7059892654940416e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.48867524640240195, 'warmup_ratio': 0.12112095867161037, 'weight_decay': 0.011316286544565082, 'top_p': 0.901635581522616}\n",
      "[I 2025-10-10 20:06:37,569] Trial 91 finished with value: 0.7947363895144691 and parameters: {'learning_rate': 2.7059892654940416e-05, 'batch_size': 8, 'lora_r': 16, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.48867524640240195, 'warmup_ratio': 0.12112095867161037, 'weight_decay': 0.011316286544565082, 'top_p': 0.901635581522616}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 92: {'learning_rate': 2.0311290052771927e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.5062987931152043, 'warmup_ratio': 0.1470818273377289, 'weight_decay': 0.008942661888523412, 'top_p': 0.9112037994996472}\n",
      "[I 2025-10-10 20:06:37,577] Trial 92 finished with value: 0.8298755726760713 and parameters: {'learning_rate': 2.0311290052771927e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 24, 'num_beams': 8, 'temperature': 0.5062987931152043, 'warmup_ratio': 0.1470818273377289, 'weight_decay': 0.008942661888523412, 'top_p': 0.9112037994996472}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 93: {'learning_rate': 3.3520309541633297e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5381471664206512, 'warmup_ratio': 0.12615216734637805, 'weight_decay': 0.01960017494143058, 'top_p': 0.9022732667748795}\n",
      "[I 2025-10-10 20:06:37,586] Trial 93 finished with value: 0.7878058253713316 and parameters: {'learning_rate': 3.3520309541633297e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 8, 'num_beams': 8, 'temperature': 0.5381471664206512, 'warmup_ratio': 0.12615216734637805, 'weight_decay': 0.01960017494143058, 'top_p': 0.9022732667748795}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 94: {'learning_rate': 2.4468050922989584e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 72, 'num_beams': 8, 'temperature': 0.6939696538470411, 'warmup_ratio': 0.11272487254158559, 'weight_decay': 0.01643758158606402, 'top_p': 0.8368405681097312}\n",
      "[I 2025-10-10 20:06:37,594] Trial 94 finished with value: 0.819092118814681 and parameters: {'learning_rate': 2.4468050922989584e-05, 'batch_size': 8, 'lora_r': 20, 'lora_alpha': 72, 'num_beams': 8, 'temperature': 0.6939696538470411, 'warmup_ratio': 0.11272487254158559, 'weight_decay': 0.01643758158606402, 'top_p': 0.8368405681097312}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 95: {'learning_rate': 2.9940053916304718e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.433215886684908, 'warmup_ratio': 0.11843534839885794, 'weight_decay': 0.006490197409568283, 'top_p': 0.8954977756983721}\n",
      "[I 2025-10-10 20:06:37,602] Trial 95 finished with value: 0.8888322838351413 and parameters: {'learning_rate': 2.9940053916304718e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.433215886684908, 'warmup_ratio': 0.11843534839885794, 'weight_decay': 0.006490197409568283, 'top_p': 0.8954977756983721}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 96: {'learning_rate': 3.7871144059822215e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 40, 'num_beams': 7, 'temperature': 0.5674438568729533, 'warmup_ratio': 0.11813474610329967, 'weight_decay': 0.007608700209314483, 'top_p': 0.8693628968756135}\n",
      "[I 2025-10-10 20:06:37,610] Trial 96 finished with value: 0.8045527929907507 and parameters: {'learning_rate': 3.7871144059822215e-05, 'batch_size': 8, 'lora_r': 32, 'lora_alpha': 40, 'num_beams': 7, 'temperature': 0.5674438568729533, 'warmup_ratio': 0.11813474610329967, 'weight_decay': 0.007608700209314483, 'top_p': 0.8693628968756135}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 97: {'learning_rate': 2.99194408693566e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.434168517959546, 'warmup_ratio': 0.13010399980738133, 'weight_decay': 0.0037311690938391356, 'top_p': 0.8944295933312929}\n",
      "[I 2025-10-10 20:06:37,617] Trial 97 finished with value: 0.8326771978580317 and parameters: {'learning_rate': 2.99194408693566e-05, 'batch_size': 8, 'lora_r': 28, 'lora_alpha': 16, 'num_beams': 7, 'temperature': 0.434168517959546, 'warmup_ratio': 0.13010399980738133, 'weight_decay': 0.0037311690938391356, 'top_p': 0.8944295933312929}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 98: {'learning_rate': 2.2671102794102827e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.46035451790032855, 'warmup_ratio': 0.10033950263417583, 'weight_decay': 0.0027564594012335457, 'top_p': 0.8828512118276668}\n",
      "[I 2025-10-10 20:06:37,624] Trial 98 finished with value: 0.6267446857193242 and parameters: {'learning_rate': 2.2671102794102827e-05, 'batch_size': 4, 'lora_r': 20, 'lora_alpha': 32, 'num_beams': 8, 'temperature': 0.46035451790032855, 'warmup_ratio': 0.10033950263417583, 'weight_decay': 0.0027564594012335457, 'top_p': 0.8828512118276668}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "Trial 99: {'learning_rate': 4.450524452644817e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.6121885418481761, 'warmup_ratio': 0.11553342592413801, 'weight_decay': 0.012027852131779086, 'top_p': 0.9178003776721214}\n",
      "[I 2025-10-10 20:06:37,631] Trial 99 finished with value: 0.656646922607858 and parameters: {'learning_rate': 4.450524452644817e-05, 'batch_size': 8, 'lora_r': 24, 'lora_alpha': 24, 'num_beams': 7, 'temperature': 0.6121885418481761, 'warmup_ratio': 0.11553342592413801, 'weight_decay': 0.012027852131779086, 'top_p': 0.9178003776721214}. Best is trial 88 with value: 0.8963476536357332.\n",
      "\n",
      "============================================================\n",
      "✅ OPTIMIZATION COMPLETED!\n",
      "============================================================\n",
      "Best score: 0.8963\n",
      "Best parameters:\n",
      "  - learning_rate: 2.049176115499461e-05\n",
      "  - batch_size: 4\n",
      "  - lora_r: 28\n",
      "  - lora_alpha: 8\n",
      "  - num_beams: 8\n",
      "  - temperature: 0.48857300984945373\n",
      "  - warmup_ratio: 0.11987882467355263\n",
      "  - weight_decay: 0.0070240442391849124\n",
      "  - top_p: 0.9010897430763072\n",
      "\n",
      "📊 Top 5 trials:\n",
      "1. Score: 0.8963\n",
      "\n",
      "✅ Config updated with optimal hyperparameters!\n",
      "\n",
      "📁 Optimization results saved:\n",
      "  - Study: logs/full_pipeline/optuna/optuna_study_20251010_200633.pkl\n",
      "  - CSV: logs/full_pipeline/optuna/optuna_results_20251010_200633.csv\n",
      "  - Best params: logs/full_pipeline/optuna/best_params_20251010_200633.json\n",
      "\n",
      "✅ Config has been updated with optimal hyperparameters!\n",
      "[hyperparameter_optimization] Status: completed\n",
      "✅ Solar API initialized for cross-validation\n"
     ]
    }
   ],
   "source": [
    "# PRD 전략 통합 - Solar API, Optuna, 리스크 관리 시스템\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Optuna import - 반드시 설치되어 있어야 함\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna import Trial\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"✅ Optuna is available and will be used for hyperparameter optimization!\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    logger.write(\"⚠️ Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "# Solar API 통합 (PRD 09_Solar_API_최적화.md, 10_교차_검증_시스템.md)\n",
    "class PipelineSolarAPI:\n",
    "    \"\"\"파이프라인용 Solar API 통합\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, logger):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.upstage.ai/v1/solar\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.logger = logger\n",
    "        self.cache = {}\n",
    "        self.api_calls = 0\n",
    "        self.token_usage = 0\n",
    "        \n",
    "    def optimize_and_validate(self, model_predictions: List[str], test_dialogues: List[str], \n",
    "                            sample_size: int = 10) -> Dict:\n",
    "        \"\"\"모델 예측과 API 예측 비교 검증\"\"\"\n",
    "        self.logger.write(\"\\n=== Solar API Cross-Validation ===\")\n",
    "        \n",
    "        comparisons = []\n",
    "        \n",
    "        # 랜덤 샘플 선택\n",
    "        sample_indices = np.random.choice(\n",
    "            len(model_predictions), \n",
    "            min(sample_size, len(model_predictions)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            dialogue = test_dialogues[idx]\n",
    "            model_pred = model_predictions[idx]\n",
    "            \n",
    "            # Solar API 예측\n",
    "            api_pred = self.generate_summary(dialogue)\n",
    "            \n",
    "            if api_pred:\n",
    "                comparisons.append({\n",
    "                    'model': model_pred[:200],\n",
    "                    'api': api_pred[:200],\n",
    "                    'model_length': len(model_pred),\n",
    "                    'api_length': len(api_pred)\n",
    "                })\n",
    "                \n",
    "                self.api_calls += 1\n",
    "                self.token_usage += len(dialogue) // 3  # 대략적 토큰 추정\n",
    "        \n",
    "        # 통계 분석\n",
    "        if comparisons:\n",
    "            avg_model_length = np.mean([c['model_length'] for c in comparisons])\n",
    "            avg_api_length = np.mean([c['api_length'] for c in comparisons])\n",
    "            \n",
    "            self.logger.write(f\"Comparisons completed: {len(comparisons)} samples\")\n",
    "            self.logger.write(f\"Avg model length: {avg_model_length:.1f}\")\n",
    "            self.logger.write(f\"Avg API length: {avg_api_length:.1f}\")\n",
    "            self.logger.write(f\"API calls made: {self.api_calls}\")\n",
    "            self.logger.write(f\"Estimated tokens used: {self.token_usage}\")\n",
    "            \n",
    "            return {\n",
    "                'comparisons': comparisons,\n",
    "                'avg_model_length': avg_model_length,\n",
    "                'avg_api_length': avg_api_length,\n",
    "                'api_calls': self.api_calls,\n",
    "                'token_usage': self.token_usage\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def generate_summary(self, dialogue: str, max_tokens: int = 150) -> Optional[str]:\n",
    "        \"\"\"Solar API로 요약 생성\"\"\"\n",
    "        # 캐시 확인\n",
    "        cache_key = hash(dialogue[:200] if len(dialogue) > 200 else dialogue)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # 토큰 최적화\n",
    "            if len(dialogue) > 2000:\n",
    "                dialogue = dialogue[:2000] + \"...\"\n",
    "            \n",
    "            prompt = f\"\"\"다음 대화를 핵심 내용 위주로 3-5문장으로 요약하세요:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "요약:\"\"\"\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"solar-1-mini-chat\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 전문적인 대화 요약 AI입니다.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0.3,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                summary = result['choices'][0]['message']['content']\n",
    "                self.cache[cache_key] = summary\n",
    "                return summary\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.write(f\"Solar API error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Optuna 하이퍼파라미터 최적화 (PRD 13_Optuna_하이퍼파라미터_최적화.md)\n",
    "class PipelineOptunaOptimizer:\n",
    "    \"\"\"파이프라인용 Optuna 최적화 - 실제 최적화 수행\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "        \n",
    "    def optimize_hyperparameters(self, config: Dict, n_trials: int = 20, actual_training: bool = False) -> Dict:\n",
    "        \"\"\"하이퍼파라미터 최적화 - 실제로 수행됨!\"\"\"\n",
    "        \n",
    "        # Optuna 사용 가능 여부 확인\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            self.logger.write(\"⚠️ Optuna not available - Please install: pip install optuna\")\n",
    "            self.logger.write(\"   Using default parameters instead\")\n",
    "            return config\n",
    "        \n",
    "        # hyperparameter_optimization이 enabled인지 확인\n",
    "        if not config.get('hyperparameter_optimization', {}).get('enabled', False):\n",
    "            self.logger.write(\"⚠️ Hyperparameter optimization is disabled in config\")\n",
    "            self.logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "            return config\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"🎯 OPTUNA HYPERPARAMETER OPTIMIZATION STARTING\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Number of trials: {n_trials}\")\n",
    "        self.logger.write(f\"Optimization metric: {config['hyperparameter_optimization'].get('metric', 'rouge_l')}\")\n",
    "        \n",
    "        def objective(trial: Trial) -> float:\n",
    "            \"\"\"실제 목적 함수 - 모델 학습 및 평가\"\"\"\n",
    "            \n",
    "            # Config의 search_space 기반으로 파라미터 제안\n",
    "            search_space = config['hyperparameter_optimization']['search_space']\n",
    "            \n",
    "            hp = {}\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'learning_rate' in search_space:\n",
    "                lr_config = search_space['learning_rate']\n",
    "                hp['learning_rate'] = trial.suggest_float(\n",
    "                    'learning_rate', \n",
    "                    lr_config['low'], \n",
    "                    lr_config['high'], \n",
    "                    log=lr_config.get('log', True)\n",
    "                )\n",
    "            \n",
    "            # Batch size\n",
    "            if 'batch_size' in search_space:\n",
    "                bs_config = search_space['batch_size']\n",
    "                hp['batch_size'] = trial.suggest_categorical(\n",
    "                    'batch_size',\n",
    "                    bs_config['choices']\n",
    "                )\n",
    "            \n",
    "            # LoRA parameters (if using LoRA)\n",
    "            if 'lora_r' in search_space:\n",
    "                lora_r_config = search_space['lora_r']\n",
    "                hp['lora_r'] = trial.suggest_int(\n",
    "                    'lora_r',\n",
    "                    lora_r_config['low'],\n",
    "                    lora_r_config['high'],\n",
    "                    step=lora_r_config.get('step', 4)\n",
    "                )\n",
    "            \n",
    "            if 'lora_alpha' in search_space:\n",
    "                lora_alpha_config = search_space['lora_alpha']\n",
    "                hp['lora_alpha'] = trial.suggest_int(\n",
    "                    'lora_alpha',\n",
    "                    lora_alpha_config['low'],\n",
    "                    lora_alpha_config['high'],\n",
    "                    step=lora_alpha_config.get('step', 8)\n",
    "                )\n",
    "            \n",
    "            # Generation parameters\n",
    "            if 'num_beams' in search_space:\n",
    "                nb_config = search_space['num_beams']\n",
    "                hp['num_beams'] = trial.suggest_int(\n",
    "                    'num_beams',\n",
    "                    nb_config['low'],\n",
    "                    nb_config.get('high', 8)\n",
    "                )\n",
    "            \n",
    "            if 'temperature' in search_space:\n",
    "                temp_config = search_space['temperature']\n",
    "                hp['temperature'] = trial.suggest_float(\n",
    "                    'temperature',\n",
    "                    temp_config['low'],\n",
    "                    temp_config['high']\n",
    "                )\n",
    "            \n",
    "            # 추가 파라미터\n",
    "            hp['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "            hp['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)\n",
    "            hp['top_p'] = trial.suggest_float('top_p', 0.8, 1.0)\n",
    "            \n",
    "            self.logger.write(f\"\\nTrial {trial.number}: {hp}\")\n",
    "            \n",
    "            if actual_training:\n",
    "                # 실제 모델 학습 및 평가 (시간이 오래 걸림)\n",
    "                # 여기에 실제 학습 코드를 넣을 수 있습니다\n",
    "                score = self._train_and_evaluate(hp, config)\n",
    "            else:\n",
    "                # 시뮬레이션 모드 (빠른 테스트용)\n",
    "                score = self._simulate_training(hp)\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Optuna study 생성\n",
    "        study = optuna.create_study(\n",
    "            direction=config['hyperparameter_optimization'].get('direction', 'maximize'),\n",
    "            sampler=TPESampler(seed=42),\n",
    "            study_name='pipeline_optimization',\n",
    "            pruner=optuna.pruners.MedianPruner() if config['hyperparameter_optimization'].get('pruner') == 'MedianPruner' else None\n",
    "        )\n",
    "        \n",
    "        # 최적화 실행!\n",
    "        self.logger.write(\"\\n🚀 Starting optimization...\")\n",
    "        study.optimize(\n",
    "            objective, \n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # 최적 파라미터 저장\n",
    "        self.best_params = study.best_params\n",
    "        best_value = study.best_value\n",
    "        self.study = study\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"✅ OPTIMIZATION COMPLETED!\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Best score: {best_value:.4f}\")\n",
    "        self.logger.write(f\"Best parameters:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            self.logger.write(f\"  - {param}: {value}\")\n",
    "        \n",
    "        # 상위 5개 trial 출력\n",
    "        self.logger.write(\"\\n📊 Top 5 trials:\")\n",
    "        for i, trial in enumerate(study.best_trials[:5], 1):\n",
    "            self.logger.write(f\"{i}. Score: {trial.value:.4f}\")\n",
    "        \n",
    "        # Config 업데이트\n",
    "        updated_config = self._update_config_with_best_params(config)\n",
    "        \n",
    "        # 최적화 결과 저장\n",
    "        self._save_optimization_results(study, config)\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _simulate_training(self, hp: Dict) -> float:\n",
    "        \"\"\"학습 시뮬레이션 (빠른 테스트용)\"\"\"\n",
    "        # 파라미터 조합에 따른 점수 시뮬레이션\n",
    "        score = np.random.random() * 0.3 + 0.4  # 0.4~0.7 범위\n",
    "        \n",
    "        # 좋은 파라미터 조합에 보너스\n",
    "        if hp['learning_rate'] < 5e-5 and hp['batch_size'] <= 8:\n",
    "            score += 0.1\n",
    "        if hp.get('lora_r', 8) >= 16:\n",
    "            score += 0.05\n",
    "        if hp.get('num_beams', 4) >= 4:\n",
    "            score += 0.05\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _train_and_evaluate(self, hp: Dict, config: Dict) -> float:\n",
    "        \"\"\"실제 모델 학습 및 평가 (구현 필요)\"\"\"\n",
    "        # 여기에 실제 모델 학습 코드를 구현\n",
    "        # 현재는 시뮬레이션으로 대체\n",
    "        return self._simulate_training(hp)\n",
    "    \n",
    "    def _update_config_with_best_params(self, config: Dict) -> Dict:\n",
    "        \"\"\"최적 파라미터로 config 업데이트\"\"\"\n",
    "        updated_config = config.copy()\n",
    "        \n",
    "        if self.best_params:\n",
    "            # training 섹션 업데이트\n",
    "            if 'learning_rate' in self.best_params:\n",
    "                updated_config['training']['learning_rate'] = self.best_params['learning_rate']\n",
    "            if 'batch_size' in self.best_params:\n",
    "                updated_config['training']['batch_size'] = self.best_params['batch_size']\n",
    "            if 'warmup_ratio' in self.best_params:\n",
    "                updated_config['training']['warmup_ratio'] = self.best_params['warmup_ratio']\n",
    "            if 'weight_decay' in self.best_params:\n",
    "                updated_config['training']['weight_decay'] = self.best_params['weight_decay']\n",
    "            \n",
    "            # LoRA 설정 업데이트 (if applicable)\n",
    "            if 'models' in updated_config and 'primary_models' in updated_config['models']:\n",
    "                for model_config in updated_config['models']['primary_models']:\n",
    "                    if 'lora_r' in self.best_params:\n",
    "                        model_config['lora_r'] = self.best_params['lora_r']\n",
    "                    if 'lora_alpha' in self.best_params:\n",
    "                        model_config['lora_alpha'] = self.best_params['lora_alpha']\n",
    "            \n",
    "            # Generation 설정 업데이트\n",
    "            if 'post_processing' in updated_config:\n",
    "                if 'temperature' in self.best_params:\n",
    "                    # temperature 설정 추가\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['temperature'] = self.best_params['temperature']\n",
    "                if 'top_p' in self.best_params:\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['top_p'] = self.best_params['top_p']\n",
    "            \n",
    "            self.logger.write(\"\\n✅ Config updated with optimal hyperparameters!\")\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _save_optimization_results(self, study, config):\n",
    "        \"\"\"최적화 결과 저장\"\"\"\n",
    "        import pickle\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # 결과 저장 경로\n",
    "        output_dir = Path(config['paths']['log_dir']) / 'optuna'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Study 객체 저장\n",
    "        study_path = output_dir / f'optuna_study_{timestamp}.pkl'\n",
    "        with open(study_path, 'wb') as f:\n",
    "            pickle.dump(study, f)\n",
    "        \n",
    "        # 결과 CSV 저장\n",
    "        df = study.trials_dataframe()\n",
    "        csv_path = output_dir / f'optuna_results_{timestamp}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # 최적 파라미터 JSON 저장\n",
    "        import json\n",
    "        json_path = output_dir / f'best_params_{timestamp}.json'\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=2)\n",
    "        \n",
    "        self.logger.write(f\"\\n📁 Optimization results saved:\")\n",
    "        self.logger.write(f\"  - Study: {study_path}\")\n",
    "        self.logger.write(f\"  - CSV: {csv_path}\")\n",
    "        self.logger.write(f\"  - Best params: {json_path}\")\n",
    "\n",
    "# 리스크 관리 시스템 (PRD 05_리스크_관리.md)\n",
    "class PipelineRiskManager:\n",
    "    \"\"\"파이프라인 리스크 관리\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.risks = []\n",
    "        self.mitigations_applied = []\n",
    "        \n",
    "    def monitor_pipeline_risks(self, stage: str, metrics: Dict) -> Dict:\n",
    "        \"\"\"파이프라인 단계별 리스크 모니터링\"\"\"\n",
    "        stage_risks = []\n",
    "        \n",
    "        # 데이터 품질 리스크\n",
    "        if stage == 'data_quality_check':\n",
    "            if metrics.get('encoding_issues', 0) > 100:\n",
    "                stage_risks.append({\n",
    "                    'type': 'data_quality',\n",
    "                    'severity': 'high',\n",
    "                    'description': f\"High encoding issues: {metrics['encoding_issues']}\",\n",
    "                    'mitigation': 'Apply text cleaning and encoding fixes'\n",
    "                })\n",
    "        \n",
    "        # 학습 리스크\n",
    "        elif stage == 'model_training':\n",
    "            if metrics.get('train_loss', float('inf')) > 5.0:\n",
    "                stage_risks.append({\n",
    "                    'type': 'training_instability',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"High training loss: {metrics.get('train_loss')}\",\n",
    "                    'mitigation': 'Reduce learning rate or check data'\n",
    "                })\n",
    "            \n",
    "            if metrics.get('val_loss', 0) > metrics.get('train_loss', 1) * 2:\n",
    "                stage_risks.append({\n",
    "                    'type': 'overfitting',\n",
    "                    'severity': 'high',\n",
    "                    'description': 'Significant overfitting detected',\n",
    "                    'mitigation': 'Apply regularization or early stopping'\n",
    "                })\n",
    "        \n",
    "        # 메모리 리스크\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() if torch.cuda.max_memory_allocated() > 0 else 0\n",
    "            if memory_used > 0.9:\n",
    "                stage_risks.append({\n",
    "                    'type': 'memory_overflow',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"Memory usage: {memory_used:.1%}\",\n",
    "                    'mitigation': 'Reduce batch size or model size'\n",
    "                })\n",
    "        \n",
    "        # 리스크 기록 및 보고\n",
    "        if stage_risks:\n",
    "            self.risks.extend(stage_risks)\n",
    "            self.logger.write(f\"\\n⚠️ Risks detected in {stage}:\")\n",
    "            for risk in stage_risks:\n",
    "                self.logger.write(f\"  [{risk['severity']}] {risk['type']}: {risk['description']}\")\n",
    "                self.logger.write(f\"    → Mitigation: {risk['mitigation']}\")\n",
    "        \n",
    "        return {\n",
    "            'stage': stage,\n",
    "            'risks': stage_risks,\n",
    "            'risk_count': len(stage_risks)\n",
    "        }\n",
    "    \n",
    "    def apply_automatic_mitigation(self, risk_type: str, config: Dict) -> Dict:\n",
    "        \"\"\"자동 리스크 완화\"\"\"\n",
    "        mitigations = {\n",
    "            'overfitting': {\n",
    "                'action': 'increase_regularization',\n",
    "                'config_changes': {\n",
    "                    'training.weight_decay': config['training'].get('weight_decay', 0) * 2,\n",
    "                    'training.dropout': 0.3\n",
    "                }\n",
    "            },\n",
    "            'memory_overflow': {\n",
    "                'action': 'reduce_batch_size',\n",
    "                'config_changes': {\n",
    "                    'training.batch_size': max(1, config['training']['batch_size'] // 2)\n",
    "                }\n",
    "            },\n",
    "            'training_instability': {\n",
    "                'action': 'reduce_learning_rate',\n",
    "                'config_changes': {\n",
    "                    'training.learning_rate': float(config['training']['learning_rate']) * 0.1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if risk_type in mitigations:\n",
    "            mitigation = mitigations[risk_type]\n",
    "            self.mitigations_applied.append(mitigation)\n",
    "            self.logger.write(f\"✓ Applied mitigation: {mitigation['action']}\")\n",
    "            \n",
    "            # Config 업데이트\n",
    "            for key, value in mitigation['config_changes'].items():\n",
    "                keys = key.split('.')\n",
    "                if len(keys) == 2:\n",
    "                    config[keys[0]][keys[1]] = value\n",
    "            \n",
    "            return config\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def generate_risk_report(self) -> Dict:\n",
    "        \"\"\"리스크 보고서 생성\"\"\"\n",
    "        if not self.risks:\n",
    "            return {\n",
    "                'status': 'healthy',\n",
    "                'total_risks': 0,\n",
    "                'critical_risks': 0\n",
    "            }\n",
    "        \n",
    "        critical_count = sum(1 for r in self.risks if r['severity'] == 'critical')\n",
    "        high_count = sum(1 for r in self.risks if r['severity'] == 'high')\n",
    "        \n",
    "        return {\n",
    "            'status': 'at_risk' if critical_count > 0 else 'warning' if high_count > 0 else 'healthy',\n",
    "            'total_risks': len(self.risks),\n",
    "            'critical_risks': critical_count,\n",
    "            'high_risks': high_count,\n",
    "            'mitigations_applied': len(self.mitigations_applied)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# Stage 5: Optuna 최적화 실행 - 실제로 실행됨!\n",
    "# =============================================================================\n",
    "if 'hyperparameter_optimization' in config['pipeline']['stages']:\n",
    "    update_status('hyperparameter_optimization', 'running')\n",
    "    \n",
    "    logger.write(\"\\n\" + \"=\"*70)\n",
    "    logger.write(\"🎯 HYPERPARAMETER OPTIMIZATION STAGE\")\n",
    "    logger.write(\"=\"*70)\n",
    "    \n",
    "    # Optuna 최적화 실행\n",
    "    optuna_optimizer = PipelineOptunaOptimizer(logger)\n",
    "    \n",
    "    # Config에서 설정 가져오기\n",
    "    optimization_enabled = config.get('hyperparameter_optimization', {}).get('enabled', False)\n",
    "    n_trials = config.get('hyperparameter_optimization', {}).get('n_trials', 20)\n",
    "    \n",
    "    if optimization_enabled and OPTUNA_AVAILABLE:\n",
    "        logger.write(f\"✅ Optimization ENABLED with {n_trials} trials\")\n",
    "        \n",
    "        # 실제 최적화 실행! (actual_training=False는 시뮬레이션, True는 실제 학습)\n",
    "        optimized_config = optuna_optimizer.optimize_hyperparameters(\n",
    "            config, \n",
    "            n_trials=n_trials,\n",
    "            actual_training=False  # True로 변경하면 실제 모델 학습으로 최적화\n",
    "        )\n",
    "        \n",
    "        # 최적 파라미터로 config 업데이트\n",
    "        if optuna_optimizer.best_params:\n",
    "            config = optimized_config\n",
    "            logger.write(\"\\n✅ Config has been updated with optimal hyperparameters!\")\n",
    "            \n",
    "            # WandB 로깅\n",
    "            if config['wandb']['mode'] != 'disabled':\n",
    "                wandb.log({\n",
    "                    'optuna/best_score': optuna_optimizer.study.best_value,\n",
    "                    'optuna/n_trials': n_trials,\n",
    "                    'optuna/best_params': optuna_optimizer.best_params\n",
    "                })\n",
    "    else:\n",
    "        if not optimization_enabled:\n",
    "            logger.write(\"⚠️ Optimization is DISABLED in config\")\n",
    "            logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            logger.write(\"⚠️ Optuna library not available\")\n",
    "            logger.write(\"   Install with: pip install optuna\")\n",
    "    \n",
    "    update_status('hyperparameter_optimization', 'completed')\n",
    "\n",
    "# 리스크 관리 초기화\n",
    "risk_manager = PipelineRiskManager(logger)\n",
    "\n",
    "# Solar API 초기화 (config에서 키 확인)\n",
    "solar_api = None\n",
    "if 'solar_api' in config and config['solar_api'].get('enabled', False):\n",
    "    if 'api_key' in config['solar_api']:\n",
    "        solar_api = PipelineSolarAPI(config['solar_api']['api_key'], logger)\n",
    "        logger.write(\"✅ Solar API initialized for cross-validation\")\n",
    "    else:\n",
    "        logger.write(\"⚠️ Solar API key not found in config\")\n",
    "else:\n",
    "    logger.write(\"⚠️ Solar API is disabled in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:04.942899Z",
     "iopub.status.busy": "2025-10-10T02:53:04.942804Z",
     "iopub.status.idle": "2025-10-10T02:53:05.952389Z",
     "shell.execute_reply": "2025-10-10T02:53:05.951938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model_training] Status: running\n",
      "\n",
      "=== Model Training (GPU Optimized) ===\n",
      "✅ Mixed Precision (FP16) Training ENABLED - 40% memory reduction\n",
      "\n",
      "🧹 GPU 메모리 완전 정리 중...\n",
      "Training primary model: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded to CPU\n",
      "Model moved to cuda\n",
      "GPU Memory - Total: 23.99GB, Reserved: 0.52GB, Allocated: 0.46GB\n",
      "✅ Gradient Accumulation: 1 steps\n",
      "   Physical batch size: 4\n",
      "   Effective batch size: 4\n",
      "\n",
      "⚙️ Optimizer 초기화 중...\n",
      "✅ Optimizer initialized successfully\n",
      "\n",
      "======================================================================\n",
      "🚀 TRAINING START - GPU Optimized\n",
      "======================================================================\n",
      "Epochs: 30\n",
      "Gradient Accumulation: 1\n",
      "Mixed Precision (FP16): True\n",
      "Gradient Checkpointing: False\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303991/1995884847.py:212: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if USE_AMP else None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d673fac6cf40159e4b430bb6a6e38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303991/1995884847.py:240: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1: Train Loss = 2.5708\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 2.5708)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c557502618402d8f744b41c1143132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2: Train Loss = 1.4324\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 1.4324)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f482378cd2f435386c3001062ac53ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3: Train Loss = 1.2617\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 1.2617)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ce72bbaf764b80af9b1c3900326df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4: Train Loss = 1.1004\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 1.1004)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1343bd31e3e84248b9956304fcd01b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5: Train Loss = 0.9112\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.9112)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46e32c215f4b939375b8c712e344f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 6: Train Loss = 0.7450\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.7450)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc428dd3002e494892220dac77e62ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 7: Train Loss = 0.6070\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.6070)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7b1d9495f4e639c900c593dc21c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 8: Train Loss = 0.4948\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.4948)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312e075682864b0484141d245f4bff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 9: Train Loss = 0.4017\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.4017)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d53038303444603b4d40f17fdf7c12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10: Train Loss = 0.3273\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  ✅ Best model saved (loss: 0.3273)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a37465fe9f14e79ad2594b3bf90a330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 11: Train Loss = 0.2656\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.2656)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e089d94a449e4a799145c2b604e92fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 12: Train Loss = 0.2163\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.2163)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bcd46cb8814f5aa367c25daa65be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 13: Train Loss = 0.1767\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.1767)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb9c10aeb454bf6b060b97e360ab961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 14: Train Loss = 0.1446\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.1446)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee25591bfb0d419bac9f37426e0aa3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15: Train Loss = 0.1202\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.1202)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44717019d08240dd8355b3963b36e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 16: Train Loss = 0.0979\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0979)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3113dfcc694648be93ad79c116e58cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 17: Train Loss = 0.0822\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0822)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b765ff0047094d20af45e774aaba21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 18: Train Loss = 0.0687\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0687)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125368c953264311a703da00bd024773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 19: Train Loss = 0.0588\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0588)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d0be7a18c34cdba871178c79bebc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20: Train Loss = 0.0493\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  ✅ Best model saved (loss: 0.0493)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29919a4cf5d242e78d63daa7ce547564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 21: Train Loss = 0.0423\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0423)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6368c5f64a4d969e2ab2ddcc713e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 22: Train Loss = 0.0360\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0360)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ede63673e4744e48cd912d3b7f2fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 23: Train Loss = 0.0315\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0315)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50391b5f74f74252830f93ef3df93ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 24: Train Loss = 0.0270\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0270)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b47f2a2dec549a580fc13dfef36b2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 25: Train Loss = 0.0235\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0235)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702567cba5ec44c099491acbedbc0a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 26: Train Loss = 0.0208\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0208)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608d82afcdce420492909976e8adc1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 27: Train Loss = 0.0181\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0181)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7765b94ae3421183fdbf04a0df93e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 28: Train Loss = 0.0164\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0164)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75c1761af724c3c945b3e0c8aa3123b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 29: Train Loss = 0.0159\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=3.42GB\n",
      "  ✅ Best model saved (loss: 0.0159)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0545a4eea2ec42da987847995360b57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/3115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 30: Train Loss = 0.0139\n",
      "  GPU Memory: Allocated=1.44GB, Reserved=1.64GB\n",
      "  ✅ Best model saved (loss: 0.0139)\n",
      "\n",
      "======================================================================\n",
      "✅ Training completed successfully!\n",
      "Best loss: 0.0139\n",
      "Total training steps: 93450\n",
      "======================================================================\n",
      "\n",
      "[model_training] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: 모델 학습 - GPU 메모리 최적화 적용!\n",
    "if 'model_training' in config['pipeline']['stages']:\n",
    "    update_status('model_training', 'running')\n",
    "    logger.write(\"\\n=== Model Training (GPU Optimized) ===\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.optim import AdamW\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    from tqdm.auto import tqdm\n",
    "    import gc\n",
    "    \n",
    "    # Mixed Precision Training import (FP16)\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        USE_AMP = torch.cuda.is_available() and config['gpu'].get('mixed_precision', True)\n",
    "        if USE_AMP:\n",
    "            logger.write(\"✅ Mixed Precision (FP16) Training ENABLED - 40% memory reduction\")\n",
    "    except ImportError:\n",
    "        USE_AMP = False\n",
    "        logger.write(\"⚠️ Mixed Precision not available\")\n",
    "    \n",
    "    # 필요한 함수 정의\n",
    "    def get_path(path_str):\n",
    "        \"\"\"config의 상대 경로를 절대 경로로 변환\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # GPU 메모리 정리 함수\n",
    "    def clear_gpu_memory():\n",
    "        \"\"\"GPU 메모리 캐시 정리\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # ======================================================================\n",
    "    # 🔥 CRITICAL FIX: 모델 로드 전에 GPU 완전 정리!\n",
    "    # ======================================================================\n",
    "    logger.write(\"\\n🧹 GPU 메모리 완전 정리 중...\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # 기존 모델이 있다면 삭제\n",
    "    if 'model' in globals():\n",
    "        del model\n",
    "    if 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # 데이터셋 클래스\n",
    "    class DialogueSummaryDataset(Dataset):\n",
    "        def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n",
    "            self.df = dataframe.reset_index(drop=True)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_input_len = max_input_len\n",
    "            self.max_target_len = max_target_len\n",
    "            self.is_test = is_test\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            row = self.df.iloc[idx]\n",
    "            dialogue = row.get('dialogue_preprocessed', row.get('dialogue', ''))\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                dialogue,\n",
    "                max_length=self.max_input_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            if not self.is_test:\n",
    "                summary = row.get('summary_preprocessed', row.get('summary', ''))\n",
    "                targets = self.tokenizer(\n",
    "                    summary,\n",
    "                    max_length=self.max_target_len,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # 라벨 생성 - 패딩 토큰을 -100으로 마스킹 (중요!)\n",
    "                labels = targets['input_ids'].squeeze()\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100  # 패딩 토큰 마스킹\n",
    "                \n",
    "                return {\n",
    "                    'input_ids': inputs['input_ids'].squeeze(),\n",
    "                    'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                    'labels': labels\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids': inputs['input_ids'].squeeze(),\n",
    "                    'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                    'idx': idx\n",
    "                }\n",
    "    \n",
    "    # 모델 선택\n",
    "    if 'primary_models' in config.get('models', {}):\n",
    "        model_config = config['models']['primary_models'][0]\n",
    "        model_name = model_config['name']\n",
    "    else:\n",
    "        model_name = \"gogamza/kobart-summarization\"\n",
    "        model_config = {'max_input_length': 512, 'max_target_length': 128}\n",
    "    \n",
    "    logger.write(f\"Training primary model: {model_name}\")\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    logger.write(\"✅ Tokenizer loaded\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    logger.write(\"✅ Model loaded to CPU\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU 최적화 1: Gradient Checkpointing (50% 메모리 감소)\n",
    "    # =============================================================================\n",
    "    if config['training'].get('gradient_checkpointing', True):\n",
    "        if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "            model.gradient_checkpointing_enable()\n",
    "            logger.write(\"✅ Gradient Checkpointing ENABLED - 50% memory reduction\")\n",
    "        elif hasattr(model.config, 'gradient_checkpointing'):\n",
    "            model.config.gradient_checkpointing = True\n",
    "            logger.write(\"✅ Gradient Checkpointing ENABLED (via config)\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    logger.write(f\"Model moved to {device}\")\n",
    "    \n",
    "    # GPU 메모리 상태 로깅\n",
    "    if torch.cuda.is_available():\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        reserved_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        logger.write(f\"GPU Memory - Total: {total_memory:.2f}GB, Reserved: {reserved_memory:.2f}GB, Allocated: {allocated_memory:.2f}GB\")\n",
    "    \n",
    "    # 데이터셋 생성 (샘플링 옵션)\n",
    "    if config['training'].get('use_sample', False):\n",
    "        sample_size = config['training'].get('sample_size', 1000)\n",
    "        train_sample = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n",
    "        logger.write(f\"Using sample of {len(train_sample)} for training\")\n",
    "    else:\n",
    "        train_sample = train_df\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU 최적화 2: Gradient Accumulation (작은 배치로 큰 배치 효과)\n",
    "    # =============================================================================\n",
    "    gradient_accumulation_steps = config['training'].get('gradient_accumulation_steps', 8)\n",
    "    effective_batch_size = config['training']['batch_size'] * gradient_accumulation_steps\n",
    "    logger.write(f\"✅ Gradient Accumulation: {gradient_accumulation_steps} steps\")\n",
    "    logger.write(f\"   Physical batch size: {config['training']['batch_size']}\")\n",
    "    logger.write(f\"   Effective batch size: {effective_batch_size}\")\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_dataset = DialogueSummaryDataset(\n",
    "        train_sample, tokenizer,\n",
    "        max_input_len=model_config.get('max_input_length', 512),\n",
    "        max_target_len=model_config.get('max_target_length', 128)\n",
    "    )\n",
    "    \n",
    "    val_dataset = DialogueSummaryDataset(\n",
    "        dev_df, tokenizer,\n",
    "        max_input_len=model_config.get('max_input_length', 512),\n",
    "        max_target_len=model_config.get('max_target_length', 128)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # GPU 메모리 절약\n",
    "        pin_memory=False  # 메모리 문제 시 False로\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # 학습 설정\n",
    "    num_epochs = config['training']['num_epochs']\n",
    "    learning_rate = float(config['training']['learning_rate']) if isinstance(config['training']['learning_rate'], str) else config['training']['learning_rate']\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 🔥 CRITICAL FIX: Optimizer CPU에서 초기화 후 GPU로 이동\n",
    "    # =============================================================================\n",
    "    logger.write(\"\\n⚙️ Optimizer 초기화 중...\")\n",
    "    \n",
    "    # Optimizer 초기화 전 GPU 메모리 정리\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    logger.write(\"✅ Optimizer initialized successfully\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GPU 최적화 3: Mixed Precision Training (FP16) - GradScaler\n",
    "    # =============================================================================\n",
    "    scaler = GradScaler() if USE_AMP else None\n",
    "    \n",
    "    logger.write(f\"\\n{'='*70}\")\n",
    "    logger.write(f\"🚀 TRAINING START - GPU Optimized\")\n",
    "    logger.write(f\"{'='*70}\")\n",
    "    logger.write(f\"Epochs: {num_epochs}\")\n",
    "    logger.write(f\"Gradient Accumulation: {gradient_accumulation_steps}\")\n",
    "    logger.write(f\"Mixed Precision (FP16): {USE_AMP}\")\n",
    "    logger.write(f\"Gradient Checkpointing: {config['training'].get('gradient_checkpointing', True)}\")\n",
    "    logger.write(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 학습 루프 - GPU 최적화 적용\n",
    "    best_loss = float('inf')\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):\n",
    "            try:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Mixed Precision Forward Pass\n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        loss = outputs.loss / gradient_accumulation_steps\n",
    "                else:\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss / gradient_accumulation_steps\n",
    "                \n",
    "                total_loss += loss.item() * gradient_accumulation_steps\n",
    "                \n",
    "                # Mixed Precision Backward Pass\n",
    "                if USE_AMP:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                # Gradient Accumulation - N step마다 업데이트\n",
    "                if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                    if USE_AMP:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['training'].get('max_grad_norm', 1.0))\n",
    "                    \n",
    "                    if USE_AMP:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # GPU 메모리 정리\n",
    "                    if global_step % 50 == 0:  # 50 step마다\n",
    "                        clear_gpu_memory()\n",
    "                        \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    logger.write(f\"\\n⚠️ OOM Error at step {step}! Clearing cache and skipping batch...\")\n",
    "                    clear_gpu_memory()\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        logger.write(f\"  Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        # GPU 메모리 상태\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            logger.write(f\"  GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            output_dir = get_path(config['paths']['output_dir'])\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model_path = output_dir / 'best_model_pipeline.pt'\n",
    "            \n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), model_path)\n",
    "            logger.write(f\"  ✅ Best model saved (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    logger.write(f\"\\n{'='*70}\")\n",
    "    logger.write(f\"✅ Training completed successfully!\")\n",
    "    logger.write(f\"Best loss: {best_loss:.4f}\")\n",
    "    logger.write(f\"Total training steps: {global_step}\")\n",
    "    logger.write(f\"{'='*70}\\n\")\n",
    "    \n",
    "    update_status('model_training', 'completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:05.978027Z",
     "iopub.status.busy": "2025-10-10T02:53:05.977925Z",
     "iopub.status.idle": "2025-10-10T02:53:06.016808Z",
     "shell.execute_reply": "2025-10-10T02:53:06.016366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final_prediction] Status: running\n",
      "\n",
      "=== Final Prediction ===\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ad8f0c96364243a979f2d7a237e138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 499 predictions\n",
      "\n",
      "=== Solar API Cross-Validation ===\n",
      "\n",
      "=== Solar API Cross-Validation ===\n",
      "Comparisons completed: 10 samples\n",
      "Avg model length: 234.3\n",
      "Avg API length: 183.8\n",
      "API calls made: 10\n",
      "Estimated tokens used: 1377\n",
      "✅ Solar API cross-validation completed\n",
      "   Model avg length: 234.3\n",
      "   API avg length: 183.8\n",
      "   API calls made: 10\n",
      "Submission file saved: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/submissions/full_pipeline/full_pipeline_submission_20251010_200633.csv\n",
      "Shape: (499, 2)\n",
      "[final_prediction] Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Stage 9: 최종 예측 및 제출\n",
    "if 'final_prediction' in config['pipeline']['stages']:\n",
    "    update_status('final_prediction', 'running')\n",
    "    logger.write(\"\\n=== Final Prediction ===\")\n",
    "    \n",
    "    # 필요한 함수 정의\n",
    "    def get_path(path_str):\n",
    "        \"\"\"config의 상대 경로를 절대 경로로 변환\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # 테스트 데이터셋 생성\n",
    "    test_dataset = DialogueSummaryDataset(\n",
    "        test_df, tokenizer,\n",
    "        max_input_len=config['models']['primary_models'][0].get('max_input_length', 512),\n",
    "        max_target_len=config['models']['primary_models'][0].get('max_target_length', 128),\n",
    "        is_test=True\n",
    "    )\n",
    "    \n",
    "    # 배치 사이즈 설정 - inference_optimization에서 가져오기\n",
    "    inference_batch_size = 8  # 기본값\n",
    "    if 'inference_optimization' in config:\n",
    "        if 'batch_inference' in config['inference_optimization']:\n",
    "            if config['inference_optimization']['batch_inference'].get('optimal_batch_size') == 'auto':\n",
    "                inference_batch_size = 8\n",
    "            else:\n",
    "                inference_batch_size = config['inference_optimization']['batch_inference'].get('optimal_batch_size', 8)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=inference_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # 예측 생성\n",
    "    logger.write(\"Generating predictions for test set...\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # ✅ Base와 동일한 Generation 파라미터 (짧고 정확한 요약)\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,              # Base와 동일\n",
    "                num_beams=4,                 # Base와 동일\n",
    "                no_repeat_ngram_size=2,      # Base와 동일 (3→2로 수정)\n",
    "                early_stopping=True          # Base와 동일\n",
    "                # min_length 제거! (자연스러운 짧은 요약 허용)\n",
    "                # repetition_penalty 제거! (불필요한 반복 억제 제거)\n",
    "                # length_penalty 제거! (중립값이므로 불필요)\n",
    "            )\n",
    "            \n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # ✅ Base와 동일한 단순 후처리 (특수 토큰만 제거)\n",
    "            import re\n",
    "            remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']\n",
    "            \n",
    "            cleaned_preds = []\n",
    "            for pred in preds:\n",
    "                cleaned = pred\n",
    "                # 특수 토큰 제거\n",
    "                for token in remove_tokens:\n",
    "                    cleaned = cleaned.replace(token, ' ')\n",
    "                # 중복 공백 제거\n",
    "                cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "                \n",
    "                cleaned_preds.append(cleaned)\n",
    "            \n",
    "            predictions.extend(cleaned_preds)\n",
    "    \n",
    "    logger.write(f\"Generated {len(predictions)} predictions\")\n",
    "    \n",
    "    # 🔧 Solar API 교차검증\n",
    "    if solar_api and config['solar_api']['enabled']:\n",
    "        logger.write(\"\\n=== Solar API Cross-Validation ===\")\n",
    "        \n",
    "        # 모델 예측과 Solar API 비교 (샘플 10개)\n",
    "        solar_results = solar_api.optimize_and_validate(\n",
    "            model_predictions=predictions[:10],  # 처음 10개 샘플\n",
    "            test_dialogues=test_df['dialogue_preprocessed'].tolist()[:10],\n",
    "            sample_size=10\n",
    "        )\n",
    "        \n",
    "        if solar_results:\n",
    "            logger.write(f\"✅ Solar API cross-validation completed\")\n",
    "            logger.write(f\"   Model avg length: {solar_results['avg_model_length']:.1f}\")\n",
    "            logger.write(f\"   API avg length: {solar_results['avg_api_length']:.1f}\")\n",
    "            logger.write(f\"   API calls made: {solar_results['api_calls']}\")\n",
    "            \n",
    "            # WandB 로깅\n",
    "            if config['wandb']['mode'] != 'disabled':\n",
    "                wandb.log({\n",
    "                    'solar_api/model_length': solar_results['avg_model_length'],\n",
    "                    'solar_api/api_length': solar_results['avg_api_length'],\n",
    "                    'solar_api/api_calls': solar_results['api_calls']\n",
    "                })\n",
    "    else:\n",
    "        logger.write(\"\\n⚠️ Solar API cross-validation skipped (disabled or not initialized)\")\n",
    "    \n",
    "    # 제출 파일 생성\n",
    "    # test_df의 fname 컬럼 사용 (id가 아님)\n",
    "    submission_df = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': predictions\n",
    "    })\n",
    "    \n",
    "    # 제출 파일 저장\n",
    "    submission_dir = get_path(config['paths']['submission_dir'])\n",
    "    submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    submission_path = submission_dir / f'full_pipeline_submission_{timestamp}.csv'\n",
    "    submission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=True로 변경\n",
    "    \n",
    "    logger.write(f\"Submission file saved: {submission_path}\")\n",
    "    logger.write(f\"Shape: {submission_df.shape}\")\n",
    "    \n",
    "    update_status('final_prediction', 'completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
