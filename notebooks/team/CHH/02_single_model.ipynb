{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ë‹¨ì¼ ëª¨ë¸ ì‹¤í—˜ - SOLAR/Polyglot-Ko LLM íŒŒì¸íŠœë‹\n",
    "> PRD ê³„íšì— ë”°ë¥¸ LLM ëª¨ë¸ LoRA íŒŒì¸íŠœë‹\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 70-73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "Using LoRA: True\n",
      "  - LoRA r: 16\n",
      "  - LoRA alpha: 32\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_single_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ëª¨ë¸ ì„ íƒ\n",
    "current_model = config['current_model']\n",
    "model_config = config['models'][current_model]\n",
    "\n",
    "print(f\"Selected Model: {model_config['name']}\")\n",
    "print(f\"Using LoRA: {model_config['use_lora']}\")\n",
    "if model_config['use_lora']:\n",
    "    print(f\"  - LoRA r: {model_config['lora_r']}\")\n",
    "    print(f\"  - LoRA alpha: {model_config['lora_alpha']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: logs/single_model\n",
      "==================================================\n",
      "Single Model Experiment: solar\n",
      "Timestamp: 20251010_090241\n",
      "Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'single_model_{current_model}_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write(f'Single Model Experiment: {current_model}')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Model: {model_config[\"name\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n"
     ]
    }
   ],
   "source": [
    "# GPU ì²´í¬\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    device = torch.device('cuda')\n",
    "    logger.write(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    logger.write(\"WARNING: No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration created\n",
      "  - r: 16\n",
      "  - alpha: 32\n",
      "  - dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# LoRA ì„¤ì •\n",
    "if model_config['use_lora']:\n",
    "    lora_config = LoraConfig(\n",
    "        r=model_config['lora_r'],\n",
    "        lora_alpha=model_config['lora_alpha'],\n",
    "        lora_dropout=model_config['lora_dropout'],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=config['peft']['target_modules']\n",
    "    )\n",
    "    logger.write(\"LoRA configuration created\")\n",
    "    logger.write(f\"  - r: {model_config['lora_r']}\")\n",
    "    logger.write(f\"  - alpha: {model_config['lora_alpha']}\")\n",
    "    logger.write(f\"  - dropout: {model_config['lora_dropout']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template loaded\n",
      "\n",
      "Sample Prompt:\n",
      "### Instruction:\n",
      "ë‹¤ìŒ ëŒ€í™”ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì •ë³´ë¥¼ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.\n",
      "\n",
      "### Input:\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”?\n",
      "#Person2#: ì•ˆë…•í•˜ì„¸ìš”!\n",
      "\n",
      "### Response:\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™•ì¸\n",
    "sample_dialogue = \"#Person1#: ì•ˆë…•í•˜ì„¸ìš”?\\n#Person2#: ì•ˆë…•í•˜ì„¸ìš”!\"\n",
    "prompt = config['prompt_template']['instruction_format'].format(\n",
    "    dialogue=sample_dialogue,\n",
    "    summary=\"\"\n",
    ")\n",
    "logger.write(\"Prompt template loaded\")\n",
    "print(\"\\nSample Prompt:\")\n",
    "print(prompt[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (PRD 14_í”„ë¡¬í”„íŠ¸_ì—”ì§€ë‹ˆì–´ë§.md)\nimport requests\nimport json\nfrom typing import Optional, Dict, List\n\nclass AdvancedPromptEngineering:\n    \"\"\"ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self):\n        self.prompt_templates = {\n            'few_shot': self._create_few_shot_template,\n            'chain_of_thought': self._create_cot_template,\n            'role_based': self._create_role_based_template,\n            'structured': self._create_structured_template,\n            'topic_specific': self._create_topic_specific_template\n        }\n        \n    def _create_few_shot_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"Few-shot í•™ìŠµ í”„ë¡¬í”„íŠ¸\"\"\"\n        examples = \"\"\"\nì˜ˆì‹œ 1:\nëŒ€í™”: í™”ì1: ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. í™”ì2: ë„¤, ì‚°ì±…í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤.\nìš”ì•½: ë‘ ì‚¬ëŒì´ ì¢‹ì€ ë‚ ì”¨ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ë©° ì‚°ì±…í•˜ê¸° ì¢‹ë‹¤ê³  ë™ì˜í•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ 2:\nëŒ€í™”: í™”ì1: í”„ë¡œì íŠ¸ ë§ˆê°ì¼ì´ ì–¸ì œì£ ? í™”ì2: ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì…ë‹ˆë‹¤. í™”ì1: ì„œë‘˜ëŸ¬ì•¼ê² ë„¤ìš”.\nìš”ì•½: í”„ë¡œì íŠ¸ ë§ˆê°ì¼ì´ ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì´ë©°, ì„œë‘˜ëŸ¬ ì™„ì„±í•´ì•¼ í•œë‹¤ê³  ëŒ€í™”í•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ 3:\nëŒ€í™”: í™”ì1: ê°ê¸° ì¦ìƒì´ ìˆì–´ìš”. í™”ì2: ì¶©ë¶„í•œ íœ´ì‹ì„ ì·¨í•˜ì„¸ìš”. ì•½ë„ ì²˜ë°©í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\nìš”ì•½: í™˜ìê°€ ê°ê¸° ì¦ìƒì„ í˜¸ì†Œí•˜ì ì˜ì‚¬ê°€ íœ´ì‹ê³¼ ì•½ ì²˜ë°©ì„ ê¶Œí•©ë‹ˆë‹¤.\n\"\"\"\n        \n        return f\"\"\"### Instruction:\nì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{examples}\n\n### Input:\n{dialogue}\n\n### Response:\"\"\"\n    \n    def _create_cot_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"Chain-of-Thought í”„ë¡¬í”„íŠ¸\"\"\"\n        return f\"\"\"### Instruction:\në‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n1ë‹¨ê³„: ëŒ€í™” ì°¸ì—¬ìë“¤ì„ íŒŒì•…í•˜ì„¸ìš”\n2ë‹¨ê³„: ëŒ€í™”ì˜ ì£¼ìš” ì£¼ì œë¥¼ ì°¾ìœ¼ì„¸ìš”\n3ë‹¨ê³„: í•µì‹¬ ì •ë³´ì™€ ê²°ë¡ ì„ ì¶”ì¶œí•˜ì„¸ìš”\n4ë‹¨ê³„: 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”\n\n### Input:\n{dialogue}\n\n### Analysis and Response:\n1ë‹¨ê³„ ë¶„ì„:\n2ë‹¨ê³„ ë¶„ì„:\n3ë‹¨ê³„ ë¶„ì„:\n4ë‹¨ê³„ ìµœì¢… ìš”ì•½:\"\"\"\n    \n    def _create_role_based_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"ì—­í•  ê¸°ë°˜ í”„ë¡¬í”„íŠ¸\"\"\"\n        role_description = {\n            'ì˜ë£Œ': 'ì˜ë£Œ ì „ë¬¸ê°€ë¡œì„œ í™˜ìì˜ ì¦ìƒê³¼ ì¹˜ë£Œ ê³„íšì„ ëª…í™•íˆ',\n            'ë¹„ì¦ˆë‹ˆìŠ¤': 'ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ë¡œì„œ ì£¼ìš” ê²°ì •ì‚¬í•­ê³¼ ì•¡ì…˜ì•„ì´í…œì„',\n            'êµìœ¡': 'êµìœ¡ ì „ë¬¸ê°€ë¡œì„œ í•™ìŠµ ë‚´ìš©ê³¼ í•µì‹¬ ê°œë…ì„',\n            'ê¸°ìˆ ': 'ê¸°ìˆ  ì „ë¬¸ê°€ë¡œì„œ ê¸°ìˆ ì  ë…¼ì˜ì‚¬í•­ê³¼ í•´ê²°ì±…ì„'\n        }.get(topic, 'ì „ë¬¸ ìš”ì•½ê°€ë¡œì„œ í•µì‹¬ ë‚´ìš©ì„')\n        \n        return f\"\"\"### Instruction:\në‹¹ì‹ ì€ {role_description} ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\nëŒ€í™”ì˜ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì—¬ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n### Input:\n{dialogue}\n\n### Expert Summary:\"\"\"\n    \n    def _create_structured_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"êµ¬ì¡°í™”ëœ ì¶œë ¥ í”„ë¡¬í”„íŠ¸\"\"\"\n        return f\"\"\"### Instruction:\në‹¤ìŒ í˜•ì‹ì— ë§ì¶° ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n[ì£¼ì œ]: ëŒ€í™”ì˜ ì£¼ìš” ì£¼ì œ\n[ì°¸ì—¬ì]: ëŒ€í™” ì°¸ì—¬ìë“¤ì˜ ì—­í• \n[í•µì‹¬ë‚´ìš©]: ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ 2-3ê°€ì§€\n[ê²°ë¡ /ê²°ê³¼]: ëŒ€í™”ì˜ ê²°ë¡ ì´ë‚˜ í•©ì˜ì‚¬í•­\n\n### Input:\n{dialogue}\n\n### Structured Summary:\"\"\"\n    \n    def _create_topic_specific_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"ì£¼ì œë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸\"\"\"\n        topic_instructions = {\n            'ê±´ê°•ê²€ì§„': 'í™˜ìì˜ ê±´ê°• ìƒíƒœ, ê²€ì§„ í•­ëª©, ì˜ì‚¬ì˜ ê¶Œê³ ì‚¬í•­ì„ ì¤‘ì‹¬ìœ¼ë¡œ',\n            'ë°±ì‹  ì ‘ì¢…': 'ë°±ì‹  ì¢…ë¥˜, ì ‘ì¢… ì¼ì •, ë¶€ì‘ìš© ì„¤ëª…ì„ í¬í•¨í•˜ì—¬',\n            'íšŒì˜': 'íšŒì˜ ì•ˆê±´, ë…¼ì˜ì‚¬í•­, ê²°ì •ì‚¬í•­ì„ ëª…í™•íˆ',\n            'ìƒë‹´': 'ìƒë‹´ ëª©ì , ë¬¸ì œì , í•´ê²°ë°©ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ',\n            'ì˜ˆì•½': 'ì˜ˆì•½ ì¼ì‹œ, ì¥ì†Œ, ëª©ì ì„ ì •í™•íˆ'\n        }\n        \n        instruction = topic_instructions.get(topic, 'í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ')\n        \n        return f\"\"\"### Instruction:\n{instruction} ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.\në¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œì™¸í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.\n\n### Input:\n{dialogue}\n\n### Response:\"\"\"\n    \n    def get_best_prompt(self, dialogue: str, topic: str = None, strategy: str = 'auto') -> str:\n        \"\"\"ìµœì  í”„ë¡¬í”„íŠ¸ ì„ íƒ\"\"\"\n        if strategy == 'auto':\n            # ëŒ€í™” ê¸¸ì´ì— ë”°ë¼ ì „ëµ ì„ íƒ\n            if len(dialogue) < 500:\n                strategy = 'structured'\n            elif len(dialogue) < 1000:\n                strategy = 'few_shot'\n            elif topic:\n                strategy = 'topic_specific'\n            else:\n                strategy = 'chain_of_thought'\n        \n        if strategy in self.prompt_templates:\n            return self.prompt_templates[strategy](dialogue, topic)\n        else:\n            # ê¸°ë³¸ í…œí”Œë¦¿\n            return self.prompt_templates['few_shot'](dialogue, topic)\n\n# Solar API êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 09_Solar_API_ìµœì í™”.md, 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md)\nclass SolarAPIValidator:\n    \"\"\"Solar API êµì°¨ ê²€ì¦ í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://api.upstage.ai/v1/solar\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        self.cache = {}  # API ì‘ë‹µ ìºì‹±\n        \n    def generate_with_solar(self, dialogue: str, max_tokens: int = 150, temperature: float = 0.3) -> Optional[str]:\n        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n        # ìºì‹œ í™•ì¸\n        cache_key = hash(dialogue[:200])\n        if cache_key in self.cache:\n            logger.write(\"Using cached Solar API response\")\n            return self.cache[cache_key]\n        \n        try:\n            # í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° ì ˆì•½)\n            if len(dialogue) > 2000:\n                dialogue = dialogue[:2000] + \"...\"\n            \n            prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:\n\n{dialogue}\n\nìš”ì•½:\"\"\"\n            \n            payload = {\n                \"model\": \"solar-1-mini-chat\",\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ëŒ€í™” ìš”ì•½ AIì…ë‹ˆë‹¤.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n                \"top_p\": 0.9\n            }\n            \n            response = requests.post(\n                f\"{self.base_url}/chat/completions\",\n                headers=self.headers,\n                json=payload,\n                timeout=30\n            )\n            \n            if response.status_code == 200:\n                result = response.json()\n                summary = result['choices'][0]['message']['content']\n                \n                # ìºì‹±\n                self.cache[cache_key] = summary\n                \n                return summary\n            else:\n                logger.write(f\"Solar API error: {response.status_code}\")\n                return None\n                \n        except Exception as e:\n            logger.write(f\"Solar API exception: {e}\")\n            return None\n    \n    def compare_with_model(self, model_summary: str, api_summary: str, reference: str = None) -> Dict:\n        \"\"\"ëª¨ë¸ê³¼ API ìš”ì•½ ë¹„êµ\"\"\"\n        from rouge import Rouge\n        \n        result = {\n            'model_summary': model_summary,\n            'api_summary': api_summary\n        }\n        \n        if reference:\n            rouge = Rouge()\n            try:\n                # ëª¨ë¸ ì ìˆ˜\n                model_scores = rouge.get_scores(model_summary, reference)[0]\n                result['model_rouge_l'] = model_scores['rouge-l']['f']\n                \n                # API ì ìˆ˜\n                if api_summary:\n                    api_scores = rouge.get_scores(api_summary, reference)[0]\n                    result['api_rouge_l'] = api_scores['rouge-l']['f']\n                else:\n                    result['api_rouge_l'] = 0\n                \n                # ìµœì„  ì„ íƒ\n                result['best'] = 'model' if result['model_rouge_l'] >= result.get('api_rouge_l', 0) else 'api'\n                result['improvement'] = abs(result['model_rouge_l'] - result.get('api_rouge_l', 0))\n                \n            except Exception as e:\n                logger.write(f\"Error computing ROUGE: {e}\")\n        \n        return result\n\n# í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì´ˆê¸°í™”\nprompt_engineer = AdvancedPromptEngineering()\n\n# Solar API ì´ˆê¸°í™” (configì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°)\nsolar_validator = None\nif 'solar_api' in config and 'api_key' in config['solar_api']:\n    solar_validator = SolarAPIValidator(config['solar_api']['api_key'])\n    logger.write(\"Solar API validator initialized for cross-validation\")\nelse:\n    logger.write(\"Solar API key not found - skipping API cross-validation\")\n\n# í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸\nsample_dialogue = train_df['dialogue'].iloc[0]\nsample_topic = train_df.get('topic', pd.Series([None] * len(train_df))).iloc[0]\n\n# ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ì „ëµ í…ŒìŠ¤íŠ¸\nlogger.write(\"\\n=== Testing Prompt Engineering Strategies ===\")\nfor strategy in ['few_shot', 'chain_of_thought', 'topic_specific']:\n    test_prompt = prompt_engineer.prompt_templates[strategy](sample_dialogue[:500], sample_topic)\n    logger.write(f\"\\n{strategy.upper()} Strategy (first 300 chars):\")\n    logger.write(test_prompt[:300] + \"...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "Data loaded successfully!\n",
      "Train samples: 12457\n",
      "Dev samples: 499\n",
      "Test samples: 499\n",
      "\n",
      "Sample train data:\n",
      "     fname  topic\n",
      "0  train_0   ê±´ê°•ê²€ì§„\n",
      "1  train_1  ë°±ì‹  ì ‘ì¢…\n",
      "2  train_2  ì—´ì‡  ë¶„ì‹¤\n",
      "\n",
      "Dialogue sample (first 200 chars):\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° ë¡œë“œ\n",
    "# config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"Data loaded successfully!\")\n",
    "logger.write(f\"Train samples: {len(train_df)}\")\n",
    "logger.write(f\"Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nSample train data:\")\n",
    "print(train_df[['fname', 'topic']].head(3))\n",
    "print(f\"\\nDialogue sample (first 200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 16_ë°ì´í„°_í’ˆì§ˆ_ê²€ì¦_ì‹œìŠ¤í…œ.md)\nimport numpy as np\nfrom typing import Dict, List\n\nclass DataQualityValidator:\n    \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤\"\"\"\n    def __init__(self):\n        self.quality_report = {}\n        \n    def validate_llm_data(self, df: pd.DataFrame) -> Dict:\n        \"\"\"LLM í•™ìŠµìš© ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"\n        report = {}\n        \n        # 1. êµ¬ì¡°ì  ê²€ì¦\n        report['structure'] = {\n            'null_values': df.isnull().sum().sum(),\n            'duplicates': df.duplicated().sum(),\n            'empty_dialogues': (df['dialogue'].str.len() == 0).sum(),\n            'empty_summaries': (df['summary'].str.len() == 0).sum() if 'summary' in df.columns else 0\n        }\n        \n        # 2. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì¦ (LLM ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê³ ë ¤)\n        dialogue_lengths = df['dialogue'].str.len()\n        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n        \n        report['text_length'] = {\n            'avg_dialogue_length': dialogue_lengths.mean(),\n            'max_dialogue_length': dialogue_lengths.max(),\n            'dialogues_over_2048': (dialogue_lengths > 2048).sum(),\n            'dialogues_over_4096': (dialogue_lengths > 4096).sum(),\n            'summary_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0\n        }\n        \n        # 3. í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ëŠ” ëŒ€ëµ 3ì = 1í† í°)\n        estimated_tokens = dialogue_lengths / 3\n        report['token_estimation'] = {\n            'avg_tokens': estimated_tokens.mean(),\n            'max_tokens': estimated_tokens.max(),\n            'samples_over_512_tokens': (estimated_tokens > 512).sum(),\n            'samples_over_1024_tokens': (estimated_tokens > 1024).sum()\n        }\n        \n        # 4. íŠ¹ìˆ˜ ë¬¸ì ë° ì¸ì½”ë”© ê²€ì¦\n        report['encoding'] = {\n            'person_tags_present': df['dialogue'].str.contains('#Person').sum(),\n            'encoding_issues': df['dialogue'].str.contains('[ï¿½\\\\?]').sum(),\n            'special_chars': df['dialogue'].str.contains('[^\\w\\s#:.,!?ê°€-í£a-zA-Z0-9]').sum()\n        }\n        \n        # 5. ì£¼ì œ ë¶„í¬ (ë¶ˆê· í˜• ì²´í¬)\n        if 'topic' in df.columns:\n            topic_counts = df['topic'].value_counts()\n            report['topic_distribution'] = {\n                'unique_topics': len(topic_counts),\n                'most_common_topic': topic_counts.index[0] if len(topic_counts) > 0 else None,\n                'most_common_count': topic_counts.iloc[0] if len(topic_counts) > 0 else 0,\n                'least_common_topic': topic_counts.index[-1] if len(topic_counts) > 0 else None,\n                'least_common_count': topic_counts.iloc[-1] if len(topic_counts) > 0 else 0,\n                'imbalance_ratio': topic_counts.iloc[0] / topic_counts.iloc[-1] if len(topic_counts) > 1 and topic_counts.iloc[-1] > 0 else 0\n            }\n        \n        self.quality_report = report\n        return report\n    \n    def recommend_preprocessing(self) -> List[str]:\n        \"\"\"ì „ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"\n        recommendations = []\n        \n        if self.quality_report.get('structure', {}).get('null_values', 0) > 0:\n            recommendations.append(\"Remove or impute null values\")\n        \n        if self.quality_report.get('structure', {}).get('duplicates', 0) > 0:\n            recommendations.append(\"Remove duplicate samples\")\n        \n        if self.quality_report.get('text_length', {}).get('dialogues_over_4096', 0) > 0:\n            recommendations.append(\"Truncate or split long dialogues (>4096 chars)\")\n        \n        if self.quality_report.get('encoding', {}).get('encoding_issues', 0) > 0:\n            recommendations.append(\"Fix encoding issues in text\")\n        \n        if self.quality_report.get('topic_distribution', {}).get('imbalance_ratio', 0) > 10:\n            recommendations.append(\"Consider data augmentation for underrepresented topics\")\n        \n        return recommendations\n\n# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰\nvalidator = DataQualityValidator()\nquality_report = validator.validate_llm_data(train_df)\n\nlogger.write(\"\\n=== Data Quality Validation Report ===\")\nfor category, metrics in quality_report.items():\n    logger.write(f\"\\n{category.upper()}:\")\n    for key, value in metrics.items():\n        if isinstance(value, float):\n            logger.write(f\"  - {key}: {value:.2f}\")\n        else:\n            logger.write(f\"  - {key}: {value}\")\n\n# ì „ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­\nrecommendations = validator.recommend_preprocessing()\nif recommendations:\n    logger.write(\"\\nğŸ“‹ Preprocessing Recommendations:\")\n    for rec in recommendations:\n        logger.write(f\"  â€¢ {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì¶”ë¡  ìµœì í™” ì‹œìŠ¤í…œ (PRD 17_ì¶”ë¡ _ìµœì í™”.md)\nimport gc\nfrom typing import List\n\nclass LLMInferenceOptimizer:\n    \"\"\"LLM ì¶”ë¡  ìµœì í™” í´ë˜ìŠ¤\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.cache = {}\n        \n        # ìµœì í™” ê¸°ë²• ì ìš©\n        self._apply_optimizations()\n    \n    def _apply_optimizations(self):\n        \"\"\"ëª¨ë¸ ìµœì í™” ê¸°ë²• ì ìš©\"\"\"\n        # 1. Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)\n        if hasattr(self.model, 'gradient_checkpointing_enable'):\n            self.model.gradient_checkpointing_enable()\n            logger.write(\"Gradient checkpointing enabled\")\n        \n        # 2. Flash Attention (ê°€ëŠ¥í•œ ê²½ìš°)\n        try:\n            if hasattr(self.model.config, 'use_flash_attention'):\n                self.model.config.use_flash_attention = True\n                logger.write(\"Flash attention enabled\")\n        except:\n            pass\n        \n        # 3. KV ìºì‹œ í™œì„±í™”\n        if hasattr(self.model.config, 'use_cache'):\n            self.model.config.use_cache = True\n            logger.write(\"KV cache enabled\")\n    \n    def batch_generate(self, texts: List[str], batch_size: int = 8, **generation_kwargs) -> List[str]:\n        \"\"\"ë°°ì¹˜ ìƒì„± ìµœì í™”\"\"\"\n        all_outputs = []\n        \n        # ê¸¸ì´ë³„ë¡œ ì •ë ¬ (ë™ì  ë°°ì¹­)\n        sorted_texts = sorted(enumerate(texts), key=lambda x: len(x[1]))\n        \n        for i in range(0, len(sorted_texts), batch_size):\n            batch = sorted_texts[i:i+batch_size]\n            batch_indices = [idx for idx, _ in batch]\n            batch_texts = [text for _, text in batch]\n            \n            # í† í°í™”\n            inputs = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=generation_kwargs.get('max_length', 1024),\n                return_tensors='pt'\n            ).to(self.device)\n            \n            # ìƒì„±\n            with torch.no_grad():\n                with torch.cuda.amp.autocast():  # Mixed precision\n                    outputs = self.model.generate(\n                        **inputs,\n                        **generation_kwargs\n                    )\n            \n            # ë””ì½”ë”©\n            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            \n            # ì›ë˜ ìˆœì„œë¡œ ì €ì¥\n            for idx, output in zip(batch_indices, decoded):\n                all_outputs.append((idx, output))\n        \n        # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬\n        all_outputs.sort(key=lambda x: x[0])\n        return [output for _, output in all_outputs]\n    \n    def cached_generate(self, text: str, **kwargs) -> str:\n        \"\"\"ìºì‹±ì„ í™œìš©í•œ ìƒì„±\"\"\"\n        # ìºì‹œ í‚¤ ìƒì„±\n        cache_key = hash(text[:200])\n        \n        if cache_key in self.cache:\n            logger.write(\"Cache hit - returning cached result\")\n            return self.cache[cache_key]\n        \n        # ìºì‹œ ë¯¸ìŠ¤ - ìƒˆë¡œ ìƒì„±\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=kwargs.get('max_length', 1024),\n            return_tensors='pt'\n        ).to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, **kwargs)\n        \n        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # ìºì‹œ ì €ì¥ (ìµœëŒ€ 100ê°œ)\n        if len(self.cache) < 100:\n            self.cache[cache_key] = result\n        \n        return result\n\n# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (PRD 05_ë¦¬ìŠ¤í¬_ê´€ë¦¬.md)\nclass LLMRiskManager:\n    \"\"\"LLM í•™ìŠµ ë¦¬ìŠ¤í¬ ê´€ë¦¬\"\"\"\n    \n    def __init__(self):\n        self.risk_log = []\n        self.mitigation_applied = []\n    \n    def check_training_risks(self, epoch: int, train_loss: float, val_loss: float, \n                           rouge_score: float, memory_used: float) -> Dict:\n        \"\"\"í•™ìŠµ ë¦¬ìŠ¤í¬ ì²´í¬\"\"\"\n        risks = []\n        \n        # 1. ê³¼ì í•© ë¦¬ìŠ¤í¬\n        if val_loss > train_loss * 1.5 and epoch > 2:\n            risks.append({\n                'type': 'overfitting',\n                'severity': 'high',\n                'metric': f'val/train ratio: {val_loss/train_loss:.2f}',\n                'mitigation': ['Increase dropout', 'Early stopping', 'Reduce learning rate']\n            })\n        \n        # 2. ê³¼ì†Œì í•© ë¦¬ìŠ¤í¬\n        if epoch > 3 and train_loss > 2.0:\n            risks.append({\n                'type': 'underfitting',\n                'severity': 'medium',\n                'metric': f'train_loss: {train_loss:.4f}',\n                'mitigation': ['Increase model capacity', 'Adjust learning rate', 'Check data quality']\n            })\n        \n        # 3. ì„±ëŠ¥ ì •ì²´ ë¦¬ìŠ¤í¬\n        if epoch > 5 and rouge_score < 0.3:\n            risks.append({\n                'type': 'performance_plateau',\n                'severity': 'high',\n                'metric': f'ROUGE-L: {rouge_score:.4f}',\n                'mitigation': ['Change prompt strategy', 'Adjust hyperparameters', 'Use different model']\n            })\n        \n        # 4. ë©”ëª¨ë¦¬ ë¦¬ìŠ¤í¬\n        if memory_used > 0.9:\n            risks.append({\n                'type': 'memory_overflow',\n                'severity': 'critical',\n                'metric': f'Memory usage: {memory_used:.1%}',\n                'mitigation': ['Reduce batch size', 'Enable gradient accumulation', 'Use smaller model']\n            })\n        \n        # 5. í•™ìŠµ ë¶ˆì•ˆì • ë¦¬ìŠ¤í¬\n        if train_loss > 10 or np.isnan(train_loss):\n            risks.append({\n                'type': 'training_instability',\n                'severity': 'critical',\n                'metric': f'Unstable loss: {train_loss}',\n                'mitigation': ['Reduce learning rate', 'Check for NaN values', 'Reset training']\n            })\n        \n        # ë¦¬ìŠ¤í¬ ë¡œê·¸ ì €ì¥\n        if risks:\n            self.risk_log.extend(risks)\n            for risk in risks:\n                logger.write(f\"âš ï¸ Risk: {risk['type']} ({risk['severity']}) - {risk['metric']}\")\n                logger.write(f\"  Suggested mitigations: {', '.join(risk['mitigation'])}\")\n        \n        return {'risks': risks, 'total_risks': len(risks)}\n    \n    def apply_automatic_mitigation(self, risk_type: str, config: Dict) -> Dict:\n        \"\"\"ìë™ ë¦¬ìŠ¤í¬ ì™„í™”\"\"\"\n        mitigations = {\n            'overfitting': {\n                'action': 'reduce_learning_rate',\n                'new_lr': config['training']['learning_rate'] * 0.5\n            },\n            'memory_overflow': {\n                'action': 'reduce_batch_size',\n                'new_batch_size': max(1, config['training']['batch_size'] // 2)\n            },\n            'training_instability': {\n                'action': 'reset_optimizer',\n                'new_lr': config['training']['learning_rate'] * 0.1\n            }\n        }\n        \n        if risk_type in mitigations:\n            mitigation = mitigations[risk_type]\n            self.mitigation_applied.append(mitigation)\n            logger.write(f\"âœ“ Applied automatic mitigation: {mitigation['action']}\")\n            return mitigation\n        \n        return {}\n\n# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (PRD 13_Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°_ìµœì í™”.md)\ntry:\n    import optuna\n    from optuna import Trial\n    from optuna.samplers import TPESampler\n    \n    class LLMOptunaOptimizer:\n        \"\"\"LLMìš© Optuna ìµœì í™”\"\"\"\n        \n        def __init__(self, model_name: str, tokenizer, train_dataset, val_dataset):\n            self.model_name = model_name\n            self.tokenizer = tokenizer\n            self.train_dataset = train_dataset\n            self.val_dataset = val_dataset\n            \n        def objective(self, trial: Trial) -> float:\n            \"\"\"Optuna ëª©ì  í•¨ìˆ˜\"\"\"\n            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ\n            hp = {\n                'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True),\n                'batch_size': trial.suggest_categorical('batch_size', [1, 2, 4, 8]),\n                'lora_r': trial.suggest_int('lora_r', 4, 32),\n                'lora_alpha': trial.suggest_int('lora_alpha', 8, 64),\n                'warmup_ratio': trial.suggest_float('warmup_ratio', 0.0, 0.2),\n                'weight_decay': trial.suggest_float('weight_decay', 0.0, 0.1),\n                'temperature': trial.suggest_float('temperature', 0.1, 1.0),\n                'top_p': trial.suggest_float('top_p', 0.8, 1.0),\n                'num_beams': trial.suggest_int('num_beams', 1, 5)\n            }\n            \n            logger.write(f\"\\nTrial {trial.number}: {hp}\")\n            \n            # ê°„ë‹¨í•œ í‰ê°€ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ í•™ìŠµ í•„ìš”)\n            # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ì ìˆ˜ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì‹œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€)\n            dummy_score = np.random.random() * 0.5 + 0.3  # 0.3~0.8 ë²”ìœ„\n            \n            return dummy_score\n        \n        def optimize(self, n_trials: int = 10, timeout: int = 3600):\n            \"\"\"ìµœì í™” ì‹¤í–‰\"\"\"\n            study = optuna.create_study(\n                direction='maximize',\n                sampler=TPESampler(seed=42),\n                study_name=f'llm_optimization_{self.model_name}'\n            )\n            \n            study.optimize(self.objective, n_trials=n_trials, timeout=timeout)\n            \n            # ìµœì  íŒŒë¼ë¯¸í„°\n            best_params = study.best_params\n            best_value = study.best_value\n            \n            logger.write(f\"\\n=== Optuna Optimization Results ===\")\n            logger.write(f\"Best ROUGE-L: {best_value:.4f}\")\n            logger.write(f\"Best parameters: {best_params}\")\n            \n            return best_params, best_value\n    \n    # Optuna ìµœì í™” ì‹¤í–‰ (configì—ì„œ í™œì„±í™”ëœ ê²½ìš°)\n    if config.get('optuna', {}).get('enabled', False):\n        logger.write(\"\\n=== Starting Optuna Hyperparameter Optimization ===\")\n        # ë‚˜ì¤‘ì— ë°ì´í„°ì…‹ ìƒì„± í›„ ì‹¤í–‰\n        optuna_optimizer = LLMOptunaOptimizer(\n            model_name=current_model,\n            tokenizer=None,  # ë‚˜ì¤‘ì— ì„¤ì •\n            train_dataset=None,  # ë‚˜ì¤‘ì— ì„¤ì •\n            val_dataset=None  # ë‚˜ì¤‘ì— ì„¤ì •\n        )\n    else:\n        optuna_optimizer = None\n        logger.write(\"Optuna optimization disabled\")\n        \nexcept ImportError:\n    logger.write(\"Optuna not installed - skipping hyperparameter optimization\")\n    optuna_optimizer = None\n\n# ë¦¬ìŠ¤í¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”\nrisk_manager = LLMRiskManager()\nlogger.write(\"Risk management system initialized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Dict, List\nimport re\n\nclass LLMDialogueSummaryDataset(Dataset):\n    \"\"\"LLM íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, prompt_template, max_length=1024, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.prompt_template = prompt_template\n        self.max_length = max_length\n        self.is_test = is_test\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"ëŒ€í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n        # ë…¸ì´ì¦ˆ ì œê±°\n        text = text.replace('\\\\n', '\\n')\n        text = text.replace('<br>', '\\n')\n        text = text.strip()\n        \n        # #Person íƒœê·¸ í•œêµ­ì–´ë¡œ ë³€ê²½\n        text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n        \n        return text\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # ëŒ€í™” ì „ì²˜ë¦¬\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        if not self.is_test:\n            # í•™ìŠµ ëª¨ë“œ: í”„ë¡¬í”„íŠ¸ì™€ ìš”ì•½ì„ í•¨ê»˜ í† í°í™”\n            summary = row.get('summary', '')\n            \n            # í”„ë¡¬í”„íŠ¸ ìƒì„±\n            full_prompt = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=summary\n            )\n            \n            # í† í°í™”\n            encoded = self.tokenizer(\n                full_prompt,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # ë¼ë²¨ ì„¤ì • (ìš”ì•½ ë¶€ë¶„ë§Œ loss ê³„ì‚°)\n            labels = encoded['input_ids'].clone()\n            \n            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n            prompt_without_summary = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=\"\"\n            )\n            prompt_tokens = self.tokenizer(\n                prompt_without_summary,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )['input_ids']\n            \n            if prompt_tokens.shape[1] < labels.shape[1]:\n                labels[0, :prompt_tokens.shape[1]] = -100\n            \n            # íŒ¨ë”© í† í°ë„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ì¤‘ìš”!)\n            labels[labels == self.tokenizer.pad_token_id] = -100\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'labels': labels.squeeze()\n            }\n        else:\n            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: í”„ë¡¬í”„íŠ¸ë§Œ í† í°í™”\n            prompt = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=\"\"\n            )\n            \n            encoded = self.tokenizer(\n                prompt,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ\nprompt_template = config['prompt_template']['instruction_format']\n\nlogger.write(\"Creating datasets...\")\n\n# ë°ì´í„°ì…‹ ìƒì„±ì€ ë‚˜ì¤‘ì— í† í¬ë‚˜ì´ì € ë¡œë“œ í›„ì— ì§„í–‰\nlogger.write(\"Dataset class defined successfully\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)\nlogger.write(f\"\\nLoading model: {model_config['name']}\")\nlogger.write(\"This may take some time due to model size...\")\n\n# 8bit/4bit ì–‘ìí™” ì„¤ì •\nload_kwargs = {}\nif model_config.get('load_in_8bit', False):\n    load_kwargs['load_in_8bit'] = True\n    load_kwargs['device_map'] = 'auto'\n    logger.write(\"Using 8-bit quantization\")\nelif model_config.get('load_in_4bit', False):\n    from transformers import BitsAndBytesConfig\n    load_kwargs['quantization_config'] = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    load_kwargs['device_map'] = 'auto'\n    logger.write(\"Using 4-bit quantization\")\n\n# í† í¬ë‚˜ì´ì € ë¡œë“œ\ntokenizer = AutoTokenizer.from_pretrained(\n    model_config['name'],\n    padding_side='left'  # ìƒì„± ëª¨ë¸ìš©\n)\n\n# íŒ¨ë”© í† í° ì„¤ì •\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    logger.write(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n\n# ëª¨ë¸ ë¡œë“œ\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.float16,\n        **load_kwargs\n    )\n    logger.write(f\"Model loaded successfully\")\n    \n    # LoRA ì ìš©\n    if model_config['use_lora']:\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        logger.write(\"LoRA applied to model\")\n    \nexcept Exception as e:\n    logger.write(f\"Error loading model: {e}\")\n    logger.write(\"Falling back to smaller model or mock mode\")\n    \n    # Fallback: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©\n    fallback_model = \"skt/kogpt2-base-v2\"\n    logger.write(f\"Loading fallback model: {fallback_model}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        fallback_model,\n        torch_dtype=torch.float16\n    ).to(device)\n    \n    if model_config['use_lora']:\n        # Fallback ëª¨ë¸ìš© LoRA config ìˆ˜ì •\n        from peft import LoraConfig, get_peft_model, TaskType\n        \n        fallback_lora_config = LoraConfig(\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.1,\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=[\"c_attn\", \"c_proj\"]  # GPT2ìš© ëª¨ë“ˆ\n        )\n        model = get_peft_model(model, fallback_lora_config)\n        model.print_trainable_parameters()\n\nlogger.write(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\nlogger.write(\"\\nCreating datasets and dataloaders...\")\n\n# í•™ìŠµìš© ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)\nif config['training'].get('use_sample', False):\n    sample_size = config['training'].get('sample_size', 1000)\n    train_df_sample = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n    logger.write(f\"Using sample of {len(train_df_sample)} training examples\")\nelse:\n    train_df_sample = train_df\n\n# ë°ì´í„°ì…‹ ìƒì„±\ntrain_dataset = LLMDialogueSummaryDataset(\n    train_df_sample,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=False\n)\n\nval_dataset = LLMDialogueSummaryDataset(\n    dev_df,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=False\n)\n\ntest_dataset = LLMDialogueSummaryDataset(\n    test_df,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=True\n)\n\n# DataLoader ìƒì„±\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['training']['batch_size'],\n    shuffle=True,\n    num_workers=0,  # ì–‘ìí™” ëª¨ë¸ì€ ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ë°©ì§€\n    pin_memory=False\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['evaluation']['batch_size'],\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=config['inference']['batch_size'],\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\nlogger.write(f\"DataLoaders created:\")\nlogger.write(f\"  - Train batches: {len(train_loader)}\")\nlogger.write(f\"  - Val batches: {len(val_loader)}\")\nlogger.write(f\"  - Test batches: {len(test_loader)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# í•™ìŠµ ë£¨í”„ (ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§ ë° Solar API êµì°¨ ê²€ì¦ í¬í•¨)\nfor epoch in range(num_epochs):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"Epoch {epoch + 1}/{num_epochs}\")\n    logger.write(f\"{'='*30}\")\n    \n    # í•™ìŠµ\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n    logger.write(f\"Average training loss: {train_loss:.4f}\")\n    training_history['train_loss'].append(train_loss)\n    \n    # ê²€ì¦ (ì ì€ ìƒ˜í”Œë¡œ)\n    val_loss, rouge_scores, sample_preds = evaluate(\n        model, \n        val_loader, \n        tokenizer,\n        max_samples=config['evaluation'].get('max_samples', 50)\n    )\n    \n    logger.write(f\"\\nValidation Results:\")\n    logger.write(f\"  - Loss: {val_loss:.4f}\")\n    logger.write(f\"  - ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}\")\n    logger.write(f\"  - ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}\")\n    logger.write(f\"  - ROUGE-L F1: {rouge_scores['rouge-l']:.4f}\")\n    \n    # ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§\n    if torch.cuda.is_available():\n        memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n    else:\n        memory_used = 0\n    \n    risk_status = risk_manager.check_training_risks(\n        epoch=epoch,\n        train_loss=train_loss,\n        val_loss=val_loss,\n        rouge_score=rouge_scores['rouge-l'],\n        memory_used=memory_used\n    )\n    \n    # ë¦¬ìŠ¤í¬ ì™„í™” ì ìš©\n    if risk_status['total_risks'] > 0:\n        logger.write(f\"\\nâš ï¸ {risk_status['total_risks']} risks detected\")\n        for risk in risk_status['risks']:\n            if risk['severity'] == 'critical':\n                mitigation = risk_manager.apply_automatic_mitigation(risk['type'], config)\n                \n                # í•™ìŠµë¥  ì¡°ì •\n                if 'new_lr' in mitigation:\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = mitigation['new_lr']\n                    logger.write(f\"  â†’ Learning rate reduced to {mitigation['new_lr']}\")\n    \n    # Solar API êµì°¨ ê²€ì¦ (ë§¤ 2 ì—í­ë§ˆë‹¤)\n    if solar_validator and epoch % 2 == 0:\n        logger.write(\"\\nğŸ”„ Solar API Cross-Validation...\")\n        \n        # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n        sample_idx = np.random.randint(0, len(dev_df))\n        sample = dev_df.iloc[sample_idx]\n        \n        # ëª¨ë¸ ì˜ˆì¸¡ ìƒì„±\n        model_prompt = prompt_engineer.get_best_prompt(\n            sample['dialogue'], \n            sample.get('topic', None)\n        )\n        \n        model_inputs = tokenizer(\n            model_prompt,\n            max_length=1024,\n            truncation=True,\n            return_tensors='pt'\n        ).to(device)\n        \n        with torch.no_grad():\n            model_output = model.generate(\n                **model_inputs,\n                max_new_tokens=150,\n                temperature=0.3,\n                top_p=0.9\n            )\n        \n        model_summary = tokenizer.decode(model_output[0], skip_special_tokens=True)\n        \n        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°\n        if \"### Response:\" in model_summary:\n            model_summary = model_summary.split(\"### Response:\")[-1].strip()\n        \n        # Solar API ì˜ˆì¸¡\n        api_summary = solar_validator.generate_with_solar(sample['dialogue'])\n        \n        # ë¹„êµ\n        comparison = solar_validator.compare_with_model(\n            model_summary=model_summary,\n            api_summary=api_summary,\n            reference=sample.get('summary', None)\n        )\n        \n        if comparison.get('model_rouge_l') and comparison.get('api_rouge_l'):\n            logger.write(f\"  Model ROUGE-L: {comparison['model_rouge_l']:.4f}\")\n            logger.write(f\"  Solar ROUGE-L: {comparison['api_rouge_l']:.4f}\")\n            logger.write(f\"  Best: {comparison['best']}\")\n            \n            # WandB ë¡œê¹…\n            if config['wandb']['mode'] != 'disabled':\n                wandb.log({\n                    'model_vs_api/model_rouge': comparison['model_rouge_l'],\n                    'model_vs_api/api_rouge': comparison.get('api_rouge_l', 0),\n                    'model_vs_api/best': 1 if comparison['best'] == 'model' else 0\n                })\n    \n    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n    training_history['val_loss'].append(val_loss)\n    training_history['rouge_1'].append(rouge_scores['rouge-1'])\n    training_history['rouge_2'].append(rouge_scores['rouge-2'])\n    training_history['rouge_l'].append(rouge_scores['rouge-l'])\n    \n    # WandB ë¡œê¹…\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss_epoch': train_loss,\n            'val_loss': val_loss,\n            'rouge_1': rouge_scores['rouge-1'],\n            'rouge_2': rouge_scores['rouge-2'],\n            'rouge_l': rouge_scores['rouge-l'],\n            'risks_detected': risk_status['total_risks'],\n            'memory_usage': memory_used\n        })\n    \n    # ìƒ˜í”Œ ì˜ˆì¸¡ ì¶œë ¥\n    if sample_preds:\n        logger.write(\"\\nSample Predictions:\")\n        for i, pred in enumerate(sample_preds[:2]):\n            logger.write(f\"  Sample {i+1}: {pred[:150]}...\")\n    \n    # Best model ì €ì¥\n    if rouge_scores['rouge-l'] > best_rouge_l:\n        best_rouge_l = rouge_scores['rouge-l']\n        patience_counter = 0\n        \n        # ëª¨ë¸ ì €ì¥\n        if hasattr(model, 'module'):\n            model_to_save = model.module\n        else:\n            model_to_save = model\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'rouge_scores': rouge_scores,\n            'config': config,\n            'model_config': model_config,\n            'risk_report': risk_manager.risk_log,  # ë¦¬ìŠ¤í¬ ë³´ê³ ì„œ ì¶”ê°€\n            'prompt_strategy': 'advanced_prompting'  # í”„ë¡¬í”„íŠ¸ ì „ëµ ê¸°ë¡\n        }, best_model_path)\n        \n        logger.write(f\"âœ“ New best model saved! (ROUGE-L: {best_rouge_l:.4f})\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            logger.write(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# ìµœì¢… ë¦¬ìŠ¤í¬ ë³´ê³ ì„œ\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"RISK MANAGEMENT REPORT\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Total risks encountered: {len(risk_manager.risk_log)}\")\nlogger.write(f\"Mitigations applied: {len(risk_manager.mitigation_applied)}\")\n\nif risk_manager.risk_log:\n    # ë¦¬ìŠ¤í¬ íƒ€ì…ë³„ ì§‘ê³„\n    risk_types = {}\n    for risk in risk_manager.risk_log:\n        risk_type = risk['type']\n        if risk_type not in risk_types:\n            risk_types[risk_type] = 0\n        risk_types[risk_type] += 1\n    \n    logger.write(\"\\nRisk Summary:\")\n    for risk_type, count in risk_types.items():\n        logger.write(f\"  - {risk_type}: {count} occurrences\")\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Training completed!\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# WandB ì´ˆê¸°í™”\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=f\"{config['wandb']['name']}_{current_model}\",\n        tags=config['wandb']['tags'] + [current_model],\n        config={\n            **config,\n            'model': model_config\n        }\n    )\n    logger.write(\"WandB initialized\")\n\n# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\nnum_epochs = config['training']['num_epochs']\n\n# learning_rateê°€ ë¬¸ìì—´ì¸ ê²½ìš° floatë¡œ ë³€í™˜\nlearning_rate = config['training']['learning_rate']\nif isinstance(learning_rate, str):\n    learning_rate = float(learning_rate)\n\nnum_training_steps = num_epochs * len(train_loader)\n\n# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\noptimizer = AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=config['training']['weight_decay']\n)\n\n# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n    num_training_steps=num_training_steps\n)\n\nlogger.write(f\"\\nTraining Configuration:\")\nlogger.write(f\"  - Epochs: {num_epochs}\")\nlogger.write(f\"  - Learning rate: {learning_rate}\")\nlogger.write(f\"  - Batch size: {config['training']['batch_size']}\")\nlogger.write(f\"  - Total training steps: {num_training_steps}\")\nlogger.write(f\"  - Warmup steps: {int(num_training_steps * config['training']['warmup_ratio'])}\")\n\n# í•™ìŠµ ê¸°ë¡\ntraining_history = {\n    'train_loss': [],\n    'val_loss': [],\n    'rouge_1': [],\n    'rouge_2': [],\n    'rouge_l': []\n}\n\n# ëª¨ë¸ ì €ì¥ ê²½ë¡œ\ndef get_path(path_str):\n    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n    path = Path(path_str)\n    if not path.is_absolute():\n        path = notebook_dir / path\n    return path\n\noutput_dir = get_path(config['paths']['output_dir'])\noutput_dir.mkdir(parents=True, exist_ok=True)\nbest_model_path = output_dir / f'best_model_{current_model}.pt'\n\n# Early stopping ì„¤ì •\nbest_rouge_l = 0\npatience = config['training']['early_stopping_patience']\npatience_counter = 0\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Starting training...\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ì¶”ë¡  ìµœì í™” ì ìš©)\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"TEST PREDICTION WITH OPTIMIZATION\")\nlogger.write(\"=\"*50)\n\n# ìµœì  ëª¨ë¸ ë¡œë“œ\nif best_model_path.exists():\n    checkpoint = torch.load(best_model_path)\n    \n    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ\n    if hasattr(model, 'module'):\n        model.module.load_state_dict(checkpoint['model_state_dict'])\n    else:\n        model.load_state_dict(checkpoint['model_state_dict'])\n    \n    logger.write(f\"Best model loaded from epoch {checkpoint['epoch'] + 1}\")\n    logger.write(f\"Best ROUGE scores: {checkpoint['rouge_scores']}\")\n\n# ì¶”ë¡  ìµœì í™” ì´ˆê¸°í™”\ninference_optimizer = LLMInferenceOptimizer(model, tokenizer, device)\n\n# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ìƒì„± (ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬)\ndef generate_test_predictions(model, test_df, tokenizer, prompt_engineer, inference_optimizer):\n    \"\"\"ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ìƒì„±\"\"\"\n    test_predictions = []\n    \n    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n    test_prompts = []\n    for idx, row in test_df.iterrows():\n        dialogue = row['dialogue']\n        topic = row.get('topic', None)\n        \n        # ìµœì  í”„ë¡¬í”„íŠ¸ ì„ íƒ\n        prompt = prompt_engineer.get_best_prompt(dialogue, topic, strategy='auto')\n        test_prompts.append(prompt)\n    \n    # ë°°ì¹˜ ìƒì„± (ìµœì í™”)\n    generation_config = config['inference']['generation_config']\n    \n    logger.write(f\"Generating predictions for {len(test_prompts)} test samples...\")\n    logger.write(\"Using optimized batch inference...\")\n    \n    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡\n    batch_size = config['inference'].get('batch_size', 4)\n    \n    for i in tqdm(range(0, len(test_prompts), batch_size), desc=\"Test predictions\"):\n        batch_prompts = test_prompts[i:i+batch_size]\n        \n        # ë°°ì¹˜ ìƒì„±\n        batch_predictions = inference_optimizer.batch_generate(\n            batch_prompts,\n            batch_size=batch_size,\n            max_new_tokens=generation_config['max_new_tokens'],\n            temperature=generation_config['temperature'],\n            top_p=generation_config['top_p'],\n            num_beams=generation_config['num_beams'],\n            no_repeat_ngram_size=generation_config['no_repeat_ngram_size'],\n            do_sample=generation_config['do_sample']\n        )\n        \n        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°\n        for pred in batch_predictions:\n            if \"### Response:\" in pred:\n                pred = pred.split(\"### Response:\")[-1].strip()\n            elif \"ìš”ì•½:\" in pred:\n                pred = pred.split(\"ìš”ì•½:\")[-1].strip()\n            \n            test_predictions.append(pred)\n    \n    return test_predictions\n\n# ì˜ˆì¸¡ ìˆ˜í–‰\ntest_predictions = generate_test_predictions(\n    model, test_df, tokenizer, prompt_engineer, inference_optimizer\n)\n\nlogger.write(f\"\\nâœ“ Generated {len(test_predictions)} test predictions\")\n\n# Solar APIì™€ ë¹„êµ (ìƒ˜í”Œ)\nif solar_validator and len(test_predictions) > 0:\n    logger.write(\"\\n=== Solar API Comparison (Test Samples) ===\")\n    \n    # ëœë¤ 3ê°œ ìƒ˜í”Œ ë¹„êµ\n    for i in range(min(3, len(test_df))):\n        sample_idx = np.random.randint(0, len(test_df))\n        \n        model_pred = test_predictions[sample_idx]\n        api_pred = solar_validator.generate_with_solar(test_df.iloc[sample_idx]['dialogue'])\n        \n        logger.write(f\"\\nSample {i+1}:\")\n        logger.write(f\"  Model: {model_pred[:100]}...\")\n        if api_pred:\n            logger.write(f\"  Solar: {api_pred[:100]}...\")\n\n# ìƒ˜í”Œ ì¶œë ¥\nlogger.write(\"\\n=== Sample Test Predictions ===\")\nfor i in range(min(5, len(test_predictions))):\n    logger.write(f\"\\nTest {i+1}: {test_predictions[i][:150]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# í•™ìŠµ ë£¨í”„\nfor epoch in range(num_epochs):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"Epoch {epoch + 1}/{num_epochs}\")\n    logger.write(f\"{'='*30}\")\n    \n    # í•™ìŠµ\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n    logger.write(f\"Average training loss: {train_loss:.4f}\")\n    training_history['train_loss'].append(train_loss)\n    \n    # ê²€ì¦ (ì ì€ ìƒ˜í”Œë¡œ)\n    val_loss, rouge_scores, sample_preds = evaluate(\n        model, \n        val_loader, \n        tokenizer,\n        max_samples=config['evaluation'].get('max_samples', 50)\n    )\n    \n    logger.write(f\"\\nValidation Results:\")\n    logger.write(f\"  - Loss: {val_loss:.4f}\")\n    logger.write(f\"  - ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}\")\n    logger.write(f\"  - ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}\")\n    logger.write(f\"  - ROUGE-L F1: {rouge_scores['rouge-l']:.4f}\")\n    \n    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n    training_history['val_loss'].append(val_loss)\n    training_history['rouge_1'].append(rouge_scores['rouge-1'])\n    training_history['rouge_2'].append(rouge_scores['rouge-2'])\n    training_history['rouge_l'].append(rouge_scores['rouge-l'])\n    \n    # WandB ë¡œê¹…\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss_epoch': train_loss,\n            'val_loss': val_loss,\n            'rouge_1': rouge_scores['rouge-1'],\n            'rouge_2': rouge_scores['rouge-2'],\n            'rouge_l': rouge_scores['rouge-l']\n        })\n    \n    # ìƒ˜í”Œ ì˜ˆì¸¡ ì¶œë ¥\n    if sample_preds:\n        logger.write(\"\\nSample Predictions:\")\n        for i, pred in enumerate(sample_preds[:2]):\n            logger.write(f\"  Sample {i+1}: {pred[:150]}...\")\n    \n    # Best model ì €ì¥\n    if rouge_scores['rouge-l'] > best_rouge_l:\n        best_rouge_l = rouge_scores['rouge-l']\n        patience_counter = 0\n        \n        # ëª¨ë¸ ì €ì¥\n        if hasattr(model, 'module'):\n            model_to_save = model.module\n        else:\n            model_to_save = model\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'rouge_scores': rouge_scores,\n            'config': config,\n            'model_config': model_config\n        }, best_model_path)\n        \n        logger.write(f\"âœ“ New best model saved! (ROUGE-L: {best_rouge_l:.4f})\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            logger.write(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    torch.cuda.empty_cache()\n    gc.collect()\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Training completed!\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì œì¶œ íŒŒì¼ ìƒì„±\nsubmission_df = pd.DataFrame({\n    'fname': test_df['fname'],\n    'summary': test_predictions\n})\n\n# ì œì¶œ íŒŒì¼ ì €ì¥ - configì˜ ê²½ë¡œ ì‚¬ìš©\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'{current_model}_submission_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\n# index=Trueë¡œ ì„¤ì •í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ í¬í•¨ì‹œí‚´\nsubmission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=False -> index=Trueë¡œ ë³€ê²½\nlogger.write(f\"\\nSubmission file saved: {submission_path}\")\n\n# ì œì¶œ íŒŒì¼ í™•ì¸\nprint(f\"\\nSubmission file created: {submission_filename}\")\nprint(f\"Shape: {submission_df.shape}\")\nprint(\"\\nFirst 3 submissions:\")\nprint(submission_df.head(3))\n\n# ì‹¤í—˜ ìš”ì•½\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"SINGLE MODEL EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Model: {current_model} ({model_config['name']})\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(f\"Training epochs completed: {len(training_history['train_loss'])}\")\nif training_history['train_loss']:\n    logger.write(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\nif training_history['val_loss']:\n    logger.write(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\nlogger.write(f\"Submission file: {submission_filename}\")\nlogger.write(\"=\"*50)\n\n# ì‹œê°í™”\nif config.get('visualization', {}).get('enabled', False):\n    from src.utils.visualizations.training_viz import TrainingVisualizer\n    \n    viz = TrainingVisualizer()\n    \n    # ì‹œê°í™” ì €ì¥ ê²½ë¡œ\n    viz_dir = get_path(config.get('visualization', {}).get('save_path', 'visualizations'))\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    \n    # í•™ìŠµ íˆìŠ¤í† ë¦¬ í”Œë¡¯\n    if len(training_history['train_loss']) > 0:\n        viz.plot_training_history(\n            training_history,\n            save_path=viz_dir / f'{current_model}_training_history.png'\n        )\n        logger.write(f\"Visualization saved to {viz_dir}\")\n\n# WandB ì¢…ë£Œ\nif config['wandb']['mode'] != 'disabled':\n    wandb.finish()\n\nlogger.write(f\"\\nâœ… Single model experiment ({current_model}) completed successfully!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì œì¶œ íŒŒì¼ ìƒì„±\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'summary': test_predictions\n})\n\n# ì œì¶œ íŒŒì¼ ì €ì¥ - configì˜ ê²½ë¡œ ì‚¬ìš©\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'{current_model}_submission_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\nsubmission_df.to_csv(submission_path, index=False, encoding='utf-8')\nlogger.write(f\"\\nSubmission file saved: {submission_path}\")\n\n# ì œì¶œ íŒŒì¼ í™•ì¸\nprint(f\"\\nSubmission file created: {submission_filename}\")\nprint(f\"Shape: {submission_df.shape}\")\nprint(\"\\nFirst 3 submissions:\")\nprint(submission_df.head(3))\n\n# ì‹¤í—˜ ìš”ì•½\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"SINGLE MODEL EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Model: {current_model} ({model_config['name']})\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(f\"Training epochs completed: {len(training_history['train_loss'])}\")\nif training_history['train_loss']:\n    logger.write(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\nif training_history['val_loss']:\n    logger.write(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\nlogger.write(f\"Submission file: {submission_filename}\")\nlogger.write(\"=\"*50)\n\n# ì‹œê°í™”\nif config.get('visualization', {}).get('enabled', False):\n    from src.utils.visualizations.training_viz import TrainingVisualizer\n    \n    viz = TrainingVisualizer()\n    \n    # ì‹œê°í™” ì €ì¥ ê²½ë¡œ\n    viz_dir = get_path(config.get('visualization', {}).get('save_path', 'visualizations'))\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    \n    # í•™ìŠµ íˆìŠ¤í† ë¦¬ í”Œë¡¯\n    if len(training_history['train_loss']) > 0:\n        viz.plot_training_history(\n            training_history,\n            save_path=viz_dir / f'{current_model}_training_history.png'\n        )\n        logger.write(f\"Visualization saved to {viz_dir}\")\n\n# WandB ì¢…ë£Œ\nif config['wandb']['mode'] != 'disabled':\n    wandb.finish()\n\nlogger.write(f\"\\nâœ… Single model experiment ({current_model}) completed successfully!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}