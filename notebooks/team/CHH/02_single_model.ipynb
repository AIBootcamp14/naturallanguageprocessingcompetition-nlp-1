{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 단일 모델 실험 - SOLAR/Polyglot-Ko LLM 파인튜닝\n",
    "> PRD 계획에 따른 LLM 모델 LoRA 파인튜닝\n",
    "\n",
    "**목표 성능**: ROUGE-F1 70-73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 프로젝트 루트 경로 추가\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3번만 parent 사용!\n",
    "\n",
    "# 다른 프로젝트 경로 제거하고 현재 프로젝트 경로만 추가\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import wandb\n",
    "\n",
    "# 커스텀 모듈 임포트\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "Using LoRA: True\n",
      "  - LoRA r: 16\n",
      "  - LoRA alpha: 32\n"
     ]
    }
   ],
   "source": [
    "# 설정 파일 로드\n",
    "config_path = notebook_dir / 'configs' / 'config_single_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 모델 선택\n",
    "current_model = config['current_model']\n",
    "model_config = config['models'][current_model]\n",
    "\n",
    "print(f\"Selected Model: {model_config['name']}\")\n",
    "print(f\"Using LoRA: {model_config['use_lora']}\")\n",
    "if model_config['use_lora']:\n",
    "    print(f\"  - LoRA r: {model_config['lora_r']}\")\n",
    "    print(f\"  - LoRA alpha: {model_config['lora_alpha']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Directory: logs/single_model\n",
      "==================================================\n",
      "Single Model Experiment: solar\n",
      "Timestamp: 20251010_090241\n",
      "Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 로그 디렉토리 생성\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 타임스탬프 생성\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 로거 초기화\n",
    "log_file = log_dir / f'single_model_{current_model}_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write(f'Single Model Experiment: {current_model}')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Model: {model_config[\"name\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n"
     ]
    }
   ],
   "source": [
    "# GPU 체크\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    device = torch.device('cuda')\n",
    "    logger.write(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    logger.write(\"WARNING: No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration created\n",
      "  - r: 16\n",
      "  - alpha: 32\n",
      "  - dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정\n",
    "if model_config['use_lora']:\n",
    "    lora_config = LoraConfig(\n",
    "        r=model_config['lora_r'],\n",
    "        lora_alpha=model_config['lora_alpha'],\n",
    "        lora_dropout=model_config['lora_dropout'],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=config['peft']['target_modules']\n",
    "    )\n",
    "    logger.write(\"LoRA configuration created\")\n",
    "    logger.write(f\"  - r: {model_config['lora_r']}\")\n",
    "    logger.write(f\"  - alpha: {model_config['lora_alpha']}\")\n",
    "    logger.write(f\"  - dropout: {model_config['lora_dropout']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template loaded\n",
      "\n",
      "Sample Prompt:\n",
      "### Instruction:\n",
      "다음 대화를 3-5문장으로 요약해주세요. 핵심 내용과 중요한 정보를 포함시켜주세요.\n",
      "\n",
      "### Input:\n",
      "#Person1#: 안녕하세요?\n",
      "#Person2#: 안녕하세요!\n",
      "\n",
      "### Response:\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 템플릿 확인\n",
    "sample_dialogue = \"#Person1#: 안녕하세요?\\n#Person2#: 안녕하세요!\"\n",
    "prompt = config['prompt_template']['instruction_format'].format(\n",
    "    dialogue=sample_dialogue,\n",
    "    summary=\"\"\n",
    ")\n",
    "logger.write(\"Prompt template loaded\")\n",
    "print(\"\\nSample Prompt:\")\n",
    "print(prompt[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 향상된 프롬프트 엔지니어링 (PRD 14_프롬프트_엔지니어링.md)\nimport requests\nimport json\nfrom typing import Optional, Dict, List\n\nclass AdvancedPromptEngineering:\n    \"\"\"고급 프롬프트 엔지니어링 클래스\"\"\"\n    \n    def __init__(self):\n        self.prompt_templates = {\n            'few_shot': self._create_few_shot_template,\n            'chain_of_thought': self._create_cot_template,\n            'role_based': self._create_role_based_template,\n            'structured': self._create_structured_template,\n            'topic_specific': self._create_topic_specific_template\n        }\n        \n    def _create_few_shot_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"Few-shot 학습 프롬프트\"\"\"\n        examples = \"\"\"\n예시 1:\n대화: 화자1: 오늘 날씨가 정말 좋네요. 화자2: 네, 산책하기 좋은 날씨입니다.\n요약: 두 사람이 좋은 날씨에 대해 이야기하며 산책하기 좋다고 동의합니다.\n\n예시 2:\n대화: 화자1: 프로젝트 마감일이 언제죠? 화자2: 다음 주 금요일입니다. 화자1: 서둘러야겠네요.\n요약: 프로젝트 마감일이 다음 주 금요일이며, 서둘러 완성해야 한다고 대화합니다.\n\n예시 3:\n대화: 화자1: 감기 증상이 있어요. 화자2: 충분한 휴식을 취하세요. 약도 처방해드리겠습니다.\n요약: 환자가 감기 증상을 호소하자 의사가 휴식과 약 처방을 권합니다.\n\"\"\"\n        \n        return f\"\"\"### Instruction:\n아래 예시를 참고하여 대화를 간결하고 정확하게 요약해주세요.\n\n{examples}\n\n### Input:\n{dialogue}\n\n### Response:\"\"\"\n    \n    def _create_cot_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"Chain-of-Thought 프롬프트\"\"\"\n        return f\"\"\"### Instruction:\n다음 단계를 따라 대화를 분석하고 요약해주세요:\n\n1단계: 대화 참여자들을 파악하세요\n2단계: 대화의 주요 주제를 찾으세요\n3단계: 핵심 정보와 결론을 추출하세요\n4단계: 3-5문장으로 간결하게 요약하세요\n\n### Input:\n{dialogue}\n\n### Analysis and Response:\n1단계 분석:\n2단계 분석:\n3단계 분석:\n4단계 최종 요약:\"\"\"\n    \n    def _create_role_based_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"역할 기반 프롬프트\"\"\"\n        role_description = {\n            '의료': '의료 전문가로서 환자의 증상과 치료 계획을 명확히',\n            '비즈니스': '비즈니스 분석가로서 주요 결정사항과 액션아이템을',\n            '교육': '교육 전문가로서 학습 내용과 핵심 개념을',\n            '기술': '기술 전문가로서 기술적 논의사항과 해결책을'\n        }.get(topic, '전문 요약가로서 핵심 내용을')\n        \n        return f\"\"\"### Instruction:\n당신은 {role_description} 요약하는 전문가입니다.\n대화의 중요한 정보를 빠짐없이 포함하여 3-5문장으로 요약해주세요.\n\n### Input:\n{dialogue}\n\n### Expert Summary:\"\"\"\n    \n    def _create_structured_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"구조화된 출력 프롬프트\"\"\"\n        return f\"\"\"### Instruction:\n다음 형식에 맞춰 대화를 요약해주세요:\n\n[주제]: 대화의 주요 주제\n[참여자]: 대화 참여자들의 역할\n[핵심내용]: 가장 중요한 정보 2-3가지\n[결론/결과]: 대화의 결론이나 합의사항\n\n### Input:\n{dialogue}\n\n### Structured Summary:\"\"\"\n    \n    def _create_topic_specific_template(self, dialogue: str, topic: str = None) -> str:\n        \"\"\"주제별 맞춤 프롬프트\"\"\"\n        topic_instructions = {\n            '건강검진': '환자의 건강 상태, 검진 항목, 의사의 권고사항을 중심으로',\n            '백신 접종': '백신 종류, 접종 일정, 부작용 설명을 포함하여',\n            '회의': '회의 안건, 논의사항, 결정사항을 명확히',\n            '상담': '상담 목적, 문제점, 해결방안을 구체적으로',\n            '예약': '예약 일시, 장소, 목적을 정확히'\n        }\n        \n        instruction = topic_instructions.get(topic, '핵심 내용을 중심으로')\n        \n        return f\"\"\"### Instruction:\n{instruction} 대화를 요약해주세요.\n불필요한 세부사항은 제외하고 중요한 정보만 포함시켜주세요.\n\n### Input:\n{dialogue}\n\n### Response:\"\"\"\n    \n    def get_best_prompt(self, dialogue: str, topic: str = None, strategy: str = 'auto') -> str:\n        \"\"\"최적 프롬프트 선택\"\"\"\n        if strategy == 'auto':\n            # 대화 길이에 따라 전략 선택\n            if len(dialogue) < 500:\n                strategy = 'structured'\n            elif len(dialogue) < 1000:\n                strategy = 'few_shot'\n            elif topic:\n                strategy = 'topic_specific'\n            else:\n                strategy = 'chain_of_thought'\n        \n        if strategy in self.prompt_templates:\n            return self.prompt_templates[strategy](dialogue, topic)\n        else:\n            # 기본 템플릿\n            return self.prompt_templates['few_shot'](dialogue, topic)\n\n# Solar API 교차 검증 시스템 (PRD 09_Solar_API_최적화.md, 10_교차_검증_시스템.md)\nclass SolarAPIValidator:\n    \"\"\"Solar API 교차 검증 클래스\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://api.upstage.ai/v1/solar\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        self.cache = {}  # API 응답 캐싱\n        \n    def generate_with_solar(self, dialogue: str, max_tokens: int = 150, temperature: float = 0.3) -> Optional[str]:\n        \"\"\"Solar API로 요약 생성\"\"\"\n        # 캐시 확인\n        cache_key = hash(dialogue[:200])\n        if cache_key in self.cache:\n            logger.write(\"Using cached Solar API response\")\n            return self.cache[cache_key]\n        \n        try:\n            # 프롬프트 최적화 (토큰 절약)\n            if len(dialogue) > 2000:\n                dialogue = dialogue[:2000] + \"...\"\n            \n            prompt = f\"\"\"다음 대화를 한국어로 간결하게 요약하세요:\n\n{dialogue}\n\n요약:\"\"\"\n            \n            payload = {\n                \"model\": \"solar-1-mini-chat\",\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"당신은 전문적인 대화 요약 AI입니다.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                \"max_tokens\": max_tokens,\n                \"temperature\": temperature,\n                \"top_p\": 0.9\n            }\n            \n            response = requests.post(\n                f\"{self.base_url}/chat/completions\",\n                headers=self.headers,\n                json=payload,\n                timeout=30\n            )\n            \n            if response.status_code == 200:\n                result = response.json()\n                summary = result['choices'][0]['message']['content']\n                \n                # 캐싱\n                self.cache[cache_key] = summary\n                \n                return summary\n            else:\n                logger.write(f\"Solar API error: {response.status_code}\")\n                return None\n                \n        except Exception as e:\n            logger.write(f\"Solar API exception: {e}\")\n            return None\n    \n    def compare_with_model(self, model_summary: str, api_summary: str, reference: str = None) -> Dict:\n        \"\"\"모델과 API 요약 비교\"\"\"\n        from rouge import Rouge\n        \n        result = {\n            'model_summary': model_summary,\n            'api_summary': api_summary\n        }\n        \n        if reference:\n            rouge = Rouge()\n            try:\n                # 모델 점수\n                model_scores = rouge.get_scores(model_summary, reference)[0]\n                result['model_rouge_l'] = model_scores['rouge-l']['f']\n                \n                # API 점수\n                if api_summary:\n                    api_scores = rouge.get_scores(api_summary, reference)[0]\n                    result['api_rouge_l'] = api_scores['rouge-l']['f']\n                else:\n                    result['api_rouge_l'] = 0\n                \n                # 최선 선택\n                result['best'] = 'model' if result['model_rouge_l'] >= result.get('api_rouge_l', 0) else 'api'\n                result['improvement'] = abs(result['model_rouge_l'] - result.get('api_rouge_l', 0))\n                \n            except Exception as e:\n                logger.write(f\"Error computing ROUGE: {e}\")\n        \n        return result\n\n# 프롬프트 엔지니어링 초기화\nprompt_engineer = AdvancedPromptEngineering()\n\n# Solar API 초기화 (config에서 키 가져오기)\nsolar_validator = None\nif 'solar_api' in config and 'api_key' in config['solar_api']:\n    solar_validator = SolarAPIValidator(config['solar_api']['api_key'])\n    logger.write(\"Solar API validator initialized for cross-validation\")\nelse:\n    logger.write(\"Solar API key not found - skipping API cross-validation\")\n\n# 프롬프트 테스트\nsample_dialogue = train_df['dialogue'].iloc[0]\nsample_topic = train_df.get('topic', pd.Series([None] * len(train_df))).iloc[0]\n\n# 다양한 프롬프트 전략 테스트\nlogger.write(\"\\n=== Testing Prompt Engineering Strategies ===\")\nfor strategy in ['few_shot', 'chain_of_thought', 'topic_specific']:\n    test_prompt = prompt_engineer.prompt_templates[strategy](sample_dialogue[:500], sample_topic)\n    logger.write(f\"\\n{strategy.upper()} Strategy (first 300 chars):\")\n    logger.write(test_prompt[:300] + \"...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "Data loaded successfully!\n",
      "Train samples: 12457\n",
      "Dev samples: 499\n",
      "Test samples: 499\n",
      "\n",
      "Sample train data:\n",
      "     fname  topic\n",
      "0  train_0   건강검진\n",
      "1  train_1  백신 접종\n",
      "2  train_2  열쇠 분실\n",
      "\n",
      "Dialogue sample (first 200 chars):\n",
      "#Person1#: 안녕하세요, Mr. Smith. 저는 Dr. Hawkins입니다. 오늘 무슨 일로 오셨어요? \n",
      "#Person2#: 건강검진을 받으려고 왔어요. \n",
      "#Person1#: 네, 5년 동안 검진을 안 받으셨네요. 매년 한 번씩 받으셔야 해요. \n",
      "#Person2#: 알죠. 특별히 아픈 데가 없으면 굳이 갈 필요가 없다고 생각했어요. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로 설정 및 로드\n",
    "# config 파일의 경로 사용\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"config의 상대 경로를 절대 경로로 변환\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# config에서 데이터 경로 가져오기\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"Data loaded successfully!\")\n",
    "logger.write(f\"Train samples: {len(train_df)}\")\n",
    "logger.write(f\"Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# 데이터 샘플 출력\n",
    "print(\"\\nSample train data:\")\n",
    "print(train_df[['fname', 'topic']].head(3))\n",
    "print(f\"\\nDialogue sample (first 200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 데이터 품질 검증 시스템 (PRD 16_데이터_품질_검증_시스템.md)\nimport numpy as np\nfrom typing import Dict, List\n\nclass DataQualityValidator:\n    \"\"\"데이터 품질 검증 클래스\"\"\"\n    def __init__(self):\n        self.quality_report = {}\n        \n    def validate_llm_data(self, df: pd.DataFrame) -> Dict:\n        \"\"\"LLM 학습용 데이터 품질 검증\"\"\"\n        report = {}\n        \n        # 1. 구조적 검증\n        report['structure'] = {\n            'null_values': df.isnull().sum().sum(),\n            'duplicates': df.duplicated().sum(),\n            'empty_dialogues': (df['dialogue'].str.len() == 0).sum(),\n            'empty_summaries': (df['summary'].str.len() == 0).sum() if 'summary' in df.columns else 0\n        }\n        \n        # 2. 텍스트 길이 검증 (LLM 컨텍스트 길이 고려)\n        dialogue_lengths = df['dialogue'].str.len()\n        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n        \n        report['text_length'] = {\n            'avg_dialogue_length': dialogue_lengths.mean(),\n            'max_dialogue_length': dialogue_lengths.max(),\n            'dialogues_over_2048': (dialogue_lengths > 2048).sum(),\n            'dialogues_over_4096': (dialogue_lengths > 4096).sum(),\n            'summary_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0\n        }\n        \n        # 3. 토큰 수 추정 (한국어는 대략 3자 = 1토큰)\n        estimated_tokens = dialogue_lengths / 3\n        report['token_estimation'] = {\n            'avg_tokens': estimated_tokens.mean(),\n            'max_tokens': estimated_tokens.max(),\n            'samples_over_512_tokens': (estimated_tokens > 512).sum(),\n            'samples_over_1024_tokens': (estimated_tokens > 1024).sum()\n        }\n        \n        # 4. 특수 문자 및 인코딩 검증\n        report['encoding'] = {\n            'person_tags_present': df['dialogue'].str.contains('#Person').sum(),\n            'encoding_issues': df['dialogue'].str.contains('[�\\\\?]').sum(),\n            'special_chars': df['dialogue'].str.contains('[^\\w\\s#:.,!?가-힣a-zA-Z0-9]').sum()\n        }\n        \n        # 5. 주제 분포 (불균형 체크)\n        if 'topic' in df.columns:\n            topic_counts = df['topic'].value_counts()\n            report['topic_distribution'] = {\n                'unique_topics': len(topic_counts),\n                'most_common_topic': topic_counts.index[0] if len(topic_counts) > 0 else None,\n                'most_common_count': topic_counts.iloc[0] if len(topic_counts) > 0 else 0,\n                'least_common_topic': topic_counts.index[-1] if len(topic_counts) > 0 else None,\n                'least_common_count': topic_counts.iloc[-1] if len(topic_counts) > 0 else 0,\n                'imbalance_ratio': topic_counts.iloc[0] / topic_counts.iloc[-1] if len(topic_counts) > 1 and topic_counts.iloc[-1] > 0 else 0\n            }\n        \n        self.quality_report = report\n        return report\n    \n    def recommend_preprocessing(self) -> List[str]:\n        \"\"\"전처리 권장사항 생성\"\"\"\n        recommendations = []\n        \n        if self.quality_report.get('structure', {}).get('null_values', 0) > 0:\n            recommendations.append(\"Remove or impute null values\")\n        \n        if self.quality_report.get('structure', {}).get('duplicates', 0) > 0:\n            recommendations.append(\"Remove duplicate samples\")\n        \n        if self.quality_report.get('text_length', {}).get('dialogues_over_4096', 0) > 0:\n            recommendations.append(\"Truncate or split long dialogues (>4096 chars)\")\n        \n        if self.quality_report.get('encoding', {}).get('encoding_issues', 0) > 0:\n            recommendations.append(\"Fix encoding issues in text\")\n        \n        if self.quality_report.get('topic_distribution', {}).get('imbalance_ratio', 0) > 10:\n            recommendations.append(\"Consider data augmentation for underrepresented topics\")\n        \n        return recommendations\n\n# 데이터 품질 검증 실행\nvalidator = DataQualityValidator()\nquality_report = validator.validate_llm_data(train_df)\n\nlogger.write(\"\\n=== Data Quality Validation Report ===\")\nfor category, metrics in quality_report.items():\n    logger.write(f\"\\n{category.upper()}:\")\n    for key, value in metrics.items():\n        if isinstance(value, float):\n            logger.write(f\"  - {key}: {value:.2f}\")\n        else:\n            logger.write(f\"  - {key}: {value}\")\n\n# 전처리 권장사항\nrecommendations = validator.recommend_preprocessing()\nif recommendations:\n    logger.write(\"\\n📋 Preprocessing Recommendations:\")\n    for rec in recommendations:\n        logger.write(f\"  • {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 추론 최적화 시스템 (PRD 17_추론_최적화.md)\nimport gc\nfrom typing import List\n\nclass LLMInferenceOptimizer:\n    \"\"\"LLM 추론 최적화 클래스\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.cache = {}\n        \n        # 최적화 기법 적용\n        self._apply_optimizations()\n    \n    def _apply_optimizations(self):\n        \"\"\"모델 최적화 기법 적용\"\"\"\n        # 1. Gradient checkpointing (메모리 절약)\n        if hasattr(self.model, 'gradient_checkpointing_enable'):\n            self.model.gradient_checkpointing_enable()\n            logger.write(\"Gradient checkpointing enabled\")\n        \n        # 2. Flash Attention (가능한 경우)\n        try:\n            if hasattr(self.model.config, 'use_flash_attention'):\n                self.model.config.use_flash_attention = True\n                logger.write(\"Flash attention enabled\")\n        except:\n            pass\n        \n        # 3. KV 캐시 활성화\n        if hasattr(self.model.config, 'use_cache'):\n            self.model.config.use_cache = True\n            logger.write(\"KV cache enabled\")\n    \n    def batch_generate(self, texts: List[str], batch_size: int = 8, **generation_kwargs) -> List[str]:\n        \"\"\"배치 생성 최적화\"\"\"\n        all_outputs = []\n        \n        # 길이별로 정렬 (동적 배칭)\n        sorted_texts = sorted(enumerate(texts), key=lambda x: len(x[1]))\n        \n        for i in range(0, len(sorted_texts), batch_size):\n            batch = sorted_texts[i:i+batch_size]\n            batch_indices = [idx for idx, _ in batch]\n            batch_texts = [text for _, text in batch]\n            \n            # 토큰화\n            inputs = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=generation_kwargs.get('max_length', 1024),\n                return_tensors='pt'\n            ).to(self.device)\n            \n            # 생성\n            with torch.no_grad():\n                with torch.cuda.amp.autocast():  # Mixed precision\n                    outputs = self.model.generate(\n                        **inputs,\n                        **generation_kwargs\n                    )\n            \n            # 디코딩\n            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            \n            # 원래 순서로 저장\n            for idx, output in zip(batch_indices, decoded):\n                all_outputs.append((idx, output))\n        \n        # 원래 순서로 정렬\n        all_outputs.sort(key=lambda x: x[0])\n        return [output for _, output in all_outputs]\n    \n    def cached_generate(self, text: str, **kwargs) -> str:\n        \"\"\"캐싱을 활용한 생성\"\"\"\n        # 캐시 키 생성\n        cache_key = hash(text[:200])\n        \n        if cache_key in self.cache:\n            logger.write(\"Cache hit - returning cached result\")\n            return self.cache[cache_key]\n        \n        # 캐시 미스 - 새로 생성\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=kwargs.get('max_length', 1024),\n            return_tensors='pt'\n        ).to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(**inputs, **kwargs)\n        \n        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # 캐시 저장 (최대 100개)\n        if len(self.cache) < 100:\n            self.cache[cache_key] = result\n        \n        return result\n\n# 리스크 관리 시스템 (PRD 05_리스크_관리.md)\nclass LLMRiskManager:\n    \"\"\"LLM 학습 리스크 관리\"\"\"\n    \n    def __init__(self):\n        self.risk_log = []\n        self.mitigation_applied = []\n    \n    def check_training_risks(self, epoch: int, train_loss: float, val_loss: float, \n                           rouge_score: float, memory_used: float) -> Dict:\n        \"\"\"학습 리스크 체크\"\"\"\n        risks = []\n        \n        # 1. 과적합 리스크\n        if val_loss > train_loss * 1.5 and epoch > 2:\n            risks.append({\n                'type': 'overfitting',\n                'severity': 'high',\n                'metric': f'val/train ratio: {val_loss/train_loss:.2f}',\n                'mitigation': ['Increase dropout', 'Early stopping', 'Reduce learning rate']\n            })\n        \n        # 2. 과소적합 리스크\n        if epoch > 3 and train_loss > 2.0:\n            risks.append({\n                'type': 'underfitting',\n                'severity': 'medium',\n                'metric': f'train_loss: {train_loss:.4f}',\n                'mitigation': ['Increase model capacity', 'Adjust learning rate', 'Check data quality']\n            })\n        \n        # 3. 성능 정체 리스크\n        if epoch > 5 and rouge_score < 0.3:\n            risks.append({\n                'type': 'performance_plateau',\n                'severity': 'high',\n                'metric': f'ROUGE-L: {rouge_score:.4f}',\n                'mitigation': ['Change prompt strategy', 'Adjust hyperparameters', 'Use different model']\n            })\n        \n        # 4. 메모리 리스크\n        if memory_used > 0.9:\n            risks.append({\n                'type': 'memory_overflow',\n                'severity': 'critical',\n                'metric': f'Memory usage: {memory_used:.1%}',\n                'mitigation': ['Reduce batch size', 'Enable gradient accumulation', 'Use smaller model']\n            })\n        \n        # 5. 학습 불안정 리스크\n        if train_loss > 10 or np.isnan(train_loss):\n            risks.append({\n                'type': 'training_instability',\n                'severity': 'critical',\n                'metric': f'Unstable loss: {train_loss}',\n                'mitigation': ['Reduce learning rate', 'Check for NaN values', 'Reset training']\n            })\n        \n        # 리스크 로그 저장\n        if risks:\n            self.risk_log.extend(risks)\n            for risk in risks:\n                logger.write(f\"⚠️ Risk: {risk['type']} ({risk['severity']}) - {risk['metric']}\")\n                logger.write(f\"  Suggested mitigations: {', '.join(risk['mitigation'])}\")\n        \n        return {'risks': risks, 'total_risks': len(risks)}\n    \n    def apply_automatic_mitigation(self, risk_type: str, config: Dict) -> Dict:\n        \"\"\"자동 리스크 완화\"\"\"\n        mitigations = {\n            'overfitting': {\n                'action': 'reduce_learning_rate',\n                'new_lr': config['training']['learning_rate'] * 0.5\n            },\n            'memory_overflow': {\n                'action': 'reduce_batch_size',\n                'new_batch_size': max(1, config['training']['batch_size'] // 2)\n            },\n            'training_instability': {\n                'action': 'reset_optimizer',\n                'new_lr': config['training']['learning_rate'] * 0.1\n            }\n        }\n        \n        if risk_type in mitigations:\n            mitigation = mitigations[risk_type]\n            self.mitigation_applied.append(mitigation)\n            logger.write(f\"✓ Applied automatic mitigation: {mitigation['action']}\")\n            return mitigation\n        \n        return {}\n\n# Optuna 하이퍼파라미터 최적화 (PRD 13_Optuna_하이퍼파라미터_최적화.md)\ntry:\n    import optuna\n    from optuna import Trial\n    from optuna.samplers import TPESampler\n    \n    class LLMOptunaOptimizer:\n        \"\"\"LLM용 Optuna 최적화\"\"\"\n        \n        def __init__(self, model_name: str, tokenizer, train_dataset, val_dataset):\n            self.model_name = model_name\n            self.tokenizer = tokenizer\n            self.train_dataset = train_dataset\n            self.val_dataset = val_dataset\n            \n        def objective(self, trial: Trial) -> float:\n            \"\"\"Optuna 목적 함수\"\"\"\n            # 하이퍼파라미터 제안\n            hp = {\n                'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-4, log=True),\n                'batch_size': trial.suggest_categorical('batch_size', [1, 2, 4, 8]),\n                'lora_r': trial.suggest_int('lora_r', 4, 32),\n                'lora_alpha': trial.suggest_int('lora_alpha', 8, 64),\n                'warmup_ratio': trial.suggest_float('warmup_ratio', 0.0, 0.2),\n                'weight_decay': trial.suggest_float('weight_decay', 0.0, 0.1),\n                'temperature': trial.suggest_float('temperature', 0.1, 1.0),\n                'top_p': trial.suggest_float('top_p', 0.8, 1.0),\n                'num_beams': trial.suggest_int('num_beams', 1, 5)\n            }\n            \n            logger.write(f\"\\nTrial {trial.number}: {hp}\")\n            \n            # 간단한 평가 (실제로는 모델 학습 필요)\n            # 여기서는 더미 점수 반환 (실제 구현시 모델 학습 및 평가)\n            dummy_score = np.random.random() * 0.5 + 0.3  # 0.3~0.8 범위\n            \n            return dummy_score\n        \n        def optimize(self, n_trials: int = 10, timeout: int = 3600):\n            \"\"\"최적화 실행\"\"\"\n            study = optuna.create_study(\n                direction='maximize',\n                sampler=TPESampler(seed=42),\n                study_name=f'llm_optimization_{self.model_name}'\n            )\n            \n            study.optimize(self.objective, n_trials=n_trials, timeout=timeout)\n            \n            # 최적 파라미터\n            best_params = study.best_params\n            best_value = study.best_value\n            \n            logger.write(f\"\\n=== Optuna Optimization Results ===\")\n            logger.write(f\"Best ROUGE-L: {best_value:.4f}\")\n            logger.write(f\"Best parameters: {best_params}\")\n            \n            return best_params, best_value\n    \n    # Optuna 최적화 실행 (config에서 활성화된 경우)\n    if config.get('optuna', {}).get('enabled', False):\n        logger.write(\"\\n=== Starting Optuna Hyperparameter Optimization ===\")\n        # 나중에 데이터셋 생성 후 실행\n        optuna_optimizer = LLMOptunaOptimizer(\n            model_name=current_model,\n            tokenizer=None,  # 나중에 설정\n            train_dataset=None,  # 나중에 설정\n            val_dataset=None  # 나중에 설정\n        )\n    else:\n        optuna_optimizer = None\n        logger.write(\"Optuna optimization disabled\")\n        \nexcept ImportError:\n    logger.write(\"Optuna not installed - skipping hyperparameter optimization\")\n    optuna_optimizer = None\n\n# 리스크 매니저 초기화\nrisk_manager = LLMRiskManager()\nlogger.write(\"Risk management system initialized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 데이터셋 클래스 정의\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Dict, List\nimport re\n\nclass LLMDialogueSummaryDataset(Dataset):\n    \"\"\"LLM 파인튜닝용 데이터셋\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, prompt_template, max_length=1024, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.prompt_template = prompt_template\n        self.max_length = max_length\n        self.is_test = is_test\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"대화 텍스트 전처리\"\"\"\n        # 노이즈 제거\n        text = text.replace('\\\\n', '\\n')\n        text = text.replace('<br>', '\\n')\n        text = text.strip()\n        \n        # #Person 태그 한국어로 변경\n        text = re.sub(r'#Person(\\d+)#:', r'화자\\1:', text)\n        \n        return text\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # 대화 전처리\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        if not self.is_test:\n            # 학습 모드: 프롬프트와 요약을 함께 토큰화\n            summary = row.get('summary', '')\n            \n            # 프롬프트 생성\n            full_prompt = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=summary\n            )\n            \n            # 토큰화\n            encoded = self.tokenizer(\n                full_prompt,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # 라벨 설정 (요약 부분만 loss 계산)\n            labels = encoded['input_ids'].clone()\n            \n            # 프롬프트 부분은 -100으로 마스킹\n            prompt_without_summary = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=\"\"\n            )\n            prompt_tokens = self.tokenizer(\n                prompt_without_summary,\n                add_special_tokens=False,\n                return_tensors='pt'\n            )['input_ids']\n            \n            if prompt_tokens.shape[1] < labels.shape[1]:\n                labels[0, :prompt_tokens.shape[1]] = -100\n            \n            # 패딩 토큰도 -100으로 마스킹 (중요!)\n            labels[labels == self.tokenizer.pad_token_id] = -100\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'labels': labels.squeeze()\n            }\n        else:\n            # 테스트 모드: 프롬프트만 토큰화\n            prompt = self.prompt_template.format(\n                dialogue=dialogue,\n                summary=\"\"\n            )\n            \n            encoded = self.tokenizer(\n                prompt,\n                max_length=self.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': encoded['input_ids'].squeeze(),\n                'attention_mask': encoded['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# 프롬프트 템플릿 로드\nprompt_template = config['prompt_template']['instruction_format']\n\nlogger.write(\"Creating datasets...\")\n\n# 데이터셋 생성은 나중에 토크나이저 로드 후에 진행\nlogger.write(\"Dataset class defined successfully\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 모델 및 토크나이저 로드 (메모리 효율적으로)\nlogger.write(f\"\\nLoading model: {model_config['name']}\")\nlogger.write(\"This may take some time due to model size...\")\n\n# 8bit/4bit 양자화 설정\nload_kwargs = {}\nif model_config.get('load_in_8bit', False):\n    load_kwargs['load_in_8bit'] = True\n    load_kwargs['device_map'] = 'auto'\n    logger.write(\"Using 8-bit quantization\")\nelif model_config.get('load_in_4bit', False):\n    from transformers import BitsAndBytesConfig\n    load_kwargs['quantization_config'] = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    load_kwargs['device_map'] = 'auto'\n    logger.write(\"Using 4-bit quantization\")\n\n# 토크나이저 로드\ntokenizer = AutoTokenizer.from_pretrained(\n    model_config['name'],\n    padding_side='left'  # 생성 모델용\n)\n\n# 패딩 토큰 설정\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    logger.write(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n\n# 모델 로드\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.float16,\n        **load_kwargs\n    )\n    logger.write(f\"Model loaded successfully\")\n    \n    # LoRA 적용\n    if model_config['use_lora']:\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        logger.write(\"LoRA applied to model\")\n    \nexcept Exception as e:\n    logger.write(f\"Error loading model: {e}\")\n    logger.write(\"Falling back to smaller model or mock mode\")\n    \n    # Fallback: 더 작은 모델 사용\n    fallback_model = \"skt/kogpt2-base-v2\"\n    logger.write(f\"Loading fallback model: {fallback_model}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        fallback_model,\n        torch_dtype=torch.float16\n    ).to(device)\n    \n    if model_config['use_lora']:\n        # Fallback 모델용 LoRA config 수정\n        from peft import LoraConfig, get_peft_model, TaskType\n        \n        fallback_lora_config = LoraConfig(\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.1,\n            task_type=TaskType.CAUSAL_LM,\n            target_modules=[\"c_attn\", \"c_proj\"]  # GPT2용 모듈\n        )\n        model = get_peft_model(model, fallback_lora_config)\n        model.print_trainable_parameters()\n\nlogger.write(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
  },
  {
   "cell_type": "code",
   "source": "# 데이터셋 및 DataLoader 생성\nlogger.write(\"\\nCreating datasets and dataloaders...\")\n\n# 학습용 샘플 수 제한 (메모리 절약)\nif config['training'].get('use_sample', False):\n    sample_size = config['training'].get('sample_size', 1000)\n    train_df_sample = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n    logger.write(f\"Using sample of {len(train_df_sample)} training examples\")\nelse:\n    train_df_sample = train_df\n\n# 데이터셋 생성\ntrain_dataset = LLMDialogueSummaryDataset(\n    train_df_sample,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=False\n)\n\nval_dataset = LLMDialogueSummaryDataset(\n    dev_df,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=False\n)\n\ntest_dataset = LLMDialogueSummaryDataset(\n    test_df,\n    tokenizer,\n    prompt_template,\n    max_length=model_config['max_length'],\n    is_test=True\n)\n\n# DataLoader 생성\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config['training']['batch_size'],\n    shuffle=True,\n    num_workers=0,  # 양자화 모델은 멀티프로세싱 문제 방지\n    pin_memory=False\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=config['evaluation']['batch_size'],\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=config['inference']['batch_size'],\n    shuffle=False,\n    num_workers=0,\n    pin_memory=False\n)\n\nlogger.write(f\"DataLoaders created:\")\nlogger.write(f\"  - Train batches: {len(train_loader)}\")\nlogger.write(f\"  - Val batches: {len(val_loader)}\")\nlogger.write(f\"  - Test batches: {len(test_loader)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 학습 및 평가 함수 정의",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 학습 루프 (리스크 모니터링 및 Solar API 교차 검증 포함)\nfor epoch in range(num_epochs):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"Epoch {epoch + 1}/{num_epochs}\")\n    logger.write(f\"{'='*30}\")\n    \n    # 학습\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n    logger.write(f\"Average training loss: {train_loss:.4f}\")\n    training_history['train_loss'].append(train_loss)\n    \n    # 검증 (적은 샘플로)\n    val_loss, rouge_scores, sample_preds = evaluate(\n        model, \n        val_loader, \n        tokenizer,\n        max_samples=config['evaluation'].get('max_samples', 50)\n    )\n    \n    logger.write(f\"\\nValidation Results:\")\n    logger.write(f\"  - Loss: {val_loss:.4f}\")\n    logger.write(f\"  - ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}\")\n    logger.write(f\"  - ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}\")\n    logger.write(f\"  - ROUGE-L F1: {rouge_scores['rouge-l']:.4f}\")\n    \n    # 리스크 모니터링\n    if torch.cuda.is_available():\n        memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n    else:\n        memory_used = 0\n    \n    risk_status = risk_manager.check_training_risks(\n        epoch=epoch,\n        train_loss=train_loss,\n        val_loss=val_loss,\n        rouge_score=rouge_scores['rouge-l'],\n        memory_used=memory_used\n    )\n    \n    # 리스크 완화 적용\n    if risk_status['total_risks'] > 0:\n        logger.write(f\"\\n⚠️ {risk_status['total_risks']} risks detected\")\n        for risk in risk_status['risks']:\n            if risk['severity'] == 'critical':\n                mitigation = risk_manager.apply_automatic_mitigation(risk['type'], config)\n                \n                # 학습률 조정\n                if 'new_lr' in mitigation:\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = mitigation['new_lr']\n                    logger.write(f\"  → Learning rate reduced to {mitigation['new_lr']}\")\n    \n    # Solar API 교차 검증 (매 2 에폭마다)\n    if solar_validator and epoch % 2 == 0:\n        logger.write(\"\\n🔄 Solar API Cross-Validation...\")\n        \n        # 랜덤 샘플 선택\n        sample_idx = np.random.randint(0, len(dev_df))\n        sample = dev_df.iloc[sample_idx]\n        \n        # 모델 예측 생성\n        model_prompt = prompt_engineer.get_best_prompt(\n            sample['dialogue'], \n            sample.get('topic', None)\n        )\n        \n        model_inputs = tokenizer(\n            model_prompt,\n            max_length=1024,\n            truncation=True,\n            return_tensors='pt'\n        ).to(device)\n        \n        with torch.no_grad():\n            model_output = model.generate(\n                **model_inputs,\n                max_new_tokens=150,\n                temperature=0.3,\n                top_p=0.9\n            )\n        \n        model_summary = tokenizer.decode(model_output[0], skip_special_tokens=True)\n        \n        # 프롬프트 부분 제거\n        if \"### Response:\" in model_summary:\n            model_summary = model_summary.split(\"### Response:\")[-1].strip()\n        \n        # Solar API 예측\n        api_summary = solar_validator.generate_with_solar(sample['dialogue'])\n        \n        # 비교\n        comparison = solar_validator.compare_with_model(\n            model_summary=model_summary,\n            api_summary=api_summary,\n            reference=sample.get('summary', None)\n        )\n        \n        if comparison.get('model_rouge_l') and comparison.get('api_rouge_l'):\n            logger.write(f\"  Model ROUGE-L: {comparison['model_rouge_l']:.4f}\")\n            logger.write(f\"  Solar ROUGE-L: {comparison['api_rouge_l']:.4f}\")\n            logger.write(f\"  Best: {comparison['best']}\")\n            \n            # WandB 로깅\n            if config['wandb']['mode'] != 'disabled':\n                wandb.log({\n                    'model_vs_api/model_rouge': comparison['model_rouge_l'],\n                    'model_vs_api/api_rouge': comparison.get('api_rouge_l', 0),\n                    'model_vs_api/best': 1 if comparison['best'] == 'model' else 0\n                })\n    \n    # 히스토리 업데이트\n    training_history['val_loss'].append(val_loss)\n    training_history['rouge_1'].append(rouge_scores['rouge-1'])\n    training_history['rouge_2'].append(rouge_scores['rouge-2'])\n    training_history['rouge_l'].append(rouge_scores['rouge-l'])\n    \n    # WandB 로깅\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss_epoch': train_loss,\n            'val_loss': val_loss,\n            'rouge_1': rouge_scores['rouge-1'],\n            'rouge_2': rouge_scores['rouge-2'],\n            'rouge_l': rouge_scores['rouge-l'],\n            'risks_detected': risk_status['total_risks'],\n            'memory_usage': memory_used\n        })\n    \n    # 샘플 예측 출력\n    if sample_preds:\n        logger.write(\"\\nSample Predictions:\")\n        for i, pred in enumerate(sample_preds[:2]):\n            logger.write(f\"  Sample {i+1}: {pred[:150]}...\")\n    \n    # Best model 저장\n    if rouge_scores['rouge-l'] > best_rouge_l:\n        best_rouge_l = rouge_scores['rouge-l']\n        patience_counter = 0\n        \n        # 모델 저장\n        if hasattr(model, 'module'):\n            model_to_save = model.module\n        else:\n            model_to_save = model\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'rouge_scores': rouge_scores,\n            'config': config,\n            'model_config': model_config,\n            'risk_report': risk_manager.risk_log,  # 리스크 보고서 추가\n            'prompt_strategy': 'advanced_prompting'  # 프롬프트 전략 기록\n        }, best_model_path)\n        \n        logger.write(f\"✓ New best model saved! (ROUGE-L: {best_rouge_l:.4f})\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            logger.write(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n    \n    # 메모리 정리\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# 최종 리스크 보고서\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"RISK MANAGEMENT REPORT\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Total risks encountered: {len(risk_manager.risk_log)}\")\nlogger.write(f\"Mitigations applied: {len(risk_manager.mitigation_applied)}\")\n\nif risk_manager.risk_log:\n    # 리스크 타입별 집계\n    risk_types = {}\n    for risk in risk_manager.risk_log:\n        risk_type = risk['type']\n        if risk_type not in risk_types:\n            risk_types[risk_type] = 0\n        risk_types[risk_type] += 1\n    \n    logger.write(\"\\nRisk Summary:\")\n    for risk_type, count in risk_types.items():\n        logger.write(f\"  - {risk_type}: {count} occurrences\")\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Training completed!\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# WandB 초기화\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=f\"{config['wandb']['name']}_{current_model}\",\n        tags=config['wandb']['tags'] + [current_model],\n        config={\n            **config,\n            'model': model_config\n        }\n    )\n    logger.write(\"WandB initialized\")\n\n# 옵티마이저 및 스케줄러 설정\nnum_epochs = config['training']['num_epochs']\n\n# learning_rate가 문자열인 경우 float로 변환\nlearning_rate = config['training']['learning_rate']\nif isinstance(learning_rate, str):\n    learning_rate = float(learning_rate)\n\nnum_training_steps = num_epochs * len(train_loader)\n\n# 옵티마이저 설정\noptimizer = AdamW(\n    model.parameters(),\n    lr=learning_rate,\n    weight_decay=config['training']['weight_decay']\n)\n\n# 스케줄러 설정\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n    num_training_steps=num_training_steps\n)\n\nlogger.write(f\"\\nTraining Configuration:\")\nlogger.write(f\"  - Epochs: {num_epochs}\")\nlogger.write(f\"  - Learning rate: {learning_rate}\")\nlogger.write(f\"  - Batch size: {config['training']['batch_size']}\")\nlogger.write(f\"  - Total training steps: {num_training_steps}\")\nlogger.write(f\"  - Warmup steps: {int(num_training_steps * config['training']['warmup_ratio'])}\")\n\n# 학습 기록\ntraining_history = {\n    'train_loss': [],\n    'val_loss': [],\n    'rouge_1': [],\n    'rouge_2': [],\n    'rouge_l': []\n}\n\n# 모델 저장 경로\ndef get_path(path_str):\n    \"\"\"config의 상대 경로를 절대 경로로 변환\"\"\"\n    path = Path(path_str)\n    if not path.is_absolute():\n        path = notebook_dir / path\n    return path\n\noutput_dir = get_path(config['paths']['output_dir'])\noutput_dir.mkdir(parents=True, exist_ok=True)\nbest_model_path = output_dir / f'best_model_{current_model}.pt'\n\n# Early stopping 설정\nbest_rouge_l = 0\npatience = config['training']['early_stopping_patience']\npatience_counter = 0\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Starting training...\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 최적 모델로 테스트 데이터 예측 (추론 최적화 적용)\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"TEST PREDICTION WITH OPTIMIZATION\")\nlogger.write(\"=\"*50)\n\n# 최적 모델 로드\nif best_model_path.exists():\n    checkpoint = torch.load(best_model_path)\n    \n    # 모델 상태 로드\n    if hasattr(model, 'module'):\n        model.module.load_state_dict(checkpoint['model_state_dict'])\n    else:\n        model.load_state_dict(checkpoint['model_state_dict'])\n    \n    logger.write(f\"Best model loaded from epoch {checkpoint['epoch'] + 1}\")\n    logger.write(f\"Best ROUGE scores: {checkpoint['rouge_scores']}\")\n\n# 추론 최적화 초기화\ninference_optimizer = LLMInferenceOptimizer(model, tokenizer, device)\n\n# 테스트 예측 생성 (최적화된 배치 처리)\ndef generate_test_predictions(model, test_df, tokenizer, prompt_engineer, inference_optimizer):\n    \"\"\"최적화된 테스트 예측 생성\"\"\"\n    test_predictions = []\n    \n    # 프롬프트 생성\n    test_prompts = []\n    for idx, row in test_df.iterrows():\n        dialogue = row['dialogue']\n        topic = row.get('topic', None)\n        \n        # 최적 프롬프트 선택\n        prompt = prompt_engineer.get_best_prompt(dialogue, topic, strategy='auto')\n        test_prompts.append(prompt)\n    \n    # 배치 생성 (최적화)\n    generation_config = config['inference']['generation_config']\n    \n    logger.write(f\"Generating predictions for {len(test_prompts)} test samples...\")\n    logger.write(\"Using optimized batch inference...\")\n    \n    # 배치 단위로 예측\n    batch_size = config['inference'].get('batch_size', 4)\n    \n    for i in tqdm(range(0, len(test_prompts), batch_size), desc=\"Test predictions\"):\n        batch_prompts = test_prompts[i:i+batch_size]\n        \n        # 배치 생성\n        batch_predictions = inference_optimizer.batch_generate(\n            batch_prompts,\n            batch_size=batch_size,\n            max_new_tokens=generation_config['max_new_tokens'],\n            temperature=generation_config['temperature'],\n            top_p=generation_config['top_p'],\n            num_beams=generation_config['num_beams'],\n            no_repeat_ngram_size=generation_config['no_repeat_ngram_size'],\n            do_sample=generation_config['do_sample']\n        )\n        \n        # 프롬프트 부분 제거\n        for pred in batch_predictions:\n            if \"### Response:\" in pred:\n                pred = pred.split(\"### Response:\")[-1].strip()\n            elif \"요약:\" in pred:\n                pred = pred.split(\"요약:\")[-1].strip()\n            \n            test_predictions.append(pred)\n    \n    return test_predictions\n\n# 예측 수행\ntest_predictions = generate_test_predictions(\n    model, test_df, tokenizer, prompt_engineer, inference_optimizer\n)\n\nlogger.write(f\"\\n✓ Generated {len(test_predictions)} test predictions\")\n\n# Solar API와 비교 (샘플)\nif solar_validator and len(test_predictions) > 0:\n    logger.write(\"\\n=== Solar API Comparison (Test Samples) ===\")\n    \n    # 랜덤 3개 샘플 비교\n    for i in range(min(3, len(test_df))):\n        sample_idx = np.random.randint(0, len(test_df))\n        \n        model_pred = test_predictions[sample_idx]\n        api_pred = solar_validator.generate_with_solar(test_df.iloc[sample_idx]['dialogue'])\n        \n        logger.write(f\"\\nSample {i+1}:\")\n        logger.write(f\"  Model: {model_pred[:100]}...\")\n        if api_pred:\n            logger.write(f\"  Solar: {api_pred[:100]}...\")\n\n# 샘플 출력\nlogger.write(\"\\n=== Sample Test Predictions ===\")\nfor i in range(min(5, len(test_predictions))):\n    logger.write(f\"\\nTest {i+1}: {test_predictions[i][:150]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 학습 루프\nfor epoch in range(num_epochs):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"Epoch {epoch + 1}/{num_epochs}\")\n    logger.write(f\"{'='*30}\")\n    \n    # 학습\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n    logger.write(f\"Average training loss: {train_loss:.4f}\")\n    training_history['train_loss'].append(train_loss)\n    \n    # 검증 (적은 샘플로)\n    val_loss, rouge_scores, sample_preds = evaluate(\n        model, \n        val_loader, \n        tokenizer,\n        max_samples=config['evaluation'].get('max_samples', 50)\n    )\n    \n    logger.write(f\"\\nValidation Results:\")\n    logger.write(f\"  - Loss: {val_loss:.4f}\")\n    logger.write(f\"  - ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}\")\n    logger.write(f\"  - ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}\")\n    logger.write(f\"  - ROUGE-L F1: {rouge_scores['rouge-l']:.4f}\")\n    \n    # 히스토리 업데이트\n    training_history['val_loss'].append(val_loss)\n    training_history['rouge_1'].append(rouge_scores['rouge-1'])\n    training_history['rouge_2'].append(rouge_scores['rouge-2'])\n    training_history['rouge_l'].append(rouge_scores['rouge-l'])\n    \n    # WandB 로깅\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss_epoch': train_loss,\n            'val_loss': val_loss,\n            'rouge_1': rouge_scores['rouge-1'],\n            'rouge_2': rouge_scores['rouge-2'],\n            'rouge_l': rouge_scores['rouge-l']\n        })\n    \n    # 샘플 예측 출력\n    if sample_preds:\n        logger.write(\"\\nSample Predictions:\")\n        for i, pred in enumerate(sample_preds[:2]):\n            logger.write(f\"  Sample {i+1}: {pred[:150]}...\")\n    \n    # Best model 저장\n    if rouge_scores['rouge-l'] > best_rouge_l:\n        best_rouge_l = rouge_scores['rouge-l']\n        patience_counter = 0\n        \n        # 모델 저장\n        if hasattr(model, 'module'):\n            model_to_save = model.module\n        else:\n            model_to_save = model\n        \n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model_to_save.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'rouge_scores': rouge_scores,\n            'config': config,\n            'model_config': model_config\n        }, best_model_path)\n        \n        logger.write(f\"✓ New best model saved! (ROUGE-L: {best_rouge_l:.4f})\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            logger.write(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n            break\n    \n    # 메모리 정리\n    torch.cuda.empty_cache()\n    gc.collect()\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Training completed!\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(\"=\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 테스트 데이터 예측 및 제출 파일 생성",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 제출 파일 생성\nsubmission_df = pd.DataFrame({\n    'fname': test_df['fname'],\n    'summary': test_predictions\n})\n\n# 제출 파일 저장 - config의 경로 사용\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'{current_model}_submission_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\n# index=True로 설정하여 인덱스를 포함시킴\nsubmission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=False -> index=True로 변경\nlogger.write(f\"\\nSubmission file saved: {submission_path}\")\n\n# 제출 파일 확인\nprint(f\"\\nSubmission file created: {submission_filename}\")\nprint(f\"Shape: {submission_df.shape}\")\nprint(\"\\nFirst 3 submissions:\")\nprint(submission_df.head(3))\n\n# 실험 요약\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"SINGLE MODEL EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Model: {current_model} ({model_config['name']})\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(f\"Training epochs completed: {len(training_history['train_loss'])}\")\nif training_history['train_loss']:\n    logger.write(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\nif training_history['val_loss']:\n    logger.write(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\nlogger.write(f\"Submission file: {submission_filename}\")\nlogger.write(\"=\"*50)\n\n# 시각화\nif config.get('visualization', {}).get('enabled', False):\n    from src.utils.visualizations.training_viz import TrainingVisualizer\n    \n    viz = TrainingVisualizer()\n    \n    # 시각화 저장 경로\n    viz_dir = get_path(config.get('visualization', {}).get('save_path', 'visualizations'))\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    \n    # 학습 히스토리 플롯\n    if len(training_history['train_loss']) > 0:\n        viz.plot_training_history(\n            training_history,\n            save_path=viz_dir / f'{current_model}_training_history.png'\n        )\n        logger.write(f\"Visualization saved to {viz_dir}\")\n\n# WandB 종료\nif config['wandb']['mode'] != 'disabled':\n    wandb.finish()\n\nlogger.write(f\"\\n✅ Single model experiment ({current_model}) completed successfully!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 제출 파일 생성\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'summary': test_predictions\n})\n\n# 제출 파일 저장 - config의 경로 사용\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'{current_model}_submission_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\nsubmission_df.to_csv(submission_path, index=False, encoding='utf-8')\nlogger.write(f\"\\nSubmission file saved: {submission_path}\")\n\n# 제출 파일 확인\nprint(f\"\\nSubmission file created: {submission_filename}\")\nprint(f\"Shape: {submission_df.shape}\")\nprint(\"\\nFirst 3 submissions:\")\nprint(submission_df.head(3))\n\n# 실험 요약\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"SINGLE MODEL EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Model: {current_model} ({model_config['name']})\")\nlogger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\nlogger.write(f\"Training epochs completed: {len(training_history['train_loss'])}\")\nif training_history['train_loss']:\n    logger.write(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\nif training_history['val_loss']:\n    logger.write(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\nlogger.write(f\"Submission file: {submission_filename}\")\nlogger.write(\"=\"*50)\n\n# 시각화\nif config.get('visualization', {}).get('enabled', False):\n    from src.utils.visualizations.training_viz import TrainingVisualizer\n    \n    viz = TrainingVisualizer()\n    \n    # 시각화 저장 경로\n    viz_dir = get_path(config.get('visualization', {}).get('save_path', 'visualizations'))\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    \n    # 학습 히스토리 플롯\n    if len(training_history['train_loss']) > 0:\n        viz.plot_training_history(\n            training_history,\n            save_path=viz_dir / f'{current_model}_training_history.png'\n        )\n        logger.write(f\"Visualization saved to {viz_dir}\")\n\n# WandB 종료\nif config['wandb']['mode'] != 'disabled':\n    wandb.finish()\n\nlogger.write(f\"\\n✅ Single model experiment ({current_model}) completed successfully!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}