{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≠ Îã§Ï§ë Î™®Îç∏ ÏïôÏÉÅÎ∏î - 5Í∞ú Î™®Îç∏ ÌÜµÌï©\n",
    "> PRD Í≥ÑÌöçÏóê Îî∞Î•∏ 5Í∞ú Î™®Îç∏ ÏïôÏÉÅÎ∏î + TTA Ï†ÑÎûµ\n",
    "\n",
    "**Î™©Ìëú ÏÑ±Îä•**: ROUGE-F1 75-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "‚úÖ ÎÇòÎàîÍ≥†Îîï Ìè∞Ìä∏ Î°úÎìú ÏÑ±Í≥µ\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3Î≤àÎßå parent ÏÇ¨Ïö©!\n",
    "\n",
    "# Îã§Î•∏ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï†úÍ±∞ÌïòÍ≥† ÌòÑÏû¨ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°úÎßå Ï∂îÍ∞Ä\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# Ïª§Ïä§ÌÖÄ Î™®Îìà ÏûÑÌè¨Ìä∏\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Models (5):\n",
      "  - solar: weight=0.30\n",
      "  - polyglot: weight=0.25\n",
      "  - kullm: weight=0.20\n",
      "  - kobart: weight=0.15\n",
      "  - koalpaca: weight=0.10\n"
     ]
    }
   ],
   "source": [
    "# ÏÑ§Ï†ï ÌååÏùº Î°úÎìú\n",
    "config_path = notebook_dir / 'configs' / 'config_multi_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ÌôúÏÑ±ÌôîÎêú Î™®Îç∏ ÌôïÏù∏\n",
    "enabled_models = [name for name, cfg in config['ensemble_models'].items() if cfg['enabled']]\n",
    "print(f\"Enabled Models ({len(enabled_models)}):\")\n",
    "for model_name in enabled_models:\n",
    "    weight = config['ensemble_models'][model_name]['weight']\n",
    "    print(f\"  - {model_name}: weight={weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Multi-Model Ensemble Experiment\n",
      "Timestamp: 20251010_090607\n",
      "Models: ['solar', 'polyglot', 'kullm', 'kobart', 'koalpaca']\n",
      "Ensemble Method: weighted_average\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "# configÏùò Î°úÍ∑∏ Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configÏùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configÏóê log_dirÏù¥ Ï†ïÏùòÎêòÏñ¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í\n",
    "if 'log_dir' in config['paths']:\n",
    "    log_dir = get_path(config['paths']['log_dir'])\n",
    "else:\n",
    "    # Í∏∞Î≥∏Í∞í: notebook_dir/logs/multi_model\n",
    "    log_dir = notebook_dir / 'logs' / 'multi_model'\n",
    "\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÏÉùÏÑ±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Î°úÍ±∞ Ï¥àÍ∏∞Ìôî\n",
    "log_file = log_dir / f'ensemble_{len(enabled_models)}models_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('Multi-Model Ensemble Experiment')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Models: {enabled_models}')\n",
    "logger.write(f'Ensemble Method: {config[\"ensemble_strategy\"][\"method\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TTA Configuration:\n",
      "  - Augmentations: 3\n",
      "  - Aggregation: mean\n",
      "  - paraphrase: enabled\n",
      "TTA: paraphrase enabled\n",
      "  - reorder: enabled\n",
      "TTA: reorder enabled\n"
     ]
    }
   ],
   "source": [
    "# TTA ÏÑ§Ï†ï ÌôïÏù∏\n",
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\nTTA Configuration:\")\n",
    "    logger.write(f\"  - Augmentations: {config['tta']['num_augmentations']}\")\n",
    "    logger.write(f\"  - Aggregation: {config['tta']['aggregation']}\")\n",
    "    \n",
    "    for technique, settings in config['tta']['techniques'].items():\n",
    "        if settings['enabled']:\n",
    "            logger.write(f\"  - {technique}: enabled\")\n",
    "            print(f\"TTA: {technique} enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n",
      "Will clear GPU cache between models\n"
     ]
    }
   ],
   "source": [
    "# GPU Ï≤¥ÌÅ¨ Î∞è Î©ÄÌã∞ GPU ÏÑ§Ï†ï\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    \n",
    "    # Î©ÄÌã∞ GPU Ï≤¥ÌÅ¨\n",
    "    if config['gpu']['multi_gpu']['enabled'] and torch.cuda.device_count() > 1:\n",
    "        logger.write(f\"Multi-GPU available: {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏÑ§Ï†ï\n",
    "    if config['gpu']['empty_cache_between_models']:\n",
    "        logger.write(\"Will clear GPU cache between models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight visualization saved to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/multi_model/visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109226/3820716939.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπò ÏãúÍ∞ÅÌôî\n",
    "weights = [config['ensemble_models'][name]['weight'] for name in enabled_models]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(enabled_models, weights, color=colors[:len(enabled_models)])\n",
    "plt.title('Ensemble Model Weights Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Weight', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "\n",
    "# Í∞ÄÏ§ëÏπò Í∞í ÌëúÏãú\n",
    "for bar, weight in zip(bars, weights):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{weight:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî Ï†ÄÏû• - config Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "if 'visualization_dir' in config['paths']:\n",
    "    viz_dir = get_path(config['paths']['visualization_dir'])\n",
    "else:\n",
    "    # Í∏∞Î≥∏Í∞í\n",
    "    viz_dir = log_dir / 'visualizations'\n",
    "\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(viz_dir / f'ensemble_weights_{timestamp}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.write(f\"Weight visualization saved to {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solar API Comparison Settings:\n",
      "  - API Key: up_rMJWNzz...\n",
      "  - Use as baseline: True\n",
      "  - Include in ensemble: False\n",
      "\n",
      "Solar API configured for comparison\n"
     ]
    }
   ],
   "source": [
    "# Solar API ÎπÑÍµê ÏÑ§Ï†ï\n",
    "if config['solar_api_comparison']['enabled']:\n",
    "    logger.write(\"\\nSolar API Comparison Settings:\")\n",
    "    logger.write(f\"  - API Key: {config['solar_api_comparison']['api_key'][:10]}...\")\n",
    "    logger.write(f\"  - Use as baseline: {config['solar_api_comparison']['use_as_baseline']}\")\n",
    "    logger.write(f\"  - Include in ensemble: {config['solar_api_comparison']['include_in_ensemble']}\")\n",
    "    \n",
    "    print(\"\\nSolar API configured for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Weight Optimization:\n",
      "  - Trials: 50\n",
      "  - Study: ensemble_weight_optimization\n",
      "  - Metric: rouge_l\n",
      "Optuna configured for ensemble weight optimization\n"
     ]
    }
   ],
   "source": [
    "# Optuna ÏµúÏ†ÅÌôî ÏÑ§Ï†ï (ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπò)\n",
    "if config['optuna']['enabled']:\n",
    "    logger.write(\"\\nOptuna Weight Optimization:\")\n",
    "    logger.write(f\"  - Trials: {config['optuna']['n_trials']}\")\n",
    "    logger.write(f\"  - Study: {config['optuna']['study_name']}\")\n",
    "    logger.write(f\"  - Metric: {config['optuna']['metric']}\")\n",
    "    \n",
    "    import optuna\n",
    "    print(\"Optuna configured for ensemble weight optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Î™®Îç∏Î≥Ñ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
    "\n",
    "Í∞Å Î™®Îç∏ÏùÑ Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú ÌïôÏäµÌïòÍ≥† ÌèâÍ∞ÄÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Loading ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "\n",
      "Data loaded successfully!\n",
      "  - Train samples: 12457\n",
      "  - Dev samples: 499\n",
      "  - Test samples: 499\n",
      "\n",
      "Train data topics:\n",
      "topic\n",
      "ÏùåÏãù Ï£ºÎ¨∏     130\n",
      "Ï∑®ÏóÖ Î©¥Ï†ë     109\n",
      "Í∏∏ ÏïàÎÇ¥       66\n",
      "Ìò∏ÌÖî Ï≤¥ÌÅ¨Ïù∏     40\n",
      "ÏïÑÌååÌä∏ ÏûÑÎåÄ     30\n",
      "ÏùºÏÉÅ ÎåÄÌôî      29\n",
      "ÏáºÌïë         27\n",
      "Ï£ºÎßê Í≥ÑÌöç      26\n",
      "Î©¥Ï†ë         25\n",
      "Ìò∏ÌÖî ÏòàÏïΩ      25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First dialogue (200 chars):\n",
      "#Person1#: ÏïàÎÖïÌïòÏÑ∏Ïöî, Mr. Smith. Ï†ÄÎäî Dr. HawkinsÏûÖÎãàÎã§. Ïò§Îäò Î¨¥Ïä® ÏùºÎ°ú Ïò§ÏÖ®Ïñ¥Ïöî? \n",
      "#Person2#: Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏúºÎ†§Í≥† ÏôîÏñ¥Ïöî. \n",
      "#Person1#: ÎÑ§, 5ÎÖÑ ÎèôÏïà Í≤ÄÏßÑÏùÑ Ïïà Î∞õÏúºÏÖ®ÎÑ§Ïöî. Îß§ÎÖÑ Ìïú Î≤àÏî© Î∞õÏúºÏÖîÏïº Ìï¥Ïöî. \n",
      "#Person2#: ÏïåÏ£†. ÌäπÎ≥ÑÌûà ÏïÑÌîà Îç∞Í∞Ä ÏóÜÏúºÎ©¥ Íµ≥Ïù¥ Í∞à ÌïÑÏöîÍ∞Ä ÏóÜÎã§Í≥† ÏÉùÍ∞ÅÌñàÏñ¥Ïöî. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "# config ÌååÏùºÏùò Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configÏùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(\"\\n=== Data Loading ===\")\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"\\nData loaded successfully!\")\n",
    "logger.write(f\"  - Train samples: {len(train_df)}\")\n",
    "logger.write(f\"  - Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"  - Test samples: {len(test_df)}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå Ï∂úÎ†•\n",
    "print(\"\\nTrain data topics:\")\n",
    "print(train_df['topic'].value_counts().head(10))\n",
    "print(f\"\\nFirst dialogue (200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== solar Model ===\n",
      "  - Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "  - Weight: 0.3\n",
      "  - LoRA: r=16, alpha=32\n",
      "Would train solar here...\n",
      "\n",
      "=== polyglot Model ===\n",
      "  - Model: EleutherAI/polyglot-ko-12.8b\n",
      "  - Weight: 0.25\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train polyglot here...\n",
      "\n",
      "=== kullm Model ===\n",
      "  - Model: nlpai-lab/kullm-v2\n",
      "  - Weight: 0.2\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train kullm here...\n",
      "\n",
      "=== kobart Model ===\n",
      "  - Model: digit82/kobart-summarization\n",
      "  - Weight: 0.15\n",
      "Would train kobart here...\n",
      "\n",
      "=== koalpaca Model ===\n",
      "  - Model: beomi/KoAlpaca-Polyglot-12.8B\n",
      "  - Weight: 0.1\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train koalpaca here...\n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏Î≥Ñ Í≤∞Í≥º Ï†ÄÏû•\n",
    "model_results = {}\n",
    "\n",
    "for model_name in enabled_models:\n",
    "    logger.write(f\"\\n=== {model_name} Model ===\")\n",
    "    model_config = config['ensemble_models'][model_name]\n",
    "    logger.write(f\"  - Model: {model_config['name']}\")\n",
    "    logger.write(f\"  - Weight: {model_config['weight']}\")\n",
    "    \n",
    "    if model_config.get('use_lora', False):\n",
    "        logger.write(f\"  - LoRA: r={model_config['lora_config']['r']}, alpha={model_config['lora_config']['alpha']}\")\n",
    "    \n",
    "    # Ïã§Ï†ú ÌïôÏäµ ÏΩîÎìúÎäî Ïó¨Í∏∞Ïóê Íµ¨ÌòÑ\n",
    "    print(f\"Would train {model_name} here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA (Text Test Augmentation)\n",
    "\n",
    "ÌÖçÏä§Ìä∏ Ï¶ùÍ∞ïÏùÑ ÌÜµÌïú ÏÑ±Îä• Ìñ•ÏÉÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TTA Implementation ===\n",
      "Paraphrase augmentation enabled\n",
      "  - Model: lcw99/t5-base-korean-paraphrase\n",
      "  - Variants: 2\n",
      "TTA would be applied here...\n"
     ]
    }
   ],
   "source": [
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\n=== TTA Implementation ===\")\n",
    "    \n",
    "    # Paraphrase\n",
    "    if config['tta']['techniques']['paraphrase']['enabled']:\n",
    "        logger.write(\"Paraphrase augmentation enabled\")\n",
    "        logger.write(f\"  - Model: {config['tta']['techniques']['paraphrase']['model']}\")\n",
    "        logger.write(f\"  - Variants: {config['tta']['techniques']['paraphrase']['num_variants']}\")\n",
    "    \n",
    "    # Ïã§Ï†ú TTA Íµ¨ÌòÑÏùÄ Ïó¨Í∏∞Ïóê\n",
    "    print(\"TTA would be applied here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏïôÏÉÅÎ∏î Î∞è ÏµúÏ¢Ö ÏòàÏ∏°\n",
    "\n",
    "Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏°ÏùÑ ÏïôÏÉÅÎ∏îÌïòÏó¨ ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
