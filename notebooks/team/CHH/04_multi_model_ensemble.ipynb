{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” - 5ê°œ ëª¨ë¸ í†µí•©\n",
    "> PRD ê³„íšì— ë”°ë¥¸ 5ê°œ ëª¨ë¸ ì•™ìƒë¸” + TTA ì „ëµ\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 75-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "âœ… ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ë¡œë“œ ì„±ê³µ\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Models (5):\n",
      "  - solar: weight=0.30\n",
      "  - polyglot: weight=0.25\n",
      "  - kullm: weight=0.20\n",
      "  - kobart: weight=0.15\n",
      "  - koalpaca: weight=0.10\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_multi_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# í™œì„±í™”ëœ ëª¨ë¸ í™•ì¸\n",
    "enabled_models = [name for name, cfg in config['ensemble_models'].items() if cfg['enabled']]\n",
    "print(f\"Enabled Models ({len(enabled_models)}):\")\n",
    "for model_name in enabled_models:\n",
    "    weight = config['ensemble_models'][model_name]['weight']\n",
    "    print(f\"  - {model_name}: weight={weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Multi-Model Ensemble Experiment\n",
      "Timestamp: 20251010_090607\n",
      "Models: ['solar', 'polyglot', 'kullm', 'kobart', 'koalpaca']\n",
      "Ensemble Method: weighted_average\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "# configì˜ ë¡œê·¸ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì— log_dirì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "if 'log_dir' in config['paths']:\n",
    "    log_dir = get_path(config['paths']['log_dir'])\n",
    "else:\n",
    "    # ê¸°ë³¸ê°’: notebook_dir/logs/multi_model\n",
    "    log_dir = notebook_dir / 'logs' / 'multi_model'\n",
    "\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'ensemble_{len(enabled_models)}models_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('Multi-Model Ensemble Experiment')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Models: {enabled_models}')\n",
    "logger.write(f'Ensemble Method: {config[\"ensemble_strategy\"][\"method\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TTA Configuration:\n",
      "  - Augmentations: 3\n",
      "  - Aggregation: mean\n",
      "  - paraphrase: enabled\n",
      "TTA: paraphrase enabled\n",
      "  - reorder: enabled\n",
      "TTA: reorder enabled\n"
     ]
    }
   ],
   "source": [
    "# TTA ì„¤ì • í™•ì¸\n",
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\nTTA Configuration:\")\n",
    "    logger.write(f\"  - Augmentations: {config['tta']['num_augmentations']}\")\n",
    "    logger.write(f\"  - Aggregation: {config['tta']['aggregation']}\")\n",
    "    \n",
    "    for technique, settings in config['tta']['techniques'].items():\n",
    "        if settings['enabled']:\n",
    "            logger.write(f\"  - {technique}: enabled\")\n",
    "            print(f\"TTA: {technique} enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n",
      "Will clear GPU cache between models\n"
     ]
    }
   ],
   "source": [
    "# GPU ì²´í¬ ë° ë©€í‹° GPU ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    \n",
    "    # ë©€í‹° GPU ì²´í¬\n",
    "    if config['gpu']['multi_gpu']['enabled'] and torch.cuda.device_count() > 1:\n",
    "        logger.write(f\"Multi-GPU available: {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •\n",
    "    if config['gpu']['empty_cache_between_models']:\n",
    "        logger.write(\"Will clear GPU cache between models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight visualization saved to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/multi_model/visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109226/3820716939.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì‹œê°í™”\n",
    "weights = [config['ensemble_models'][name]['weight'] for name in enabled_models]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(enabled_models, weights, color=colors[:len(enabled_models)])\n",
    "plt.title('Ensemble Model Weights Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Weight', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ê°’ í‘œì‹œ\n",
    "for bar, weight in zip(bars, weights):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{weight:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ì‹œê°í™” ì €ì¥ - config ê²½ë¡œ ì‚¬ìš©\n",
    "if 'visualization_dir' in config['paths']:\n",
    "    viz_dir = get_path(config['paths']['visualization_dir'])\n",
    "else:\n",
    "    # ê¸°ë³¸ê°’\n",
    "    viz_dir = log_dir / 'visualizations'\n",
    "\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(viz_dir / f'ensemble_weights_{timestamp}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.write(f\"Weight visualization saved to {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solar API Comparison Settings:\n",
      "  - API Key: up_rMJWNzz...\n",
      "  - Use as baseline: True\n",
      "  - Include in ensemble: False\n",
      "\n",
      "Solar API configured for comparison\n"
     ]
    }
   ],
   "source": [
    "# Solar API ë¹„êµ ì„¤ì •\n",
    "if config['solar_api_comparison']['enabled']:\n",
    "    logger.write(\"\\nSolar API Comparison Settings:\")\n",
    "    logger.write(f\"  - API Key: {config['solar_api_comparison']['api_key'][:10]}...\")\n",
    "    logger.write(f\"  - Use as baseline: {config['solar_api_comparison']['use_as_baseline']}\")\n",
    "    logger.write(f\"  - Include in ensemble: {config['solar_api_comparison']['include_in_ensemble']}\")\n",
    "    \n",
    "    print(\"\\nSolar API configured for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Weight Optimization:\n",
      "  - Trials: 50\n",
      "  - Study: ensemble_weight_optimization\n",
      "  - Metric: rouge_l\n",
      "Optuna configured for ensemble weight optimization\n"
     ]
    }
   ],
   "source": [
    "# Optuna ìµœì í™” ì„¤ì • (ì•™ìƒë¸” ê°€ì¤‘ì¹˜)\n",
    "if config['optuna']['enabled']:\n",
    "    logger.write(\"\\nOptuna Weight Optimization:\")\n",
    "    logger.write(f\"  - Trials: {config['optuna']['n_trials']}\")\n",
    "    logger.write(f\"  - Study: {config['optuna']['study_name']}\")\n",
    "    logger.write(f\"  - Metric: {config['optuna']['metric']}\")\n",
    "    \n",
    "    import optuna\n",
    "    print(\"Optuna configured for ensemble weight optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ë³„ í•™ìŠµ ë° í‰ê°€\n",
    "\n",
    "ê° ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Loading ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "\n",
      "Data loaded successfully!\n",
      "  - Train samples: 12457\n",
      "  - Dev samples: 499\n",
      "  - Test samples: 499\n",
      "\n",
      "Train data topics:\n",
      "topic\n",
      "ìŒì‹ ì£¼ë¬¸     130\n",
      "ì·¨ì—… ë©´ì ‘     109\n",
      "ê¸¸ ì•ˆë‚´       66\n",
      "í˜¸í…” ì²´í¬ì¸     40\n",
      "ì•„íŒŒíŠ¸ ì„ëŒ€     30\n",
      "ì¼ìƒ ëŒ€í™”      29\n",
      "ì‡¼í•‘         27\n",
      "ì£¼ë§ ê³„íš      26\n",
      "ë©´ì ‘         25\n",
      "í˜¸í…” ì˜ˆì•½      25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First dialogue (200 chars):\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "# config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(\"\\n=== Data Loading ===\")\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"\\nData loaded successfully!\")\n",
    "logger.write(f\"  - Train samples: {len(train_df)}\")\n",
    "logger.write(f\"  - Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"  - Test samples: {len(test_df)}\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nTrain data topics:\")\n",
    "print(train_df['topic'].value_counts().head(10))\n",
    "print(f\"\\nFirst dialogue (200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== solar Model ===\n",
      "  - Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "  - Weight: 0.3\n",
      "  - LoRA: r=16, alpha=32\n",
      "Would train solar here...\n",
      "\n",
      "=== polyglot Model ===\n",
      "  - Model: EleutherAI/polyglot-ko-12.8b\n",
      "  - Weight: 0.25\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train polyglot here...\n",
      "\n",
      "=== kullm Model ===\n",
      "  - Model: nlpai-lab/kullm-v2\n",
      "  - Weight: 0.2\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train kullm here...\n",
      "\n",
      "=== kobart Model ===\n",
      "  - Model: digit82/kobart-summarization\n",
      "  - Weight: 0.15\n",
      "Would train kobart here...\n",
      "\n",
      "=== koalpaca Model ===\n",
      "  - Model: beomi/KoAlpaca-Polyglot-12.8B\n",
      "  - Weight: 0.1\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train koalpaca here...\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥\n",
    "model_results = {}\n",
    "\n",
    "for model_name in enabled_models:\n",
    "    logger.write(f\"\\n=== {model_name} Model ===\")\n",
    "    model_config = config['ensemble_models'][model_name]\n",
    "    logger.write(f\"  - Model: {model_config['name']}\")\n",
    "    logger.write(f\"  - Weight: {model_config['weight']}\")\n",
    "    \n",
    "    if model_config.get('use_lora', False):\n",
    "        logger.write(f\"  - LoRA: r={model_config['lora_config']['r']}, alpha={model_config['lora_config']['alpha']}\")\n",
    "    \n",
    "    # ì‹¤ì œ í•™ìŠµ ì½”ë“œëŠ” ì—¬ê¸°ì— êµ¬í˜„\n",
    "    print(f\"Would train {model_name} here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA (Text Test Augmentation)\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì¦ê°•ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TTA Implementation ===\n",
      "Paraphrase augmentation enabled\n",
      "  - Model: lcw99/t5-base-korean-paraphrase\n",
      "  - Variants: 2\n",
      "TTA would be applied here...\n"
     ]
    }
   ],
   "source": [
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\n=== TTA Implementation ===\")\n",
    "    \n",
    "    # Paraphrase\n",
    "    if config['tta']['techniques']['paraphrase']['enabled']:\n",
    "        logger.write(\"Paraphrase augmentation enabled\")\n",
    "        logger.write(f\"  - Model: {config['tta']['techniques']['paraphrase']['model']}\")\n",
    "        logger.write(f\"  - Variants: {config['tta']['techniques']['paraphrase']['num_variants']}\")\n",
    "    \n",
    "    # ì‹¤ì œ TTA êµ¬í˜„ì€ ì—¬ê¸°ì—\n",
    "    print(\"TTA would be applied here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì•™ìƒë¸” ë° ìµœì¢… ì˜ˆì¸¡\n",
    "\n",
    "ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
