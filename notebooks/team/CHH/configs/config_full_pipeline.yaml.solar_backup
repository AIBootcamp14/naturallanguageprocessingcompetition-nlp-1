augmentation:
  enabled: true
  target_ratio: 1.5
  techniques:
    back_translation:
      enabled: true
      languages:
      - en
      - ja
      translation_model: Helsinki-NLP/opus-mt
    dialogue_reordering:
      enabled: false
    paraphrase:
      enabled: true
      model: lcw99/t5-base-korean-paraphrase
      num_variants: 2
      quality_threshold: 0.7
    token_replacement:
      enabled: true
      preserve_entities: true
      replacement_ratio: 0.15
cross_validation:
  enabled: true
  ensemble_folds: true
  n_splits: 5
  random_state: 42
  save_all_folds: true
  shuffle: true
  train_all_folds: true
data_quality:
  checks:
    semantic:
      check_information_loss: true
      max_compression_ratio: 0.5
      min_compression_ratio: 0.1
    statistical:
      outlier_detection: true
      outlier_method: isolation_forest
      outlier_threshold: 0.05
    structural:
      check_duplicates: true
      check_encoding: true
      check_nulls: true
  enabled: true
  handle_issues:
    fix_encoding: true
    remove_duplicates: true
    remove_outliers: true
deployment:
  monitoring:
    enabled: true
    health_check_interval: 60
    metrics_port: 9090
  serving:
    framework: fastapi
    port: 8000
    workers: 4
  versioning:
    enabled: true
    registry: local
ensemble:
  advanced:
    blending:
      enabled: false
      validation_size: 0.2
    stacking:
      cv_folds: 3
      enabled: true
      meta_learner: lgbm
  base_method: weighted_average
  test_time_augmentation:
    aggregation: mean
    enabled: true
    num_augmentations: 5
experiment:
  checkpointing:
    keep_last_n: 3
    save_best_only: false
    save_every_n_epochs: 1
  description: 모든 최적화 기법이 적용된 최종 파이프라인
  name: full_pipeline_v1
  results_format:
  - json
  - csv
  - pickle
  save_all_results: true
  timestamp: true
  version: 1.0.0
gpu:
  auto_optimization:
    enabled: true
    find_optimal_batch_size: true
    gradient_accumulation_auto: true
  cuda_device: 0
  device: cuda
  gpu_check_path: ../../../src/utils/gpu_optimization/team_gpu_check.py
  memory_fraction: 0.95
  mixed_precision: true
  use_gpu_optimization: true
hyperparameter_optimization:
  direction: maximize
  enabled: true
  metric: rouge_l
  n_trials: 100
  pruner: MedianPruner
  sampler: TPESampler
  search_space:
    batch_size:
      choices:
      - 4
      - 8
      - 16
      type: categorical
    learning_rate:
      high: 0.001
      log: true
      low: 1.0e-05
      type: float
    lora_alpha:
      high: 128
      low: 8
      step: 8
      type: int
    lora_r:
      high: 64
      low: 4
      step: 4
      type: int
    num_beams:
      high: 8
      low: 2
      type: int
    temperature:
      high: 1.0
      low: 0.1
      type: float
inference_optimization:
  batch_inference:
    dynamic_batching: true
    enabled: true
    optimal_batch_size: auto
  onnx_conversion:
    enabled: true
    optimize: true
    quantization: dynamic
  tensorrt:
    enabled: false
    precision: fp16
logging:
  format: '%(asctime)s - [%(pipeline_stage)s] - %(name)s - %(levelname)s - %(message)s'
  level: INFO
  loggers:
  - pipeline
  - training
  - evaluation
  - inference
  notebook_logger_path: ../../../src/logging/notebook_logger.py
  save_to_file: true
  use_notebook_logger: true
models:
  auxiliary_models:
  - model_type: seq2seq
    name: digit82/kobart-summarization
    weight: 0.1
  - model_type: seq2seq
    name: gogamza/kobart-summarization
    weight: 0.05
  primary_models:
  - lora_alpha: 32
    lora_r: 16
    name: upstage/SOLAR-10.7B-Instruct-v1.0
    use_lora: true
    weight: 0.35
  - lora_alpha: 16
    lora_r: 8
    name: EleutherAI/polyglot-ko-12.8b
    use_lora: true
    weight: 0.3
  - lora_alpha: 16
    lora_r: 8
    name: nlpai-lab/kullm-v2
    use_lora: true
    weight: 0.2
paths:
  augmented_data_dir: ./data/augmented
  cache_dir: ./cache/full_pipeline
  data_dir: ../../../data/raw
  dev_file: ../../../data/raw/dev.csv
  log_dir: ./logs/full_pipeline
  output_dir: ./models/full_pipeline
  preprocessed_data_dir: ./data/preprocessed
  submission_dir: ./submissions/full_pipeline
  test_file: ../../../data/raw/test.csv
  train_file: ../../../data/raw/train.csv
  visualization_dir: ./logs/full_pipeline/visualizations
performance_targets:
  overall: 0.85
  rouge_1: 0.45
  rouge_2: 0.3
  rouge_l: 0.4
pipeline:
  parallel_stages:
  - - model_training
    - prompt_optimization
  - - cross_validation
    - tta_preparation
  stages:
  - data_quality_check
  - data_preprocessing
  - data_augmentation
  - model_training
  - cross_validation
  - ensemble
  - hyperparameter_optimization
  - inference_optimization
  - final_prediction
post_processing:
  grammar_correction:
    enabled: true
    tool: py-hanspell
  length_adjustment:
    max_length: 150
    min_length: 30
    target_length: 80
  quality_check:
    check_coherence: true
    check_completeness: true
    min_rouge_score: 0.3
preprocessing:
  data_split:
    seed: 42
    stratify: false
    validation_ratio: 0.1
  noise_removal:
    fix_escaped_chars: true
    normalize_whitespace: true
    remove_html_tags: true
    remove_special_tokens: false
  token_normalization:
    masking_tokens:
      preserve: true
      tokens:
      - '#PhoneNumber#'
      - '#Address#'
      - '#SSN#'
      - '#Email#'
    person_tokens:
      format: '#Person{id}#'
      standardize: true
prompt_engineering:
  ab_testing:
    enabled: true
    num_variants: 5
    selection_metric: rouge_l
  enabled: true
  templates:
    chain_of_thought:
      enabled: true
      template: '다음 대화를 단계적으로 분석하여 요약하세요.


        1단계: 주요 주제 파악

        2단계: 핵심 정보 추출

        3단계: 간결한 요약 작성


        대화: {dialogue}


        분석 및 요약:

        '
    few_shot:
      enabled: true
      example_selection: random
      num_examples: 3
    zero_shot:
      enabled: true
      template: '다음 대화를 3-5문장으로 요약하세요:

        {dialogue}


        요약:

        '
reproducibility:
  benchmark: false
  deterministic: true
  seed: 42
  worker_init_fn: true
solar_api:
  api_key: up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT
  enabled: true
  hybrid_approach:
    confidence_threshold: 0.7
    use_for_difficult_samples: true
    use_for_validation: true
  optimization:
    batch_size: 10
    cache_responses: true
    token_budget: 100000
training:
  batch_size: 4
  early_stopping:
    metric: eval_rouge_l
    patience: 3
    threshold: 0.001
  fp16: true
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  learning_rate: 2e-5
  max_grad_norm: 0.3
  num_epochs: 5
  scheduler: cosine
  seed: 42
  warmup_ratio: 0.1
  weight_decay: 0.01
visualization:
  enabled: true
  plots:
  - training_curves
  - model_comparison
  - confusion_matrix
  - rouge_distribution
  - sample_difficulty_heatmap
  - ensemble_weights
  - hyperparameter_importance
  save_path: ../logs/full_pipeline/visualizations
  training_viz_path: ../../../src/utils/visualizations/training_viz.py
  use_training_viz: true
wandb:
  entity: ieyeppo
  log_artifacts: true
  log_datasets: true
  log_models: true
  mode: online
  name: full-pipeline-integrated
  notes: 모든 기법이 통합된 최종 파이프라인
  project: nlp-competition
  tags:
  - full_pipeline
  - production
  - all_techniques
