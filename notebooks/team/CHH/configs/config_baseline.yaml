# ============================================================================
# 베이스라인 구현 설정 파일
# ============================================================================
# 용도: KoBART 기반 기본 모델 학습
# 설명: 프로젝트의 기본 성능을 측정하기 위한 베이스라인 실험
# 최종 수정: 2025-10-10
# ============================================================================

# ----------------------------------------------------------------------------
# 경로 설정 (Path Configuration)
# ----------------------------------------------------------------------------
paths:
  data_dir: "../../../data/raw"  # 원본 데이터 디렉토리 경로 (상대 경로)
  train_file: "../../../data/raw/train.csv"  # 학습 데이터 파일 경로
  dev_file: "../../../data/raw/dev.csv"  # 검증 데이터 파일 경로
  test_file: "../../../data/raw/test.csv"  # 테스트 데이터 파일 경로
  output_dir: "./models/baseline"  # 학습된 모델 저장 디렉토리
  log_dir: "./logs/baseline"  # 로그 파일 저장 디렉토리
  submission_dir: "./submissions/baseline"  # 제출 파일 저장 디렉토리
  visualization_dir: "./logs/baseline/visualizations"  # 시각화 파일 저장 디렉토리

# ----------------------------------------------------------------------------
# 모델 설정 (Model Configuration)
# ----------------------------------------------------------------------------
model:
  name: "digit82/kobart-summarization"  # 사용할 모델 이름 (Hugging Face 모델 ID)
  model_type: "bart"  # 모델 타입 (BART = Bidirectional and Auto-Regressive Transformers)
  max_input_length: 512  # 입력 시퀀스 최대 길이 (토큰 단위)
  max_target_length: 100  # 출력 시퀀스 최대 길이 (토큰 단위)
  use_cache: true  # 모델 캐시 사용 여부 (추론 속도 향상)

# ----------------------------------------------------------------------------
# 토크나이저 설정 (Tokenizer Configuration)
# ----------------------------------------------------------------------------
tokenizer:
  name: "digit82/kobart-summarization"  # 사용할 토크나이저 (모델과 동일)
  max_length: 512  # 토크나이저 최대 길이
  padding: "max_length"  # 패딩 전략 (max_length: 최대 길이까지 패딩)
  truncation: true  # 최대 길이 초과 시 잘라내기 활성화

  # 특수 토큰 목록: 대화 참여자 및 개인정보 마스킹용
  special_tokens:
    - "#Person1#"  # 대화 참여자 1 식별 토큰
    - "#Person2#"  # 대화 참여자 2 식별 토큰
    - "#Person3#"  # 대화 참여자 3 식별 토큰
    - "#Person4#"  # 대화 참여자 4 식별 토큰
    - "#Person5#"  # 대화 참여자 5 식별 토큰
    - "#Person6#"  # 대화 참여자 6 식별 토큰
    - "#Person7#"  # 대화 참여자 7 식별 토큰
    - "#PhoneNumber#"  # 전화번호 마스킹 토큰
    - "#Address#"  # 주소 마스킹 토큰
    - "#PassportNumber#"  # 여권번호 마스킹 토큰
    - "#SSN#"  # 주민등록번호 마스킹 토큰
    - "#CreditCard#"  # 신용카드번호 마스킹 토큰
    - "#CarNumber#"  # 차량번호 마스킹 토큰
    - "#Email#"  # 이메일 마스킹 토큰

# ----------------------------------------------------------------------------
# 학습 설정 (Training Configuration)
# ----------------------------------------------------------------------------
training:
  seed: 42  # 랜덤 시드 (재현성 보장)
  num_epochs: 3  # 전체 학습 에폭 수
  batch_size: 16  # GPU당 배치 크기
  gradient_accumulation_steps: 2  # 그래디언트 누적 스텝 (실제 배치 = 16 * 2 = 32)
  learning_rate: 5e-5  # 학습률 (5 x 10^-5)
  warmup_ratio: 0.1  # Warmup 비율 (전체 스텝의 10%)
  weight_decay: 0.01  # 가중치 감쇠 (L2 정규화 계수)
  max_grad_norm: 1.0  # 그래디언트 클리핑 최대 노름값 (그래디언트 폭발 방지)

  # ===== 옵티마이저 설정 =====
  optimizer: "adamw"  # 옵티마이저 종류 (AdamW = Adam + Weight Decay)
  adam_epsilon: 1e-8  # Adam 옵티마이저 epsilon 값 (수치 안정성)

  # ===== 스케줄러 설정 =====
  scheduler_type: "linear"  # 학습률 스케줄러 타입 (선형 감소)

  # ===== 조기 종료 설정 =====
  early_stopping_patience: 3  # 조기 종료 인내 횟수 (3 epoch 동안 개선 없으면 중단)
  early_stopping_threshold: 0.001  # 조기 종료 임계값 (개선 최소 기준)

  # ===== 체크포인트 설정 =====
  save_strategy: "epoch"  # 저장 전략 (매 에폭마다 저장)
  evaluation_strategy: "epoch"  # 평가 전략 (매 에폭마다 평가)
  save_total_limit: 3  # 최대 저장 체크포인트 수 (디스크 공간 절약)
  load_best_model_at_end: true  # 학습 종료 시 최고 성능 모델 로드
  metric_for_best_model: "eval_rouge_l"  # 최고 모델 선택 기준 메트릭

  # ===== Mixed Precision 설정 =====
  fp16: true  # FP16 (16비트 부동소수점) 학습 활성화
  fp16_opt_level: "O1"  # FP16 최적화 레벨 (O1 = Mixed Precision)

# ----------------------------------------------------------------------------
# 평가 설정 (Evaluation Configuration)
# ----------------------------------------------------------------------------
evaluation:
  # ===== 평가 메트릭 =====
  metrics:  # 사용할 평가 메트릭 목록
    - "rouge"  # ROUGE 점수 (요약 품질 평가)

  # ===== ROUGE 타입 =====
  rouge_types:  # 계산할 ROUGE 타입
    - "rouge-1"  # ROUGE-1 (단어 단위 겹침)
    - "rouge-2"  # ROUGE-2 (2-gram 겹침)
    - "rouge-l"  # ROUGE-L (최장 공통 부분 수열)

  # ===== Generation 파라미터 =====
  num_beams: 4  # Beam Search 빔 개수
  no_repeat_ngram_size: 2  # 반복 방지 n-gram 크기
  length_penalty: 1.0  # 길이 페널티 (1.0 = 중립)

# ----------------------------------------------------------------------------
# 추론 설정 (Inference Configuration)
# ----------------------------------------------------------------------------
inference:
  batch_size: 32  # 추론 배치 크기
  num_beams: 4  # Beam Search 빔 개수
  max_length: 100  # 생성 최대 길이 (토큰 단위)
  min_length: 10  # 생성 최소 길이 (토큰 단위)
  no_repeat_ngram_size: 2  # 반복 방지 n-gram 크기
  length_penalty: 1.0  # 길이 페널티 (1.0 = 중립, >1 = 짧게, <1 = 길게)
  early_stopping: true  # 조기 종료 활성화
  temperature: 1.0  # 샘플링 온도 (1.0 = 기본, >1 = 더 무작위, <1 = 더 결정적)
  top_k: 50  # Top-K 샘플링 (상위 50개 토큰 중 선택)
  top_p: 0.95  # Nucleus 샘플링 (누적 확률 95% 이내 토큰 선택)

# ----------------------------------------------------------------------------
# WandB 설정 (Weights & Biases)
# ----------------------------------------------------------------------------
wandb:
  project: "nlp-competition"  # WandB 프로젝트 이름
  entity: "ieyeppo"  # WandB 엔티티 (사용자/팀 이름)
  name: "baseline-kobart"  # 실험 이름
  tags:  # 실험 태그 (검색 및 필터링용)
    - "baseline"  # 베이스라인 실험
    - "kobart"  # KoBART 모델
  notes: "KoBART 베이스라인 실험"  # 실험 설명
  mode: "online"  # 로깅 모드 (online: 실시간, offline: 로컬, disabled: 비활성화)

# ----------------------------------------------------------------------------
# API 키 설정
# ----------------------------------------------------------------------------
api_keys:
  upstage_api_key: "up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT"  # Upstage Solar API 키
  huggingface_token: ""  # Hugging Face 토큰 (비공개 모델 사용 시 필요)

# ----------------------------------------------------------------------------
# Solar API 설정 (교차 검증 시스템)
# ----------------------------------------------------------------------------
solar_api:
  api_key: "up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT"  # Solar API 키
  enabled: true  # Solar API 사용 여부
  base_url: "https://api.upstage.ai/v1/solar"  # Solar API 기본 URL
  model: "solar-1-mini-chat"  # 사용할 Solar 모델
  max_tokens: 150  # 생성 최대 토큰 수
  temperature: 0.3  # 샘플링 온도 (낮을수록 결정적)
  top_p: 0.9  # Nucleus 샘플링 임계값
  timeout: 30  # API 요청 타임아웃 (초)

# ----------------------------------------------------------------------------
# 로깅 설정 (Logging Configuration)
# ----------------------------------------------------------------------------
logging:
  level: "INFO"  # 로그 레벨 (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"  # 로그 포맷
  save_to_file: true  # 파일로 로그 저장 여부
  log_every_n_steps: 100  # N 스텝마다 로그 출력

# ----------------------------------------------------------------------------
# GPU 설정 (GPU Configuration)
# ----------------------------------------------------------------------------
gpu:
  device: "cuda"  # 사용할 디바이스 (cuda: GPU, cpu: CPU)
  cuda_device: 0  # 사용할 CUDA 디바이스 번호
  mixed_precision: true  # Mixed Precision 학습 활성화
  memory_fraction: 0.9  # GPU 메모리 사용 비율 (0.9 = 90%)

# ----------------------------------------------------------------------------
# 실험 추적 (Experiment Tracking)
# ----------------------------------------------------------------------------
experiment:
  name: "baseline_kobart_v1"  # 실험 고유 이름
  description: "KoBART 기반 베이스라인 모델"  # 실험 설명
  version: "1.0.0"  # 실험 버전
  timestamp: true  # 타임스탬프 자동 추가 여부
