# ğŸ® ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

**ìš°ì„ ìˆœìœ„**: ğŸ”¥ ìµœìš°ì„  (PRD 14ë²ˆ)
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 12-16ì‹œê°„
**ë‚œì´ë„**: â˜…â˜…â˜…â˜…â˜†

---

## ğŸ“‹ ëª©í‘œ

**í˜„ì¬ `train.py`ë¥¼ PRD 14ë²ˆì— ëª…ì‹œëœ í†µí•© ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤ë¡œ ì™„ì „íˆ ì¬ì‘ì„±**

### Before (í˜„ì¬)
```bash
python scripts/train.py --experiment baseline_kobart --debug
```

### After (ëª©í‘œ)
```bash
# ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
python scripts/train.py --mode single --models kobart --epochs 20

# K-Fold êµì°¨ ê²€ì¦
python scripts/train.py --mode kfold --models solar-10.7b --k_folds 5

# ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
python scripts/train.py --mode multi_model --models kobart llama-3.2-3b qwen3-4b

# Optuna ìµœì í™”
python scripts/train.py --mode optuna --models kobart --optuna_trials 100

# í’€ íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ì˜µì…˜)
python scripts/train.py --mode full --models all --use_tta --k_folds 5
```

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. Trainer í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡°

```
src/trainers/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_trainer.py          # BaseTrainer ì¶”ìƒ í´ë˜ìŠ¤
â”œâ”€â”€ single_trainer.py         # SingleModelTrainer
â”œâ”€â”€ kfold_trainer.py          # KFoldTrainer
â”œâ”€â”€ multi_model_trainer.py    # MultiModelEnsembleTrainer
â”œâ”€â”€ optuna_trainer.py         # OptunaOptimizer
â””â”€â”€ full_pipeline_trainer.py  # FullPipelineTrainer
```

#### BaseTrainer (ì¶”ìƒ í´ë˜ìŠ¤)
```python
# src/trainers/base_trainer.py
from abc import ABC, abstractmethod
from pathlib import Path

class BaseTrainer(ABC):
    """ëª¨ë“  Trainerì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, args, logger, wandb_logger=None):
        self.args = args
        self.logger = logger
        self.wandb_logger = wandb_logger
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    @abstractmethod
    def train(self):
        """í•™ìŠµ ë©”ì¸ ë¡œì§ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    @abstractmethod
    def save_results(self, results):
        """ê²°ê³¼ ì €ì¥ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def log(self, message, level='INFO'):
        """í†µí•© ë¡œê¹…"""
        self.logger.write(message)
        if self.wandb_logger:
            self.wandb_logger.log_text(message)

    def load_data(self):
        """ê³µí†µ ë°ì´í„° ë¡œë”© ë¡œì§"""
        import pandas as pd
        train_df = pd.read_csv(self.args.train_data)
        eval_df = pd.read_csv(self.args.dev_data)

        if self.args.debug:
            train_df = train_df.head(100)
            eval_df = eval_df.head(20)

        return train_df, eval_df
```

#### SingleModelTrainer
```python
# src/trainers/single_trainer.py
from src.trainers.base_trainer import BaseTrainer
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer

class SingleModelTrainer(BaseTrainer):
    """ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ"""

    def train(self):
        self.log("=" * 60)
        self.log("SINGLE MODEL ëª¨ë“œ í•™ìŠµ ì‹œì‘")
        self.log(f"ëª¨ë¸: {self.args.models[0]}")
        self.log("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        train_df, eval_df = self.load_data()
        self.log(f"í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
        self.log(f"ê²€ì¦ ë°ì´í„°: {len(eval_df)}ê°œ")

        # 2. Config ë¡œë“œ (ëª¨ë¸ë³„)
        from src.config import load_model_config
        config = load_model_config(self.args.models[0])

        # 3. ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)

        # 4. Dataset ìƒì„±
        train_dataset = DialogueSummarizationDataset(
            dialogues=train_df['dialogue'].tolist(),
            summaries=train_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True
        )

        eval_dataset = DialogueSummarizationDataset(
            dialogues=eval_df['dialogue'].tolist(),
            summaries=eval_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True
        )

        # 5. Trainer ìƒì„±
        trainer = create_trainer(
            config=config,
            model=model,
            tokenizer=tokenizer,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            use_wandb=self.args.use_wandb,
            logger=self.logger
        )

        # 6. í•™ìŠµ ì‹¤í–‰
        results = trainer.train()

        self.log("=" * 60)
        self.log("SINGLE MODEL í•™ìŠµ ì™„ë£Œ!")
        self.log("=" * 60)

        return {
            'mode': 'single',
            'model': self.args.models[0],
            'results': results,
            'model_path': trainer.model_save_path
        }

    def save_results(self, results):
        """ê²°ê³¼ ì €ì¥"""
        import json
        result_path = self.output_dir / "single_model_results.json"

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        self.log(f"ê²°ê³¼ ì €ì¥: {result_path}")
```

#### KFoldTrainer
```python
# src/trainers/kfold_trainer.py
from src.trainers.base_trainer import BaseTrainer
from sklearn.model_selection import KFold

class KFoldTrainer(BaseTrainer):
    """K-Fold êµì°¨ ê²€ì¦ í•™ìŠµ"""

    def train(self):
        self.log("=" * 60)
        self.log("K-FOLD ëª¨ë“œ í•™ìŠµ ì‹œì‘")
        self.log(f"K-Folds: {self.args.k_folds}")
        self.log(f"ëª¨ë¸: {self.args.models[0]}")
        self.log("=" * 60)

        # 1. ì „ì²´ ë°ì´í„° ë¡œë“œ
        train_df, _ = self.load_data()

        # 2. K-Fold ë¶„í• 
        kf = KFold(n_splits=self.args.k_folds, shuffle=True, random_state=self.args.fold_seed)
        fold_results = []

        for fold_idx, (train_indices, val_indices) in enumerate(kf.split(train_df)):
            self.log(f"\n{'=' * 40}")
            self.log(f"Fold {fold_idx + 1}/{self.args.k_folds} ì‹œì‘")
            self.log(f"{'=' * 40}")

            # Foldë³„ ë°ì´í„° ë¶„í• 
            fold_train_df = train_df.iloc[train_indices]
            fold_val_df = train_df.iloc[val_indices]

            # Foldë³„ í•™ìŠµ (SingleModelTrainer ì¬í™œìš©)
            fold_result = self._train_fold(fold_idx, fold_train_df, fold_val_df)
            fold_results.append(fold_result)

            self.log(f"Fold {fold_idx + 1} ì™„ë£Œ: ROUGE = {fold_result['rouge']:.4f}")

        # 3. í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_rouge = sum(r['rouge'] for r in fold_results) / len(fold_results)

        self.log("\n" + "=" * 60)
        self.log(f"K-FOLD í‰ê·  ì„±ëŠ¥: ROUGE = {avg_rouge:.4f}")
        self.log("=" * 60)

        return {
            'mode': 'kfold',
            'k_folds': self.args.k_folds,
            'fold_results': fold_results,
            'avg_rouge': avg_rouge
        }

    def _train_fold(self, fold_idx, train_df, val_df):
        """ê°œë³„ Fold í•™ìŠµ"""
        from src.config import load_model_config
        from src.models import load_model_and_tokenizer
        from src.data import DialogueSummarizationDataset
        from src.training import create_trainer

        config = load_model_config(self.args.models[0])
        model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)

        train_dataset = DialogueSummarizationDataset(
            dialogues=train_df['dialogue'].tolist(),
            summaries=train_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True
        )

        val_dataset = DialogueSummarizationDataset(
            dialogues=val_df['dialogue'].tolist(),
            summaries=val_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True
        )

        trainer = create_trainer(
            config=config,
            model=model,
            tokenizer=tokenizer,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            use_wandb=self.args.use_wandb,
            logger=self.logger,
            experiment_name=f"{self.args.experiment_name}_fold{fold_idx}"
        )

        results = trainer.train()

        return {
            'fold': fold_idx,
            'rouge': results['eval_metrics'].get('rouge_sum', 0),
            'model_path': trainer.model_save_path
        }

    def save_results(self, results):
        """ê²°ê³¼ ì €ì¥"""
        import json
        result_path = self.output_dir / "kfold_results.json"

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        self.log(f"ê²°ê³¼ ì €ì¥: {result_path}")
```

#### MultiModelEnsembleTrainer (ê°œìš”ë§Œ)
```python
# src/trainers/multi_model_trainer.py
from src.trainers.base_trainer import BaseTrainer

class MultiModelEnsembleTrainer(BaseTrainer):
    """ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” í•™ìŠµ"""

    def train(self):
        self.log("=" * 60)
        self.log("MULTI-MODEL ENSEMBLE ëª¨ë“œ í•™ìŠµ ì‹œì‘")
        self.log(f"ëª¨ë¸ ìˆ˜: {len(self.args.models)}")
        self.log(f"ëª¨ë¸ ëª©ë¡: {', '.join(self.args.models)}")
        self.log(f"ì•™ìƒë¸” ì „ëµ: {self.args.ensemble_strategy}")
        self.log("=" * 60)

        # 1. ê° ëª¨ë¸ ê°œë³„ í•™ìŠµ
        model_results = []
        for model_name in self.args.models:
            self.log(f"\n{'=' * 40}")
            self.log(f"ëª¨ë¸ í•™ìŠµ: {model_name}")
            self.log(f"{'=' * 40}")

            # SingleModelTrainer ì¬í™œìš©
            result = self._train_single_model(model_name)
            model_results.append(result)

        # 2. ì•™ìƒë¸” (src/ensemble/ ëª¨ë“ˆ ì‚¬ìš©)
        from src.ensemble import create_ensemble
        ensemble = create_ensemble(
            models=model_results,
            strategy=self.args.ensemble_strategy,
            weights=self.args.ensemble_weights
        )

        # 3. ì•™ìƒë¸” í‰ê°€
        ensemble_score = self._evaluate_ensemble(ensemble)

        return {
            'mode': 'multi_model',
            'models': self.args.models,
            'model_results': model_results,
            'ensemble_score': ensemble_score
        }

    def _train_single_model(self, model_name):
        # SingleModelTrainer ë¡œì§ ì¬í™œìš©
        pass

    def save_results(self, results):
        # ê²°ê³¼ ì €ì¥
        pass
```

#### OptunaOptimizer (ê°œìš”ë§Œ)
```python
# src/trainers/optuna_trainer.py
from src.trainers.base_trainer import BaseTrainer
import optuna

class OptunaOptimizer(BaseTrainer):
    """Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""

    def train(self):
        self.log("=" * 60)
        self.log("OPTUNA ìµœì í™” ëª¨ë“œ ì‹œì‘")
        self.log(f"Trials: {self.args.optuna_trials}")
        self.log(f"Sampler: {self.args.optuna_sampler}")
        self.log("=" * 60)

        # Optuna Study ìƒì„±
        study = optuna.create_study(
            study_name=self.args.experiment_name,
            direction='maximize',
            sampler=self._get_sampler(),
            pruner=self._get_pruner()
        )

        # ìµœì í™” ì‹¤í–‰
        study.optimize(
            self.objective,
            n_trials=self.args.optuna_trials,
            timeout=self.args.optuna_timeout
        )

        self.log(f"\nBest Trial: {study.best_trial.number}")
        self.log(f"Best Value: {study.best_value:.4f}")
        self.log(f"Best Params: {study.best_params}")

        return {
            'mode': 'optuna',
            'best_params': study.best_params,
            'best_value': study.best_value,
            'study': study
        }

    def objective(self, trial):
        """Optuna ëª©ì  í•¨ìˆ˜"""
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        lr = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)
        batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32])

        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        score = self._train_with_params(lr, batch_size)

        return score

    def save_results(self, results):
        # Optuna ê²°ê³¼ ì €ì¥
        pass
```

---

## ğŸ“ train.py ì¬ì‘ì„±

### ì™„ì „í•œ train.py (PRD 14 ê¸°ë°˜)

```python
#!/usr/bin/env python3
# scripts/train.py
"""
NLP ëŒ€í™” ìš”ì•½ í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
PRD 14ë²ˆ "ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œ" êµ¬í˜„

ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ëª¨ë¸
    python train.py --mode single --models kobart

    # K-Fold
    python train.py --mode kfold --models solar-10.7b --k_folds 5

    # ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
    python train.py --mode multi_model --models kobart llama-3.2-3b

    # Optuna ìµœì í™”
    python train.py --mode optuna --optuna_trials 50

    # í’€ íŒŒì´í”„ë¼ì¸
    python train.py --mode full --models all --use_tta
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.logging import Logger
from src.utils.config.seed import set_seed


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ - ìœ ì—°í•œ ì‹¤í–‰ ì˜µì…˜',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # ==================== ê¸°ë³¸ ì„¤ì • ====================
    parser.add_argument(
        '--mode',
        type=str,
        default='single',
        choices=['single', 'kfold', 'multi_model', 'optuna', 'full'],
        help='''ì‹¤í–‰ ëª¨ë“œ ì„ íƒ:
        single: ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ ì‹¤í—˜)
        kfold: K-Fold êµì°¨ ê²€ì¦ (ì•ˆì •ì„±)
        multi_model: ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” (ì„±ëŠ¥)
        optuna: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ìë™í™”)
        full: ì „ì²´ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ì œì¶œ)'''
    )

    parser.add_argument(
        '--config',
        type=str,
        default='configs/train_config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )

    parser.add_argument(
        '--experiment_name',
        type=str,
        default=None,
        help='ì‹¤í—˜ëª… (ìë™ ìƒì„±: {mode}_{model}_{timestamp})'
    )

    # ==================== ëª¨ë¸ ì„ íƒ ====================
    parser.add_argument(
        '--models',
        type=str,
        nargs='+',
        default=['kobart'],
        choices=[
            'kobart',
            'solar-10.7b',
            'polyglot-ko-12.8b',
            'llama-3.2-korean-3b',
            'qwen3-4b',
            'kullm-v2',
            'all'  # ëª¨ë“  ëª¨ë¸
        ],
        help='ì‚¬ìš©í•  ëª¨ë¸ (multi_model ëª¨ë“œì—ì„œ ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)'
    )

    # ==================== í•™ìŠµ ì„¤ì • ====================
    parser.add_argument(
        '--epochs',
        type=int,
        default=None,
        help='ì—í­ ìˆ˜ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='ë°°ì¹˜ í¬ê¸° (None: config íŒŒì¼ ê°’ ì‚¬ìš© ë˜ëŠ” ìë™ íƒìƒ‰)'
    )

    parser.add_argument(
        '--learning_rate',
        type=float,
        default=None,
        help='í•™ìŠµë¥  (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    # ==================== K-Fold ì„¤ì • ====================
    parser.add_argument(
        '--k_folds',
        type=int,
        default=5,
        help='K-Fold ìˆ˜ (kfold ëª¨ë“œ)'
    )

    parser.add_argument(
        '--fold_seed',
        type=int,
        default=42,
        help='Fold ë¶„í•  ì‹œë“œ'
    )

    # ==================== ì•™ìƒë¸” ì„¤ì • ====================
    parser.add_argument(
        '--ensemble_strategy',
        type=str,
        default='weighted_avg',
        choices=[
            'averaging',
            'weighted_avg',
            'majority_vote',
            'stacking',
            'blending',
            'rouge_weighted'
        ],
        help='ì•™ìƒë¸” ì „ëµ'
    )

    parser.add_argument(
        '--ensemble_weights',
        type=float,
        nargs='+',
        default=None,
        help='ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ìë™ ìµœì í™” ê°€ëŠ¥)'
    )

    # ==================== TTA ì„¤ì • ====================
    parser.add_argument(
        '--use_tta',
        action='store_true',
        help='Test Time Augmentation ì‚¬ìš©'
    )

    parser.add_argument(
        '--tta_strategies',
        type=str,
        nargs='+',
        default=['paraphrase'],
        choices=['paraphrase', 'reorder', 'synonym', 'mask'],
        help='TTA ì „ëµ'
    )

    parser.add_argument(
        '--tta_num_aug',
        type=int,
        default=3,
        help='TTA ì¦ê°• ìˆ˜'
    )

    # ==================== Optuna ì„¤ì • ====================
    parser.add_argument(
        '--optuna_trials',
        type=int,
        default=100,
        help='Optuna ì‹œë„ íšŸìˆ˜'
    )

    parser.add_argument(
        '--optuna_timeout',
        type=int,
        default=7200,
        help='Optuna ì œí•œ ì‹œê°„ (ì´ˆ)'
    )

    parser.add_argument(
        '--optuna_sampler',
        type=str,
        default='tpe',
        choices=['tpe', 'gp', 'random', 'cmaes'],
        help='Optuna ìƒ˜í”ŒëŸ¬'
    )

    parser.add_argument(
        '--optuna_pruner',
        type=str,
        default='median',
        choices=['median', 'percentile', 'hyperband'],
        help='Optuna ê°€ì§€ì¹˜ê¸°'
    )

    # ==================== ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ====================
    parser.add_argument(
        '--use_wandb',
        action='store_true',
        help='WandB ì‚¬ìš©'
    )

    parser.add_argument(
        '--wandb_project',
        type=str,
        default='dialogue-summarization',
        help='WandB í”„ë¡œì íŠ¸ëª…'
    )

    parser.add_argument(
        '--save_visualizations',
        action='store_true',
        help='ì‹œê°í™” ì €ì¥'
    )

    # ==================== ê¸°íƒ€ ì˜µì…˜ ====================
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='ëœë¤ ì‹œë“œ'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ (ì ì€ ë°ì´í„°)'
    )

    # ==================== ë°ì´í„° ê²½ë¡œ ====================
    parser.add_argument(
        '--train_data',
        type=str,
        default='data/raw/train.csv',
        help='í•™ìŠµ ë°ì´í„° ê²½ë¡œ'
    )

    parser.add_argument(
        '--dev_data',
        type=str,
        default='data/raw/dev.csv',
        help='ê²€ì¦ ë°ì´í„° ê²½ë¡œ'
    )

    return parser.parse_args()


def setup_environment(args):
    """í™˜ê²½ ì„¤ì •"""
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)

    # ì‹¤í—˜ëª… ìë™ ìƒì„±
    if args.experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = args.models[0].replace('-', '_') if args.models else 'default'
        args.experiment_name = f"{args.mode}_{model_name}_{timestamp}"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(f"experiments/{args.experiment_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    args.output_dir = str(output_dir)

    # ë¡œê±° ì„¤ì •
    log_path = output_dir / "train.log"
    logger = Logger(log_path, print_also=True)
    logger.start_redirect()

    return logger


def get_trainer(args, logger):
    """ëª¨ë“œì— ë”°ë¥¸ Trainer ì„ íƒ"""
    from src.trainers import (
        SingleModelTrainer,
        KFoldTrainer,
        MultiModelEnsembleTrainer,
        OptunaOptimizer,
        FullPipelineTrainer
    )

    trainer_map = {
        'single': SingleModelTrainer,
        'kfold': KFoldTrainer,
        'multi_model': MultiModelEnsembleTrainer,
        'optuna': OptunaOptimizer,
        'full': FullPipelineTrainer
    }

    trainer_class = trainer_map[args.mode]
    return trainer_class(args, logger)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    args = parse_arguments()

    print("=" * 60)
    print("ğŸš€ NLP ëŒ€í™” ìš”ì•½ í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“‹ ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ğŸ¤– ëª¨ë¸: {', '.join(args.models)}")
    print("=" * 60)

    # í™˜ê²½ ì„¤ì •
    logger = setup_environment(args)

    try:
        # Trainer ìƒì„±
        trainer = get_trainer(args, logger)

        # í•™ìŠµ ì‹¤í–‰
        print(f"\nğŸ“Š {args.mode.upper()} ëª¨ë“œ ì‹¤í–‰ ì¤‘...")
        results = trainer.train()

        # ê²°ê³¼ ì €ì¥
        trainer.save_results(results)

        # ì‹œê°í™”
        if args.save_visualizations:
            print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
            from src.utils.visualizations import create_training_visualizations
            create_training_visualizations(
                results=results,
                output_dir=args.output_dir
            )

        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {args.output_dir}")

    except Exception as e:
        logger.write(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", print_error=True)
        raise

    finally:
        # ì •ë¦¬
        logger.stop_redirect()
        logger.close()


if __name__ == "__main__":
    main()
```

---

## ğŸ”§ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± (30ë¶„)
- [ ] `src/trainers/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] `src/trainers/__init__.py` ì‘ì„±
- [ ] ê° Trainer íŒŒì¼ ìƒì„± (ë¹ˆ í´ë˜ìŠ¤)

### Phase 2: BaseTrainer êµ¬í˜„ (2ì‹œê°„)
- [ ] `base_trainer.py` ì™„ì„±
- [ ] ê³µí†µ ë©”ì„œë“œ êµ¬í˜„ (`load_data`, `log` ë“±)
- [ ] ì¶”ìƒ ë©”ì„œë“œ ì •ì˜

### Phase 3: SingleModelTrainer êµ¬í˜„ (3ì‹œê°„)
- [ ] `single_trainer.py` ì™„ì„±
- [ ] ê¸°ì¡´ `train.py` ë¡œì§ ì´ì „
- [ ] í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### Phase 4: KFoldTrainer êµ¬í˜„ (4ì‹œê°„)
- [ ] `kfold_trainer.py` ì™„ì„±
- [ ] K-Fold ë¶„í•  ë¡œì§
- [ ] Foldë³„ í•™ìŠµ ë£¨í”„
- [ ] í‰ê·  ì„±ëŠ¥ ê³„ì‚°

### Phase 5: train.py ì¬ì‘ì„± (3ì‹œê°„)
- [ ] ê¸°ì¡´ `train.py` ë°±ì—…
- [ ] ìƒˆë¡œìš´ `train.py` ì‘ì„±
- [ ] ì¸ì íŒŒì‹± (50+ ì˜µì…˜)
- [ ] Trainer ì„ íƒ ë¡œì§

### Phase 6: í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
- [ ] Single ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] KFold ëª¨ë“œ í…ŒìŠ¤íŠ¸
- [ ] ê°ì¢… ì˜µì…˜ ì¡°í•© í…ŒìŠ¤íŠ¸

### Phase 7: ê³ ê¸‰ Trainer êµ¬í˜„ (ë‚˜ì¤‘ì—, 8ì‹œê°„)
- [ ] `multi_model_trainer.py`
- [ ] `optuna_trainer.py`
- [ ] `full_pipeline_trainer.py`

---

## ğŸš€ ì¦‰ì‹œ ì‹œì‘

### 1. ë””ë ‰í† ë¦¬ ìƒì„±
```bash
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition

mkdir -p src/trainers

touch src/trainers/__init__.py
touch src/trainers/base_trainer.py
touch src/trainers/single_trainer.py
touch src/trainers/kfold_trainer.py
touch src/trainers/multi_model_trainer.py
touch src/trainers/optuna_trainer.py
touch src/trainers/full_pipeline_trainer.py
```

### 2. ê¸°ì¡´ íŒŒì¼ ë°±ì—…
```bash
cp scripts/train.py scripts/train_old.py
```

### 3. BaseTrainer ì‘ì„± ì‹œì‘
```bash
# src/trainers/base_trainer.py íŒŒì¼ì„ í¸ì§‘ê¸°ë¡œ ì—´ê¸°
# ìœ„ì˜ BaseTrainer ì½”ë“œ ë¶™ì—¬ë„£ê¸°
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ê³¼

### êµ¬í˜„ í›„ ì‚¬ìš© ì˜ˆì‹œ
```bash
# ë¹ ë¥¸ ì‹¤í—˜
python train.py --mode single --models kobart --debug

# í”„ë¡œë•ì…˜ í•™ìŠµ
python train.py --mode kfold --models solar-10.7b --k_folds 5 --use_wandb

# ìµœì¢… ì œì¶œ
python train.py --mode full --models all --use_tta --save_visualizations
```

### ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
- âœ… í•˜ë‚˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë“  ì‹¤í—˜ ê°€ëŠ¥
- âœ… ì˜µì…˜ìœ¼ë¡œ ê¸°ëŠ¥ ì„ íƒ ê°€ëŠ¥
- âœ… ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ ê´€ë¦¬
- âœ… PRD ìš”êµ¬ì‚¬í•­ 100% ì¶©ì¡±

---

**ë‹¤ìŒ ì‘ì—…**: `03_LLM_í†µí•©_ê°€ì´ë“œ.md`
