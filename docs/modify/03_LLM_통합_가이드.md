# ğŸ¤– LLM íŒŒì¸íŠœë‹ í†µí•© ê°€ì´ë“œ

**âš ï¸ ìƒíƒœ**: ì„ íƒì  (2025-10-11 ì—…ë°ì´íŠ¸)
**ìš°ì„ ìˆœìœ„**: âš ï¸ ì„ íƒì  ê³ ê¸‰ ê¸°ëŠ¥ (PRD 08ë²ˆ)
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 4-6ì‹œê°„
**ë‚œì´ë„**: â˜…â˜…â˜…â˜…â˜†
**ì„ í–‰ ì‘ì—…**: 02_ì‹¤í–‰_ì˜µì…˜_ì‹œìŠ¤í…œ_êµ¬í˜„_ê°€ì´ë“œ.md

> **ğŸ“Œ ì°¸ê³ **: ì´ ë¬¸ì„œëŠ” ì„ íƒì  ê³ ê¸‰ ê¸°ëŠ¥ì— ëŒ€í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.
> í˜„ì¬ scripts/train_llm.pyê°€ ë³„ë„ë¡œ ì¡´ì¬í•˜ì—¬ LLM íŒŒì¸íŠœë‹ì´ ê°€ëŠ¥í•˜ë©°,
> train.pyì™€ì˜ í†µí•©ì€ í•„ìš”ì‹œ ì§„í–‰í•˜ë©´ ë©ë‹ˆë‹¤.

---

## ğŸ“‹ ë¬¸ì œ ì •ì˜

### í˜„ì¬ ìƒí™©
```
scripts/
â”œâ”€â”€ train.py          # Encoder-Decoder ì „ìš© (KoBART)
â””â”€â”€ train_llm.py      # Causal LM ì „ìš© (Llama, Qwen) - ë¶„ë¦¬ë¨!
```

**ë¬¸ì œì **:
1. ë‘ ê°œì˜ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ â†’ ì‚¬ìš©ì í˜¼ë€
2. `train.py`ì—ì„œ LLM ì‚¬ìš© ë¶ˆê°€ëŠ¥
3. í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë¶ˆê°€
4. PRD 08ë²ˆ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±

### ëª©í‘œ ìƒíƒœ
```python
# í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ ëª¨ë“  ëª¨ë¸ ì‚¬ìš©
python train.py --mode single --models kobart
python train.py --mode single --models llama-3.2-korean-3b
python train.py --mode multi_model --models kobart llama-3.2-korean-3b qwen3-4b
```

---

## ğŸ—ï¸ í†µí•© ì•„í‚¤í…ì²˜

### 1. ëª¨ë¸ íƒ€ì… ë¶„ë¦¬

```
src/models/
â”œâ”€â”€ __init__.py                    # í†µí•© ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ model_loader.py                # ê¸°ì¡´ (Encoder-Decoder)
â”œâ”€â”€ llm_loader.py                  # ì‹ ê·œ (Causal LM)
â”œâ”€â”€ model_config.py                # ëª¨ë¸ë³„ ì„¤ì •
â””â”€â”€ generation/
    â”œâ”€â”€ encoder_decoder_generator.py
    â””â”€â”€ causal_lm_generator.py
```

### 2. Config êµ¬ì¡° í™•ì¥

```yaml
# configs/models/kobart.yaml
model:
  name: "kobart"
  type: "encoder_decoder"  # â† íƒ€ì… ì¶”ê°€!
  checkpoint: "digit82/kobart-summarization"

# configs/models/llama_3.2_3b.yaml
model:
  name: "llama-3.2-korean-3b"
  type: "causal_lm"  # â† íƒ€ì… ì¶”ê°€!
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"

  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true

  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
```

---

## ğŸ“ êµ¬í˜„ ë‹¨ê³„

### Phase 1: LLM Loader ë¶„ë¦¬ (2ì‹œê°„)

#### src/models/llm_loader.py (ì‹ ê·œ ìƒì„±)

```python
# src/models/llm_loader.py
"""Causal LM ëª¨ë¸ ë¡œë” (Llama, Qwen, ë“±)"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


def load_causal_lm(config, logger=None):
    """
    Causal LM ëª¨ë¸ ë¡œë“œ (QLoRA í¬í•¨)

    Args:
        config: ëª¨ë¸ ì„¤ì •
        logger: ë¡œê±°

    Returns:
        model, tokenizer
    """
    if logger:
        logger.write(f"Loading Causal LM: {config.model.checkpoint}")

    # 1. Quantization ì„¤ì •
    quantization_config = None
    if config.model.get('quantization'):
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=config.model.quantization.get('load_in_4bit', True),
            bnb_4bit_compute_dtype=getattr(torch, config.model.quantization.get('bnb_4bit_compute_dtype', 'bfloat16')),
            bnb_4bit_quant_type=config.model.quantization.get('bnb_4bit_quant_type', 'nf4'),
            bnb_4bit_use_double_quant=config.model.quantization.get('bnb_4bit_use_double_quant', True)
        )

    # 2. ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        config.model.checkpoint,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
        trust_remote_code=True
    )

    # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        config.model.checkpoint,
        trust_remote_code=True
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id

    # Chat í…œí”Œë¦¿ í† í° ì¶”ê°€
    if config.tokenizer.get('chat_template_tokens'):
        special_tokens = {
            'additional_special_tokens': config.tokenizer.chat_template_tokens
        }
        tokenizer.add_special_tokens(special_tokens)
        model.resize_token_embeddings(len(tokenizer))

    # 4. LoRA ì„¤ì •
    if config.model.get('lora'):
        if logger:
            logger.write("Applying LoRA configuration...")

        # K-bit training ì¤€ë¹„
        model = prepare_model_for_kbit_training(model)

        # LoRA Config
        lora_config = LoraConfig(
            r=config.model.lora.get('r', 16),
            lora_alpha=config.model.lora.get('lora_alpha', 32),
            lora_dropout=config.model.lora.get('lora_dropout', 0.05),
            bias=config.model.lora.get('bias', 'none'),
            task_type=config.model.lora.get('task_type', 'CAUSAL_LM'),
            target_modules=config.model.lora.get('target_modules', [
                'q_proj', 'k_proj', 'v_proj', 'o_proj',
                'gate_proj', 'up_proj', 'down_proj'
            ])
        )

        # PEFT ëª¨ë¸ ìƒì„±
        model = get_peft_model(model, lora_config)

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_percentage = 100 * trainable_params / total_params

        if logger:
            logger.write(f"  Trainable params: {trainable_params:,} ({trainable_percentage:.2f}%)")
            logger.write(f"  Total params: {total_params:,}")

    # 5. Gradient Checkpointing
    if config.training.get('gradient_checkpointing', True):
        model.gradient_checkpointing_enable()
        model.config.use_cache = False  # Gradient checkpointingê³¼ í•¨ê»˜ ì‚¬ìš© ì‹œ í•„ìˆ˜

    return model, tokenizer


def format_llm_prompt(dialogue: str, tokenizer) -> str:
    """
    LLMìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…

    Args:
        dialogue: ëŒ€í™” ì›ë¬¸
        tokenizer: í† í¬ë‚˜ì´ì €

    Returns:
        í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸
    """
    # Chat í…œí”Œë¦¿ ì‚¬ìš© (Llama 3.x ìŠ¤íƒ€ì¼)
    messages = [
        {
            "role": "system",
            "content": "You are an expert in dialogue summarization. Summarize the given dialogue concisely and accurately."
        },
        {
            "role": "user",
            "content": f"Dialogue:\n{dialogue}\n\nSummary:"
        }
    ]

    # Chat í…œí”Œë¦¿ ì ìš©
    if hasattr(tokenizer, 'apply_chat_template'):
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    else:
        # Fallback: ìˆ˜ë™ í¬ë§·íŒ…
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{messages[0]['content']}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{messages[1]['content']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

    return prompt
```

---

### Phase 2: í†µí•© Model Loader ìˆ˜ì • (1ì‹œê°„)

#### src/models/__init__.py ìˆ˜ì •

```python
# src/models/__init__.py
"""ëª¨ë¸ ë¡œë” í†µí•© ì¸í„°í˜ì´ìŠ¤"""

from src.models.model_loader import load_encoder_decoder
from src.models.llm_loader import load_causal_lm


def load_model_and_tokenizer(config, logger=None):
    """
    ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë¡œë” ì„ íƒ

    Args:
        config: ëª¨ë¸ ì„¤ì •
        logger: ë¡œê±°

    Returns:
        model, tokenizer

    Raises:
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…
    """
    model_type = config.model.get('type', 'encoder_decoder')

    if logger:
        logger.write(f"Model Type: {model_type}")

    if model_type == 'encoder_decoder':
        return load_encoder_decoder(config, logger)
    elif model_type == 'causal_lm':
        return load_causal_lm(config, logger)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")


__all__ = [
    'load_model_and_tokenizer',
    'load_encoder_decoder',
    'load_causal_lm'
]
```

#### src/models/model_loader.py í•¨ìˆ˜ëª… ë³€ê²½

```python
# src/models/model_loader.py
"""Encoder-Decoder ëª¨ë¸ ë¡œë” (KoBART ë“±)"""

from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer
)


def load_encoder_decoder(config, logger=None):
    """
    Encoder-Decoder ëª¨ë¸ ë¡œë“œ

    Args:
        config: ëª¨ë¸ ì„¤ì •
        logger: ë¡œê±°

    Returns:
        model, tokenizer
    """
    if logger:
        logger.write(f"Loading Encoder-Decoder: {config.model.checkpoint}")

    # ê¸°ì¡´ load_model_and_tokenizer ì½”ë“œì™€ ë™ì¼
    model = AutoModelForSeq2SeqLM.from_pretrained(config.model.checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(config.model.checkpoint)

    # Special tokens ì¶”ê°€
    if config.tokenizer.get('special_tokens'):
        special_tokens = {'additional_special_tokens': config.tokenizer.special_tokens}
        tokenizer.add_special_tokens(special_tokens)
        model.resize_token_embeddings(len(tokenizer))

    return model, tokenizer
```

---

### Phase 3: Dataset ë° Trainer ìˆ˜ì • (2ì‹œê°„)

#### src/data/dataset.py ìˆ˜ì • (LLM ì§€ì›)

```python
# src/data/dataset.py
"""Dataset í´ë˜ìŠ¤ - Encoder-Decoderì™€ Causal LM ëª¨ë‘ ì§€ì›"""

import torch
from torch.utils.data import Dataset


class DialogueSummarizationDataset(Dataset):
    """ëŒ€í™” ìš”ì•½ Dataset (Encoder-Decoder & Causal LM ì§€ì›)"""

    def __init__(
        self,
        dialogues,
        summaries,
        tokenizer,
        encoder_max_len=512,
        decoder_max_len=100,
        preprocess=True,
        model_type='encoder_decoder'  # â† ì¶”ê°€!
    ):
        self.dialogues = dialogues
        self.summaries = summaries
        self.tokenizer = tokenizer
        self.encoder_max_len = encoder_max_len
        self.decoder_max_len = decoder_max_len
        self.preprocess = preprocess
        self.model_type = model_type  # â† ì¶”ê°€!

        # ì „ì²˜ë¦¬
        if self.preprocess:
            from src.data.preprocessor import preprocess_dialogue, preprocess_summary
            self.dialogues = [preprocess_dialogue(d) for d in dialogues]
            self.summaries = [preprocess_summary(s) for s in summaries]

    def __len__(self):
        return len(self.dialogues)

    def __getitem__(self, idx):
        dialogue = self.dialogues[idx]
        summary = self.summaries[idx]

        if self.model_type == 'encoder_decoder':
            return self._get_encoder_decoder_item(dialogue, summary)
        elif self.model_type == 'causal_lm':
            return self._get_causal_lm_item(dialogue, summary)
        else:
            raise ValueError(f"Unsupported model_type: {self.model_type}")

    def _get_encoder_decoder_item(self, dialogue, summary):
        """Encoder-Decoderìš© ë°ì´í„°"""
        # ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼
        encoder_inputs = self.tokenizer(
            dialogue,
            max_length=self.encoder_max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        decoder_inputs = self.tokenizer(
            summary,
            max_length=self.decoder_max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        labels = decoder_inputs['input_ids'].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': encoder_inputs['input_ids'].squeeze(),
            'attention_mask': encoder_inputs['attention_mask'].squeeze(),
            'labels': labels.squeeze()
        }

    def _get_causal_lm_item(self, dialogue, summary):
        """Causal LMìš© ë°ì´í„° (Instruction Tuning ìŠ¤íƒ€ì¼)"""
        from src.models.llm_loader import format_llm_prompt

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = format_llm_prompt(dialogue, self.tokenizer)
        full_text = prompt + summary + self.tokenizer.eos_token

        # í† í¬ë‚˜ì´ì§•
        encoding = self.tokenizer(
            full_text,
            max_length=self.encoder_max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Labels ìƒì„± (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹)
        prompt_encoding = self.tokenizer(
            prompt,
            max_length=self.encoder_max_len,
            truncation=True,
            return_tensors='pt'
        )
        prompt_length = prompt_encoding['input_ids'].shape[1]

        labels = encoding['input_ids'].clone()
        labels[:, :prompt_length] = -100  # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': labels.squeeze()
        }
```

#### src/training/trainer_factory.py ìˆ˜ì •

```python
# src/training/trainer_factory.py
"""Trainer ìƒì„± íŒ©í† ë¦¬"""

from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    Trainer,
    TrainingArguments
)
from src.training.callbacks import create_callbacks


def create_trainer(
    config,
    model,
    tokenizer,
    train_dataset,
    eval_dataset,
    use_wandb=False,
    logger=None,
    experiment_name=None
):
    """
    ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ Trainer ìƒì„±

    Args:
        config: í•™ìŠµ ì„¤ì •
        model: ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
        eval_dataset: ê²€ì¦ ë°ì´í„°ì…‹
        use_wandb: WandB ì‚¬ìš© ì—¬ë¶€
        logger: ë¡œê±°
        experiment_name: ì‹¤í—˜ëª…

    Returns:
        Trainer
    """
    model_type = config.model.get('type', 'encoder_decoder')

    if model_type == 'encoder_decoder':
        return _create_seq2seq_trainer(
            config, model, tokenizer, train_dataset, eval_dataset,
            use_wandb, logger, experiment_name
        )
    elif model_type == 'causal_lm':
        return _create_causal_lm_trainer(
            config, model, tokenizer, train_dataset, eval_dataset,
            use_wandb, logger, experiment_name
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")


def _create_seq2seq_trainer(config, model, tokenizer, train_dataset, eval_dataset,
                              use_wandb, logger, experiment_name):
    """Seq2Seq Trainer (ê¸°ì¡´ ì½”ë“œ)"""
    training_args = Seq2SeqTrainingArguments(
        output_dir=config.paths.model_save_dir,
        num_train_epochs=config.training.epochs,
        per_device_train_batch_size=config.training.batch_size,
        # ... ê¸°ì¡´ ì„¤ì •
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        # ... ê¸°ì¡´ ì„¤ì •
    )

    return trainer


def _create_causal_lm_trainer(config, model, tokenizer, train_dataset, eval_dataset,
                                use_wandb, logger, experiment_name):
    """Causal LM Trainer (ì‹ ê·œ)"""
    training_args = TrainingArguments(
        output_dir=config.paths.model_save_dir,
        num_train_epochs=config.training.get('num_train_epochs', 3),  # LLMì€ 3 epoch
        per_device_train_batch_size=config.training.get('per_device_train_batch_size', 8),
        per_device_eval_batch_size=config.training.get('per_device_eval_batch_size', 8),
        gradient_accumulation_steps=config.training.get('gradient_accumulation_steps', 8),
        learning_rate=config.training.get('learning_rate', 2e-5),
        warmup_ratio=config.training.get('warmup_ratio', 0.1),
        weight_decay=config.training.get('weight_decay', 0.1),
        lr_scheduler_type=config.training.get('lr_scheduler_type', 'cosine'),
        max_grad_norm=config.training.get('max_grad_norm', 1.2),
        bf16=config.training.get('bf16', True),
        fp16=config.training.get('fp16', False),
        logging_steps=10,
        save_strategy='epoch',
        evaluation_strategy='epoch',
        load_best_model_at_end=True,
        metric_for_best_model='eval_loss',
        greater_is_better=False,
        report_to=['wandb'] if use_wandb else [],
        run_name=experiment_name if use_wandb else None,
        gradient_checkpointing=config.training.get('gradient_checkpointing', True),
        optim=config.training.get('optim', 'paged_adamw_32bit'),
    )

    # Callbacks
    callbacks = create_callbacks(config, logger)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        callbacks=callbacks
    )

    return trainer
```

---

## ğŸ”§ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: LLM Loader ìƒì„± (2ì‹œê°„)
- [ ] `src/models/llm_loader.py` ìƒì„±
- [ ] `load_causal_lm()` í•¨ìˆ˜ êµ¬í˜„
- [ ] QLoRA ì„¤ì • êµ¬í˜„
- [ ] Chat í…œí”Œë¦¿ í¬ë§·íŒ… êµ¬í˜„

### Phase 2: í†µí•© (1ì‹œê°„)
- [ ] `src/models/__init__.py` ìˆ˜ì •
- [ ] `src/models/model_loader.py` í•¨ìˆ˜ëª… ë³€ê²½
- [ ] ëª¨ë¸ íƒ€ì… ê¸°ë°˜ ë¼ìš°íŒ… êµ¬í˜„

### Phase 3: Dataset & Trainer ìˆ˜ì • (2ì‹œê°„)
- [ ] `DialogueSummarizationDataset`ì— `model_type` íŒŒë¼ë¯¸í„° ì¶”ê°€
- [ ] `_get_causal_lm_item()` ë©”ì„œë“œ êµ¬í˜„
- [ ] `create_trainer()`ì— Causal LM ë¶„ê¸° ì¶”ê°€
- [ ] `_create_causal_lm_trainer()` êµ¬í˜„

### Phase 4: Config íŒŒì¼ ìƒì„± (1ì‹œê°„)
- [ ] `configs/models/llama_3.2_3b.yaml` ìƒì„±
- [ ] `configs/models/qwen3_4b.yaml` ìƒì„±
- [ ] `configs/base/causal_lm.yaml` ìƒì„±

---

## ğŸš€ ì¦‰ì‹œ ì‹œì‘

### 1. íŒŒì¼ ìƒì„±
```bash
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition

touch src/models/llm_loader.py

mkdir -p configs/models
touch configs/models/llama_3.2_3b.yaml
touch configs/models/qwen3_4b.yaml

mkdir -p configs/base
touch configs/base/causal_lm.yaml
```

### 2. llm_loader.py ì‘ì„±
ìœ„ì˜ ì½”ë“œë¥¼ `src/models/llm_loader.py`ì— ë¶™ì—¬ë„£ê¸°

### 3. __init__.py ìˆ˜ì •
ìœ„ì˜ ìˆ˜ì • ì‚¬í•­ì„ `src/models/__init__.py`ì— ì ìš©

---

## ğŸ“Š í…ŒìŠ¤íŠ¸

### í†µí•© í›„ í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´
```bash
# Encoder-Decoder (ê¸°ì¡´)
python train.py --mode single --models kobart --debug

# Causal LM (ì‹ ê·œ)
python train.py --mode single --models llama-3.2-korean-3b --debug

# ë‘˜ ë‹¤ ì•™ìƒë¸”
python train.py --mode multi_model --models kobart llama-3.2-korean-3b --debug
```

---

**ë‹¤ìŒ ì‘ì—…**: `04_ë‚˜ë¨¸ì§€_ëª¨ë“ˆ_êµ¬í˜„_ê°€ì´ë“œ.md` (Solar API, K-Fold, Ensemble, Optuna ë“±)
