# 04. 모델 학습 및 추론 시스템

## 목차
- [Part 1: 모델 로딩](#part-1-모델-로딩)
  - [모델 로더 개요](#모델-로더-개요)
  - [ModelLoader 클래스](#modelloader-클래스)
  - [모델 로더 사용 방법](#모델-로더-사용-방법)
  - [디바이스 관리](#디바이스-관리)
  - [특수 토큰 처리](#특수-토큰-처리)
- [Part 2: 학습 시스템](#part-2-학습-시스템)
  - [학습 시스템 개요](#학습-시스템-개요)
  - [ModelTrainer 클래스](#modeltrainer-클래스)
  - [학습 시스템 사용 방법](#학습-시스템-사용-방법)
  - [학습 인자 설정](#학습-인자-설정)
  - [WandB 통합](#wandb-통합)
  - [체크포인트 관리](#체크포인트-관리)
- [Part 3: 추론 시스템](#part-3-추론-시스템)
  - [추론 시스템 개요](#추론-시스템-개요)
  - [Predictor 클래스](#predictor-클래스)
  - [추론 시스템 사용 방법](#추론-시스템-사용-방법)
  - [생성 파라미터](#생성-파라미터)
  - [제출 파일 생성](#제출-파일-생성)
- [Part 4: LLM 파인튜닝](#part-4-llm-파인튜닝)
  - [LLM 파인튜닝 개요](#llm-파인튜닝-개요)
  - [LoRA Loader](#lora-loader)
  - [LLM Dataset](#llm-dataset)
  - [LLM Trainer](#llm-trainer)
  - [LLM 실행 명령어](#llm-실행-명령어)

---

# Part 1: 모델 로딩

## 모델 로더 개요

### 목적
- HuggingFace 사전학습 모델 자동 로드
- 토크나이저 초기화 및 특수 토큰 추가
- 디바이스 자동 감지 및 배치
- 임베딩 크기 자동 조정

### 핵심 기능
- ✅ Config 기반 모델 로딩
- ✅ 특수 토큰 자동 추가 및 임베딩 리사이즈
- ✅ GPU/CPU 자동 감지
- ✅ 모델 파라미터 정보 출력
- ✅ Logger 통합 지원

---

## ModelLoader 클래스

### 파일 위치
```
src/models/model_loader.py
```

### 클래스 구조

```python
class ModelLoader:
    def __init__(self, config: DictConfig, logger=None):
        """모델 로더 초기화"""

    def _get_device(self) -> torch.device:
        """사용할 디바이스 결정 (GPU/CPU)"""

    def load_tokenizer(self) -> PreTrainedTokenizer:
        """토크나이저 로드 및 특수 토큰 추가"""

    def load_model(self, tokenizer=None) -> PreTrainedModel:
        """사전학습 모델 로드"""

    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """모델과 토크나이저를 함께 로드"""
```

---

## 모델 로더 사용 방법

### 1. 기본 사용법

```python
from src.config import load_config
from src.models import load_model_and_tokenizer

# Config 로드
config = load_config("baseline_kobart")

# 모델 및 토크나이저 로드
model, tokenizer = load_model_and_tokenizer(config)

print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. Logger와 함께 사용

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.models import load_model_and_tokenizer

# Logger 초기화
log_path = create_log_path("train", "model_load.log")
logger = Logger(log_path, print_also=True)

# Logger를 전달하여 모델 로드
model, tokenizer = load_model_and_tokenizer(config, logger=logger)
```

**출력 예시:**
```
============================================================
모델 및 토크나이저 로딩 시작
============================================================
토크나이저 로딩: digit82/kobart-summarization
  → 특수 토큰 7개 추가됨
  → pad_token 설정: </s>

모델 로딩: digit82/kobart-summarization
  → 임베딩 크기 조정: 51200 → 51207
  → 디바이스: cuda
  → 전체 파라미터: 123,859,968
  → 학습 가능 파라미터: 123,859,968
============================================================
✅ 모델 및 토크나이저 로딩 완료
============================================================
```

### 3. ModelLoader 클래스 직접 사용

```python
from src.models.model_loader import ModelLoader

# ModelLoader 인스턴스 생성
loader = ModelLoader(config, logger=logger)

# 단계별 로드
tokenizer = loader.load_tokenizer()      # 1. 토크나이저만 로드
model = loader.load_model(tokenizer)     # 2. 모델 로드

# 또는 한 번에 로드
model, tokenizer = loader.load_model_and_tokenizer()
```

---

## 디바이스 관리

### 디바이스 자동 감지

ModelLoader는 다음 우선순위로 디바이스를 결정합니다:

1. **Config 설정 확인**
   ```yaml
   # configs/base/default.yaml
   training:
     device: "cuda"  # 또는 "cpu", "cuda:0", "cuda:1" 등
   ```

2. **자동 감지 (Config 없는 경우)**
   ```python
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   ```

### 특정 GPU 지정

```yaml
# configs/experiments/my_experiment.yaml
training:
  device: "cuda:1"  # 두 번째 GPU 사용
```

### 디바이스 관련 동작

```python
# 1. 디바이스 결정
device = loader._get_device()

# 2. 모델을 디바이스로 이동
model = model.to(device)

# 3. GPU 사용 불가 시 경고
# CUDA가 설정되었으나 사용 불가능합니다. CPU를 사용합니다.
```

---

## 특수 토큰 처리

### 특수 토큰 자동 추가

Config에서 특수 토큰 리스트를 정의하면 자동으로 추가됩니다:

```yaml
# configs/base/encoder_decoder.yaml
tokenizer:
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateAndTime#'
```

### 처리 과정

1. **토크나이저 로드**
   ```python
   tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)
   ```

2. **특수 토큰 추가**
   ```python
   special_tokens = list(config.model.special_tokens)
   num_added = tokenizer.add_special_tokens({
       'additional_special_tokens': special_tokens
   })
   # 출력: "특수 토큰 7개 추가됨"
   ```

3. **패딩 토큰 설정** (BART 계열 모델)
   ```python
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

4. **모델 임베딩 리사이즈**
   ```python
   vocab_size = len(tokenizer)
   model_vocab_size = model.config.vocab_size

   if vocab_size != model_vocab_size:
       model.resize_token_embeddings(vocab_size)
       # 출력: "임베딩 크기 조정: 51200 → 51207"
   ```

### 임베딩 크기 조정 이유

특수 토큰을 추가하면 토크나이저 어휘 크기가 증가하므로, 모델의 임베딩 레이어 크기도 함께 늘려야 합니다.

**변경 전:**
- 토크나이저 어휘: 51,200개
- 모델 임베딩: 51,200개

**변경 후:**
- 토크나이저 어휘: 51,207개 (+7)
- 모델 임베딩: 51,207개 (+7)

---

# Part 2: 학습 시스템

## 학습 시스템 개요

### 목적
- HuggingFace Seq2SeqTrainer 래핑
- Config 기반 학습 자동화
- WandB 로깅 통합
- ROUGE 평가 자동 실행
- 체크포인트 자동 관리

### 핵심 기능
- ✅ Seq2SeqTrainer 자동 설정
- ✅ WandB 로깅 통합
- ✅ ROUGE 자동 평가
- ✅ Early Stopping 지원
- ✅ 최상 모델 자동 저장
- ✅ Logger 통합 지원

---

## ModelTrainer 클래스

### 파일 위치
```
src/training/trainer.py
```

### 클래스 구조

```python
class ModelTrainer:
    def __init__(self, config, model, tokenizer, train_dataset,
                 eval_dataset=None, use_wandb=True, logger=None):
        """학습 시스템 초기화"""

    def _create_training_args(self) -> Seq2SeqTrainingArguments:
        """HuggingFace 학습 인자 생성"""

    def compute_metrics(self, eval_preds) -> Dict[str, float]:
        """평가 메트릭 계산 (ROUGE)"""

    def _create_trainer(self) -> Seq2SeqTrainer:
        """HuggingFace Seq2SeqTrainer 생성"""

    def train(self) -> Dict[str, Any]:
        """모델 학습 실행"""

    def evaluate(self) -> Dict[str, float]:
        """모델 평가 실행"""
```

---

## 학습 시스템 사용 방법

### 1. 기본 사용법

```python
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# 1. Config 로드
config = load_config("baseline_kobart")

# 2. 모델 로드
model, tokenizer = load_model_and_tokenizer(config)

# 3. 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")
eval_df = pd.read_csv("data/raw/dev.csv")

# 4. Dataset 생성
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=config.tokenizer.encoder_max_len,
    decoder_max_len=config.tokenizer.decoder_max_len
)

eval_dataset = DialogueSummarizationDataset(
    dialogues=eval_df['dialogue'].tolist(),
    summaries=eval_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=config.tokenizer.encoder_max_len,
    decoder_max_len=config.tokenizer.decoder_max_len
)

# 5. Trainer 생성 및 학습
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    use_wandb=True
)

# 6. 학습 실행
results = trainer.train()

print(f"최종 모델 경로: {results['final_model_path']}")
print(f"ROUGE Sum: {results['eval_metrics']['eval_rouge_sum']:.4f}")
```

### 2. Logger와 함께 사용

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# Logger 초기화
log_path = create_log_path("train", "train.log")
logger = Logger(log_path, print_also=True)
logger.start_redirect()

try:
    # Trainer 생성 (Logger 전달)
    trainer = create_trainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        use_wandb=True,
        logger=logger
    )

    # 학습 실행
    results = trainer.train()

finally:
    logger.stop_redirect()
    logger.close()
```

### 3. WandB 없이 학습

```python
# WandB 비활성화
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    use_wandb=False  # WandB 사용 안 함
)

results = trainer.train()
```

---

## 학습 인자 설정

### Config에서 자동 생성

`src/training/trainer.py`의 `_create_training_args` 함수가 Config를 기반으로 학습 인자를 자동 생성합니다:

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  epochs: 20
  batch_size: 50
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_steps: 500
  save_total_limit: 3
  logging_steps: 100
```

### 생성된 학습 인자

```python
Seq2SeqTrainingArguments(
    output_dir="outputs/baseline_kobart",
    overwrite_output_dir=True,

    # 학습 하이퍼파라미터
    num_train_epochs=20,
    per_device_train_batch_size=50,
    per_device_eval_batch_size=50,
    learning_rate=1e-5,
    weight_decay=0.01,
    warmup_steps=500,

    # 평가 및 저장
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model='eval_rouge_sum',

    # 로깅
    logging_dir="outputs/baseline_kobart/logs",
    logging_steps=100,
    report_to=['wandb'] if use_wandb else [],

    # Seq2Seq 특화
    predict_with_generate=True,
    generation_max_length=100,
    generation_num_beams=4,

    # 기타
    fp16=torch.cuda.is_available(),
    dataloader_num_workers=4
)
```

---

## WandB 통합

### WandB 설정

```yaml
# configs/experiments/baseline_kobart.yaml
wandb:
  enabled: true
  project: "nlp-competition"
  entity: "ieyeppo"

experiment:
  name: "baseline_kobart"
  tags:
    - "baseline"
    - "kobart"
```

### 자동 로깅 항목

WandB에 자동으로 로깅되는 항목:

1. **학습 메트릭**
   - train_loss
   - train_runtime
   - train_samples_per_second
   - train_steps_per_second

2. **평가 메트릭**
   - eval_rouge1
   - eval_rouge2
   - eval_rougeL
   - eval_rouge_sum
   - eval_loss

3. **Config 정보**
   - 전체 Config 파라미터
   - 실험 태그
   - 모델 체크포인트 이름

4. **시스템 정보**
   - GPU 사용률
   - 메모리 사용량
   - CPU 사용률

### WandB 초기화 및 종료

```python
# WandB Logger 초기화
if use_wandb and config.wandb.enabled:
    self.wandb_logger = WandbLogger(
        project_name=config.wandb.project,
        entity=config.wandb.entity,
        experiment_name=config.experiment.name,
        config=dict(config),
        tags=config.experiment.tags
    )

# 학습 시작 시 WandB Run 초기화
self.wandb_logger.init_run()

# 학습 종료 시 WandB Run 종료
self.wandb_logger.finish()
```

---

## 체크포인트 관리

### 자동 저장

학습 중 자동으로 체크포인트가 저장됩니다:

```
outputs/baseline_kobart/
├── checkpoint-500/          # 500 스텝 체크포인트
│   ├── config.json
│   ├── pytorch_model.bin
│   └── trainer_state.json
├── checkpoint-1000/         # 1000 스텝 체크포인트
├── checkpoint-1500/         # 1500 스텝 체크포인트
└── final_model/             # 최종 모델
    ├── config.json
    ├── pytorch_model.bin
    ├── tokenizer_config.json
    └── special_tokens_map.json
```

### 최상 모델 자동 로드

```python
# Config 설정
save_strategy='epoch',
load_best_model_at_end=True,           # 최상 모델 자동 로드
metric_for_best_model='eval_rouge_sum' # ROUGE Sum 기준
```

학습 종료 후 자동으로 가장 높은 ROUGE Sum을 달성한 체크포인트가 로드됩니다.

### 체크포인트 개수 제한

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  save_total_limit: 3  # 최대 3개 체크포인트만 유지
```

오래된 체크포인트는 자동으로 삭제되어 디스크 공간을 절약합니다.

---

# Part 3: 추론 시스템

## 추론 시스템 개요

### 목적
- 학습된 모델로 대화 요약 예측
- 배치 추론 및 진행 표시
- 제출 파일 자동 생성
- 생성 파라미터 유연한 설정

### 핵심 기능
- ✅ 단일/배치 추론 지원
- ✅ DataFrame 직접 처리
- ✅ 제출 파일 자동 생성
- ✅ 생성 파라미터 오버라이드
- ✅ Logger 통합 지원

---

## Predictor 클래스

### 파일 위치
```
src/inference/predictor.py
```

### 클래스 구조

```python
class Predictor:
    def __init__(self, model, tokenizer, config=None, device=None, logger=None):
        """추론 시스템 초기화"""

    def _setup_generation_config(self) -> Dict:
        """생성 파라미터 설정"""

    def predict_single(self, dialogue: str, **generation_kwargs) -> str:
        """단일 대화 요약 예측"""

    def predict_batch(self, dialogues: List[str], batch_size=32,
                     show_progress=True, **generation_kwargs) -> List[str]:
        """배치 대화 요약 예측"""

    def predict_dataframe(self, df: pd.DataFrame, batch_size=32,
                          show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """DataFrame에 대해 예측 수행"""

    def create_submission(self, test_df: pd.DataFrame, output_path: str,
                         batch_size=32, show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """제출 파일 생성"""
```

---

## 추론 시스템 사용 방법

### 1. 기본 사용법 (단일 예측)

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# 모델 및 토크나이저 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# Predictor 생성
predictor = create_predictor(model, tokenizer)

# 단일 예측
dialogue = "#Person1#: 안녕하세요 #Person2#: 네 안녕하세요"
summary = predictor.predict_single(dialogue)

print(f"예측 요약: {summary}")
```

### 2. 배치 예측

```python
dialogues = [
    "#Person1#: 저녁 뭐 먹을까? #Person2#: 김치찌개 어때?",
    "#Person1#: 내일 회의 몇 시야? #Person2#: 3시로 잡혔어",
    "#Person1#: 날씨 좋네 #Person2#: 산책 가자"
]

# 배치 예측
summaries = predictor.predict_batch(
    dialogues,
    batch_size=32,
    show_progress=True
)

for dialogue, summary in zip(dialogues, summaries):
    print(f"대화: {dialogue}")
    print(f"요약: {summary}\n")
```

### 3. DataFrame 예측

```python
import pandas as pd

# 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# DataFrame 예측
result_df = predictor.predict_dataframe(
    test_df,
    batch_size=32,
    show_progress=True
)

# 결과 확인
print(result_df[['fname', 'dialogue', 'summary']].head())
```

### 4. 제출 파일 생성

```python
# 제출 파일 자동 생성
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32,
    show_progress=True
)

print(f"제출 파일 생성 완료: submissions/submission.csv")
print(f"샘플 수: {len(submission_df)}")
```

---

## 생성 파라미터

### 기본 생성 파라미터

```python
default_config = {
    'max_length': 100,              # 최대 생성 길이
    'num_beams': 4,                 # Beam search 빔 개수
    'early_stopping': True,         # EOS 토큰 생성 시 조기 종료
    'no_repeat_ngram_size': 2,      # n-gram 반복 방지
    'length_penalty': 1.0,          # 길이 페널티
}
```

### Config에서 자동 로드

```yaml
# configs/experiments/baseline_kobart.yaml
inference:
  generate_max_length: 100
  num_beams: 4
  early_stopping: true
  no_repeat_ngram_size: 2
  length_penalty: 1.0
```

### 런타임 오버라이드

```python
# Config 값 무시하고 파라미터 지정
summaries = predictor.predict_batch(
    dialogues,
    batch_size=16,
    num_beams=8,            # 오버라이드
    max_length=150,         # 오버라이드
    temperature=0.8         # 추가 파라미터
)
```

### 주요 생성 파라미터 설명

| 파라미터 | 기본값 | 설명 |
|---------|-------|------|
| `max_length` | 100 | 생성할 최대 토큰 수 |
| `num_beams` | 4 | Beam search 빔 개수 (높을수록 품질↑, 속도↓) |
| `early_stopping` | True | EOS 토큰 생성 시 즉시 종료 |
| `no_repeat_ngram_size` | 2 | n-gram 반복 방지 크기 |
| `length_penalty` | 1.0 | 길이 페널티 (>1: 긴 문장 선호, <1: 짧은 문장 선호) |
| `temperature` | 1.0 | 샘플링 온도 (낮을수록 결정적, 높을수록 다양) |
| `top_k` | 50 | Top-k 샘플링 |
| `top_p` | 1.0 | Nucleus 샘플링 |

---

## 제출 파일 생성

### 기본 사용법

```python
import pandas as pd
from src.inference import create_predictor

# 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# Predictor 생성
predictor = create_predictor(model, tokenizer)

# 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# 제출 파일 생성
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32,
    show_progress=True
)
```

### Logger와 함께 사용

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# Logger 초기화
log_path = create_log_path("inference", "inference.log")
logger = Logger(log_path, print_also=True)
logger.start_redirect()

try:
    # Predictor 생성 (Logger 전달)
    predictor = create_predictor(model, tokenizer, logger=logger)

    # 제출 파일 생성
    submission_df = predictor.create_submission(
        test_df=test_df,
        output_path="submissions/submission.csv",
        batch_size=32,
        show_progress=True
    )

finally:
    logger.stop_redirect()
    logger.close()
```

### 출력 예시

```
============================================================
제출 파일 생성 시작
============================================================

샘플 수: 2500
Predicting: 100%|██████████| 79/79 [02:15<00:00,  1.71s/it]

✅ 제출 파일 저장 완료: submissions/submission.csv
============================================================
```

### 제출 파일 형식

```csv
fname,summary
test_001,두 사람이 저녁 약속을 잡았다
test_002,회의 시간을 3시로 정했다
test_003,내일 점심 메뉴는 김치찌개다
...
```

---

# Part 4: LLM 파인튜닝

## LLM 파인튜닝 개요

### 목적
- Causal LM (Llama, Qwen) 파인튜닝
- QLoRA 4-bit 양자화 지원
- LoRA (Low-Rank Adaptation) 지원
- Instruction/Chat Format 지원

### 핵심 기능
- ✅ QLoRA 4-bit 양자화
- ✅ LoRA 파라미터 효율적 학습
- ✅ Chat template 토큰 자동 추가
- ✅ Prompt truncation 방지
- ✅ Instruction Tuning 데이터 증강

---

## LoRA Loader

### 파일 위치
```
src/models/lora_loader.py
```

### 클래스 구조

```python
class LoRALoader:
    def __init__(self, config, logger=None)
    def load_model_and_tokenizer(use_lora=True, use_qlora=False)
    def _load_tokenizer(checkpoint)
    def _create_bnb_config()
    def _load_causal_lm(checkpoint, bnb_config)
    def _add_chat_tokens(model, tokenizer)
    def _apply_lora(model)
    def _configure_tokenizer(tokenizer)
```

### 주요 기능

#### 1. QLoRA 4-bit 양자화
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # Llama: bf16, Qwen: fp16
)
```

#### 2. LoRA 설정
```python
lora_config = LoraConfig(
    r=16,                    # LoRA rank
    lora_alpha=32,           # alpha = r * 2
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"       # MLP
    ],
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM
)
```

#### 3. Chat Template 토큰 추가
```python
# Llama 모델
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# Qwen 모델
chat_tokens = ["<|im_start|>", "<|im_end|>"]

tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
model.resize_token_embeddings(len(tokenizer))
```

#### 4. Prompt Truncation 방지
```python
# Left padding/truncation (Causal LM 필수)
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
```

---

## LLM Dataset

### 파일 위치
```
src/data/llm_dataset.py
```

### 클래스 구조

```python
class LLMSummarizationDataset(Dataset):
    def __init__(dialogues, summaries, tokenizer,
                 encoder_max_len=1024, decoder_max_len=200,
                 format_type="instruction")
    def __getitem__(idx)
    def _format_instruction(dialogue, summary)
    def _format_chat(dialogue, summary)

class InstructionAugmentedDataset(Dataset):
    # 5가지 instruction 템플릿으로 데이터 증강
```

### 데이터 포맷

#### Instruction Format
```
### Instruction:
다음 대화를 간결하게 요약해주세요.

### Input:
{dialogue}

### Response:
{summary}
```

#### Chat Format (Llama)
```
<|start_header_id|>system<|end_header_id|>
당신은 대화를 요약하는 전문가입니다.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
다음 대화를 요약해주세요:
{dialogue}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
{summary}<|eot_id|>
```

---

## LLM Trainer

### 파일 위치
```
src/training/llm_trainer.py
```

### 클래스 구조

```python
class LLMTrainer:
    def __init__(config, model, tokenizer, train_dataset, eval_dataset)
    def train()
    def evaluate()
    def _create_training_args()
    def _create_trainer()
```

### 학습 설정 (QLoRA 최적화)

```python
training_args = TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=8,      # effective batch=64
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    weight_decay=0.1,
    max_grad_norm=1.2,

    # QLoRA 최적화
    optim="paged_adamw_32bit",
    bf16=True,                          # Llama: bf16
    gradient_checkpointing=True,

    # 평가 및 저장
    eval_strategy='epoch',
    save_strategy='epoch',
    metric_for_best_model='eval_loss',  # Causal LM은 loss 사용
    greater_is_better=False
)
```

---

## LLM 실행 명령어

### 1. 기본 LLM 학습 (Llama-3.2-3B + QLoRA)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**결과 파일:**
- 모델: `outputs/llama_3.2_3b_qlora/final_model/`
- 로그: `logs/YYYYMMDD/train/train_llm_llama_3.2_3b_YYYYMMDD_HHMMSS.log`

### 2. Qwen 모델 학습

```bash
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

**결과 파일:**
- 모델: `outputs/qwen3_4b_qlora/final_model/`
- 로그: `logs/YYYYMMDD/train/train_llm_qwen3_4b_YYYYMMDD_HHMMSS.log`

### 3. Instruction Tuning (데이터 5배 증강)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**효과:**
- 학습 데이터: 12,457개 → 62,285개 (5배)
- 5가지 instruction 템플릿 적용

### 4. 디버그 모드 (빠른 테스트)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**디버그 모드 설정:**
- 데이터: 학습 50개, 검증 10개
- 에포크: 1회
- 배치 크기: 2
- WandB: 비활성화

---

## Config 파일

### Causal LM 기본 설정
**파일:** `configs/base/causal_lm.yaml`

```yaml
model:
  type: causal_lm
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"

lora:
  r: 16
  alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  dropout: 0.05
  use_qlora: true

tokenizer:
  encoder_max_len: 1024  # Prompt truncation 방지
  decoder_max_len: 200

training:
  epochs: 3
  batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.1
  max_grad_norm: 1.2
  gradient_checkpointing: true
```

### GPU 메모리 사용량

| 모델 | 파라미터 | QLoRA (4-bit) | Full Fine-tuning |
|------|----------|---------------|------------------|
| Llama-3.2-3B | 3B | **8GB** | 24GB |
| Qwen3-4B | 4B | **10GB** | 32GB |
| Llama-3-8B | 8B | **16GB** | 64GB |

### 성능 목표

#### Zero-shot 성능 (검증됨)

| 모델 | ROUGE-1 | ROUGE-2 | ROUGE-L | **ROUGE Sum** |
|------|---------|---------|---------|---------------|
| Llama-3.2-Korean-3B | 26.96 | 11.08 | 24.22 | **49.52** (1위) |
| Qwen3-4B | 24.22 | 9.23 | 21.79 | **45.02** (4위) |

#### 파인튜닝 목표

| 모델 | Zero-shot | 파인튜닝 목표 | 개선 목표 |
|------|-----------|---------------|-----------|
| Llama-3.2-Korean-3B | 49.52 | **95+** | +45 포인트 |
| Qwen3-4B | 45.02 | **95+** | +50 포인트 |

---

## 관련 파일

**소스 코드:**
- `src/models/model_loader.py` - ModelLoader 클래스
- `src/models/lora_loader.py` - LoRA Loader
- `src/training/trainer.py` - ModelTrainer 클래스
- `src/training/llm_trainer.py` - LLM Trainer
- `src/inference/predictor.py` - Predictor 클래스
- `src/data/llm_dataset.py` - LLM Dataset

**Config:**
- `configs/base/default.yaml` - 기본 설정
- `configs/base/encoder_decoder.yaml` - Seq2Seq 설정
- `configs/base/causal_lm.yaml` - Causal LM 설정
- `configs/models/kobart.yaml` - KoBART 설정
- `configs/models/llama_3.2_3b.yaml` - Llama 설정
- `configs/models/qwen3_4b.yaml` - Qwen 설정

**스크립트:**
- `scripts/train.py` - 학습 스크립트
- `scripts/train_llm.py` - LLM 학습 스크립트
- `scripts/inference.py` - 추론 스크립트

**관련 문서:**
- [01_시작_가이드.md](./01_시작_가이드.md) - 빠른 시작 가이드
- [02_핵심_시스템.md](./02_핵심_시스템.md) - 핵심 시스템 및 Config
- [06_데이터_파이프라인.md](./06_데이터_파이프라인.md) - 데이터 처리 및 증강
- [08_평가_최적화.md](./08_평가_최적화.md) - 평가 및 최적화
- [04_명령어_옵션_완전_가이드.md](./04_명령어_옵션_완전_가이드.md) - 전체 명령어 가이드
