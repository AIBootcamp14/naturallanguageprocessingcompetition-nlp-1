# 04. 모델 학습 및 추론 시스템

## 목차
- [Part 1: 모델 로딩](#part-1-모델-로딩)
  - [모델 로더 개요](#모델-로더-개요)
  - [ModelLoader 클래스](#modelloader-클래스)
  - [모델 로더 사용 방법](#모델-로더-사용-방법)
  - [디바이스 관리](#디바이스-관리)
  - [특수 토큰 처리](#특수-토큰-처리)
- [Part 2: 학습 시스템](#part-2-학습-시스템)
  - [학습 시스템 개요](#학습-시스템-개요)
  - [ModelTrainer 클래스](#modeltrainer-클래스)
  - [학습 시스템 사용 방법](#학습-시스템-사용-방법)
  - [학습 인자 설정](#학습-인자-설정)
  - [WandB 통합](#wandb-통합)
  - [체크포인트 관리](#체크포인트-관리)
- [Part 3: 추론 시스템](#part-3-추론-시스템)
  - [추론 시스템 개요](#추론-시스템-개요)
  - [Predictor 클래스](#predictor-클래스)
  - [추론 시스템 사용 방법](#추론-시스템-사용-방법)
  - [생성 파라미터](#생성-파라미터)
  - [제출 파일 생성](#제출-파일-생성)
- [Part 4: LLM 파인튜닝](#part-4-llm-파인튜닝)
  - [LLM 파인튜닝 개요](#llm-파인튜닝-개요)
  - [LoRA Loader](#lora-loader)
  - [LLM Dataset](#llm-dataset)
  - [LLM Trainer](#llm-trainer)
  - [LLM 실행 명령어](#llm-실행-명령어)

---

# Part 1: 모델 로딩

## 모델 로더 개요

### 목적
- HuggingFace 사전학습 모델 자동 로드
- 토크나이저 초기화 및 특수 토큰 추가
- 디바이스 자동 감지 및 배치
- 임베딩 크기 자동 조정

### 핵심 기능
- ✅ Config 기반 모델 로딩
- ✅ 특수 토큰 자동 추가 및 임베딩 리사이즈
- ✅ GPU/CPU 자동 감지
- ✅ 모델 파라미터 정보 출력
- ✅ Logger 통합 지원

---

## ModelLoader 클래스

### 파일 위치
```
src/models/model_loader.py
```

### 클래스 구조

```python
class ModelLoader:
    def __init__(self, config: DictConfig, logger=None):
        """모델 로더 초기화"""

    def _get_device(self) -> torch.device:
        """사용할 디바이스 결정 (GPU/CPU)"""

    def load_tokenizer(self) -> PreTrainedTokenizer:
        """토크나이저 로드 및 특수 토큰 추가"""

    def load_model(self, tokenizer=None) -> PreTrainedModel:
        """사전학습 모델 로드"""

    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """모델과 토크나이저를 함께 로드"""
```

---

## 모델 로더 사용 방법

### 1. 기본 사용법

```python
# ==================== 모델 로더 기본 사용 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer

# ---------------------- Config 로드 ---------------------- #
# baseline_kobart 설정 파일 로드
config = load_config("baseline_kobart")

# ---------------------- 모델 및 토크나이저 로드 ---------------------- #
# Config 기반으로 사전학습 모델 및 토크나이저 자동 로드
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- 모델 정보 출력 ---------------------- #
# 전체 파라미터 수 계산 및 출력
print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. Logger와 함께 사용

```python
# ==================== Logger와 함께 모델 로딩 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.models import load_model_and_tokenizer

# ---------------------- Logger 초기화 ---------------------- #
# 로그 파일 경로 생성 및 Logger 인스턴스 생성
log_path = create_log_path("train", "model_load.log")
logger = Logger(log_path, print_also=True)

# ---------------------- Logger를 전달하여 모델 로드 ---------------------- #
# Logger를 전달하면 모델 로딩 과정이 로그 파일에 기록됨
model, tokenizer = load_model_and_tokenizer(config, logger=logger)
```

**출력 예시:**
```
============================================================
모델 및 토크나이저 로딩 시작
============================================================
토크나이저 로딩: digit82/kobart-summarization
  → 특수 토큰 7개 추가됨
  → pad_token 설정: </s>

모델 로딩: digit82/kobart-summarization
  → 임베딩 크기 조정: 51200 → 51207
  → 디바이스: cuda
  → 전체 파라미터: 123,859,968
  → 학습 가능 파라미터: 123,859,968
============================================================
✅ 모델 및 토크나이저 로딩 완료
============================================================
```

### 3. ModelLoader 클래스 직접 사용

```python
# ==================== ModelLoader 클래스 직접 사용 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.models.model_loader import ModelLoader

# ---------------------- ModelLoader 인스턴스 생성 ---------------------- #
# Config 및 Logger를 전달하여 ModelLoader 인스턴스 생성
loader = ModelLoader(config, logger=logger)

# ---------------------- 단계별 로드 ---------------------- #
# 1. 토크나이저만 먼저 로드
tokenizer = loader.load_tokenizer()
# 2. 토크나이저를 전달하여 모델 로드
model = loader.load_model(tokenizer)

# ---------------------- 또는 한 번에 로드 ---------------------- #
# 모델과 토크나이저를 동시에 로드
model, tokenizer = loader.load_model_and_tokenizer()
```

---

## 디바이스 관리

### 디바이스 자동 감지

ModelLoader는 다음 우선순위로 디바이스를 결정합니다:

1. **Config 설정 확인**
   ```yaml
   # configs/base/default.yaml
   # ------------------------------- 학습 디바이스 설정 ------------------------------- #
   training:
     device: "cuda"                                      # 디바이스 설정 (cuda, cpu, cuda:0, cuda:1 등)
   ```

2. **자동 감지 (Config 없는 경우)**
   ```python
   # ---------------------- 디바이스 자동 감지 ---------------------- #
   # CUDA 사용 가능 여부에 따라 자동으로 디바이스 결정
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   ```

### 특정 GPU 지정

```yaml
# configs/experiments/my_experiment.yaml
# ------------------------------- 특정 GPU 지정 ------------------------------- #
training:
  device: "cuda:1"                                      # 두 번째 GPU 사용 (인덱스 1번)
```

### 디바이스 관련 동작

```python
# ==================== 디바이스 관리 예시 ==================== #

# ---------------------- 디바이스 결정 ---------------------- #
# Config 설정 또는 자동 감지를 통해 사용할 디바이스 결정
device = loader._get_device()

# ---------------------- 모델을 디바이스로 이동 ---------------------- #
# 결정된 디바이스(GPU 또는 CPU)로 모델 전송
model = model.to(device)

# ---------------------- GPU 사용 불가 시 경고 ---------------------- #
# CUDA가 설정되었으나 사용 불가능합니다. CPU를 사용합니다.
```

---

## 특수 토큰 처리

### 특수 토큰 자동 추가

Config에서 특수 토큰 리스트를 정의하면 자동으로 추가됩니다:

```yaml
# configs/base/encoder_decoder.yaml
# ------------------------------- 특수 토큰 설정 ------------------------------- #
tokenizer:
  special_tokens:                                       # 추가할 특수 토큰 리스트
    - '#Person1#'                                       # 대화 참여자 1
    - '#Person2#'                                       # 대화 참여자 2
    - '#PhoneNumber#'                                   # 전화번호 마스킹 토큰
    - '#Address#'                                       # 주소 마스킹 토큰
    - '#DateAndTime#'                                   # 날짜/시간 마스킹 토큰
```

### 처리 과정

1. **토크나이저 로드**
   ```python
   # ---------------------- 사전학습 토크나이저 로드 ---------------------- #
   # HuggingFace Hub에서 체크포인트에 해당하는 토크나이저 로드
   tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)
   ```

2. **특수 토큰 추가**
   ```python
   # ---------------------- 특수 토큰 추가 ---------------------- #
   # Config에 정의된 특수 토큰 리스트 가져오기
   special_tokens = list(config.model.special_tokens)
   # 토크나이저에 특수 토큰 추가
   num_added = tokenizer.add_special_tokens({
       'additional_special_tokens': special_tokens
   })
   # 출력: "특수 토큰 7개 추가됨"
   ```

3. **패딩 토큰 설정** (BART 계열 모델)
   ```python
   # ---------------------- 패딩 토큰 설정 ---------------------- #
   # 패딩 토큰이 없는 경우 EOS 토큰을 패딩 토큰으로 지정
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

4. **모델 임베딩 리사이즈**
   ```python
   # ---------------------- 모델 임베딩 크기 조정 ---------------------- #
   # 현재 토크나이저 어휘 크기 확인
   vocab_size = len(tokenizer)
   # 모델의 기존 어휘 크기 확인
   model_vocab_size = model.config.vocab_size

   # -------------- 어휘 크기 불일치 시 임베딩 리사이즈 -------------- #
   # 특수 토큰 추가로 어휘 크기가 증가한 경우 처리
   if vocab_size != model_vocab_size:
       model.resize_token_embeddings(vocab_size)
       # 출력: "임베딩 크기 조정: 51200 → 51207"
   ```

### 임베딩 크기 조정 이유

특수 토큰을 추가하면 토크나이저 어휘 크기가 증가하므로, 모델의 임베딩 레이어 크기도 함께 늘려야 합니다.

**변경 전:**
- 토크나이저 어휘: 51,200개
- 모델 임베딩: 51,200개

**변경 후:**
- 토크나이저 어휘: 51,207개 (+7)
- 모델 임베딩: 51,207개 (+7)

---

# Part 2: 학습 시스템

## 학습 시스템 개요

### 목적
- HuggingFace Seq2SeqTrainer 래핑
- Config 기반 학습 자동화
- WandB 로깅 통합
- ROUGE 평가 자동 실행
- 체크포인트 자동 관리

### 핵심 기능
- ✅ Seq2SeqTrainer 자동 설정
- ✅ WandB 로깅 통합
- ✅ ROUGE 자동 평가
- ✅ Early Stopping 지원
- ✅ 최상 모델 자동 저장
- ✅ Logger 통합 지원

---

## ModelTrainer 클래스

### 파일 위치
```
src/training/trainer.py
```

### 클래스 구조

```python
class ModelTrainer:
    def __init__(self, config, model, tokenizer, train_dataset,
                 eval_dataset=None, use_wandb=True, logger=None):
        """학습 시스템 초기화"""

    def _create_training_args(self) -> Seq2SeqTrainingArguments:
        """HuggingFace 학습 인자 생성"""

    def compute_metrics(self, eval_preds) -> Dict[str, float]:
        """평가 메트릭 계산 (ROUGE)"""

    def _create_trainer(self) -> Seq2SeqTrainer:
        """HuggingFace Seq2SeqTrainer 생성"""

    def train(self) -> Dict[str, Any]:
        """모델 학습 실행"""

    def evaluate(self) -> Dict[str, float]:
        """모델 평가 실행"""
```

---

## 학습 시스템 사용 방법

### 1. 기본 사용법

```python
# ==================== ModelTrainer 기본 사용 예시 ==================== #

# ---------------------- 필수 모듈 임포트 ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# ---------------------- Config 로드 ---------------------- #
# 베이스라인 KoBART 실험 설정 파일 로드
config = load_config("baseline_kobart")

# ---------------------- 모델 및 토크나이저 로드 ---------------------- #
# Config 기반으로 사전학습 모델과 토크나이저 자동 로드
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- 학습/검증 데이터 로드 ---------------------- #
# CSV 파일에서 학습 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")
# CSV 파일에서 검증 데이터 로드
eval_df = pd.read_csv("data/raw/dev.csv")

# ---------------------- PyTorch Dataset 생성 ---------------------- #
# 학습용 Dataset 생성
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),           # 대화 리스트
    summaries=train_df['summary'].tolist(),            # 요약 리스트
    tokenizer=tokenizer,                               # 토크나이저
    encoder_max_len=config.tokenizer.encoder_max_len,  # 인코더 최대 길이
    decoder_max_len=config.tokenizer.decoder_max_len   # 디코더 최대 길이
)

# 검증용 Dataset 생성
eval_dataset = DialogueSummarizationDataset(
    dialogues=eval_df['dialogue'].tolist(),            # 대화 리스트
    summaries=eval_df['summary'].tolist(),             # 요약 리스트
    tokenizer=tokenizer,                               # 토크나이저
    encoder_max_len=config.tokenizer.encoder_max_len,  # 인코더 최대 길이
    decoder_max_len=config.tokenizer.decoder_max_len   # 디코더 최대 길이
)

# ---------------------- Trainer 생성 ---------------------- #
# ModelTrainer 인스턴스 생성 및 WandB 로깅 활성화
trainer = create_trainer(
    config=config,                  # 실험 설정
    model=model,                    # 모델
    tokenizer=tokenizer,            # 토크나이저
    train_dataset=train_dataset,    # 학습 데이터셋
    eval_dataset=eval_dataset,      # 검증 데이터셋
    use_wandb=True                  # WandB 로깅 사용
)

# ---------------------- 학습 실행 ---------------------- #
# 모델 학습 시작 및 결과 저장
results = trainer.train()

# ---------------------- 결과 출력 ---------------------- #
# 최종 모델 저장 경로 출력
print(f"최종 모델 경로: {results['final_model_path']}")
# 검증 데이터셋에 대한 ROUGE Sum 점수 출력
print(f"ROUGE Sum: {results['eval_metrics']['eval_rouge_sum']:.4f}")
```

### 2. Logger와 함께 사용

```python
# ==================== Logger와 함께 ModelTrainer 사용 예시 ==================== #

# ---------------------- Logger 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger 초기화 ---------------------- #
# 로그 파일 경로 생성
log_path = create_log_path("train", "train.log")
# Logger 인스턴스 생성 (콘솔 출력도 활성화)
logger = Logger(log_path, print_also=True)
# 표준 출력 리다이렉션 시작 (print도 로그 파일로 기록)
logger.start_redirect()

# ---------------------- 예외 처리 블록 ---------------------- #
try:
    # -------------- Trainer 생성 (Logger 전달) -------------- #
    # Logger를 전달하여 학습 과정 로그 기록
    trainer = create_trainer(
        config=config,                  # 실험 설정
        model=model,                    # 모델
        tokenizer=tokenizer,            # 토크나이저
        train_dataset=train_dataset,    # 학습 데이터셋
        eval_dataset=eval_dataset,      # 검증 데이터셋
        use_wandb=True,                 # WandB 로깅 사용
        logger=logger                   # Logger 인스턴스
    )

    # -------------- 학습 실행 -------------- #
    # 모델 학습 시작 및 결과 저장
    results = trainer.train()

# ---------------------- 종료 처리 ---------------------- #
finally:
    # 표준 출력 리다이렉션 중지
    logger.stop_redirect()
    # Logger 파일 핸들러 닫기
    logger.close()
```

### 3. WandB 없이 학습

```python
# ==================== WandB 없이 학습 예시 ==================== #

# ---------------------- Trainer 생성 (WandB 비활성화) ---------------------- #
# WandB 로깅 없이 로컬에서만 학습 진행
trainer = create_trainer(
    config=config,                  # 실험 설정
    model=model,                    # 모델
    tokenizer=tokenizer,            # 토크나이저
    train_dataset=train_dataset,    # 학습 데이터셋
    eval_dataset=eval_dataset,      # 검증 데이터셋
    use_wandb=False                 # WandB 사용 안 함
)

# ---------------------- 학습 실행 ---------------------- #
# 모델 학습 시작 및 결과 저장
results = trainer.train()
```

---

## 학습 인자 설정

### Config에서 자동 생성

`src/training/trainer.py`의 `_create_training_args` 함수가 Config를 기반으로 학습 인자를 자동 생성합니다:

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 학습 하이퍼파라미터 설정 ------------------------------- #
training:
  epochs: 20                                            # 학습 에포크 수
  batch_size: 50                                        # 디바이스당 배치 크기
  learning_rate: 1e-5                                   # 학습률
  weight_decay: 0.01                                    # 가중치 감소 (L2 정규화)
  warmup_steps: 500                                     # 학습률 워밍업 스텝 수
  save_total_limit: 3                                   # 최대 저장 체크포인트 수
  logging_steps: 100                                    # 로깅 스텝 간격
```

### 생성된 학습 인자

```python
# ==================== Seq2SeqTrainingArguments 자동 생성 ==================== #

Seq2SeqTrainingArguments(
    # ---------------------- 출력 디렉토리 설정 ---------------------- #
    output_dir="outputs/baseline_kobart",       # 체크포인트 및 결과 저장 경로
    overwrite_output_dir=True,                  # 기존 디렉토리 덮어쓰기 허용

    # ---------------------- 학습 하이퍼파라미터 ---------------------- #
    num_train_epochs=20,                        # 학습 에포크 수
    per_device_train_batch_size=50,             # 학습 배치 크기 (디바이스당)
    per_device_eval_batch_size=50,              # 평가 배치 크기 (디바이스당)
    learning_rate=1e-5,                         # 학습률
    weight_decay=0.01,                          # 가중치 감소 (L2 정규화)
    warmup_steps=500,                           # 학습률 워밍업 스텝 수

    # ---------------------- 평가 및 저장 전략 ---------------------- #
    eval_strategy='epoch',                      # 에포크마다 평가 수행
    save_strategy='epoch',                      # 에포크마다 체크포인트 저장
    save_total_limit=3,                         # 최대 3개 체크포인트만 유지
    load_best_model_at_end=True,                # 학습 종료 시 최상 모델 자동 로드
    metric_for_best_model='eval_rouge_sum',     # 최상 모델 선정 기준 메트릭

    # ---------------------- 로깅 설정 ---------------------- #
    logging_dir="outputs/baseline_kobart/logs", # TensorBoard 로그 저장 경로
    logging_steps=100,                          # 100 스텝마다 로그 기록
    report_to=['wandb'] if use_wandb else [],   # WandB 로깅 활성화 여부

    # ---------------------- Seq2Seq 생성 설정 ---------------------- #
    predict_with_generate=True,                 # 평가 시 생성 모드 사용
    generation_max_length=100,                  # 생성 최대 길이
    generation_num_beams=4,                     # Beam search 빔 개수

    # ---------------------- 기타 최적화 설정 ---------------------- #
    fp16=torch.cuda.is_available(),             # GPU 사용 가능 시 FP16 자동 활성화
    dataloader_num_workers=4                    # 데이터 로딩 워커 수
)
```

---

## WandB 통합

### WandB 설정

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- WandB 로깅 설정 ------------------------------- #
wandb:
  enabled: true                                         # WandB 사용 여부
  project: "nlp-competition"                            # WandB 프로젝트명
  entity: "ieyeppo"                                     # WandB 엔티티 (사용자/팀)

# ------------------------------- 실험 설정 ------------------------------- #
experiment:
  name: "baseline_kobart"                               # 실험명
  tags:                                                 # 실험 태그 리스트
    - "baseline"                                        # 베이스라인 실험
    - "kobart"                                          # KoBART 모델
```

### 자동 로깅 항목

WandB에 자동으로 로깅되는 항목:

1. **학습 메트릭**
   - train_loss
   - train_runtime
   - train_samples_per_second
   - train_steps_per_second

2. **평가 메트릭**
   - eval_rouge1
   - eval_rouge2
   - eval_rougeL
   - eval_rouge_sum
   - eval_loss

3. **Config 정보**
   - 전체 Config 파라미터
   - 실험 태그
   - 모델 체크포인트 이름

4. **시스템 정보**
   - GPU 사용률
   - 메모리 사용량
   - CPU 사용률

### WandB 초기화 및 종료

```python
# ==================== WandB Logger 생명주기 관리 ==================== #

# ---------------------- WandB Logger 초기화 ---------------------- #
# WandB 사용이 활성화된 경우에만 초기화
if use_wandb and config.wandb.enabled:
    self.wandb_logger = WandbLogger(
        project_name=config.wandb.project,          # WandB 프로젝트명
        entity=config.wandb.entity,                 # WandB 엔티티 (사용자/팀)
        experiment_name=config.experiment.name,     # 실험명
        config=dict(config),                        # Config를 딕셔너리로 변환하여 저장
        tags=config.experiment.tags                 # 실험 태그 리스트
    )

# ---------------------- WandB Run 시작 ---------------------- #
# 학습 시작 시 WandB Run 초기화
self.wandb_logger.init_run()

# ---------------------- WandB Run 종료 ---------------------- #
# 학습 종료 시 WandB Run 종료 및 최종 로그 저장
self.wandb_logger.finish()
```

---

## 체크포인트 관리

### 자동 저장

학습 중 자동으로 체크포인트가 저장됩니다:

```
outputs/baseline_kobart/
├── checkpoint-500/          # 500 스텝 체크포인트
│   ├── config.json
│   ├── pytorch_model.bin
│   └── trainer_state.json
├── checkpoint-1000/         # 1000 스텝 체크포인트
├── checkpoint-1500/         # 1500 스텝 체크포인트
└── final_model/             # 최종 모델
    ├── config.json
    ├── pytorch_model.bin
    ├── tokenizer_config.json
    └── special_tokens_map.json
```

### 최상 모델 자동 로드

```python
# ==================== 최상 모델 자동 로드 설정 ==================== #

# ---------------------- Config 설정 ---------------------- #
save_strategy='epoch',                         # 에포크마다 체크포인트 저장
load_best_model_at_end=True,                   # 최상 모델 자동 로드
metric_for_best_model='eval_rouge_sum'         # ROUGE Sum 기준으로 최상 모델 선정
```

학습 종료 후 자동으로 가장 높은 ROUGE Sum을 달성한 체크포인트가 로드됩니다.

### 체크포인트 개수 제한

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 체크포인트 저장 제한 ------------------------------- #
training:
  save_total_limit: 3                                   # 최대 3개 체크포인트만 유지 (디스크 공간 절약)
```

오래된 체크포인트는 자동으로 삭제되어 디스크 공간을 절약합니다.

---

# Part 3: 추론 시스템

## 추론 시스템 개요

### 목적
- 학습된 모델로 대화 요약 예측
- 배치 추론 및 진행 표시
- 제출 파일 자동 생성
- 생성 파라미터 유연한 설정

### 핵심 기능
- ✅ 단일/배치 추론 지원
- ✅ DataFrame 직접 처리
- ✅ 제출 파일 자동 생성
- ✅ 생성 파라미터 오버라이드
- ✅ Logger 통합 지원

---

## Predictor 클래스

### 파일 위치
```
src/inference/predictor.py
```

### 클래스 구조

```python
class Predictor:
    def __init__(self, model, tokenizer, config=None, device=None, logger=None):
        """추론 시스템 초기화"""

    def _setup_generation_config(self) -> Dict:
        """생성 파라미터 설정"""

    def predict_single(self, dialogue: str, **generation_kwargs) -> str:
        """단일 대화 요약 예측"""

    def predict_batch(self, dialogues: List[str], batch_size=32,
                     show_progress=True, **generation_kwargs) -> List[str]:
        """배치 대화 요약 예측"""

    def predict_dataframe(self, df: pd.DataFrame, batch_size=32,
                          show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """DataFrame에 대해 예측 수행"""

    def create_submission(self, test_df: pd.DataFrame, output_path: str,
                         batch_size=32, show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """제출 파일 생성"""
```

---

## 추론 시스템 사용 방법

### 1. 기본 사용법 (단일 예측)

```python
# ==================== Predictor 기본 사용 예시 (단일 예측) ==================== #

# ---------------------- Transformers 모듈 임포트 ---------------------- #
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ---------------------- 학습된 모델 및 토크나이저 로드 ---------------------- #
# 저장된 최종 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# 저장된 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- Predictor 인스턴스 생성 ---------------------- #
# 모델과 토크나이저를 전달하여 Predictor 생성
predictor = create_predictor(model, tokenizer)

# ---------------------- 단일 대화 예측 ---------------------- #
# 예측할 대화 텍스트
dialogue = "#Person1#: 안녕하세요 #Person2#: 네 안녕하세요"
# 대화 요약 생성
summary = predictor.predict_single(dialogue)

# ---------------------- 결과 출력 ---------------------- #
# 생성된 요약 출력
print(f"예측 요약: {summary}")
```

### 2. 배치 예측

```python
# ==================== Predictor 배치 예측 예시 ==================== #

# ---------------------- 예측할 대화 리스트 준비 ---------------------- #
dialogues = [
    "#Person1#: 저녁 뭐 먹을까? #Person2#: 김치찌개 어때?",
    "#Person1#: 내일 회의 몇 시야? #Person2#: 3시로 잡혔어",
    "#Person1#: 날씨 좋네 #Person2#: 산책 가자"
]

# ---------------------- 배치 예측 실행 ---------------------- #
# 여러 대화를 한 번에 처리
summaries = predictor.predict_batch(
    dialogues,              # 대화 리스트
    batch_size=32,          # 배치 크기
    show_progress=True      # 진행률 표시 활성화
)

# ---------------------- 결과 출력 ---------------------- #
# 각 대화와 생성된 요약을 순회하며 출력
for dialogue, summary in zip(dialogues, summaries):
    print(f"대화: {dialogue}")
    print(f"요약: {summary}\n")
```

### 3. DataFrame 예측

```python
# ==================== Predictor DataFrame 예측 예시 ==================== #

# ---------------------- Pandas 모듈 임포트 ---------------------- #
import pandas as pd

# ---------------------- 테스트 데이터 로드 ---------------------- #
# CSV 파일에서 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- DataFrame 예측 실행 ---------------------- #
# DataFrame에 대해 배치 예측 수행
result_df = predictor.predict_dataframe(
    test_df,                # 테스트 데이터프레임
    batch_size=32,          # 배치 크기
    show_progress=True      # 진행률 표시 활성화
)

# ---------------------- 결과 확인 ---------------------- #
# 파일명, 대화, 요약 컬럼의 상위 5개 행 출력
print(result_df[['fname', 'dialogue', 'summary']].head())
```

### 4. 제출 파일 생성

```python
# ==================== 제출 파일 자동 생성 예시 ==================== #

# ---------------------- 제출 파일 생성 실행 ---------------------- #
# 테스트 데이터에 대해 예측하고 제출 형식으로 저장
submission_df = predictor.create_submission(
    test_df=test_df,                            # 테스트 데이터프레임
    output_path="submissions/submission.csv",   # 제출 파일 저장 경로
    batch_size=32,                              # 배치 크기
    show_progress=True                          # 진행률 표시 활성화
)

# ---------------------- 결과 출력 ---------------------- #
# 제출 파일 생성 완료 메시지
print(f"제출 파일 생성 완료: submissions/submission.csv")
# 총 예측 샘플 수 출력
print(f"샘플 수: {len(submission_df)}")
```

---

## 생성 파라미터

### 기본 생성 파라미터

```python
# ==================== 기본 생성 파라미터 설정 ==================== #

# ---------------------- 생성 파라미터 딕셔너리 ---------------------- #
default_config = {
    'max_length': 100,              # 최대 생성 길이
    'num_beams': 4,                 # Beam search 빔 개수
    'early_stopping': True,         # EOS 토큰 생성 시 조기 종료
    'no_repeat_ngram_size': 2,      # n-gram 반복 방지 크기
    'length_penalty': 1.0,          # 길이 페널티 (1.0: 중립)
}
```

### Config에서 자동 로드

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 추론 생성 파라미터 설정 ------------------------------- #
inference:
  generate_max_length: 100                              # 생성 최대 길이
  num_beams: 4                                          # Beam search 빔 개수
  early_stopping: true                                  # EOS 토큰 생성 시 조기 종료
  no_repeat_ngram_size: 2                               # n-gram 반복 방지 크기
  length_penalty: 1.0                                   # 길이 페널티 (1.0: 중립)
```

### 런타임 오버라이드

```python
# ==================== 생성 파라미터 런타임 오버라이드 예시 ==================== #

# ---------------------- Config 값 무시하고 파라미터 지정 ---------------------- #
# 예측 실행 시점에 생성 파라미터를 동적으로 변경
summaries = predictor.predict_batch(
    dialogues,              # 대화 리스트
    batch_size=16,          # 배치 크기
    num_beams=8,            # 오버라이드: Beam 개수 증가
    max_length=150,         # 오버라이드: 최대 길이 증가
    temperature=0.8         # 추가 파라미터: 샘플링 온도
)
```

### 주요 생성 파라미터 설명

| 파라미터 | 기본값 | 설명 |
|---------|-------|------|
| `max_length` | 100 | 생성할 최대 토큰 수 |
| `num_beams` | 4 | Beam search 빔 개수 (높을수록 품질↑, 속도↓) |
| `early_stopping` | True | EOS 토큰 생성 시 즉시 종료 |
| `no_repeat_ngram_size` | 2 | n-gram 반복 방지 크기 |
| `length_penalty` | 1.0 | 길이 페널티 (>1: 긴 문장 선호, <1: 짧은 문장 선호) |
| `temperature` | 1.0 | 샘플링 온도 (낮을수록 결정적, 높을수록 다양) |
| `top_k` | 50 | Top-k 샘플링 |
| `top_p` | 1.0 | Nucleus 샘플링 |

---

## 제출 파일 생성

### 기본 사용법

```python
# ==================== 제출 파일 생성 기본 사용법 ==================== #

# ---------------------- 필수 모듈 임포트 ---------------------- #
import pandas as pd
from src.inference import create_predictor

# ---------------------- 학습된 모델 로드 ---------------------- #
# 최종 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- Predictor 생성 ---------------------- #
# 모델과 토크나이저를 전달하여 Predictor 생성
predictor = create_predictor(model, tokenizer)

# ---------------------- 테스트 데이터 로드 ---------------------- #
# CSV 파일에서 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- 제출 파일 생성 ---------------------- #
# 테스트 데이터에 대해 예측하고 제출 형식으로 저장
submission_df = predictor.create_submission(
    test_df=test_df,                            # 테스트 데이터프레임
    output_path="submissions/submission.csv",   # 제출 파일 저장 경로
    batch_size=32,                              # 배치 크기
    show_progress=True                          # 진행률 표시 활성화
)
```

### Logger와 함께 사용

```python
# ==================== Logger와 함께 제출 파일 생성 예시 ==================== #

# ---------------------- Logger 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger 초기화 ---------------------- #
# 추론용 로그 파일 경로 생성
log_path = create_log_path("inference", "inference.log")
# Logger 인스턴스 생성 (콘솔 출력도 활성화)
logger = Logger(log_path, print_also=True)
# 표준 출력 리다이렉션 시작
logger.start_redirect()

# ---------------------- 예외 처리 블록 ---------------------- #
try:
    # -------------- Predictor 생성 (Logger 전달) -------------- #
    # Logger를 전달하여 추론 과정 로그 기록
    predictor = create_predictor(model, tokenizer, logger=logger)

    # -------------- 제출 파일 생성 -------------- #
    # 테스트 데이터에 대해 예측 수행
    submission_df = predictor.create_submission(
        test_df=test_df,                            # 테스트 데이터프레임
        output_path="submissions/submission.csv",   # 제출 파일 저장 경로
        batch_size=32,                              # 배치 크기
        show_progress=True                          # 진행률 표시 활성화
    )

# ---------------------- 종료 처리 ---------------------- #
finally:
    # 표준 출력 리다이렉션 중지
    logger.stop_redirect()
    # Logger 파일 핸들러 닫기
    logger.close()
```

### 출력 예시

```
============================================================
제출 파일 생성 시작
============================================================

샘플 수: 2500
Predicting: 100%|██████████| 79/79 [02:15<00:00,  1.71s/it]

✅ 제출 파일 저장 완료: submissions/submission.csv
============================================================
```

### 제출 파일 형식

```csv
fname,summary
test_001,두 사람이 저녁 약속을 잡았다
test_002,회의 시간을 3시로 정했다
test_003,내일 점심 메뉴는 김치찌개다
...
```

---

# Part 4: LLM 파인튜닝

## LLM 파인튜닝 개요

### 목적
- Causal LM (Llama, Qwen) 파인튜닝
- QLoRA 4-bit 양자화 지원
- LoRA (Low-Rank Adaptation) 지원
- Instruction/Chat Format 지원

### 핵심 기능
- ✅ QLoRA 4-bit 양자화
- ✅ LoRA 파라미터 효율적 학습
- ✅ Chat template 토큰 자동 추가
- ✅ Prompt truncation 방지
- ✅ Instruction Tuning 데이터 증강

---

## LoRA Loader

### 파일 위치
```
src/models/lora_loader.py
```

### 클래스 구조

```python
class LoRALoader:
    def __init__(self, config, logger=None)
    def load_model_and_tokenizer(use_lora=True, use_qlora=False)
    def _load_tokenizer(checkpoint)
    def _create_bnb_config()
    def _load_causal_lm(checkpoint, bnb_config)
    def _add_chat_tokens(model, tokenizer)
    def _apply_lora(model)
    def _configure_tokenizer(tokenizer)
```

### 주요 기능

#### 1. QLoRA 4-bit 양자화
```python
# ==================== QLoRA 4-bit 양자화 설정 ==================== #

# ---------------------- BitsAndBytes 양자화 Config 생성 ---------------------- #
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                          # 4-bit 양자화 활성화
    bnb_4bit_use_double_quant=True,             # 이중 양자화 사용 (메모리 절약)
    bnb_4bit_quant_type="nf4",                  # NF4 양자화 타입
    bnb_4bit_compute_dtype=torch.bfloat16       # 계산 데이터 타입 (Llama: bf16, Qwen: fp16)
)
```

#### 2. LoRA 설정
```python
# ==================== LoRA (Low-Rank Adaptation) 설정 ==================== #

# ---------------------- LoRA Config 생성 ---------------------- #
lora_config = LoraConfig(
    r=16,                    # LoRA rank (저랭크 행렬 차원)
    lora_alpha=32,           # alpha 스케일링 파라미터 (일반적으로 r * 2)
    target_modules=[
        # Attention 레이어
        "q_proj", "k_proj", "v_proj", "o_proj",
        # MLP 레이어
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,       # LoRA 레이어 드롭아웃
    task_type=TaskType.CAUSAL_LM  # Causal Language Modeling 태스크
)
```

#### 3. Chat Template 토큰 추가
```python
# ==================== Chat Template 특수 토큰 추가 ==================== #

# ---------------------- Llama 모델용 Chat 토큰 ---------------------- #
# Llama 3 Chat 템플릿 특수 토큰
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# ---------------------- Qwen 모델용 Chat 토큰 ---------------------- #
# Qwen Chat 템플릿 특수 토큰
chat_tokens = ["<|im_start|>", "<|im_end|>"]

# ---------------------- 토크나이저에 특수 토큰 추가 ---------------------- #
# Chat 템플릿 토큰을 토크나이저에 추가
tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
# 토큰 추가로 인한 임베딩 크기 조정
model.resize_token_embeddings(len(tokenizer))
```

#### 4. Prompt Truncation 방지
```python
# ==================== Prompt Truncation 방지 설정 ==================== #

# ---------------------- Left Padding/Truncation 설정 ---------------------- #
# Causal LM에서 프롬프트 보존을 위해 왼쪽부터 패딩 및 잘라내기
tokenizer.padding_side = "left"         # 왼쪽에서 패딩
tokenizer.truncation_side = "left"      # 왼쪽에서 잘라내기 (프롬프트 끝부분 보존)
```

---

## LLM Dataset

### 파일 위치
```
src/data/llm_dataset.py
```

### 클래스 구조

```python
class LLMSummarizationDataset(Dataset):
    def __init__(dialogues, summaries, tokenizer,
                 encoder_max_len=1024, decoder_max_len=200,
                 format_type="instruction")
    def __getitem__(idx)
    def _format_instruction(dialogue, summary)
    def _format_chat(dialogue, summary)

class InstructionAugmentedDataset(Dataset):
    # 5가지 instruction 템플릿으로 데이터 증강
```

### 데이터 포맷

#### Instruction Format
```
### Instruction:
다음 대화를 간결하게 요약해주세요.

### Input:
{dialogue}

### Response:
{summary}
```

#### Chat Format (Llama)
```
<|start_header_id|>system<|end_header_id|>
당신은 대화를 요약하는 전문가입니다.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
다음 대화를 요약해주세요:
{dialogue}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
{summary}<|eot_id|>
```

---

## LLM Trainer

### 파일 위치
```
src/training/llm_trainer.py
```

### 클래스 구조

```python
class LLMTrainer:
    def __init__(config, model, tokenizer, train_dataset, eval_dataset)
    def train()
    def evaluate()
    def _create_training_args()
    def _create_trainer()
```

### 학습 설정 (QLoRA 최적화)

```python
# ==================== LLM 학습 인자 설정 (QLoRA 최적화) ==================== #

# ---------------------- TrainingArguments 생성 ---------------------- #
training_args = TrainingArguments(
    # -------------- 기본 학습 설정 -------------- #
    num_train_epochs=3,                         # 학습 에포크 수
    per_device_train_batch_size=8,              # 디바이스당 배치 크기
    gradient_accumulation_steps=8,              # 그래디언트 누적 스텝 (effective batch=64)
    learning_rate=2e-5,                         # 학습률
    lr_scheduler_type="cosine",                 # Cosine 학습률 스케줄러
    warmup_ratio=0.1,                           # 워밍업 비율 (전체 스텝의 10%)
    weight_decay=0.1,                           # 가중치 감소 (L2 정규화)
    max_grad_norm=1.2,                          # 그래디언트 클리핑 임계값

    # -------------- QLoRA 최적화 설정 -------------- #
    optim="paged_adamw_32bit",                  # Paged AdamW 옵티마이저 (메모리 효율적)
    bf16=True,                                  # BFloat16 사용 (Llama: bf16, Qwen: fp16)
    gradient_checkpointing=True,                # 그래디언트 체크포인팅 (메모리 절약)

    # -------------- 평가 및 저장 전략 -------------- #
    eval_strategy='epoch',                      # 에포크마다 평가
    save_strategy='epoch',                      # 에포크마다 저장
    metric_for_best_model='eval_loss',          # 최상 모델 선정 기준 (Causal LM은 loss 사용)
    greater_is_better=False                     # Loss는 낮을수록 좋음
)
```

---

## LLM 실행 명령어

### 1. 기본 LLM 학습 (Llama-3.2-3B + QLoRA)

```bash
# ==================== 기본 LLM 학습 명령어 ==================== #

# ---------------------- Llama-3.2-3B 모델 학습 (QLoRA 사용) ---------------------- #
# QLoRA를 활용하여 메모리 효율적으로 Llama-3.2-3B 모델 파인튜닝
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**결과 파일:**
- 모델: `outputs/llama_3.2_3b_qlora/final_model/`
- 로그: `logs/YYYYMMDD/train/train_llm_llama_3.2_3b_YYYYMMDD_HHMMSS.log`

### 2. Qwen 모델 학습

```bash
# ==================== Qwen 모델 학습 명령어 ==================== #

# ---------------------- Qwen3-4B 모델 학습 (QLoRA 사용) ---------------------- #
# QLoRA를 활용하여 Qwen3-4B 모델 파인튜닝
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

**결과 파일:**
- 모델: `outputs/qwen3_4b_qlora/final_model/`
- 로그: `logs/YYYYMMDD/train/train_llm_qwen3_4b_YYYYMMDD_HHMMSS.log`

### 3. Instruction Tuning (데이터 5배 증강)

```bash
# ==================== Instruction Tuning 학습 명령어 ==================== #

# ---------------------- 데이터 증강을 활용한 Llama-3.2-3B 학습 ---------------------- #
# 5가지 Instruction 템플릿으로 데이터를 5배 증강하여 학습
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**효과:**
- 학습 데이터: 12,457개 → 62,285개 (5배)
- 5가지 instruction 템플릿 적용

### 4. 디버그 모드 (빠른 테스트)

```bash
# ==================== 디버그 모드 학습 명령어 ==================== #

# ---------------------- 빠른 테스트를 위한 디버그 모드 실행 ---------------------- #
# 소량의 데이터로 빠르게 파이프라인을 검증
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**디버그 모드 설정:**
- 데이터: 학습 50개, 검증 10개
- 에포크: 1회
- 배치 크기: 2
- WandB: 비활성화

---

## Config 파일

### Causal LM 기본 설정
**파일:** `configs/base/causal_lm.yaml`

```yaml
# ------------------------------- 모델 설정 ------------------------------- #
model:
  type: causal_lm                                         # 모델 타입 (Causal Language Model)
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"    # HuggingFace 모델 체크포인트

# ------------------------------- LoRA 설정 ------------------------------- #
lora:
  r: 16                                                   # LoRA rank (저랭크 행렬 차원)
  alpha: 32                                               # alpha 스케일링 파라미터
  target_modules:                                         # LoRA 적용 대상 모듈
    - "q_proj"                                            # Query projection
    - "k_proj"                                            # Key projection
    - "v_proj"                                            # Value projection
    - "o_proj"                                            # Output projection
    - "gate_proj"                                         # Gate projection (MLP)
    - "up_proj"                                           # Up projection (MLP)
    - "down_proj"                                         # Down projection (MLP)
  dropout: 0.05                                           # LoRA 레이어 드롭아웃 비율
  use_qlora: true                                         # QLoRA 4-bit 양자화 사용

# ------------------------------- 토크나이저 설정 ------------------------------- #
tokenizer:
  encoder_max_len: 1024                                   # 인코더 최대 길이 (Prompt truncation 방지)
  decoder_max_len: 200                                    # 디코더 최대 길이

# ------------------------------- 학습 설정 ------------------------------- #
training:
  epochs: 3                                               # 학습 에포크 수
  batch_size: 8                                           # 디바이스당 배치 크기
  gradient_accumulation_steps: 8                          # 그래디언트 누적 스텝 (effective batch=64)
  learning_rate: 2e-5                                     # 학습률
  lr_scheduler_type: "cosine"                             # 학습률 스케줄러 타입
  warmup_ratio: 0.1                                       # 워밍업 비율
  weight_decay: 0.1                                       # 가중치 감소 (L2 정규화)
  max_grad_norm: 1.2                                      # 그래디언트 클리핑 임계값
  gradient_checkpointing: true                            # 그래디언트 체크포인팅 사용
```

### GPU 메모리 사용량

| 모델 | 파라미터 | QLoRA (4-bit) | Full Fine-tuning |
|------|----------|---------------|------------------|
| Llama-3.2-3B | 3B | **8GB** | 24GB |
| Qwen3-4B | 4B | **10GB** | 32GB |
| Llama-3-8B | 8B | **16GB** | 64GB |

### 성능 목표

#### Zero-shot 성능 (검증됨)

| 모델 | ROUGE-1 | ROUGE-2 | ROUGE-L | **ROUGE Sum** |
|------|---------|---------|---------|---------------|
| Llama-3.2-Korean-3B | 26.96 | 11.08 | 24.22 | **49.52** (1위) |
| Qwen3-4B | 24.22 | 9.23 | 21.79 | **45.02** (4위) |

#### 파인튜닝 목표

| 모델 | Zero-shot | 파인튜닝 목표 | 개선 목표 |
|------|-----------|---------------|-----------|
| Llama-3.2-Korean-3B | 49.52 | **95+** | +45 포인트 |
| Qwen3-4B | 45.02 | **95+** | +50 포인트 |

---

## 관련 파일

**소스 코드:**
- `src/models/model_loader.py` - ModelLoader 클래스
- `src/models/lora_loader.py` - LoRA Loader
- `src/training/trainer.py` - ModelTrainer 클래스
- `src/training/llm_trainer.py` - LLM Trainer
- `src/inference/predictor.py` - Predictor 클래스
- `src/data/llm_dataset.py` - LLM Dataset

**Config:**
- `configs/base/default.yaml` - 기본 설정
- `configs/base/encoder_decoder.yaml` - Seq2Seq 설정
- `configs/base/causal_lm.yaml` - Causal LM 설정
- `configs/models/kobart.yaml` - KoBART 설정
- `configs/models/llama_3.2_3b.yaml` - Llama 설정
- `configs/models/qwen3_4b.yaml` - Qwen 설정

**스크립트:**
- `scripts/train.py` - 학습 스크립트
- `scripts/train_llm.py` - LLM 학습 스크립트
- `scripts/inference.py` - 추론 스크립트

**관련 문서:**
- [01_시작_가이드.md](./01_시작_가이드.md) - 빠른 시작 가이드
- [02_핵심_시스템.md](./02_핵심_시스템.md) - 핵심 시스템 및 Config
- [06_데이터_파이프라인.md](./06_데이터_파이프라인.md) - 데이터 처리 및 증강
- [08_평가_최적화.md](./08_평가_최적화.md) - 평가 및 최적화
- [04_명령어_옵션_완전_가이드.md](./04_명령어_옵션_완전_가이드.md) - 전체 명령어 가이드
