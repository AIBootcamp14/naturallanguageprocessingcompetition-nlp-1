# 04. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡  ì‹œìŠ¤í…œ

## ëª©ì°¨
- [Part 1: ëª¨ë¸ ë¡œë”©](#part-1-ëª¨ë¸-ë¡œë”©)
  - [ëª¨ë¸ ë¡œë” ê°œìš”](#ëª¨ë¸-ë¡œë”-ê°œìš”)
  - [ModelLoader í´ë˜ìŠ¤](#modelloader-í´ë˜ìŠ¤)
  - [ëª¨ë¸ ë¡œë” ì‚¬ìš© ë°©ë²•](#ëª¨ë¸-ë¡œë”-ì‚¬ìš©-ë°©ë²•)
  - [ë””ë°”ì´ìŠ¤ ê´€ë¦¬](#ë””ë°”ì´ìŠ¤-ê´€ë¦¬)
  - [íŠ¹ìˆ˜ í† í° ì²˜ë¦¬](#íŠ¹ìˆ˜-í† í°-ì²˜ë¦¬)
- [Part 2: í•™ìŠµ ì‹œìŠ¤í…œ](#part-2-í•™ìŠµ-ì‹œìŠ¤í…œ)
  - [í•™ìŠµ ì‹œìŠ¤í…œ ê°œìš”](#í•™ìŠµ-ì‹œìŠ¤í…œ-ê°œìš”)
  - [ModelTrainer í´ë˜ìŠ¤](#modeltrainer-í´ë˜ìŠ¤)
  - [í•™ìŠµ ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•](#í•™ìŠµ-ì‹œìŠ¤í…œ-ì‚¬ìš©-ë°©ë²•)
  - [í•™ìŠµ ì¸ì ì„¤ì •](#í•™ìŠµ-ì¸ì-ì„¤ì •)
  - [WandB í†µí•©](#wandb-í†µí•©)
  - [ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬](#ì²´í¬í¬ì¸íŠ¸-ê´€ë¦¬)
- [Part 3: ì¶”ë¡  ì‹œìŠ¤í…œ](#part-3-ì¶”ë¡ -ì‹œìŠ¤í…œ)
  - [ì¶”ë¡  ì‹œìŠ¤í…œ ê°œìš”](#ì¶”ë¡ -ì‹œìŠ¤í…œ-ê°œìš”)
  - [Predictor í´ë˜ìŠ¤](#predictor-í´ë˜ìŠ¤)
  - [ì¶”ë¡  ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•](#ì¶”ë¡ -ì‹œìŠ¤í…œ-ì‚¬ìš©-ë°©ë²•)
  - [ìƒì„± íŒŒë¼ë¯¸í„°](#ìƒì„±-íŒŒë¼ë¯¸í„°)
  - [ì œì¶œ íŒŒì¼ ìƒì„±](#ì œì¶œ-íŒŒì¼-ìƒì„±)
- [Part 4: LLM íŒŒì¸íŠœë‹](#part-4-llm-íŒŒì¸íŠœë‹)
  - [LLM íŒŒì¸íŠœë‹ ê°œìš”](#llm-íŒŒì¸íŠœë‹-ê°œìš”)
  - [LoRA Loader](#lora-loader)
    - [ì£¼ìš” ê¸°ëŠ¥](#ì£¼ìš”-ê¸°ëŠ¥)
    - [ì£¼ì˜ì‚¬í•­ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#ì£¼ì˜ì‚¬í•­-ë°-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
  - [LLM Dataset](#llm-dataset)
  - [LLM Trainer](#llm-trainer)
  - [LLM ì‹¤í–‰ ëª…ë ¹ì–´](#llm-ì‹¤í–‰-ëª…ë ¹ì–´)

---

# Part 1: ëª¨ë¸ ë¡œë”©

## ëª¨ë¸ ë¡œë” ê°œìš”

### ëª©ì 
- HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ìë™ ë¡œë“œ
- í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€
- ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ë°°ì¹˜
- ì„ë² ë”© í¬ê¸° ìë™ ì¡°ì •

### í•µì‹¬ ê¸°ëŠ¥
- âœ… Config ê¸°ë°˜ ëª¨ë¸ ë¡œë”©
- âœ… íŠ¹ìˆ˜ í† í° ìë™ ì¶”ê°€ ë° ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ
- âœ… GPU/CPU ìë™ ê°ì§€
- âœ… ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
- âœ… Logger í†µí•© ì§€ì›

---

## ModelLoader í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/models/model_loader.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class ModelLoader:
    def __init__(self, config: DictConfig, logger=None):
        """ëª¨ë¸ ë¡œë” ì´ˆê¸°í™”"""

    def _get_device(self) -> torch.device:
        """ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì • (GPU/CPU)"""

    def load_tokenizer(self) -> PreTrainedTokenizer:
        """í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŠ¹ìˆ˜ í† í° ì¶”ê°€"""

    def load_model(self, tokenizer=None) -> PreTrainedModel:
        """ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ"""

    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•¨ê»˜ ë¡œë“œ"""
```

---

## ëª¨ë¸ ë¡œë” ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
# ==================== ëª¨ë¸ ë¡œë” ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer

# ---------------------- Config ë¡œë“œ ---------------------- #
# baseline_kobart ì„¤ì • íŒŒì¼ ë¡œë“œ
config = load_config("baseline_kobart")

# ---------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# Config ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ìë™ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- ëª¨ë¸ ì •ë³´ ì¶œë ¥ ---------------------- #
# ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ë° ì¶œë ¥
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. Loggerì™€ í•¨ê»˜ ì‚¬ìš©

```python
# ==================== Loggerì™€ í•¨ê»˜ ëª¨ë¸ ë¡œë”© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.models import load_model_and_tokenizer

# ---------------------- Logger ì´ˆê¸°í™” ---------------------- #
# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„± ë° Logger ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
log_path = create_log_path("train", "model_load.log")
logger = Logger(log_path, print_also=True)

# ---------------------- Loggerë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ ë¡œë“œ ---------------------- #
# Loggerë¥¼ ì „ë‹¬í•˜ë©´ ëª¨ë¸ ë¡œë”© ê³¼ì •ì´ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ë¨
model, tokenizer = load_model_and_tokenizer(config, logger=logger)
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
============================================================
ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘
============================================================
í† í¬ë‚˜ì´ì € ë¡œë”©: digit82/kobart-summarization
  â†’ íŠ¹ìˆ˜ í† í° 7ê°œ ì¶”ê°€ë¨
  â†’ pad_token ì„¤ì •: </s>

ëª¨ë¸ ë¡œë”©: digit82/kobart-summarization
  â†’ ì„ë² ë”© í¬ê¸° ì¡°ì •: 51200 â†’ 51207
  â†’ ë””ë°”ì´ìŠ¤: cuda
  â†’ ì „ì²´ íŒŒë¼ë¯¸í„°: 123,859,968
  â†’ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 123,859,968
============================================================
âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ
============================================================
```

### 3. ModelLoader í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš©

```python
# ==================== ModelLoader í´ë˜ìŠ¤ ì§ì ‘ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.models.model_loader import ModelLoader

# ---------------------- ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---------------------- #
# Config ë° Loggerë¥¼ ì „ë‹¬í•˜ì—¬ ModelLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
loader = ModelLoader(config, logger=logger)

# ---------------------- ë‹¨ê³„ë³„ ë¡œë“œ ---------------------- #
# 1. í† í¬ë‚˜ì´ì €ë§Œ ë¨¼ì € ë¡œë“œ
tokenizer = loader.load_tokenizer()
# 2. í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ ë¡œë“œ
model = loader.load_model(tokenizer)

# ---------------------- ë˜ëŠ” í•œ ë²ˆì— ë¡œë“œ ---------------------- #
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë™ì‹œì— ë¡œë“œ
model, tokenizer = loader.load_model_and_tokenizer()
```

---

## ë””ë°”ì´ìŠ¤ ê´€ë¦¬

### ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€

ModelLoaderëŠ” ë‹¤ìŒ ìš°ì„ ìˆœìœ„ë¡œ ë””ë°”ì´ìŠ¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤:

1. **Config ì„¤ì • í™•ì¸**
   ```yaml
   # configs/base/default.yaml
   # ------------------------------- í•™ìŠµ ë””ë°”ì´ìŠ¤ ì„¤ì • ------------------------------- #
   training:
     device: "cuda"                                      # ë””ë°”ì´ìŠ¤ ì„¤ì • (cuda, cpu, cuda:0, cuda:1 ë“±)
   ```

2. **ìë™ ê°ì§€ (Config ì—†ëŠ” ê²½ìš°)**
   ```python
   # ---------------------- ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ---------------------- #
   # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ê²°ì •
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   ```

### íŠ¹ì • GPU ì§€ì •

```yaml
# configs/experiments/my_experiment.yaml
# ------------------------------- íŠ¹ì • GPU ì§€ì • ------------------------------- #
training:
  device: "cuda:1"                                      # ë‘ ë²ˆì§¸ GPU ì‚¬ìš© (ì¸ë±ìŠ¤ 1ë²ˆ)
```

### ë””ë°”ì´ìŠ¤ ê´€ë ¨ ë™ì‘

```python
# ==================== ë””ë°”ì´ìŠ¤ ê´€ë¦¬ ì˜ˆì‹œ ==================== #

# ---------------------- ë””ë°”ì´ìŠ¤ ê²°ì • ---------------------- #
# Config ì„¤ì • ë˜ëŠ” ìë™ ê°ì§€ë¥¼ í†µí•´ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •
device = loader._get_device()

# ---------------------- ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ ---------------------- #
# ê²°ì •ëœ ë””ë°”ì´ìŠ¤(GPU ë˜ëŠ” CPU)ë¡œ ëª¨ë¸ ì „ì†¡
model = model.to(device)

# ---------------------- GPU ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²½ê³  ---------------------- #
# CUDAê°€ ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```

---

## íŠ¹ìˆ˜ í† í° ì²˜ë¦¬

### íŠ¹ìˆ˜ í† í° ìë™ ì¶”ê°€

Configì—ì„œ íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤:

```yaml
# configs/base/encoder_decoder.yaml
# ------------------------------- íŠ¹ìˆ˜ í† í° ì„¤ì • ------------------------------- #
tokenizer:
  special_tokens:                                       # ì¶”ê°€í•  íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸
    - '#Person1#'                                       # ëŒ€í™” ì°¸ì—¬ì 1
    - '#Person2#'                                       # ëŒ€í™” ì°¸ì—¬ì 2
    - '#PhoneNumber#'                                   # ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹ í† í°
    - '#Address#'                                       # ì£¼ì†Œ ë§ˆìŠ¤í‚¹ í† í°
    - '#DateAndTime#'                                   # ë‚ ì§œ/ì‹œê°„ ë§ˆìŠ¤í‚¹ í† í°
```

### ì²˜ë¦¬ ê³¼ì •

1. **í† í¬ë‚˜ì´ì € ë¡œë“œ**
   ```python
   # ---------------------- ì‚¬ì „í•™ìŠµ í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
   # HuggingFace Hubì—ì„œ ì²´í¬í¬ì¸íŠ¸ì— í•´ë‹¹í•˜ëŠ” í† í¬ë‚˜ì´ì € ë¡œë“œ
   tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)
   ```

2. **íŠ¹ìˆ˜ í† í° ì¶”ê°€**
   ```python
   # ---------------------- íŠ¹ìˆ˜ í† í° ì¶”ê°€ ---------------------- #
   # Configì— ì •ì˜ëœ íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
   special_tokens = list(config.model.special_tokens)
   # í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€
   num_added = tokenizer.add_special_tokens({
       'additional_special_tokens': special_tokens
   })
   # ì¶œë ¥: "íŠ¹ìˆ˜ í† í° 7ê°œ ì¶”ê°€ë¨"
   ```

3. **íŒ¨ë”© í† í° ì„¤ì •** (BART ê³„ì—´ ëª¨ë¸)
   ```python
   # ---------------------- íŒ¨ë”© í† í° ì„¤ì • ---------------------- #
   # íŒ¨ë”© í† í°ì´ ì—†ëŠ” ê²½ìš° EOS í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì§€ì •
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

4. **ëª¨ë¸ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ**
   ```python
   # ---------------------- ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • ---------------------- #
   # í˜„ì¬ í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° í™•ì¸
   vocab_size = len(tokenizer)
   # ëª¨ë¸ì˜ ê¸°ì¡´ ì–´íœ˜ í¬ê¸° í™•ì¸
   model_vocab_size = model.config.vocab_size

   # -------------- ì–´íœ˜ í¬ê¸° ë¶ˆì¼ì¹˜ ì‹œ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ -------------- #
   # íŠ¹ìˆ˜ í† í° ì¶”ê°€ë¡œ ì–´íœ˜ í¬ê¸°ê°€ ì¦ê°€í•œ ê²½ìš° ì²˜ë¦¬
   if vocab_size != model_vocab_size:
       model.resize_token_embeddings(vocab_size)
       # ì¶œë ¥: "ì„ë² ë”© í¬ê¸° ì¡°ì •: 51200 â†’ 51207"
   ```

### ì„ë² ë”© í¬ê¸° ì¡°ì • ì´ìœ 

íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•˜ë©´ í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸°ê°€ ì¦ê°€í•˜ë¯€ë¡œ, ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸°ë„ í•¨ê»˜ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.

**ë³€ê²½ ì „:**
- í† í¬ë‚˜ì´ì € ì–´íœ˜: 51,200ê°œ
- ëª¨ë¸ ì„ë² ë”©: 51,200ê°œ

**ë³€ê²½ í›„:**
- í† í¬ë‚˜ì´ì € ì–´íœ˜: 51,207ê°œ (+7)
- ëª¨ë¸ ì„ë² ë”©: 51,207ê°œ (+7)

---

# Part 2: í•™ìŠµ ì‹œìŠ¤í…œ

## í•™ìŠµ ì‹œìŠ¤í…œ ê°œìš”

### ëª©ì 
- HuggingFace Seq2SeqTrainer ë˜í•‘
- Config ê¸°ë°˜ í•™ìŠµ ìë™í™”
- WandB ë¡œê¹… í†µí•©
- ROUGE í‰ê°€ ìë™ ì‹¤í–‰
- ì²´í¬í¬ì¸íŠ¸ ìë™ ê´€ë¦¬

### í•µì‹¬ ê¸°ëŠ¥
- âœ… Seq2SeqTrainer ìë™ ì„¤ì •
- âœ… WandB ë¡œê¹… í†µí•©
- âœ… ROUGE ìë™ í‰ê°€
- âœ… Early Stopping ì§€ì›
- âœ… ìµœìƒ ëª¨ë¸ ìë™ ì €ì¥
- âœ… Logger í†µí•© ì§€ì›

---

## ModelTrainer í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/training/trainer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class ModelTrainer:
    def __init__(self, config, model, tokenizer, train_dataset,
                 eval_dataset=None, use_wandb=True, logger=None):
        """í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""

    def _create_training_args(self) -> Seq2SeqTrainingArguments:
        """HuggingFace í•™ìŠµ ì¸ì ìƒì„±"""

    def compute_metrics(self, eval_preds) -> Dict[str, float]:
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE)"""

    def _create_trainer(self) -> Seq2SeqTrainer:
        """HuggingFace Seq2SeqTrainer ìƒì„±"""

    def train(self) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""

    def evaluate(self) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
```

---

## í•™ìŠµ ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
# ==================== ModelTrainer ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# ---------------------- Config ë¡œë“œ ---------------------- #
# ë² ì´ìŠ¤ë¼ì¸ KoBART ì‹¤í—˜ ì„¤ì • íŒŒì¼ ë¡œë“œ
config = load_config("baseline_kobart")

# ---------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# Config ê¸°ë°˜ìœ¼ë¡œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ìë™ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¡œë“œ ---------------------- #
# CSV íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")
# CSV íŒŒì¼ì—ì„œ ê²€ì¦ ë°ì´í„° ë¡œë“œ
eval_df = pd.read_csv("data/raw/dev.csv")

# ---------------------- PyTorch Dataset ìƒì„± ---------------------- #
# í•™ìŠµìš© Dataset ìƒì„±
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),           # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    summaries=train_df['summary'].tolist(),            # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer=tokenizer,                               # í† í¬ë‚˜ì´ì €
    encoder_max_len=config.tokenizer.encoder_max_len,  # ì¸ì½”ë” ìµœëŒ€ ê¸¸ì´
    decoder_max_len=config.tokenizer.decoder_max_len   # ë””ì½”ë” ìµœëŒ€ ê¸¸ì´
)

# ê²€ì¦ìš© Dataset ìƒì„±
eval_dataset = DialogueSummarizationDataset(
    dialogues=eval_df['dialogue'].tolist(),            # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    summaries=eval_df['summary'].tolist(),             # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer=tokenizer,                               # í† í¬ë‚˜ì´ì €
    encoder_max_len=config.tokenizer.encoder_max_len,  # ì¸ì½”ë” ìµœëŒ€ ê¸¸ì´
    decoder_max_len=config.tokenizer.decoder_max_len   # ë””ì½”ë” ìµœëŒ€ ê¸¸ì´
)

# ---------------------- Trainer ìƒì„± ---------------------- #
# ModelTrainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° WandB ë¡œê¹… í™œì„±í™”
trainer = create_trainer(
    config=config,                  # ì‹¤í—˜ ì„¤ì •
    model=model,                    # ëª¨ë¸
    tokenizer=tokenizer,            # í† í¬ë‚˜ì´ì €
    train_dataset=train_dataset,    # í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=eval_dataset,      # ê²€ì¦ ë°ì´í„°ì…‹
    use_wandb=True                  # WandB ë¡œê¹… ì‚¬ìš©
)

# ---------------------- í•™ìŠµ ì‹¤í–‰ ---------------------- #
# ëª¨ë¸ í•™ìŠµ ì‹œì‘ ë° ê²°ê³¼ ì €ì¥
results = trainer.train()

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
# ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì¶œë ¥
print(f"ìµœì¢… ëª¨ë¸ ê²½ë¡œ: {results['final_model_path']}")
# ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ROUGE Sum ì ìˆ˜ ì¶œë ¥
print(f"ROUGE Sum: {results['eval_metrics']['eval_rouge_sum']:.4f}")
```

### 2. Loggerì™€ í•¨ê»˜ ì‚¬ìš©

```python
# ==================== Loggerì™€ í•¨ê»˜ ModelTrainer ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- Logger ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger ì´ˆê¸°í™” ---------------------- #
# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
log_path = create_log_path("train", "train.log")
# Logger ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì½˜ì†” ì¶œë ¥ë„ í™œì„±í™”)
logger = Logger(log_path, print_also=True)
# í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ ì‹œì‘ (printë„ ë¡œê·¸ íŒŒì¼ë¡œ ê¸°ë¡)
logger.start_redirect()

# ---------------------- ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ ---------------------- #
try:
    # -------------- Trainer ìƒì„± (Logger ì „ë‹¬) -------------- #
    # Loggerë¥¼ ì „ë‹¬í•˜ì—¬ í•™ìŠµ ê³¼ì • ë¡œê·¸ ê¸°ë¡
    trainer = create_trainer(
        config=config,                  # ì‹¤í—˜ ì„¤ì •
        model=model,                    # ëª¨ë¸
        tokenizer=tokenizer,            # í† í¬ë‚˜ì´ì €
        train_dataset=train_dataset,    # í•™ìŠµ ë°ì´í„°ì…‹
        eval_dataset=eval_dataset,      # ê²€ì¦ ë°ì´í„°ì…‹
        use_wandb=True,                 # WandB ë¡œê¹… ì‚¬ìš©
        logger=logger                   # Logger ì¸ìŠ¤í„´ìŠ¤
    )

    # -------------- í•™ìŠµ ì‹¤í–‰ -------------- #
    # ëª¨ë¸ í•™ìŠµ ì‹œì‘ ë° ê²°ê³¼ ì €ì¥
    results = trainer.train()

# ---------------------- ì¢…ë£Œ ì²˜ë¦¬ ---------------------- #
finally:
    # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ ì¤‘ì§€
    logger.stop_redirect()
    # Logger íŒŒì¼ í•¸ë“¤ëŸ¬ ë‹«ê¸°
    logger.close()
```

### 3. WandB ì—†ì´ í•™ìŠµ

```python
# ==================== WandB ì—†ì´ í•™ìŠµ ì˜ˆì‹œ ==================== #

# ---------------------- Trainer ìƒì„± (WandB ë¹„í™œì„±í™”) ---------------------- #
# WandB ë¡œê¹… ì—†ì´ ë¡œì»¬ì—ì„œë§Œ í•™ìŠµ ì§„í–‰
trainer = create_trainer(
    config=config,                  # ì‹¤í—˜ ì„¤ì •
    model=model,                    # ëª¨ë¸
    tokenizer=tokenizer,            # í† í¬ë‚˜ì´ì €
    train_dataset=train_dataset,    # í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=eval_dataset,      # ê²€ì¦ ë°ì´í„°ì…‹
    use_wandb=False                 # WandB ì‚¬ìš© ì•ˆ í•¨
)

# ---------------------- í•™ìŠµ ì‹¤í–‰ ---------------------- #
# ëª¨ë¸ í•™ìŠµ ì‹œì‘ ë° ê²°ê³¼ ì €ì¥
results = trainer.train()
```

---

## í•™ìŠµ ì¸ì ì„¤ì •

### Configì—ì„œ ìë™ ìƒì„±

`src/training/trainer.py`ì˜ `_create_training_args` í•¨ìˆ˜ê°€ Configë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì¸ìë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤:

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ------------------------------- #
training:
  epochs: 20                                            # í•™ìŠµ ì—í¬í¬ ìˆ˜
  batch_size: 50                                        # ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
  learning_rate: 1e-5                                   # í•™ìŠµë¥ 
  weight_decay: 0.01                                    # ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ì •ê·œí™”)
  warmup_steps: 500                                     # í•™ìŠµë¥  ì›Œë°ì—… ìŠ¤í… ìˆ˜
  save_total_limit: 3                                   # ìµœëŒ€ ì €ì¥ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
  logging_steps: 100                                    # ë¡œê¹… ìŠ¤í… ê°„ê²©
```

### ìƒì„±ëœ í•™ìŠµ ì¸ì

```python
# ==================== Seq2SeqTrainingArguments ìë™ ìƒì„± ==================== #

Seq2SeqTrainingArguments(
    # ---------------------- ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • ---------------------- #
    output_dir="outputs/baseline_kobart",       # ì²´í¬í¬ì¸íŠ¸ ë° ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    overwrite_output_dir=True,                  # ê¸°ì¡´ ë””ë ‰í† ë¦¬ ë®ì–´ì“°ê¸° í—ˆìš©

    # ---------------------- í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---------------------- #
    num_train_epochs=20,                        # í•™ìŠµ ì—í¬í¬ ìˆ˜
    per_device_train_batch_size=50,             # í•™ìŠµ ë°°ì¹˜ í¬ê¸° (ë””ë°”ì´ìŠ¤ë‹¹)
    per_device_eval_batch_size=50,              # í‰ê°€ ë°°ì¹˜ í¬ê¸° (ë””ë°”ì´ìŠ¤ë‹¹)
    learning_rate=1e-5,                         # í•™ìŠµë¥ 
    weight_decay=0.01,                          # ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ì •ê·œí™”)
    warmup_steps=500,                           # í•™ìŠµë¥  ì›Œë°ì—… ìŠ¤í… ìˆ˜

    # ---------------------- í‰ê°€ ë° ì €ì¥ ì „ëµ ---------------------- #
    eval_strategy='epoch',                      # ì—í¬í¬ë§ˆë‹¤ í‰ê°€ ìˆ˜í–‰
    save_strategy='epoch',                      # ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_total_limit=3,                         # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
    load_best_model_at_end=True,                # í•™ìŠµ ì¢…ë£Œ ì‹œ ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ
    metric_for_best_model='eval_rouge_sum',     # ìµœìƒ ëª¨ë¸ ì„ ì • ê¸°ì¤€ ë©”íŠ¸ë¦­

    # ---------------------- ë¡œê¹… ì„¤ì • ---------------------- #
    logging_dir="outputs/baseline_kobart/logs", # TensorBoard ë¡œê·¸ ì €ì¥ ê²½ë¡œ
    logging_steps=100,                          # 100 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡
    report_to=['wandb'] if use_wandb else [],   # WandB ë¡œê¹… í™œì„±í™” ì—¬ë¶€

    # ---------------------- Seq2Seq ìƒì„± ì„¤ì • ---------------------- #
    predict_with_generate=True,                 # í‰ê°€ ì‹œ ìƒì„± ëª¨ë“œ ì‚¬ìš©
    generation_max_length=100,                  # ìƒì„± ìµœëŒ€ ê¸¸ì´
    generation_num_beams=4,                     # Beam search ë¹” ê°œìˆ˜

    # ---------------------- ê¸°íƒ€ ìµœì í™” ì„¤ì • ---------------------- #
    fp16=torch.cuda.is_available(),             # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ FP16 ìë™ í™œì„±í™”
    dataloader_num_workers=4                    # ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜
)
```

---

## WandB í†µí•©

### WandB ì„¤ì •

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- WandB ë¡œê¹… ì„¤ì • ------------------------------- #
wandb:
  enabled: true                                         # WandB ì‚¬ìš© ì—¬ë¶€
  project: "nlp-competition"                            # WandB í”„ë¡œì íŠ¸ëª…
  entity: "ieyeppo"                                     # WandB ì—”í‹°í‹° (ì‚¬ìš©ì/íŒ€)

# ------------------------------- ì‹¤í—˜ ì„¤ì • ------------------------------- #
experiment:
  name: "baseline_kobart"                               # ì‹¤í—˜ëª…
  tags:                                                 # ì‹¤í—˜ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
    - "baseline"                                        # ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜
    - "kobart"                                          # KoBART ëª¨ë¸
```

### ìë™ ë¡œê¹… í•­ëª©

WandBì— ìë™ìœ¼ë¡œ ë¡œê¹…ë˜ëŠ” í•­ëª©:

1. **í•™ìŠµ ë©”íŠ¸ë¦­**
   - train_loss
   - train_runtime
   - train_samples_per_second
   - train_steps_per_second

2. **í‰ê°€ ë©”íŠ¸ë¦­**
   - eval_rouge1
   - eval_rouge2
   - eval_rougeL
   - eval_rouge_sum
   - eval_loss

3. **Config ì •ë³´**
   - ì „ì²´ Config íŒŒë¼ë¯¸í„°
   - ì‹¤í—˜ íƒœê·¸
   - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì´ë¦„

4. **ì‹œìŠ¤í…œ ì •ë³´**
   - GPU ì‚¬ìš©ë¥ 
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - CPU ì‚¬ìš©ë¥ 

### WandB ì´ˆê¸°í™” ë° ì¢…ë£Œ

```python
# ==================== WandB Logger ìƒëª…ì£¼ê¸° ê´€ë¦¬ ==================== #

# ---------------------- WandB Logger ì´ˆê¸°í™” ---------------------- #
# WandB ì‚¬ìš©ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì´ˆê¸°í™”
if use_wandb and config.wandb.enabled:
    self.wandb_logger = WandbLogger(
        project_name=config.wandb.project,          # WandB í”„ë¡œì íŠ¸ëª…
        entity=config.wandb.entity,                 # WandB ì—”í‹°í‹° (ì‚¬ìš©ì/íŒ€)
        experiment_name=config.experiment.name,     # ì‹¤í—˜ëª…
        config=dict(config),                        # Configë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        tags=config.experiment.tags                 # ì‹¤í—˜ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
    )

# ---------------------- WandB Run ì‹œì‘ ---------------------- #
# í•™ìŠµ ì‹œì‘ ì‹œ WandB Run ì´ˆê¸°í™”
self.wandb_logger.init_run()

# ---------------------- WandB Run ì¢…ë£Œ ---------------------- #
# í•™ìŠµ ì¢…ë£Œ ì‹œ WandB Run ì¢…ë£Œ ë° ìµœì¢… ë¡œê·¸ ì €ì¥
self.wandb_logger.finish()
```

---

## ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ìë™ ì €ì¥

í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤:

```
outputs/baseline_kobart/
â”œâ”€â”€ checkpoint-500/          # 500 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ trainer_state.json
â”œâ”€â”€ checkpoint-1000/         # 1000 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ checkpoint-1500/         # 1500 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ final_model/             # ìµœì¢… ëª¨ë¸
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ special_tokens_map.json
```

### ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ

```python
# ==================== ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ ì„¤ì • ==================== #

# ---------------------- Config ì„¤ì • ---------------------- #
save_strategy='epoch',                         # ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
load_best_model_at_end=True,                   # ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ
metric_for_best_model='eval_rouge_sum'         # ROUGE Sum ê¸°ì¤€ìœ¼ë¡œ ìµœìƒ ëª¨ë¸ ì„ ì •
```

í•™ìŠµ ì¢…ë£Œ í›„ ìë™ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ROUGE Sumì„ ë‹¬ì„±í•œ ì²´í¬í¬ì¸íŠ¸ê°€ ë¡œë“œë©ë‹ˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì œí•œ ------------------------------- #
training:
  save_total_limit: 3                                   # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€ (ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)
```

ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì‚­ì œë˜ì–´ ë””ìŠ¤í¬ ê³µê°„ì„ ì ˆì•½í•©ë‹ˆë‹¤.

---

# Part 3: ì¶”ë¡  ì‹œìŠ¤í…œ

## ì¶”ë¡  ì‹œìŠ¤í…œ ê°œìš”

### ëª©ì 
- í•™ìŠµëœ ëª¨ë¸ë¡œ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡
- ë°°ì¹˜ ì¶”ë¡  ë° ì§„í–‰ í‘œì‹œ
- ì œì¶œ íŒŒì¼ ìë™ ìƒì„±
- ìƒì„± íŒŒë¼ë¯¸í„° ìœ ì—°í•œ ì„¤ì •

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ë‹¨ì¼/ë°°ì¹˜ ì¶”ë¡  ì§€ì›
- âœ… DataFrame ì§ì ‘ ì²˜ë¦¬
- âœ… ì œì¶œ íŒŒì¼ ìë™ ìƒì„±
- âœ… ìƒì„± íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
- âœ… Logger í†µí•© ì§€ì›

---

## Predictor í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/predictor.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class Predictor:
    def __init__(self, model, tokenizer, config=None, device=None, logger=None):
        """ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""

    def _setup_generation_config(self) -> Dict:
        """ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •"""

    def predict_single(self, dialogue: str, **generation_kwargs) -> str:
        """ë‹¨ì¼ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡"""

    def predict_batch(self, dialogues: List[str], batch_size=32,
                     show_progress=True, **generation_kwargs) -> List[str]:
        """ë°°ì¹˜ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡"""

    def predict_dataframe(self, df: pd.DataFrame, batch_size=32,
                          show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """DataFrameì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""

    def create_submission(self, test_df: pd.DataFrame, output_path: str,
                         batch_size=32, show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """ì œì¶œ íŒŒì¼ ìƒì„±"""
```

---

## ì¶”ë¡  ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ì¼ ì˜ˆì¸¡)

```python
# ==================== Predictor ê¸°ë³¸ ì‚¬ìš© ì˜ˆì‹œ (ë‹¨ì¼ ì˜ˆì¸¡) ==================== #

# ---------------------- Transformers ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ---------------------- í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# ì €ì¥ëœ ìµœì¢… ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- GPUë¡œ ëª¨ë¸ ì´ë™ ---------------------- #
# CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ì¶”ë¡  ì†ë„ ëŒ€í­ í–¥ìƒ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ---------------------- Predictor ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---------------------- #
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•˜ì—¬ Predictor ìƒì„±
# deviceë¥¼ ì „ë‹¬í•˜ì—¬ GPU ì‚¬ìš© ë³´ì¥
predictor = create_predictor(model, tokenizer, device=device)

# ---------------------- ë‹¨ì¼ ëŒ€í™” ì˜ˆì¸¡ ---------------------- #
# ì˜ˆì¸¡í•  ëŒ€í™” í…ìŠ¤íŠ¸
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ë„¤ ì•ˆë…•í•˜ì„¸ìš”"
# ëŒ€í™” ìš”ì•½ ìƒì„±
summary = predictor.predict_single(dialogue)

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
# ìƒì„±ëœ ìš”ì•½ ì¶œë ¥
print(f"ì˜ˆì¸¡ ìš”ì•½: {summary}")
```

### 2. ë°°ì¹˜ ì˜ˆì¸¡

```python
# ==================== Predictor ë°°ì¹˜ ì˜ˆì¸¡ ì˜ˆì‹œ ==================== #

# ---------------------- ì˜ˆì¸¡í•  ëŒ€í™” ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ ---------------------- #
dialogues = [
    "#Person1#: ì €ë… ë­ ë¨¹ì„ê¹Œ? #Person2#: ê¹€ì¹˜ì°Œê°œ ì–´ë•Œ?",
    "#Person1#: ë‚´ì¼ íšŒì˜ ëª‡ ì‹œì•¼? #Person2#: 3ì‹œë¡œ ì¡í˜”ì–´",
    "#Person1#: ë‚ ì”¨ ì¢‹ë„¤ #Person2#: ì‚°ì±… ê°€ì"
]

# ---------------------- ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰ ---------------------- #
# ì—¬ëŸ¬ ëŒ€í™”ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
summaries = predictor.predict_batch(
    dialogues,              # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    batch_size=32,          # ë°°ì¹˜ í¬ê¸°
    show_progress=True      # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
)

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
# ê° ëŒ€í™”ì™€ ìƒì„±ëœ ìš”ì•½ì„ ìˆœíšŒí•˜ë©° ì¶œë ¥
for dialogue, summary in zip(dialogues, summaries):
    print(f"ëŒ€í™”: {dialogue}")
    print(f"ìš”ì•½: {summary}\n")
```

### 3. DataFrame ì˜ˆì¸¡

```python
# ==================== Predictor DataFrame ì˜ˆì¸¡ ì˜ˆì‹œ ==================== #

# ---------------------- Pandas ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
import pandas as pd

# ---------------------- í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---------------------- #
# CSV íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- DataFrame ì˜ˆì¸¡ ì‹¤í–‰ ---------------------- #
# DataFrameì— ëŒ€í•´ ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰
result_df = predictor.predict_dataframe(
    test_df,                # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    batch_size=32,          # ë°°ì¹˜ í¬ê¸°
    show_progress=True      # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
)

# ---------------------- ê²°ê³¼ í™•ì¸ ---------------------- #
# íŒŒì¼ëª…, ëŒ€í™”, ìš”ì•½ ì»¬ëŸ¼ì˜ ìƒìœ„ 5ê°œ í–‰ ì¶œë ¥
print(result_df[['fname', 'dialogue', 'summary']].head())
```

### 4. ì œì¶œ íŒŒì¼ ìƒì„±

```python
# ==================== ì œì¶œ íŒŒì¼ ìë™ ìƒì„± ì˜ˆì‹œ ==================== #

# ---------------------- ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤í–‰ ---------------------- #
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê³  ì œì¶œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
submission_df = predictor.create_submission(
    test_df=test_df,                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    output_path="submissions/submission.csv",   # ì œì¶œ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    batch_size=32,                              # ë°°ì¹˜ í¬ê¸°
    show_progress=True                          # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
)

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
# ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ ë©”ì‹œì§€
print(f"ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: submissions/submission.csv")
# ì´ ì˜ˆì¸¡ ìƒ˜í”Œ ìˆ˜ ì¶œë ¥
print(f"ìƒ˜í”Œ ìˆ˜: {len(submission_df)}")
```

---

## ìƒì„± íŒŒë¼ë¯¸í„°

### âš ï¸ ì¤‘ìš”: max_length vs max_new_tokens

**ë¬¸ì œì :** `max_length`ëŠ” **ì…ë ¥+ì¶œë ¥ ì „ì²´ ê¸¸ì´**ë¥¼ ì œí•œí•˜ë¯€ë¡œ, ì…ë ¥ì´ ê¸¸ë©´ ì¶œë ¥ì´ ì˜ë¦½ë‹ˆë‹¤.

**í•´ê²°ì±…:** `max_new_tokens`ëŠ” **ì¶œë ¥ë§Œì˜ ê¸¸ì´**ë¥¼ ì œí•œí•˜ì—¬ ì¼ê´€ëœ ìš”ì•½ ê¸¸ì´ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

```python
# ==================== max_lengthì˜ ë¬¸ì œì  ==================== #

# âŒ ì˜ëª»ëœ ë°©ë²•: max_length ì‚¬ìš©
outputs = model.generate(
    input_ids,
    max_length=100  # ì…ë ¥(80í† í°) + ì¶œë ¥(?) = 100
)
# ê²°ê³¼: ì¶œë ¥ì€ 20í† í°ë§Œ ìƒì„±ë¨ â†’ ë¬¸ì¥ ëŠê¹€ ë°œìƒ! âŒ

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: max_new_tokens ì‚¬ìš©
outputs = model.generate(
    input_ids,
    max_new_tokens=200,  # ì…ë ¥ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¶œë ¥ 200í† í° ìƒì„±
    min_new_tokens=30    # ìµœì†Œ 30í† í° ë³´ì¥
)
# ê²°ê³¼: í•­ìƒ ì¼ê´€ëœ ê¸¸ì´ì˜ ì™„ì „í•œ ìš”ì•½ ìƒì„±! âœ…
```

### ğŸ“Œ ë™ì‹œ ì‚¬ìš© ì‹œ ìš°ì„ ìˆœìœ„

**ì¤‘ìš”:** `max_length`ì™€ `max_new_tokens`ë¥¼ **ë™ì‹œì— ì§€ì •í•˜ë©´** `max_new_tokens`ê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

```python
# ==================== ë™ì‹œ ì‚¬ìš© ì‹œ ë™ì‘ ë°©ì‹ ==================== #

# âš ï¸ ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì‹œì— ì§€ì •í•œ ê²½ìš°
outputs = model.generate(
    input_ids,
    max_length=100,       # âŒ ë¬´ì‹œë¨
    max_new_tokens=200    # âœ… ì´ ê°’ì´ ì‚¬ìš©ë¨
)

# HuggingFace Transformers ê³µì‹ ë™ì‘:
# 1. max_new_tokensê°€ ì§€ì •ë˜ë©´ max_lengthëŠ” **ë¬´ì‹œ**ë©ë‹ˆë‹¤
# 2. max_new_tokensê°€ Noneì´ë©´ max_lengthë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤
# 3. ë‘˜ ë‹¤ ì§€ì •í•˜ë©´ ê²½ê³  ë©”ì‹œì§€ì™€ í•¨ê»˜ max_new_tokensê°€ ì ìš©ë©ë‹ˆë‹¤

# ê²½ê³  ë©”ì‹œì§€ ì˜ˆì‹œ:
# "Both `max_new_tokens` and `max_length` have been set.
#  `max_new_tokens` will take precedence."
```

### ğŸ“‹ íŒŒë¼ë¯¸í„° ë¹„êµí‘œ

| íŒŒë¼ë¯¸í„° | ì ìš© ë²”ìœ„ | ì…ë ¥ ê¸¸ì´ ì˜ì¡´ì„± | ì¶œë ¥ ì¼ê´€ì„± | ê¶Œì¥ ì—¬ë¶€ |
|---------|----------|----------------|------------|-----------|
| `max_length` | **ì…ë ¥+ì¶œë ¥ ì „ì²´** | âŒ ì˜ì¡´í•¨ | âŒ ê°€ë³€ì  | âŒ ë¹„ê¶Œì¥ |
| `max_new_tokens` | **ì¶œë ¥ë§Œ** | âœ… ë…ë¦½ì  | âœ… ì¼ê´€ì  | âœ… ê¶Œì¥ |
| `min_new_tokens` | **ì¶œë ¥ë§Œ (ìµœì†Œ)** | âœ… ë…ë¦½ì  | âœ… ë³´ì¥ë¨ | âœ… ê¶Œì¥ |

### ğŸ” Config íŒŒì¼ì—ì„œì˜ ì„¤ì • ì˜ˆì‹œ

```yaml
# configs/base/encoder_decoder.yaml
inference:
  # âš ï¸ ì„¸ ê°€ì§€ ì˜µì…˜ ëª¨ë‘ ì„¤ì • ê°€ëŠ¥
  generate_max_new_tokens: 200    # âœ… ìš°ì„ ìˆœìœ„ 1: ì¶œë ¥ 200í† í°
  generate_min_new_tokens: 30     # âœ… ìš°ì„ ìˆœìœ„ 1: ìµœì†Œ 30í† í°
  generate_max_length: 512        # âš ï¸ ë¬´ì‹œë¨ (max_new_tokensê°€ ìˆìœ¼ë¯€ë¡œ)
```

**ì‹¤ì œ ë™ì‘:**
- `max_new_tokens=200` ì‚¬ìš©ë¨ â†’ ì¶œë ¥ 200í† í° ìƒì„±
- `min_new_tokens=30` ì‚¬ìš©ë¨ â†’ ìµœì†Œ 30í† í° ë³´ì¥
- `max_length=512` ë¬´ì‹œë¨ â†’ max_new_tokensê°€ ìš°ì„ 

**ì™œ max_lengthë¥¼ Configì— ë‚¨ê²¨ë‘ë‚˜ìš”?**
- **í•˜ìœ„ í˜¸í™˜ì„±**: ì´ì „ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
- **ì•ˆì „ ì¥ì¹˜**: max_new_tokensê°€ Noneì¼ ë•Œ ëŒ€ë¹„
- **ëª…ì‹œì  ë¬¸ì„œí™”**: ì „ì²´ ê¸¸ì´ ì œí•œì„ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ

### âš ï¸ ê²½ê³  vs ì˜¤ë¥˜: ì‚­ì œ í•„ìš” ì—¬ë¶€

**Q: max_lengthë¥¼ ì‚­ì œí•´ì•¼ í•˜ë‚˜ìš”?**

**A: ì„ íƒ ì‚¬í•­ì…ë‹ˆë‹¤. ì‚­ì œí•´ë„ ë˜ê³  ê·¸ëƒ¥ ë‘¬ë„ ë©ë‹ˆë‹¤!**

```python
# ==================== max_length ì‚­ì œ ì—¬ë¶€ ==================== #

# âœ… ì˜µì…˜ 1: max_length ìœ ì§€ (ê¶Œì¥)
inference:
  generate_max_new_tokens: 200    # ì‚¬ìš©ë¨
  generate_min_new_tokens: 30     # ì‚¬ìš©ë¨
  generate_max_length: 512        # ë¬´ì‹œë˜ì§€ë§Œ ë¬¸ì„œí™”/Fallback ëª©ì 

# âœ… ì˜µì…˜ 2: max_length ì‚­ì œ (ê¹”ë”)
inference:
  generate_max_new_tokens: 200    # ì‚¬ìš©ë¨
  generate_min_new_tokens: 30     # ì‚¬ìš©ë¨
  # max_length ì—†ìŒ â†’ ë¬¸ì œ ì—†ìŒ!
```

**ë‘˜ ë‹¤ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!**

---

### ğŸ” ê²½ê³  ë©”ì‹œì§€ ìƒì„¸ ì„¤ëª…

**Q: ê²½ê³  ë©”ì‹œì§€ê°€ ë°œìƒí•˜ë©´ ì˜¤ë¥˜ì¸ê°€ìš”? ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ë‚˜ìš”?**

**A: ì•„ë‹™ë‹ˆë‹¤! ê²½ê³ ëŠ” ì˜¤ë¥˜ê°€ ì•„ë‹™ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì€ ì •ìƒ ì‹¤í–‰ë©ë‹ˆë‹¤.**

```python
# ==================== ê²½ê³  vs ì˜¤ë¥˜ ë¹„êµ ==================== #

# âš ï¸ ê²½ê³ (Warning) - ë…¸ë€ìƒ‰ ë©”ì‹œì§€
outputs = model.generate(
    input_ids,
    max_length=100,       # ë‘˜ ë‹¤ ìˆìœ¼ë©´ ê²½ê³  ë°œìƒ
    max_new_tokens=200
)
# ê²°ê³¼:
# - ë…¸ë€ìƒ‰ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥:
#   "Both `max_new_tokens` and `max_length` have been set.
#    `max_new_tokens` will take precedence."
# - âœ… í”„ë¡œê·¸ë¨ ê³„ì† ì‹¤í–‰ë¨
# - âœ… max_new_tokens=200 ì‚¬ìš©ë¨
# - âœ… ì •ìƒ ì¢…ë£Œë¨

# âŒ ì˜¤ë¥˜(Error) - ë¹¨ê°„ìƒ‰ ë©”ì‹œì§€
outputs = model.generate(
    input_ids,
    max_new_tokens=-100   # ì˜ëª»ëœ ê°’
)
# ê²°ê³¼:
# - ë¹¨ê°„ìƒ‰ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥:
#   "ValueError: max_new_tokens must be positive"
# - âŒ í”„ë¡œê·¸ë¨ ì¦‰ì‹œ ì¤‘ë‹¨ë¨
# - âŒ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìš”
```

### ğŸ“Š ê²½ê³  vs ì˜¤ë¥˜ ë¹„êµí‘œ

| êµ¬ë¶„ | ê²½ê³ (Warning) | ì˜¤ë¥˜(Error) |
|-----|-------------|-----------|
| **ìƒ‰ìƒ** | âš ï¸ ë…¸ë€ìƒ‰ | âŒ ë¹¨ê°„ìƒ‰ |
| **í”„ë¡œê·¸ë¨ ì‹¤í–‰** | âœ… ê³„ì† ì‹¤í–‰ë¨ | âŒ ì¦‰ì‹œ ì¤‘ë‹¨ë¨ |
| **ì˜ˆì™¸ ì²˜ë¦¬** | ë¶ˆí•„ìš” | í•„ìˆ˜ (try-except) |
| **ì‹¬ê°ë„** | ë‚®ìŒ (ì•Œë¦¼) | ë†’ìŒ (ì¹˜ëª…ì ) |
| **ë°œìƒ ìƒí™©** | ê¶Œì¥í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ë²• | ì˜ëª»ëœ ì‚¬ìš©ë²• |
| **ì˜ˆì‹œ** | ë‘ íŒŒë¼ë¯¸í„° ë™ì‹œ ì‚¬ìš© | ìŒìˆ˜ ê°’, ì˜ëª»ëœ íƒ€ì… |

### ğŸ’¡ ì‹¤ë¬´ ê¶Œì¥ ì‚¬í•­

**1ï¸âƒ£ max_lengthë¥¼ ìœ ì§€í•˜ëŠ” ê²½ìš° (ê¶Œì¥)**

```yaml
# configs/base/encoder_decoder.yaml
inference:
  generate_max_new_tokens: 200    # âœ… ì£¼ íŒŒë¼ë¯¸í„°
  generate_min_new_tokens: 30     # âœ… ë³´ì¡° íŒŒë¼ë¯¸í„°
  generate_max_length: 512        # âš ï¸ Fallback/ë¬¸ì„œí™”ìš© (ê²½ê³  ë°œìƒ ê°€ëŠ¥)
```

**ì¥ì :**
- âœ… í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
- âœ… max_new_tokens=Noneì¼ ë•Œ ìë™ Fallback
- âœ… ì „ì²´ ê¸¸ì´ ì œí•œ ë¬¸ì„œí™”
- âš ï¸ ê²½ê³  ë©”ì‹œì§€ ë°œìƒ ê°€ëŠ¥ (ë¬´í•´í•¨)

**2ï¸âƒ£ max_lengthë¥¼ ì‚­ì œí•˜ëŠ” ê²½ìš°**

```yaml
# configs/base/encoder_decoder.yaml
inference:
  generate_max_new_tokens: 200    # âœ… ì£¼ íŒŒë¼ë¯¸í„°
  generate_min_new_tokens: 30     # âœ… ë³´ì¡° íŒŒë¼ë¯¸í„°
  # max_length ì—†ìŒ
```

**ì¥ì :**
- âœ… ê²½ê³  ë©”ì‹œì§€ ì—†ìŒ (ê¹”ë”)
- âœ… í˜¼ë€ ë°©ì§€
- âš ï¸ Fallback ì—†ìŒ (max_new_tokens=Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

### ğŸ¯ ê²°ë¡ 

```python
# ==================== ìµœì¢… ê¶Œì¥ ì‚¬í•­ ==================== #

# ì„ íƒ 1: max_length ìœ ì§€ (ì•ˆì „)
# - ê²½ê³  ë©”ì‹œì§€ ë°œìƒí•  ìˆ˜ ìˆì§€ë§Œ ë¬´í•´í•¨
# - Fallback ì•ˆì „ì¥ì¹˜ ìˆìŒ
# - ê¶Œì¥: í”„ë¡œì íŠ¸ ì´ˆê¸° ë˜ëŠ” í•˜ìœ„ í˜¸í™˜ì„± ì¤‘ìš” ì‹œ

# ì„ íƒ 2: max_length ì‚­ì œ (ê¹”ë”)
# - ê²½ê³  ì—†ìŒ
# - ë” ëª…í™•í•œ ì„¤ì •
# - ê¶Œì¥: í”„ë¡œì íŠ¸ ì•ˆì •í™” í›„ ë˜ëŠ” ëª…í™•ì„± ì„ í˜¸ ì‹œ

# âœ… ì¤‘ìš”: ë‘˜ ë‹¤ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!
# âœ… ê²½ê³ ëŠ” ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¯€ë¡œ í”„ë¡œê·¸ë¨ì€ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤!
```

### ğŸ’¡ ê¶Œì¥ ì‚¬ìš© íŒ¨í„´

```python
# ==================== ê¶Œì¥ ì‚¬ìš© íŒ¨í„´ ==================== #

# âœ… íŒ¨í„´ 1: max_new_tokensë§Œ ì‚¬ìš© (ê°€ì¥ ê¶Œì¥)
outputs = model.generate(
    input_ids,
    max_new_tokens=200,
    min_new_tokens=30,
    num_beams=5
)

# âœ… íŒ¨í„´ 2: Configì—ì„œ ìë™ ë¡œë“œ (í”„ë¡œì íŠ¸ í‘œì¤€)
# Config íŒŒì¼ì— generate_max_new_tokens ì„¤ì •
# ì½”ë“œì—ì„œ getattrë¡œ ìë™ ë¡œë“œ
max_new_tokens = getattr(args, 'max_new_tokens', 200)
outputs = model.generate(input_ids, max_new_tokens=max_new_tokens)

# âŒ íŒ¨í„´ 3: ë‘ íŒŒë¼ë¯¸í„° ë™ì‹œ ì‚¬ìš© (ë¹„ê¶Œì¥)
# ê²½ê³  ë©”ì‹œì§€ê°€ ë°œìƒí•˜ë©° í˜¼ë€ì„ ì•¼ê¸°í•¨
outputs = model.generate(
    input_ids,
    max_length=100,      # ë¬´ì‹œë¨
    max_new_tokens=200   # ì‹¤ì œ ì‚¬ìš©ë¨
)
```

### ê¸°ë³¸ ìƒì„± íŒŒë¼ë¯¸í„°

```python
# ==================== ê¸°ë³¸ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • ==================== #

# ---------------------- ìƒì„± íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ---------------------- #
default_config = {
    'max_new_tokens': 100,          # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
    'min_new_tokens': 30,           # âœ… ìƒì„±í•  ìµœì†Œ í† í° ìˆ˜ (ë¬¸ì¥ ëŠê¹€ ë°©ì§€)
    'num_beams': 5,                 # Beam search ë¹” ê°œìˆ˜
    'early_stopping': True,         # EOS í† í° ìƒì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
    'no_repeat_ngram_size': 3,      # n-gram ë°˜ë³µ ë°©ì§€ í¬ê¸°
    'length_penalty': 1.0,          # ê¸¸ì´ í˜ë„í‹° (1.0: ì¤‘ë¦½)
    'repetition_penalty': 1.5,      # ìµœì ê°’: 1.5 (ì ì ˆí•œ ì–µì œ)
}
```

### Configì—ì„œ ìë™ ë¡œë“œ

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- ì¶”ë¡  ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì • ------------------------------- #
inference:
  generate_max_new_tokens: 100                          # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
  generate_min_new_tokens: 30                           # âœ… ìƒì„±í•  ìµœì†Œ í† í° ìˆ˜ (ê¶Œì¥)
  num_beams: 5                                          # Beam search ë¹” ê°œìˆ˜
  early_stopping: true                                  # EOS í† í° ìƒì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
  no_repeat_ngram_size: 3                               # ìµœì ê°’: 3 (ë°˜ë³µ ë°©ì§€)
  length_penalty: 1.0                                   # ê¸¸ì´ í˜ë„í‹° (1.0: ì¤‘ë¦½)
  repetition_penalty: 1.5                               # ìµœì ê°’: 1.5 (ì ì ˆí•œ ì–µì œ)
```

### CLI ëª…ë ¹ì–´ë¡œ ì˜¤ë²„ë¼ì´ë“œ

```bash
# ==================== CLIì—ì„œ ìƒì„± íŒŒë¼ë¯¸í„° ì§€ì • ==================== #

# ---------------------- ê¶Œì¥ ì„¤ì • (í•œêµ­ì–´ ìš”ì•½) ---------------------- #
python scripts/train.py \
  --mode full \
  --models kobart \
  --max_new_tokens 200 \        # âœ… í•œêµ­ì–´ëŠ” 200 ê¶Œì¥ (ì˜ì–´ì˜ 2-3ë°°)
  --min_new_tokens 30 \         # âœ… ìµœì†Œ ê¸¸ì´ ë³´ì¥
  --num_beams 5 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3
```

### ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ

```python
# ==================== ìƒì„± íŒŒë¼ë¯¸í„° ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ ì˜ˆì‹œ ==================== #

# ---------------------- Config ê°’ ë¬´ì‹œí•˜ê³  íŒŒë¼ë¯¸í„° ì§€ì • ---------------------- #
# ì˜ˆì¸¡ ì‹¤í–‰ ì‹œì ì— ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ë³€ê²½
summaries = predictor.predict_batch(
    dialogues,                  # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    batch_size=16,              # ë°°ì¹˜ í¬ê¸°
    max_new_tokens=100,         # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
    min_new_tokens=30,          # âœ… ì˜¤ë²„ë¼ì´ë“œ: ìµœì†Œ 30í† í° ë³´ì¥
    num_beams=8,                # ì˜¤ë²„ë¼ì´ë“œ: Beam ê°œìˆ˜ ì¦ê°€
    repetition_penalty=1.5      # ìµœì ê°’: 1.5 (ì ì ˆí•œ ì–µì œ)
)
```

### ì£¼ìš” ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê¶Œì¥ê°’ | ì„¤ëª… |
|---------|-------|------|
| `max_new_tokens` â­ | **100** | **ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜** (ìµœì ê°’: 99.6% ì™„ì„±ë„ ë‹¬ì„±) |
| `min_new_tokens` â­ | **30** | **ìƒì„±í•  ìµœì†Œ í† í° ìˆ˜** (ë¬¸ì¥ ëŠê¹€ ë°©ì§€) |
| `num_beams` | 5 | Beam search ë¹” ê°œìˆ˜ (ë†’ì„ìˆ˜ë¡ í’ˆì§ˆâ†‘, ì†ë„â†“) |
| `repetition_penalty` | 1.5 | ë°˜ë³µ ì–µì œ ê°•ë„ (ìµœì ê°’: 1.5, ì ì ˆí•œ ì–µì œ) |
| `early_stopping` | True | EOS í† í° ìƒì„± ì‹œ ì¦‰ì‹œ ì¢…ë£Œ |
| `no_repeat_ngram_size` | 3 | n-gram ë°˜ë³µ ë°©ì§€ í¬ê¸° |
| `length_penalty` | 1.0 | ê¸¸ì´ í˜ë„í‹° (>1: ê¸´ ë¬¸ì¥ ì„ í˜¸, <1: ì§§ì€ ë¬¸ì¥ ì„ í˜¸) |
| `temperature` | 1.0 | ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì , ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘) |
| `top_k` | 50 | Top-k ìƒ˜í”Œë§ |
| `top_p` | 1.0 | Nucleus ìƒ˜í”Œë§ |

### í•œêµ­ì–´ ìš”ì•½ì— ìµœì í™”ëœ ì„¤ì •

```python
# ==================== í•œêµ­ì–´ ìš”ì•½ ìµœì  ì„¤ì • ==================== #

# ---------------------- ê³ í’ˆì§ˆ ìš”ì•½ ì„¤ì • ---------------------- #
optimal_config = {
    # âœ… ê¸¸ì´ ì œì–´
    'max_new_tokens': 100,      # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
    'min_new_tokens': 30,       # ë„ˆë¬´ ì§§ì€ ìš”ì•½ ë°©ì§€

    # âœ… í’ˆì§ˆ í–¥ìƒ
    'num_beams': 5,             # 5ê°œ í›„ë³´ ì¤‘ ìµœì„  ì„ íƒ
    'repetition_penalty': 1.5,  # ìµœì ê°’: 1.5 (ì ì ˆí•œ ì–µì œ)
    'no_repeat_ngram_size': 3,  # ìµœì ê°’: 3 (ë°˜ë³µ ë°©ì§€)

    # âœ… ì¢…ë£Œ ì¡°ê±´
    'early_stopping': True,     # EOS í† í° ìƒì„± ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
    'length_penalty': 1.0,      # ì¤‘ë¦½ì  ê¸¸ì´ ì„ í˜¸
}

# ---------------------- ì˜ˆì¸¡ ì‹¤í–‰ ---------------------- #
summaries = predictor.predict_batch(
    dialogues,
    **optimal_config
)
```

### ë¬¸ì¥ ëŠê¹€ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

#### ë¬¸ì œ ì¦ìƒ
- ìš”ì•½ì´ ì¤‘ê°„ì— ëŠê¹€: "Person1ê³¼ Person2ëŠ” ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤. ê·¸ë“¤ì€ ê¹€ì¹˜ì°Œê°œë¥¼ ë¨¹ê¸°ë¡œ"
- ë¬¸ì¥ ë¶€í˜¸ ì—†ì´ ì¢…ë£Œ: "Person1ê³¼ Person2ëŠ” ë‚´ì¼ 3ì‹œì— íšŒì˜ë¥¼ í•˜ê¸°ë¡œ í–ˆë‹¤"
- ë¶ˆì™„ì „í•œ ë‹¨ì–´: "Person1ì€ #Pe"

#### í•´ê²° ë°©ë²•

**1ë‹¨ê³„: max_new_tokens ì‚¬ìš©**
```python
# âŒ ê¸°ì¡´ (ë¬¸ì œ ë°œìƒ)
outputs = model.generate(input_ids, max_length=100)

# âœ… ìˆ˜ì • (ë¬¸ì œ í•´ê²°)
outputs = model.generate(
    input_ids,
    max_new_tokens=100,  # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
    min_new_tokens=30    # ìµœì†Œ ê¸¸ì´ ë³´ì¥
)
```

**2ë‹¨ê³„: í›„ì²˜ë¦¬ ì¶”ê°€**
```python
def postprocess_summary(text):
    """ìš”ì•½ë¬¸ í›„ì²˜ë¦¬"""
    import re
    text = text.strip()

    # 1. ë¶ˆì™„ì „í•œ í”Œë ˆì´ìŠ¤í™€ë” ì œê±°
    text = re.sub(r'\s+#[A-Za-zê°€-í£]{0,10}$', '', text)

    # 2. ì§§ì€ ë§ˆì§€ë§‰ ë‹¨ì–´ ì œê±°
    parts = text.rsplit(' ', 1)
    if len(parts) == 2 and len(parts[1]) <= 3:
        if not parts[1].endswith(('.', '!', '?')):
            text = parts[0]

    # 3. ë¬¸ì¥ ì¢…ê²° ë³´ì¥
    if text and text[-1] not in '.!?':
        text += '.'

    return text
```

**3ë‹¨ê³„: CLI ëª…ë ¹ì–´ ì‚¬ìš©**
```bash
# ëª¨ë“  ëª¨ë“œì—ì„œ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
python scripts/train.py \
  --mode full \
  --models kobart \
  --max_new_tokens 100 \  # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
  --min_new_tokens 30 \
  --num_beams 5
```

---

## ì œì¶œ íŒŒì¼ ìƒì„±

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
# ==================== ì œì¶œ íŒŒì¼ ìƒì„± ê¸°ë³¸ ì‚¬ìš©ë²• ==================== #

# ---------------------- í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
import torch
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ---------------------- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ---------------------- #
# ìµœì¢… ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- GPUë¡œ ëª¨ë¸ ì´ë™ ---------------------- #
# CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ì¶”ë¡  ì†ë„ ëŒ€í­ í–¥ìƒ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ---------------------- Predictor ìƒì„± ---------------------- #
# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•˜ì—¬ Predictor ìƒì„±
# deviceë¥¼ ì „ë‹¬í•˜ì—¬ GPU ì‚¬ìš© ë³´ì¥
predictor = create_predictor(model, tokenizer, device=device)

# ---------------------- í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---------------------- #
# CSV íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- ì œì¶œ íŒŒì¼ ìƒì„± ---------------------- #
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê³  ì œì¶œ í˜•ì‹ìœ¼ë¡œ ì €ì¥
submission_df = predictor.create_submission(
    test_df=test_df,                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    output_path="submissions/submission.csv",   # ì œì¶œ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    batch_size=32,                              # ë°°ì¹˜ í¬ê¸°
    show_progress=True                          # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
)
```

### Loggerì™€ í•¨ê»˜ ì‚¬ìš©

```python
# ==================== Loggerì™€ í•¨ê»˜ ì œì¶œ íŒŒì¼ ìƒì„± ì˜ˆì‹œ ==================== #

# ---------------------- Logger ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger ì´ˆê¸°í™” ---------------------- #
# ì¶”ë¡ ìš© ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
log_path = create_log_path("inference", "inference.log")
# Logger ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì½˜ì†” ì¶œë ¥ë„ í™œì„±í™”)
logger = Logger(log_path, print_also=True)
# í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ ì‹œì‘
logger.start_redirect()

# ---------------------- ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ ---------------------- #
try:
    # -------------- Predictor ìƒì„± (Logger ì „ë‹¬) -------------- #
    # Loggerë¥¼ ì „ë‹¬í•˜ì—¬ ì¶”ë¡  ê³¼ì • ë¡œê·¸ ê¸°ë¡
    predictor = create_predictor(model, tokenizer, logger=logger)

    # -------------- ì œì¶œ íŒŒì¼ ìƒì„± -------------- #
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
    submission_df = predictor.create_submission(
        test_df=test_df,                            # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
        output_path="submissions/submission.csv",   # ì œì¶œ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        batch_size=32,                              # ë°°ì¹˜ í¬ê¸°
        show_progress=True                          # ì§„í–‰ë¥  í‘œì‹œ í™œì„±í™”
    )

# ---------------------- ì¢…ë£Œ ì²˜ë¦¬ ---------------------- #
finally:
    # í‘œì¤€ ì¶œë ¥ ë¦¬ë‹¤ì´ë ‰ì…˜ ì¤‘ì§€
    logger.stop_redirect()
    # Logger íŒŒì¼ í•¸ë“¤ëŸ¬ ë‹«ê¸°
    logger.close()
```

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘
============================================================

ìƒ˜í”Œ ìˆ˜: 2500
Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [02:15<00:00,  1.71s/it]

âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submissions/submission.csv
============================================================
```

### ì œì¶œ íŒŒì¼ í˜•ì‹

```csv
fname,summary
test_001,ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤
test_002,íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤
test_003,ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤
...
```

---

# Part 4: LLM íŒŒì¸íŠœë‹

## LLM íŒŒì¸íŠœë‹ ê°œìš”

### ëª©ì 
- Causal LM (Llama, Qwen) íŒŒì¸íŠœë‹
- QLoRA 4-bit ì–‘ìí™” ì§€ì›
- LoRA (Low-Rank Adaptation) ì§€ì›
- Instruction/Chat Format ì§€ì›

### í•µì‹¬ ê¸°ëŠ¥
- âœ… QLoRA 4-bit ì–‘ìí™”
- âœ… LoRA íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  í•™ìŠµ
- âœ… Chat template í† í° ìë™ ì¶”ê°€
- âœ… Prompt truncation ë°©ì§€
- âœ… Instruction Tuning ë°ì´í„° ì¦ê°•

---

## LoRA Loader

### íŒŒì¼ ìœ„ì¹˜
```
src/models/lora_loader.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LoRALoader:
    def __init__(self, config, logger=None)
    def load_model_and_tokenizer(use_lora=True, use_qlora=False)
    def _load_tokenizer(checkpoint)
    def _create_bnb_config()
    def _load_causal_lm(checkpoint, bnb_config)
    def _add_chat_tokens(model, tokenizer)
    def _apply_lora(model)
    def _configure_tokenizer(tokenizer)
```

### ì£¼ìš” ê¸°ëŠ¥

#### 1. QLoRA 4-bit ì–‘ìí™”
```python
# ==================== QLoRA 4-bit ì–‘ìí™” ì„¤ì • ==================== #

# ---------------------- BitsAndBytes ì–‘ìí™” Config ìƒì„± ---------------------- #
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                          # 4-bit ì–‘ìí™” í™œì„±í™”
    bnb_4bit_use_double_quant=True,             # ì´ì¤‘ ì–‘ìí™” ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    bnb_4bit_quant_type="nf4",                  # NF4 ì–‘ìí™” íƒ€ì…
    bnb_4bit_compute_dtype=torch.float16        # âš ï¸ FP16 ê¶Œì¥ (BF16ì€ PyTorch AMP í˜¸í™˜ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)
)
```

#### 2. LoRA ì„¤ì •
```python
# ==================== LoRA (Low-Rank Adaptation) ì„¤ì • ==================== #

# ---------------------- LoRA Config ìƒì„± ---------------------- #
lora_config = LoraConfig(
    r=16,                    # LoRA rank (ì €ë­í¬ í–‰ë ¬ ì°¨ì›)
    lora_alpha=32,           # alpha ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„° (ì¼ë°˜ì ìœ¼ë¡œ r * 2)
    target_modules=[
        # Attention ë ˆì´ì–´
        "q_proj", "k_proj", "v_proj", "o_proj",
        # MLP ë ˆì´ì–´
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,       # LoRA ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒ
    task_type=TaskType.CAUSAL_LM  # Causal Language Modeling íƒœìŠ¤í¬
)
```

#### 3. Chat Template í† í° ì¶”ê°€
```python
# ==================== Chat Template íŠ¹ìˆ˜ í† í° ì¶”ê°€ ==================== #

# ---------------------- Llama ëª¨ë¸ìš© Chat í† í° ---------------------- #
# Llama 3 Chat í…œí”Œë¦¿ íŠ¹ìˆ˜ í† í°
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# ---------------------- Qwen ëª¨ë¸ìš© Chat í† í° ---------------------- #
# Qwen Chat í…œí”Œë¦¿ íŠ¹ìˆ˜ í† í°
chat_tokens = ["<|im_start|>", "<|im_end|>"]

# ---------------------- í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€ ---------------------- #
# Chat í…œí”Œë¦¿ í† í°ì„ í† í¬ë‚˜ì´ì €ì— ì¶”ê°€
tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
# í† í° ì¶”ê°€ë¡œ ì¸í•œ ì„ë² ë”© í¬ê¸° ì¡°ì •
model.resize_token_embeddings(len(tokenizer))
```

#### 4. Prompt Truncation ë°©ì§€
```python
# ==================== Prompt Truncation ë°©ì§€ ì„¤ì • ==================== #

# ---------------------- Left Padding/Truncation ì„¤ì • ---------------------- #
# Causal LMì—ì„œ í”„ë¡¬í”„íŠ¸ ë³´ì¡´ì„ ìœ„í•´ ì™¼ìª½ë¶€í„° íŒ¨ë”© ë° ì˜ë¼ë‚´ê¸°
tokenizer.padding_side = "left"         # ì™¼ìª½ì—ì„œ íŒ¨ë”©
tokenizer.truncation_side = "left"      # ì™¼ìª½ì—ì„œ ì˜ë¼ë‚´ê¸° (í”„ë¡¬í”„íŠ¸ ëë¶€ë¶„ ë³´ì¡´)
```

### 5. ì£¼ì˜ì‚¬í•­ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### âš ï¸ BFloat16 AMP í˜¸í™˜ì„± ë¬¸ì œ (Critical)

**ë¬¸ì œ**: PyTorch AMP GradScalerëŠ” BFloat16ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ

**ì—ëŸ¬ ë©”ì‹œì§€**:
```
NotImplementedError: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
```

**ì›ì¸**:
- `src/models/lora_loader.py:108`ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ `torch.bfloat16`ì„ ì‚¬ìš©
- í•™ìŠµ ì¤‘ gradient clipping ë‹¨ê³„ì—ì„œ AMP GradScalerê°€ BFloat16 í…ì„œë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•¨
- `_amp_foreach_non_finite_check_and_unscale_cuda` ì»¤ë„ì´ Float16ë§Œ ì§€ì›

**í•´ê²° ë°©ë²•**:
`src/models/lora_loader.py:108-113`ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
# ë³€ê²½ ì „
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen ëª¨ë¸: fp16 ì‚¬ìš©")
else:
    self._log("  - Llama ëª¨ë¸: bf16 ì‚¬ìš©")

# ë³€ê²½ í›„
# PyTorch AMP GradScalerëŠ” BFloat16ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëª¨ë“  ëª¨ë¸ì— Float16 ì‚¬ìš©
compute_dtype = torch.float16
self._log("  - QLoRA compute dtype: fp16 (AMP í˜¸í™˜)")
```

**ì˜í–¥ì„ ë°›ëŠ” ëª¨ë¸**: Llama, Solar, Polyglot ë“± Qwenì´ ì•„ë‹Œ ëª¨ë“  Causal LM

**ê²€ì¦**: ì‹¤í—˜ `20251012_131535_test_full_pipeline_quick`ì—ì„œ í™•ì¸ë¨

#### âš ï¸ Device Map Offload ë¬¸ì œ

**ë¬¸ì œ**: ëŒ€í˜• ëª¨ë¸(12.8B ì´ìƒ) ë¡œë”© ì‹œ ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ í´ë” ë¯¸ì§€ì •

**ì—ëŸ¬ ë©”ì‹œì§€**:
```
ValueError: The current `device_map` had weights offloaded to the disk.
Please provide an `offload_folder` for them.
```

**ì›ì¸**:
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ `device_map="auto"`ê°€ ìë™ìœ¼ë¡œ ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ ì‹œë„
- `offload_folder` íŒŒë¼ë¯¸í„° ë¯¸ì§€ì •

**í•´ê²° ë°©ë²• 1**: offload_folder ì¶”ê°€ (ê¶Œì¥)
```python
# src/models/llm_loader.py:58
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    offload_folder=str(offload_dir),  # ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ í´ë” ì§€ì •
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

**í•´ê²° ë°©ë²• 2**: safetensors ì„¤ì¹˜
```bash
pip install safetensors
```

**ì˜í–¥ì„ ë°›ëŠ” ëª¨ë¸**: KULLM-v2 (12.8B), Solar-10.7B ë“± ëŒ€í˜• ëª¨ë¸

**ê²€ì¦**: ì‹¤í—˜ `20251012_131535_test_full_pipeline_quick`ì—ì„œ KULLM-v2 ë¡œë”© ì‹¤íŒ¨ í™•ì¸ë¨

#### âš ï¸ token_type_ids ê²½ê³ 

**ë¬¸ì œ**: Causal LM ì¶”ë¡  ì‹œ ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€

**ê²½ê³  ë©”ì‹œì§€**:
```
The following `model_kwargs` are not used by the model: ['token_type_ids']
```

**ì›ì¸**:
- Causal LMì€ `token_type_ids`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- Tokenizerê°€ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì§€ë§Œ ëª¨ë¸ì´ ë¬´ì‹œ

**í•´ê²° ë°©ë²•**:
ì¶”ë¡  ì½”ë“œì—ì„œ `token_type_ids` ì œê±°:

```python
# src/inference/predictor.py ë˜ëŠ” ê´€ë ¨ ì¶”ë¡  ì½”ë“œ
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# token_type_ids ì œê±° (Causal LMì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']

outputs = model.generate(**inputs, ...)
```

---

## LLM Dataset

### íŒŒì¼ ìœ„ì¹˜
```
src/data/llm_dataset.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LLMSummarizationDataset(Dataset):
    def __init__(dialogues, summaries, tokenizer,
                 encoder_max_len=1024, decoder_max_len=200,
                 format_type="instruction")
    def __getitem__(idx)
    def _format_instruction(dialogue, summary)
    def _format_chat(dialogue, summary)

class InstructionAugmentedDataset(Dataset):
    # 5ê°€ì§€ instruction í…œí”Œë¦¿ìœ¼ë¡œ ë°ì´í„° ì¦ê°•
```

### ë°ì´í„° í¬ë§·

#### Instruction Format
```
### Instruction:
ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

### Input:
{dialogue}

### Response:
{summary}
```

#### Chat Format (Llama)
```
<|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:
{dialogue}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
{summary}<|eot_id|>
```

---

## LLM Trainer

### íŒŒì¼ ìœ„ì¹˜
```
src/training/llm_trainer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LLMTrainer:
    def __init__(config, model, tokenizer, train_dataset, eval_dataset)
    def train()
    def evaluate()
    def _create_training_args()
    def _create_trainer()
```

### í•™ìŠµ ì„¤ì • (QLoRA ìµœì í™”)

```python
# ==================== LLM í•™ìŠµ ì¸ì ì„¤ì • (QLoRA ìµœì í™”) ==================== #

# ---------------------- TrainingArguments ìƒì„± ---------------------- #
training_args = TrainingArguments(
    # -------------- ê¸°ë³¸ í•™ìŠµ ì„¤ì • -------------- #
    num_train_epochs=3,                         # í•™ìŠµ ì—í¬í¬ ìˆ˜
    per_device_train_batch_size=8,              # ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=8,              # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… (effective batch=64)
    learning_rate=2e-5,                         # í•™ìŠµë¥ 
    lr_scheduler_type="cosine",                 # Cosine í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    warmup_ratio=0.1,                           # ì›Œë°ì—… ë¹„ìœ¨ (ì „ì²´ ìŠ¤í…ì˜ 10%)
    weight_decay=0.1,                           # ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ì •ê·œí™”)
    max_grad_norm=1.2,                          # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„ê³„ê°’

    # -------------- QLoRA ìµœì í™” ì„¤ì • -------------- #
    optim="paged_adamw_32bit",                  # Paged AdamW ì˜µí‹°ë§ˆì´ì € (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    fp16=True,                                  # âš ï¸ FP16 ê¶Œì¥ (BF16ì€ PyTorch AMP í˜¸í™˜ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥)
    gradient_checkpointing=True,                # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… (ë©”ëª¨ë¦¬ ì ˆì•½)

    # -------------- í‰ê°€ ë° ì €ì¥ ì „ëµ -------------- #
    eval_strategy='epoch',                      # ì—í¬í¬ë§ˆë‹¤ í‰ê°€
    save_strategy='epoch',                      # ì—í¬í¬ë§ˆë‹¤ ì €ì¥
    metric_for_best_model='eval_loss',          # ìµœìƒ ëª¨ë¸ ì„ ì • ê¸°ì¤€ (Causal LMì€ loss ì‚¬ìš©)
    greater_is_better=False                     # LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
)
```

---

## LLM ì‹¤í–‰ ëª…ë ¹ì–´

### 1. ê¸°ë³¸ LLM í•™ìŠµ (Llama-3.2-3B + QLoRA)

```bash
# ==================== ê¸°ë³¸ LLM í•™ìŠµ ëª…ë ¹ì–´ ==================== #

# ---------------------- Llama-3.2-3B ëª¨ë¸ í•™ìŠµ (QLoRA ì‚¬ìš©) ---------------------- #
# QLoRAë¥¼ í™œìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ Llama-3.2-3B ëª¨ë¸ íŒŒì¸íŠœë‹
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_llama_3.2_3b/final_model/`
- í•™ìŠµ ë¡œê·¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_llama_3.2_3b/train.log`
- ë¡œê·¸ ë°±ì—…: `logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_llama_3.2_3b.log`

### 2. Qwen ëª¨ë¸ í•™ìŠµ

```bash
# ==================== Qwen ëª¨ë¸ í•™ìŠµ ëª…ë ¹ì–´ ==================== #

# ---------------------- Qwen3-4B ëª¨ë¸ í•™ìŠµ (QLoRA ì‚¬ìš©) ---------------------- #
# QLoRAë¥¼ í™œìš©í•˜ì—¬ Qwen3-4B ëª¨ë¸ íŒŒì¸íŠœë‹
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_qwen3_4b/final_model/`
- í•™ìŠµ ë¡œê·¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_qwen3_4b/train.log`
- ë¡œê·¸ ë°±ì—…: `logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_qwen3_4b.log`

### 3. Instruction Tuning (ë°ì´í„° 5ë°° ì¦ê°•)

```bash
# ==================== Instruction Tuning í•™ìŠµ ëª…ë ¹ì–´ ==================== #

# ---------------------- ë°ì´í„° ì¦ê°•ì„ í™œìš©í•œ Llama-3.2-3B í•™ìŠµ ---------------------- #
# 5ê°€ì§€ Instruction í…œí”Œë¦¿ìœ¼ë¡œ ë°ì´í„°ë¥¼ 5ë°° ì¦ê°•í•˜ì—¬ í•™ìŠµ
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**íš¨ê³¼:**
- í•™ìŠµ ë°ì´í„°: 12,457ê°œ â†’ 62,285ê°œ (5ë°°)
- 5ê°€ì§€ instruction í…œí”Œë¦¿ ì ìš©

### 4. ë””ë²„ê·¸ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
# ==================== ë””ë²„ê·¸ ëª¨ë“œ í•™ìŠµ ëª…ë ¹ì–´ ==================== #

# ---------------------- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰ ---------------------- #
# ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ íŒŒì´í”„ë¼ì¸ì„ ê²€ì¦
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •:**
- ë°ì´í„°: í•™ìŠµ 50ê°œ, ê²€ì¦ 10ê°œ
- ì—í¬í¬: 1íšŒ
- ë°°ì¹˜ í¬ê¸°: 2
- WandB: ë¹„í™œì„±í™”

---

## Config íŒŒì¼

### Causal LM ê¸°ë³¸ ì„¤ì •
**íŒŒì¼:** `configs/base/causal_lm.yaml`

```yaml
# ------------------------------- ëª¨ë¸ ì„¤ì • ------------------------------- #
model:
  type: causal_lm                                         # ëª¨ë¸ íƒ€ì… (Causal Language Model)
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"    # HuggingFace ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸

# ------------------------------- LoRA ì„¤ì • ------------------------------- #
lora:
  r: 16                                                   # LoRA rank (ì €ë­í¬ í–‰ë ¬ ì°¨ì›)
  alpha: 32                                               # alpha ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
  target_modules:                                         # LoRA ì ìš© ëŒ€ìƒ ëª¨ë“ˆ
    - "q_proj"                                            # Query projection
    - "k_proj"                                            # Key projection
    - "v_proj"                                            # Value projection
    - "o_proj"                                            # Output projection
    - "gate_proj"                                         # Gate projection (MLP)
    - "up_proj"                                           # Up projection (MLP)
    - "down_proj"                                         # Down projection (MLP)
  dropout: 0.05                                           # LoRA ë ˆì´ì–´ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
  use_qlora: true                                         # QLoRA 4-bit ì–‘ìí™” ì‚¬ìš©

# ------------------------------- í† í¬ë‚˜ì´ì € ì„¤ì • ------------------------------- #
tokenizer:
  encoder_max_len: 1024                                   # ì¸ì½”ë” ìµœëŒ€ ê¸¸ì´ (Prompt truncation ë°©ì§€)
  decoder_max_len: 200                                    # ë””ì½”ë” ìµœëŒ€ ê¸¸ì´

# ------------------------------- í•™ìŠµ ì„¤ì • ------------------------------- #
training:
  epochs: 3                                               # í•™ìŠµ ì—í¬í¬ ìˆ˜
  batch_size: 8                                           # ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
  gradient_accumulation_steps: 8                          # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… (effective batch=64)
  learning_rate: 2e-5                                     # í•™ìŠµë¥ 
  lr_scheduler_type: "cosine"                             # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
  warmup_ratio: 0.1                                       # ì›Œë°ì—… ë¹„ìœ¨
  weight_decay: 0.1                                       # ê°€ì¤‘ì¹˜ ê°ì†Œ (L2 ì •ê·œí™”)
  max_grad_norm: 1.2                                      # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì„ê³„ê°’
  gradient_checkpointing: true                            # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
```

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | QLoRA (4-bit) | Full Fine-tuning |
|------|----------|---------------|------------------|
| Llama-3.2-3B | 3B | **8GB** | 24GB |
| Qwen3-4B | 4B | **10GB** | 32GB |
| Llama-3-8B | 8B | **16GB** | 64GB |

### ì„±ëŠ¥ ëª©í‘œ

#### Zero-shot ì„±ëŠ¥ (ê²€ì¦ë¨)

| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | **ROUGE Sum** |
|------|---------|---------|---------|---------------|
| Llama-3.2-Korean-3B | 26.96 | 11.08 | 24.22 | **49.52** (1ìœ„) |
| Qwen3-4B | 24.22 | 9.23 | 21.79 | **45.02** (4ìœ„) |

#### íŒŒì¸íŠœë‹ ëª©í‘œ

| ëª¨ë¸ | Zero-shot | íŒŒì¸íŠœë‹ ëª©í‘œ | ê°œì„  ëª©í‘œ |
|------|-----------|---------------|-----------|
| Llama-3.2-Korean-3B | 49.52 | **95+** | +45 í¬ì¸íŠ¸ |
| Qwen3-4B | 45.02 | **95+** | +50 í¬ì¸íŠ¸ |

---

## ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/models/model_loader.py` - ModelLoader í´ë˜ìŠ¤
- `src/models/lora_loader.py` - LoRA Loader
- `src/training/trainer.py` - ModelTrainer í´ë˜ìŠ¤
- `src/training/llm_trainer.py` - LLM Trainer
- `src/inference/predictor.py` - Predictor í´ë˜ìŠ¤
- `src/data/llm_dataset.py` - LLM Dataset

**Config:**
- `configs/base/default.yaml` - ê¸°ë³¸ ì„¤ì •
- `configs/base/encoder_decoder.yaml` - Seq2Seq ì„¤ì •
- `configs/base/causal_lm.yaml` - Causal LM ì„¤ì •
- `configs/models/kobart.yaml` - KoBART ì„¤ì •
- `configs/models/llama_3.2_3b.yaml` - Llama ì„¤ì •
- `configs/models/qwen3_4b.yaml` - Qwen ì„¤ì •

**ìŠ¤í¬ë¦½íŠ¸:**
- `scripts/train.py` - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- `scripts/train_llm.py` - LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- `scripts/inference.py` - ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

**ê´€ë ¨ ë¬¸ì„œ:**
- [01_ì‹œì‘_ê°€ì´ë“œ.md](./01_ì‹œì‘_ê°€ì´ë“œ.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) - í•µì‹¬ ì‹œìŠ¤í…œ ë° Config
- [06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) - ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°•
- [08_í‰ê°€_ìµœì í™”.md](./08_í‰ê°€_ìµœì í™”.md) - í‰ê°€ ë° ìµœì í™”
- [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ì „ì²´ ëª…ë ¹ì–´ ê°€ì´ë“œ
