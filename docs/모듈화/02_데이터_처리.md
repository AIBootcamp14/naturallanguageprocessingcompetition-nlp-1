# ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [DialoguePreprocessor](#dialoguepreprocessor)
3. [Dataset í´ë˜ìŠ¤](#dataset-í´ë˜ìŠ¤)
4. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
5. [í…ŒìŠ¤íŠ¸ ê²°ê³¼](#í…ŒìŠ¤íŠ¸-ê²°ê³¼)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- ëŒ€í™” ë°ì´í„°ì˜ ì¼ê´€ëœ ì „ì²˜ë¦¬
- PyTorch í•™ìŠµ/ì¶”ë¡ ì„ ìœ„í•œ Dataset ì œê³µ
- ë…¸ì´ì¦ˆ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ê·œí™”
- í™”ì ì •ë³´ ì¶”ì¶œ ë° í†µê³„ ìƒì„±

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ë…¸ì´ì¦ˆ ì œê±° (`\\n`, `<br>` ë“±)
- âœ… í™”ì ì¶”ì¶œ ë° í„´ ê³„ì‚°
- âœ… í•™ìŠµ/ê²€ì¦ìš© Dataset
- âœ… ì¶”ë¡ ìš© Dataset
- âœ… DataFrame ì „ì²˜ë¦¬

---

## ğŸ”§ DialoguePreprocessor

### í´ë˜ìŠ¤ êµ¬ì¡°

```mermaid
classDiagram
    class DialoguePreprocessor {
        +speaker_pattern: Pattern
        +__init__()
        +clean_dialogue(text: str) str
        +normalize_dialogue(text: str) str
        +extract_speakers(dialogue: str) List[str]
        +count_turns(dialogue: str) int
        +split_dialogue_by_speaker(dialogue: str) List[Tuple]
        +preprocess_batch(dialogues, summaries) Tuple
        +preprocess_dataframe(df) DataFrame
    }

    DialoguePreprocessor --> "uses" Dataset
```

### ì£¼ìš” ë©”ì„œë“œ

#### 1. clean_dialogue() - ë…¸ì´ì¦ˆ ì œê±°

**ì²˜ë¦¬ í•­ëª©:**
1. `\\n` â†’ `\n` ë³€í™˜ (ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ ë¬¸ì)
2. `<br>` íƒœê·¸ ì œê±°
3. ì¤‘ë³µ ê³µë°± ì œê±°
4. ê³¼ë„í•œ ê°œí–‰ ì œê±° (3ê°œ ì´ìƒ â†’ 2ê°œ)
5. ì•ë’¤ ê³µë°± ì œê±°

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from src.data import DialoguePreprocessor

preprocessor = DialoguePreprocessor()

# ë…¸ì´ì¦ˆê°€ ìˆëŠ” í…ìŠ¤íŠ¸
dirty_text = "ì•ˆë…•í•˜ì„¸ìš”\\\\n<br>ë°˜ê°‘ìŠµë‹ˆë‹¤  "

# ì „ì²˜ë¦¬ ì‹¤í–‰
clean_text = preprocessor.clean_dialogue(dirty_text)
print(clean_text)  # "ì•ˆë…•í•˜ì„¸ìš”\në°˜ê°‘ìŠµë‹ˆë‹¤"
```

#### 2. extract_speakers() - í™”ì ì¶”ì¶œ

**ì¶”ì¶œ íŒ¨í„´:** `#Person1#`, `#Person2#`, ... `#PersonN#`

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤\\n#Person1#: ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤"

speakers = preprocessor.extract_speakers(dialogue)
print(speakers)  # ['#Person1#', '#Person2#']
```

#### 3. count_turns() - í„´ ê°œìˆ˜ ê³„ì‚°

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤\\n#Person1#: ê°ì‚¬í•©ë‹ˆë‹¤"

turns = preprocessor.count_turns(dialogue)
print(turns)  # 3
```

#### 4. split_dialogue_by_speaker() - ëŒ€í™” ë¶„í• 

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤"

turns = preprocessor.split_dialogue_by_speaker(dialogue)
# [('#Person1#', 'ì•ˆë…•í•˜ì„¸ìš”'), ('#Person2#', 'ë°˜ê°‘ìŠµë‹ˆë‹¤')]

for speaker, utterance in turns:
    print(f"{speaker}: {utterance}")
```

#### 5. preprocess_dataframe() - DataFrame ì „ì²˜ë¦¬

**ì¶”ê°€ë˜ëŠ” í†µê³„ ì»¬ëŸ¼:**
- `num_speakers` - í™”ì ìˆ˜
- `num_turns` - í„´ ê°œìˆ˜

```python
import pandas as pd

df = pd.read_csv("data/raw/train.csv")
preprocessor = DialoguePreprocessor()

# DataFrame ì „ì²˜ë¦¬ (dialogue, summary ì»¬ëŸ¼ ì •ì œ + í†µê³„ ì¶”ê°€)
df_processed = preprocessor.preprocess_dataframe(df)

print(df_processed[['num_speakers', 'num_turns']].describe())
```

---

## ğŸ“¦ Dataset í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```mermaid
classDiagram
    class Dataset {
        <<interface>>
        +__len__()
        +__getitem__(idx)
    }

    class DialogueSummarizationDataset {
        +dialogues: List[str]
        +summaries: List[str]
        +tokenizer: PreTrainedTokenizer
        +encoder_max_len: int
        +decoder_max_len: int
        +__init__(...)
        +__len__() int
        +__getitem__(idx) Dict
    }

    class InferenceDataset {
        +dialogues: List[str]
        +tokenizer: PreTrainedTokenizer
        +encoder_max_len: int
        +fnames: List[str]
        +__init__(...)
        +__len__() int
        +__getitem__(idx) Dict
    }

    Dataset <|-- DialogueSummarizationDataset
    Dataset <|-- InferenceDataset
```

### 1. DialogueSummarizationDataset (í•™ìŠµ/ê²€ì¦ìš©)

**ëª©ì :** í•™ìŠµ ë° ê²€ì¦ì„ ìœ„í•œ Dataset

**ë°˜í™˜ í˜•ì‹:**
```python
{
    'input_ids': Tensor,        # ì¸ì½”ë” ì…ë ¥ (dialogue)
    'attention_mask': Tensor,   # ì–´í…ì…˜ ë§ˆìŠ¤í¬
    'labels': Tensor            # ë””ì½”ë” ë ˆì´ë¸” (summary)
}
```

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from src.data import DialogueSummarizationDataset
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# ë°ì´í„°ì…‹ ìƒì„±
dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    preprocess=True             # ì „ì²˜ë¦¬ ìë™ ì ìš©
)

# ìƒ˜í”Œ ì ‘ê·¼
sample = dataset[0]
print(f"Input shape: {sample['input_ids'].shape}")         # torch.Size([512])
print(f"Attention mask shape: {sample['attention_mask'].shape}")  # torch.Size([512])
print(f"Labels shape: {sample['labels'].shape}")           # torch.Size([100])
```

### 2. InferenceDataset (ì¶”ë¡ ìš©)

**ëª©ì :** ì¶”ë¡ (ì˜ˆì¸¡)ì„ ìœ„í•œ Dataset

**ë°˜í™˜ í˜•ì‹:**
```python
{
    'input_ids': Tensor,        # ì¸ì½”ë” ì…ë ¥ (dialogue)
    'attention_mask': Tensor,   # ì–´í…ì…˜ ë§ˆìŠ¤í¬
    'fname': str                # íŒŒì¼ëª… (ì„ íƒì )
}
```

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from src.data import InferenceDataset

# ì¶”ë¡ ìš© ë°ì´í„°ì…‹ ìƒì„±
inference_dataset = InferenceDataset(
    dialogues=test_df['dialogue'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    preprocess=True,
    fnames=test_df['fname'].tolist()  # ì œì¶œ íŒŒì¼ìš©
)

# ìƒ˜í”Œ ì ‘ê·¼
sample = inference_dataset[0]
print(f"Input shape: {sample['input_ids'].shape}")
print(f"Fname: {sample['fname']}")
```

### 3. create_dataset_from_dataframe() í¸ì˜ í•¨ìˆ˜

**DataFrameì—ì„œ Dataset ì§ì ‘ ìƒì„±:**

```python
from src.data import create_dataset_from_dataframe

# í•™ìŠµìš© Dataset ìƒì„±
train_dataset = create_dataset_from_dataframe(
    df=train_df,
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    is_train=True,              # í•™ìŠµ ëª¨ë“œ
    preprocess=True
)

# ì¶”ë¡ ìš© Dataset ìƒì„±
test_dataset = create_dataset_from_dataframe(
    df=test_df,
    tokenizer=tokenizer,
    encoder_max_len=512,
    is_train=False,             # ì¶”ë¡  ëª¨ë“œ
    preprocess=True
)
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### ë°ì´í„° ì²˜ë¦¬ í”Œë¡œìš°

```mermaid
graph LR
    A[Raw CSV] --> B[DataFrame]
    B --> C{ì „ì²˜ë¦¬ í•„ìš”?}

    C -->|Yes| D[DialoguePreprocessor]
    D --> E[ì •ì œëœ DataFrame]

    C -->|No| E

    E --> F[Dataset ìƒì„±]
    F --> G{í•™ìŠµ/ì¶”ë¡ ?}

    G -->|í•™ìŠµ| H[DialogueSummarizationDataset]
    G -->|ì¶”ë¡ | I[InferenceDataset]

    H --> J[DataLoader]
    I --> J

    J --> K[ëª¨ë¸ í•™ìŠµ/ì¶”ë¡ ]

    style A fill:#e3f2fd
    style E fill:#fff3e0
    style H fill:#c8e6c9
    style I fill:#c8e6c9
```

### ì „ì²´ ì‚¬ìš© ì˜ˆì‹œ

```python
import pandas as pd
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from src.data import DialoguePreprocessor, DialogueSummarizationDataset

# 1. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")

# 2. ì „ì²˜ë¦¬ (ì„ íƒì  - Datasetì—ì„œ ìë™ìœ¼ë¡œë„ ê°€ëŠ¥)
preprocessor = DialoguePreprocessor()
train_df = preprocessor.preprocess_dataframe(train_df)

print(f"ë°ì´í„° í¬ê¸°: {len(train_df)}")
print(f"í™”ì ìˆ˜ ë¶„í¬:\n{train_df['num_speakers'].value_counts()}")
print(f"í„´ ìˆ˜ í†µê³„:\n{train_df['num_turns'].describe()}")

# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# 4. Dataset ìƒì„±
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    preprocess=False  # ì´ë¯¸ ì „ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ False
)

# 5. DataLoader ìƒì„±
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# 6. í•™ìŠµ ë£¨í”„
for batch in train_loader:
    input_ids = batch['input_ids']          # (batch_size, 512)
    attention_mask = batch['attention_mask'] # (batch_size, 512)
    labels = batch['labels']                # (batch_size, 100)

    # ëª¨ë¸ í•™ìŠµ...
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼

### í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´

```bash
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate
python tests/test_preprocessor.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 5ê°œ)

#### 1. âœ… ë…¸ì´ì¦ˆ ì œê±° í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤:**
```python
test_cases = [
    ("ì•ˆë…•í•˜ì„¸ìš”\\\\në°˜ê°‘ìŠµë‹ˆë‹¤", "ì•ˆë…•í•˜ì„¸ìš”\\në°˜ê°‘ìŠµë‹ˆë‹¤"),  # \\\\n â†’ \\n
    ("ì•ˆë…•<br>ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì•ˆë…•\\në°˜ê°‘ìŠµë‹ˆë‹¤"),              # <br> â†’ \\n
    ("ì•ˆë…•  í•˜ì„¸ìš”", "ì•ˆë…• í•˜ì„¸ìš”"),                        # ì¤‘ë³µ ê³µë°± ì œê±°
    ("  ì•ˆë…•í•˜ì„¸ìš”  ", "ì•ˆë…•í•˜ì„¸ìš”"),                       # ì•ë’¤ ê³µë°± ì œê±°
]
```

**ê²°ê³¼:**
```
í…ŒìŠ¤íŠ¸ 1: âœ…
í…ŒìŠ¤íŠ¸ 2: âœ…
í…ŒìŠ¤íŠ¸ 3: âœ…
í…ŒìŠ¤íŠ¸ 4: âœ…

âœ… ë…¸ì´ì¦ˆ ì œê±° í…ŒìŠ¤íŠ¸ ì„±ê³µ!
```

#### 2. âœ… í™”ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤\\n#Person1#: ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤"
speakers = preprocessor.extract_speakers(dialogue)

assert speakers == ['#Person1#', '#Person2#']
```

**ê²°ê³¼:**
```
í™”ì: ['#Person1#', '#Person2#']
âœ… í™”ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
```

#### 3. âœ… í„´ ê°œìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤\\n#Person1#: ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤"
turns = preprocessor.count_turns(dialogue)

assert turns == 3
```

**ê²°ê³¼:**
```
í„´ ê°œìˆ˜: 3
âœ… í„´ ê°œìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ!
```

#### 4. âœ… ëŒ€í™” ë¶„í•  í…ŒìŠ¤íŠ¸

```python
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤"
turns = preprocessor.split_dialogue_by_speaker(dialogue)

assert len(turns) == 2
assert turns[0] == ('#Person1#', 'ì•ˆë…•í•˜ì„¸ìš”')
assert turns[1] == ('#Person2#', 'ë°˜ê°‘ìŠµë‹ˆë‹¤')
```

**ê²°ê³¼:**
```
ë¶„í•  ê²°ê³¼:
  #Person1#: ì•ˆë…•í•˜ì„¸ìš”
  #Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤

âœ… ëŒ€í™” ë¶„í•  í…ŒìŠ¤íŠ¸ ì„±ê³µ!
```

#### 5. âœ… ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

**ì‹¤ì œ í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬:**

```python
df = pd.read_csv('data/raw/train.csv')
preprocessor = DialoguePreprocessor()
df_processed = preprocessor.preprocess_dataframe(df)
```

**ê²°ê³¼:**
```
ì›ë³¸ ë°ì´í„° í¬ê¸°: 12,457
ì „ì²˜ë¦¬ í›„ í¬ê¸°: 12,457
ì¶”ê°€ëœ ì»¬ëŸ¼: ['num_speakers', 'num_turns']

í™”ì ìˆ˜ í†µê³„:
2    12335
3      107
4       13
5        1
6        1

í„´ ìˆ˜ í†µê³„:
count    12457.000000
mean         9.493425
std          4.148729
min          2.000000
25%          7.000000
50%          9.000000
75%         11.000000
max         59.000000

âœ… ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
```

### ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼

**í•µì‹¬ í†µê³„:**
- ì „ì²´ ìƒ˜í”Œ: **12,457ê°œ**
- í™”ì ë¶„í¬: ëŒ€ë¶€ë¶„ **2ëª…** (12,335ê°œ, 99%)
- í„´ ê°œìˆ˜: í‰ê·  **9.5í„´**, ìµœëŒ€ **59í„´**
- ì „ì²˜ë¦¬ ì„±ê³µë¥ : **100%**

---

## ğŸ“Š ë°ì´í„° íŠ¹ì„± ë¶„ì„

### í™”ì ë¶„í¬

```
2ëª…: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99.02% (12,335ê°œ)
3ëª…: â–ˆ 0.86% (107ê°œ)
4ëª…: â– 0.10% (13ê°œ)
5ëª…: â– 0.01% (1ê°œ)
6ëª…: â– 0.01% (1ê°œ)
```

### í„´ ê¸¸ì´ ë¶„í¬

```
 2-5í„´: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 26.8%
 6-9í„´: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 48.2%
10-13í„´: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35.4%
14-17í„´: â–ˆâ–ˆ 9.8%
18+í„´: â–ˆ 4.8%
```

---

## ğŸ¯ ì‹¤ì „ í™œìš© íŒ

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬

ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ:

```python
# ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
chunk_size = 1000

for chunk in pd.read_csv("data/raw/train.csv", chunksize=chunk_size):
    chunk_processed = preprocessor.preprocess_dataframe(chunk)
    # ì²˜ë¦¬...
```

### 2. ì „ì²˜ë¦¬ ìºì‹±

```python
import pickle

# ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
df_processed = preprocessor.preprocess_dataframe(df)
df_processed.to_pickle("data/processed/train_preprocessed.pkl")

# ë‹¤ìŒ ì‹¤í–‰ ì‹œ ë¡œë“œ
df_processed = pd.read_pickle("data/processed/train_preprocessed.pkl")
```

### 3. ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì¶”ê°€

```python
class CustomPreprocessor(DialoguePreprocessor):
    def custom_clean(self, text: str) -> str:
        # ì¶”ê°€ ì „ì²˜ë¦¬ ë¡œì§
        text = self.clean_dialogue(text)
        text = text.replace("íŠ¹ì •íŒ¨í„´", "ëŒ€ì²´í…ìŠ¤íŠ¸")
        return text

    def preprocess_batch(self, dialogues, summaries=None):
        # clean_dialogue ëŒ€ì‹  custom_clean ì‚¬ìš©
        cleaned_dialogues = [self.custom_clean(d) for d in dialogues]
        # ...
        return cleaned_dialogues, cleaned_summaries
```

---

## ğŸ“Œ ì£¼ì˜ì‚¬í•­

### 1. í† í° ê¸¸ì´ ì´ˆê³¼

```python
# ê¸´ ëŒ€í™”ëŠ” ìë™ìœ¼ë¡œ ì˜ë¦¼ (truncation=True)
dataset = DialogueSummarizationDataset(
    ...,
    encoder_max_len=512,  # 512 í† í° ì´ˆê³¼ ì‹œ ìë™ ìë¥´ê¸°
    truncation=True
)
```

### 2. íŒ¨ë”© í† í° ì²˜ë¦¬

```python
# labelsì˜ íŒ¨ë”©ì€ -100ìœ¼ë¡œ ì„¤ì •ë¨ (ì†ì‹¤ ê³„ì‚° ì‹œ ë¬´ì‹œ)
labels[labels == tokenizer.pad_token_id] = -100
```

### 3. ì „ì²˜ë¦¬ ì¤‘ë³µ ë°©ì§€

```python
# Datasetì—ì„œ preprocess=Trueë¡œ ì„¤ì •í•œ ê²½ìš°
# ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ì§€ ì•Šì•„ë„ ë¨
dataset = DialogueSummarizationDataset(
    ...,
    preprocess=True  # ìë™ ì „ì²˜ë¦¬
)
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/data/preprocessor.py` - DialoguePreprocessor í´ë˜ìŠ¤
- `src/data/dataset.py` - Dataset í´ë˜ìŠ¤ë“¤
- `src/data/__init__.py` - ì™¸ë¶€ API

**í…ŒìŠ¤íŠ¸:**
- `tests/test_preprocessor.py` - ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

**ë°ì´í„°:**
- `data/raw/train.csv` - í•™ìŠµ ë°ì´í„°
- `data/raw/dev.csv` - ê²€ì¦ ë°ì´í„°
