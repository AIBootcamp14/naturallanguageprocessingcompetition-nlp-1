# í•™ìŠµ ì‹œìŠ¤í…œ ìƒì„¸ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ModelTrainer í´ë˜ìŠ¤](#modeltrainer-í´ë˜ìŠ¤)
3. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
4. [WandB í†µí•©](#wandb-í†µí•©)
5. [ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬](#ì²´í¬í¬ì¸íŠ¸-ê´€ë¦¬)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- HuggingFace Seq2SeqTrainer ë˜í•‘
- Config ê¸°ë°˜ í•™ìŠµ ìë™í™”
- WandB ë¡œê¹… í†µí•©
- ROUGE í‰ê°€ ìë™ ì‹¤í–‰
- ì²´í¬í¬ì¸íŠ¸ ìë™ ê´€ë¦¬

### í•µì‹¬ ê¸°ëŠ¥
- âœ… Seq2SeqTrainer ìë™ ì„¤ì •
- âœ… WandB ë¡œê¹… í†µí•©
- âœ… ROUGE ìë™ í‰ê°€
- âœ… Early Stopping ì§€ì›
- âœ… ìµœìƒ ëª¨ë¸ ìë™ ì €ì¥
- âœ… Logger í†µí•© ì§€ì›

---

## ğŸ—ï¸ ModelTrainer í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/training/trainer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class ModelTrainer:
    def __init__(self, config, model, tokenizer, train_dataset,
                 eval_dataset=None, use_wandb=True, logger=None):
        """í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""

    def _create_training_args(self) -> Seq2SeqTrainingArguments:
        """HuggingFace í•™ìŠµ ì¸ì ìƒì„±"""

    def compute_metrics(self, eval_preds) -> Dict[str, float]:
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE)"""

    def _create_trainer(self) -> Seq2SeqTrainer:
        """HuggingFace Seq2SeqTrainer ìƒì„±"""

    def train(self) -> Dict[str, Any]:
        """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""

    def evaluate(self) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# 1. Config ë¡œë“œ
config = load_config("baseline_kobart")

# 2. ëª¨ë¸ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# 3. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")
eval_df = pd.read_csv("data/raw/dev.csv")

# 4. Dataset ìƒì„±
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=config.tokenizer.encoder_max_len,
    decoder_max_len=config.tokenizer.decoder_max_len
)

eval_dataset = DialogueSummarizationDataset(
    dialogues=eval_df['dialogue'].tolist(),
    summaries=eval_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=config.tokenizer.encoder_max_len,
    decoder_max_len=config.tokenizer.decoder_max_len
)

# 5. Trainer ìƒì„± ë° í•™ìŠµ
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    use_wandb=True
)

# 6. í•™ìŠµ ì‹¤í–‰
results = trainer.train()

print(f"ìµœì¢… ëª¨ë¸ ê²½ë¡œ: {results['final_model_path']}")
print(f"ROUGE Sum: {results['eval_metrics']['eval_rouge_sum']:.4f}")
```

### 2. Loggerì™€ í•¨ê»˜ ì‚¬ìš©

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# Logger ì´ˆê¸°í™”
log_path = create_log_path("train", "train.log")
logger = Logger(log_path, print_also=True)
logger.start_redirect()

try:
    # Trainer ìƒì„± (Logger ì „ë‹¬)
    trainer = create_trainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        use_wandb=True,
        logger=logger
    )

    # í•™ìŠµ ì‹¤í–‰
    results = trainer.train()

finally:
    logger.stop_redirect()
    logger.close()
```

### 3. WandB ì—†ì´ í•™ìŠµ

```python
# WandB ë¹„í™œì„±í™”
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    use_wandb=False  # WandB ì‚¬ìš© ì•ˆ í•¨
)

results = trainer.train()
```

---

## ğŸ“Š í•™ìŠµ ì¸ì (Seq2SeqTrainingArguments)

### Configì—ì„œ ìë™ ìƒì„±

`src/training/trainer.py`ì˜ `_create_training_args` í•¨ìˆ˜ê°€ Configë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ì¸ìë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤:

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  epochs: 20
  batch_size: 50
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_steps: 500
  save_total_limit: 3
  logging_steps: 100
```

### ìƒì„±ëœ í•™ìŠµ ì¸ì

```python
Seq2SeqTrainingArguments(
    output_dir="outputs/baseline_kobart",
    overwrite_output_dir=True,

    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    num_train_epochs=20,
    per_device_train_batch_size=50,
    per_device_eval_batch_size=50,
    learning_rate=1e-5,
    weight_decay=0.01,
    warmup_steps=500,

    # í‰ê°€ ë° ì €ì¥
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model='eval_rouge_sum',

    # ë¡œê¹…
    logging_dir="outputs/baseline_kobart/logs",
    logging_steps=100,
    report_to=['wandb'] if use_wandb else [],

    # Seq2Seq íŠ¹í™”
    predict_with_generate=True,
    generation_max_length=100,
    generation_num_beams=4,

    # ê¸°íƒ€
    fp16=torch.cuda.is_available(),
    dataloader_num_workers=4
)
```

---

## ğŸ”— WandB í†µí•©

### WandB ì„¤ì •

```yaml
# configs/experiments/baseline_kobart.yaml
wandb:
  enabled: true
  project: "nlp-competition"
  entity: "ieyeppo"

experiment:
  name: "baseline_kobart"
  tags:
    - "baseline"
    - "kobart"
```

### ìë™ ë¡œê¹… í•­ëª©

WandBì— ìë™ìœ¼ë¡œ ë¡œê¹…ë˜ëŠ” í•­ëª©:

1. **í•™ìŠµ ë©”íŠ¸ë¦­**
   - train_loss
   - train_runtime
   - train_samples_per_second
   - train_steps_per_second

2. **í‰ê°€ ë©”íŠ¸ë¦­**
   - eval_rouge1
   - eval_rouge2
   - eval_rougeL
   - eval_rouge_sum
   - eval_loss

3. **Config ì •ë³´**
   - ì „ì²´ Config íŒŒë¼ë¯¸í„°
   - ì‹¤í—˜ íƒœê·¸
   - ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì´ë¦„

4. **ì‹œìŠ¤í…œ ì •ë³´**
   - GPU ì‚¬ìš©ë¥ 
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - CPU ì‚¬ìš©ë¥ 

### WandB ì´ˆê¸°í™” ë° ì¢…ë£Œ

```python
# WandB Logger ì´ˆê¸°í™”
if use_wandb and config.wandb.enabled:
    self.wandb_logger = WandbLogger(
        project_name=config.wandb.project,
        entity=config.wandb.entity,
        experiment_name=config.experiment.name,
        config=dict(config),
        tags=config.experiment.tags
    )

# í•™ìŠµ ì‹œì‘ ì‹œ WandB Run ì´ˆê¸°í™”
self.wandb_logger.init_run()

# í•™ìŠµ ì¢…ë£Œ ì‹œ WandB Run ì¢…ë£Œ
self.wandb_logger.finish()
```

---

## ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ìë™ ì €ì¥

í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤:

```
outputs/baseline_kobart/
â”œâ”€â”€ checkpoint-500/          # 500 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ trainer_state.json
â”œâ”€â”€ checkpoint-1000/         # 1000 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ checkpoint-1500/         # 1500 ìŠ¤í… ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ final_model/             # ìµœì¢… ëª¨ë¸
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ special_tokens_map.json
```

### ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ

```python
# Config ì„¤ì •
save_strategy='epoch',
load_best_model_at_end=True,           # ìµœìƒ ëª¨ë¸ ìë™ ë¡œë“œ
metric_for_best_model='eval_rouge_sum' # ROUGE Sum ê¸°ì¤€
```

í•™ìŠµ ì¢…ë£Œ í›„ ìë™ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ROUGE Sumì„ ë‹¬ì„±í•œ ì²´í¬í¬ì¸íŠ¸ê°€ ë¡œë“œë©ë‹ˆë‹¤.

### ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  save_total_limit: 3  # ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
```

ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ëŠ” ìë™ìœ¼ë¡œ ì‚­ì œë˜ì–´ ë””ìŠ¤í¬ ê³µê°„ì„ ì ˆì•½í•©ë‹ˆë‹¤.

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜
```
src/tests/test_trainer.py
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python src/tests/test_trainer.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 4ê°œ)

1. âœ… Trainer ìƒì„±
2. âœ… í•™ìŠµ ì¸ì ìƒì„±
3. âœ… ROUGE í‰ê°€ í•¨ìˆ˜ (compute_metrics)
4. âœ… í¸ì˜ í•¨ìˆ˜ (create_trainer)

---

## ğŸ¯ ì‹¤ì „ í™œìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ë””ë²„ê·¸ ëª¨ë“œ í•™ìŠµ

```python
# Config ìˆ˜ì •
config.training.epochs = 2
config.training.batch_size = 4
config.wandb.enabled = False

# ì‘ì€ ë°ì´í„°ì…‹
train_df = train_df.head(100)
eval_df = eval_df.head(20)

# ë¹ ë¥¸ í•™ìŠµ
trainer = create_trainer(config, model, tokenizer, train_dataset, eval_dataset, use_wandb=False)
results = trainer.train()
```

### ì˜ˆì‹œ 2: Early Stopping ì‚¬ìš©

```yaml
# configs/experiments/my_experiment.yaml
training:
  early_stopping_patience: 3  # 3 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ
```

### ì˜ˆì‹œ 3: í•™ìŠµ ì¬ê°œ

```python
# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
trainer.train(resume_from_checkpoint="outputs/baseline_kobart/checkpoint-1000")
```

---

## ğŸ“Œ ì£¼ì˜ì‚¬í•­

### 1. ë°°ì¹˜ í¬ê¸°ì™€ GPU ë©”ëª¨ë¦¬

GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”:

```yaml
training:
  batch_size: 16  # 50 â†’ 16
  gradient_accumulation_steps: 4  # ì‹¤ì§ˆì  ë°°ì¹˜ í¬ê¸°: 16 * 4 = 64
```

### 2. WandB ë¡œê·¸ì¸

WandB ì‚¬ìš© ì „ ë¡œê·¸ì¸ í•„ìš”:

```bash
wandb login
```

### 3. FP16 ìë™ í™œì„±í™”

GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ FP16 í•™ìŠµì´ í™œì„±í™”ë©ë‹ˆë‹¤:

```python
fp16=torch.cuda.is_available()
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/training/trainer.py` - ModelTrainer í´ë˜ìŠ¤
- `src/training/__init__.py` - ì™¸ë¶€ API

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_trainer.py` - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**Config:**
- `configs/base/default.yaml` - ê¸°ë³¸ í•™ìŠµ ì„¤ì •
- `configs/experiments/baseline_kobart.yaml` - ì‹¤í—˜ë³„ ì„¤ì •
