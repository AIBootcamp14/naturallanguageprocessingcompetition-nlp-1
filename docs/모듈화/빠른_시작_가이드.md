# 빠른 시작 가이드

## 🚀 5분 안에 시작하기

### 1. 환경 설정

```bash
# 가상환경 활성화
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# 또는
pyenv activate nlp_py3_11_9

# 패키지 확인
pip list | grep -E "(torch|transformers|wandb)"
```

### 2. 전체 모듈 테스트

```bash
# 프로젝트 루트에서 실행
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition

# 모든 테스트 한 번에 실행
python tests/test_config_loader.py && \
python tests/test_preprocessor.py && \
python tests/test_model_loader.py && \
python tests/test_metrics.py && \
python tests/test_trainer.py && \
python tests/test_predictor.py
```

---

## 📝 주요 명령어

### Config 관련

```python
# Config 로드
from src.config import load_config
config = load_config("baseline_kobart")

# Config 확인
from omegaconf import OmegaConf
print(OmegaConf.to_yaml(config))
```

### 데이터 전처리

```python
from src.data import DialoguePreprocessor
import pandas as pd

# 전처리기 생성
preprocessor = DialoguePreprocessor()

# DataFrame 전처리
df = pd.read_csv("data/raw/train.csv")
df_processed = preprocessor.preprocess_dataframe(df)
```

### 모델 로딩

```python
from src.models import load_model_and_tokenizer
from src.config import load_config

config = load_config("baseline_kobart")
model, tokenizer = load_model_and_tokenizer(config)

print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### 학습 실행

```python
from src.training import create_trainer
from src.data import DialogueSummarizationDataset

# Dataset 생성
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer
)

# Trainer 생성 및 학습
trainer = create_trainer(config, model, tokenizer, train_dataset)
results = trainer.train()
```

### 추론 실행

```python
from src.inference import create_predictor

# Predictor 생성
predictor = create_predictor(model, tokenizer, config)

# 단일 예측
summary = predictor.predict_single("#Person1#: 안녕하세요")

# 제출 파일 생성
test_df = pd.read_csv("data/raw/test.csv")
submission = predictor.create_submission(
    test_df,
    output_path="submissions/submission.csv"
)
```

---

## 🔥 자주 사용하는 패턴

### 패턴 1: 빠른 실험

```python
# 1단계: Config 로드
config = load_config("baseline_kobart")

# 2단계: Config 수정 (빠른 테스트)
config.training.epochs = 2
config.training.batch_size = 4
config.wandb.enabled = False

# 3단계: 모델 로드
model, tokenizer = load_model_and_tokenizer(config)

# 4단계: 작은 데이터로 테스트
df = pd.read_csv("data/raw/train.csv").head(100)  # 100개만
# ... 학습 진행
```

### 패턴 2: 체크포인트에서 재개

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# 저장된 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/checkpoint-1000")
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/checkpoint-1000")

# 추론 진행
predictor = create_predictor(model, tokenizer, config)
```

### 패턴 3: ROUGE 평가만 실행

```python
from src.evaluation import calculate_rouge_scores

predictions = ["예측 요약1", "예측 요약2"]
references = ["정답 요약1", "정답 요약2"]

scores = calculate_rouge_scores(predictions, references)
print(scores)
```

---

## 💡 문제 해결

### 문제 1: CUDA out of memory

```python
# 해결책: 배치 크기 줄이기
config.training.batch_size = 8  # 또는 4, 2
config.training.gradient_accumulation_steps = 4  # 그래디언트 누적 사용
```

### 문제 2: 토크나이저 특수 토큰 경고

```python
# 해결책: 특수 토큰 명시적으로 추가
special_tokens = ['#Person1#', '#Person2#', ...]
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
model.resize_token_embeddings(len(tokenizer))
```

### 문제 3: WandB 로그인 필요

```bash
# 해결책: WandB 로그인
wandb login

# 또는 Config에서 비활성화
config.wandb.enabled = False
```

---

## 📚 더 알아보기

| 문서 | 설명 |
|------|------|
| [00_전체_시스템_개요.md](./00_전체_시스템_개요.md) | 시스템 아키텍처 |
| [01_Config_시스템.md](./01_Config_시스템.md) | Config 상세 가이드 |
| [02_데이터_처리.md](./02_데이터_처리.md) | 데이터 전처리 상세 |

---

