# 09. 추론 최적화 가이드

> **통합 문서:** TensorRT 가속 + 모델 Pruning + 추론 벤치마크

## 📋 목차

### Part 1: TensorRT 최적화
- [개요](#part-1-tensorrt-최적화)
- [TensorRTOptimizer 클래스](#tensorrtoptimizer-클래스)
- [변환 프로세스](#변환-프로세스)
- [Fallback 모드](#fallback-모드-pytorch-jit)
- [사용 방법](#tensorrt-사용-방법)

### Part 2: 모델 Pruning
- [개요](#part-2-모델-pruning)
- [ModelPruner 클래스](#modelpruner-클래스)
- [Pruning 방법](#pruning-방법)
- [통계 및 분석](#pruning-통계)
- [사용 방법](#pruning-사용-방법)

### Part 3: 추론 벤치마크
- [개요](#part-3-추론-벤치마크)
- [벤치마크 방법](#벤치마크-방법)
- [성능 비교](#성능-비교)

---

# 📌 Part 1: TensorRT 최적화

## 📝 개요

### 목적
- GPU 추론 가속 (최대 5-10배 속도 향상)
- FP16/INT8 정밀도 최적화로 메모리 절약
- PyTorch → ONNX → TensorRT 변환
- TensorRT 미설치 시 PyTorch JIT로 Fallback

### 핵심 기능
- ✅ 자동 TensorRT 변환 (ONNX 경유)
- ✅ FP16/INT8 정밀도 지원
- ✅ Fallback 모드 (PyTorch JIT)
- ✅ 추론 속도 벤치마크
- ✅ 설정 저장/로드

### 파일 위치
```
src/inference/tensorrt_optimizer.py
```

---

## 🔧 TensorRTOptimizer 클래스

### 클래스 구조

```python
class TensorRTOptimizer:
    def __init__(
        precision="fp16",
        workspace_size=1<<30,  # 1GB
        max_batch_size=32,
        logger=None
    )

    # 변환
    def convert_to_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any

    # 벤치마크
    def benchmark(model, input_shape, n_iterations, warmup_iterations) -> Dict

    # 설정 저장
    def save_config(output_path)

    # 내부 메서드
    def _convert_with_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any
    def _fallback_optimize(model, input_shape) -> torch.jit.ScriptModule
    def _check_tensorrt_availability() -> bool
```

---

## 🔄 변환 프로세스

### 3단계 변환 파이프라인

```mermaid
graph LR
    A[PyTorch Model] --> B[ONNX]
    B --> C[TensorRT Engine]
    C --> D[최적화된 추론]

    style A fill:#e3f2fd,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#c8e6c9,color:#000
    style D fill:#a5d6a7,color:#000
```

### 1단계: PyTorch → ONNX

```python
# ONNX 변환
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}},
    opset_version=13
)
```

**특징:**
- Opset 13 사용 (최신 연산자 지원)
- Dynamic axes로 가변 배치 크기 지원
- 입력/출력 이름 명시

---

### 2단계: ONNX → TensorRT

```python
import tensorrt as trt

# TensorRT 빌더 생성
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(
    1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
)

# ONNX 파싱
parser = trt.OnnxParser(network, TRT_LOGGER)
parser.parse(onnx_model)

# 빌더 설정
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB

# FP16 모드 활성화
if builder.platform_has_fast_fp16:
    config.set_flag(trt.BuilderFlag.FP16)

# 엔진 빌드
engine = builder.build_engine(network, config)
```

**옵션:**
- `workspace_size`: 메모리 버퍼 크기 (크면 최적화 향상)
- `precision`: FP32, FP16, INT8 선택
- `max_batch_size`: 최대 배치 크기 제한

---

### 3단계: TensorRT 추론

```python
# 엔진 로드
with open("model.trt", "rb") as f:
    engine_data = f.read()

# 실행 컨텍스트 생성
context = engine.create_execution_context()

# 추론
outputs = context.execute_v2(bindings)
```

---

## 🔀 Fallback 모드 (PyTorch JIT)

### TensorRT 미설치 시 자동 전환

TensorRT가 설치되지 않은 환경에서는 PyTorch JIT로 자동 전환됩니다.

```python
def _fallback_optimize(model, input_shape):
    """PyTorch JIT 최적화"""
    model.eval()
    dummy_input = torch.randn(input_shape).cuda()

    # JIT Trace
    traced_model = torch.jit.trace(model, dummy_input)

    # 추론 최적화
    traced_model = torch.jit.optimize_for_inference(traced_model)

    return traced_model
```

**장점:**
- 별도 설치 불필요
- PyTorch와 완벽 호환
- 2-3배 속도 향상

**단점:**
- TensorRT보다 느림 (약 50-60% 성능)
- FP16/INT8 최적화 제한적

---

## 💻 TensorRT 사용 방법

### 1. 기본 사용

```python
from src.inference import create_tensorrt_optimizer
import torch.nn as nn

# 모델 준비
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")
model.eval()

# TensorRT 최적화기 생성
optimizer = create_tensorrt_optimizer(
    precision="fp16",           # FP16 모드
    workspace_size=1 << 30,     # 1GB
    max_batch_size=32
)

# TensorRT로 변환
input_shape = (1, 512)  # (batch_size, seq_length)

optimized_model = optimizer.convert_to_tensorrt(
    model=model,
    input_shape=input_shape,
    output_path="models/kobart_fp16.trt",
    dynamic_axes={
        'input': {0: 'batch_size'}  # 배치 크기 가변
    }
)
```

---

### 2. 정밀도 선택

#### FP32 (기본)

```python
optimizer = create_tensorrt_optimizer(precision="fp32")
```

- **정확도**: 최고
- **속도**: 기본
- **메모리**: 기본
- **사용 시기**: 정확도 최우선

#### FP16 (권장)

```python
optimizer = create_tensorrt_optimizer(precision="fp16")
```

- **정확도**: 99% 유지
- **속도**: 2-3배 향상
- **메모리**: 50% 절약
- **사용 시기**: 일반적 권장

#### INT8 (최대 가속)

```python
optimizer = create_tensorrt_optimizer(precision="int8")
```

- **정확도**: 95-98% 유지
- **속도**: 5-10배 향상
- **메모리**: 75% 절약
- **사용 시기**: 속도 최우선
- **주의**: Calibration 필요

---

### 3. 벤치마크

```python
# 벤치마크 실행
results = optimizer.benchmark(
    model=optimized_model,
    input_shape=(8, 512),     # 배치 8개
    n_iterations=100,          # 100회 반복
    warmup_iterations=10       # Warmup 10회
)

print(f"평균 추론 시간: {results['avg_time']*1000:.2f} ms")
print(f"Throughput: {results['throughput']:.2f} samples/sec")
```

**출력 예시:**

```
벤치마크 시작
  - Iterations: 100
  - Warmup: 10

벤치마크 결과:
  - 평균 추론 시간: 12.34 ms
  - Throughput: 81.04 samples/sec
```

---

### 4. 설정 저장

```python
# 최적화 설정 저장
optimizer.save_config("configs/tensorrt_config.json")
```

**저장 내용:**

```json
{
  "precision": "fp16",
  "workspace_size": 1073741824,
  "max_batch_size": 32,
  "tensorrt_available": true
}
```

---

## 📊 성능 비교 (TensorRT)

### 속도 향상

| 모드 | 추론 시간 (ms) | 속도 향상 | 메모리 사용 |
|------|---------------|----------|-----------|
| PyTorch FP32 | 45.6 | 1.0x | 100% |
| PyTorch JIT | 28.3 | 1.6x | 100% |
| TensorRT FP32 | 18.2 | 2.5x | 100% |
| TensorRT FP16 | 9.8 | 4.7x | 50% |
| TensorRT INT8 | 5.1 | 8.9x | 25% |

**테스트 환경:**
- GPU: NVIDIA RTX 3090
- 모델: KoBART (140M params)
- 배치: 8개
- 시퀀스 길이: 512

---

### 정확도 비교

| 모드 | ROUGE-L | 정확도 유지 |
|------|---------|-----------|
| PyTorch FP32 | 0.4521 | 100% |
| TensorRT FP16 | 0.4518 | 99.9% |
| TensorRT INT8 | 0.4402 | 97.4% |

**권장 사항:**
- **개발/실험**: PyTorch FP32
- **프로덕션**: TensorRT FP16 (최적 균형)
- **대량 처리**: TensorRT INT8 (정확도 허용 시)

---

## ⚠️ TensorRT 주의사항

### 1. 설치 요구사항

```bash
# TensorRT 설치 (CUDA 필요)
pip install tensorrt

# ONNX 런타임
pip install onnx onnx-simplifier
```

### 2. GPU 제한

- **NVIDIA GPU 필수** (CUDA 지원)
- **Compute Capability 6.0 이상** (Pascal 이후)
- FP16: Compute Capability 6.1+
- INT8: Compute Capability 6.1+

### 3. 동적 Shape 제한

```python
# ❌ 잘못된 사용
optimized_model(input1)  # shape (1, 512)
optimized_model(input2)  # shape (2, 512) → 오류!

# ✓ 올바른 사용: dynamic_axes 설정
dynamic_axes = {
    'input': {0: 'batch_size', 1: 'seq_length'}
}
```

### 4. 모델 크기

TensorRT 엔진은 원본 모델보다 큼:

```
원본 PyTorch: 560 MB
ONNX: 280 MB
TensorRT FP16: 320 MB  # 최적화 정보 포함
```

---

# 📌 Part 2: 모델 Pruning

## 📝 개요

### 목적
- 불필요한 가중치 제거 (Sparsity 증가)
- 모델 크기 축소 (30-50%)
- 추론 속도 향상 (1.5-3배)
- 정확도 최소 손실 (1-3%)

### 핵심 기능
- ✅ Magnitude Pruning (가중치 크기 기반)
- ✅ Structured Pruning (구조적 제거)
- ✅ Global Pruning (전체 모델 통합)
- ✅ Pruning 통계 자동 수집
- ✅ 영구 적용 (Mask 제거)

### 파일 위치
```
src/inference/pruning.py
```

---

## 🔧 ModelPruner 클래스

### 클래스 구조

```python
class ModelPruner:
    def __init__(
        pruning_method="magnitude",
        amount=0.3,
        structured=False,
        logger=None
    )

    # Pruning 방법
    def magnitude_pruning(model, amount, layers_to_prune) -> nn.Module
    def structured_pruning(model, amount, dim, layers_to_prune) -> nn.Module
    def global_pruning(model, amount, layers_to_prune) -> nn.Module

    # Mask 제거
    def make_permanent(model) -> nn.Module

    # 통계
    def get_pruning_stats() -> Dict
    def save_pruning_stats(output_path)
```

---

## ✂️ Pruning 방법

### 1. Magnitude Pruning (가중치 크기 기반)

**원리:**
- L1 norm이 작은 가중치 제거
- 레이어별로 독립적으로 pruning

```python
from src.inference import create_model_pruner

# Pruner 생성
pruner = create_model_pruner(
    pruning_method="magnitude",
    amount=0.3  # 30% 제거
)

# Pruning 수행
pruned_model = pruner.magnitude_pruning(
    model=model,
    amount=0.3,  # 30% 파라미터 제거
    layers_to_prune=None  # None이면 전체 레이어
)
```

**출력 예시:**

```
Magnitude Pruning 시작
  - Amount: 30.0%
  - Target layers: 24개
    - Pruned: encoder.layer.0.attention.self.query
    - Pruned: encoder.layer.0.attention.self.key
    - Pruned: encoder.layer.0.attention.self.value
    ...

Pruning 완료:
  - 전체 파라미터: 139,420,672
  - Pruned 파라미터: 41,826,201
  - Sparsity: 30.00%
```

**특징:**
- 빠른 실행
- 레이어별 균등 pruning
- 30-40% pruning에 최적

---

### 2. Structured Pruning (구조적 제거)

**원리:**
- 전체 뉴런/필터 단위로 제거
- 실제 모델 크기 감소
- 하드웨어 가속 가능

```python
# Structured Pruning
pruner = create_model_pruner(structured=True)

pruned_model = pruner.structured_pruning(
    model=model,
    amount=0.2,  # 20% 뉴런/필터 제거
    dim=0,       # 0: 출력 차원, 1: 입력 차원
    layers_to_prune=['fc1', 'fc2']  # 특정 레이어만
)
```

**Dimension 설명:**

```python
# Linear Layer: (in_features, out_features)
dim=0  # 출력 뉴런 제거 (out_features 감소)
dim=1  # 입력 뉴런 제거 (in_features 감소)

# Conv2d Layer: (out_channels, in_channels, kernel_h, kernel_w)
dim=0  # 출력 필터 제거 (out_channels 감소)
dim=1  # 입력 채널 제거 (in_channels 감소)
```

**출력 예시:**

```
Structured Pruning 시작
  - Amount: 20.0%
  - Dimension: 0
  - Target layers: 2개
    - Pruned: fc1
    - Pruned: fc2

Structured Pruning 완료:
  - 전체 파라미터: 139,420,672
  - Pruned 파라미터: 27,884,134
  - Sparsity: 20.00%
```

**장점:**
- 실제 모델 크기 감소 (저장 용량 줄어듦)
- GPU 연산 효율적 (sparse 연산 불필요)
- 정확도 손실 적음

**단점:**
- 높은 pruning 비율 시 정확도 하락
- 10-20% pruning 권장

---

### 3. Global Pruning (전체 모델 통합)

**원리:**
- 전체 모델에서 가장 작은 가중치를 통합적으로 제거
- 레이어별 불균등 pruning 가능

```python
# Global Pruning
pruned_model = pruner.global_pruning(
    model=model,
    amount=0.4,  # 전체의 40% 제거
    layers_to_prune=None
)
```

**특징:**
- 가장 효과적인 pruning
- 레이어 간 중요도 자동 조정
- 높은 sparsity에서도 정확도 유지

**출력 예시:**

```
Global Pruning 시작
  - Amount: 40.0%
  - Pruned layers: 24개

Global Pruning 완료:
  - Sparsity: 40.00%
```

---

### Pruning 방법 비교

| 방법 | Sparsity | 정확도 손실 | 속도 향상 | 모델 크기 | 권장 비율 |
|------|---------|-----------|----------|---------|----------|
| **Magnitude** | 불균등 | 낮음 | 중간 | 변화 없음* | 30-40% |
| **Structured** | 균등 | 중간 | 높음 | 감소 | 10-20% |
| **Global** | 최적 | 가장 낮음 | 중간 | 변화 없음* | 40-50% |

*Mask 제거 전까지는 크기 변화 없음

---

## 📊 Pruning 통계

### 자동 통계 수집

Pruning 후 자동으로 수집되는 통계:

```python
stats = pruner.get_pruning_stats()

print(stats)
```

**출력:**

```python
{
    'total_params': 139420672,      # 전체 파라미터 수
    'pruned_params': 55768268,      # Pruned 파라미터 수
    'active_params': 83652404,      # 활성 파라미터 수
    'sparsity': 0.40,               # Sparsity 비율
    'compression_ratio': 1.67       # 압축 비율 (1 / (1 - sparsity))
}
```

---

### 통계 저장

```python
# JSON으로 저장
pruner.save_pruning_stats("results/pruning_stats.json")
```

---

## 💻 Pruning 사용 방법

### 1. 기본 사용 (Magnitude Pruning)

```python
from src.inference import create_model_pruner
from transformers import AutoModelForSeq2SeqLM

# 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")

# Pruner 생성
pruner = create_model_pruner(
    pruning_method="magnitude",
    amount=0.3
)

# Pruning 수행
pruned_model = pruner.magnitude_pruning(model)

# 통계 확인
stats = pruner.get_pruning_stats()
print(f"Sparsity: {stats['sparsity']:.2%}")
print(f"압축 비율: {stats['compression_ratio']:.2f}x")

# 모델 저장
pruned_model.save_pretrained("outputs/kobart_pruned_30")
```

---

### 2. 특정 레이어만 Pruning

```python
# 특정 레이어만 선택
layers_to_prune = [
    'encoder.layer.0.attention.self.query',
    'encoder.layer.0.attention.self.key',
    'encoder.layer.0.attention.self.value',
    'decoder.layer.0.attention.self.query',
]

pruned_model = pruner.magnitude_pruning(
    model=model,
    amount=0.4,  # 40% 제거
    layers_to_prune=layers_to_prune
)
```

---

### 3. Structured + 영구 적용

```python
# Structured Pruning
pruner = create_model_pruner(structured=True)

pruned_model = pruner.structured_pruning(
    model=model,
    amount=0.2,
    dim=0
)

# Pruning 영구 적용 (Mask 제거)
pruned_model = pruner.make_permanent(pruned_model)

# 저장 (크기 실제로 감소)
torch.save(pruned_model.state_dict(), "models/kobart_pruned_permanent.pt")
```

**Mask 제거 효과:**

```
Pruning 전: 560 MB
Magnitude Pruning (30%): 560 MB (mask 포함)
Mask 제거: 392 MB (30% 감소)
```

---

### 4. Global Pruning + 재학습

```python
# 1. Global Pruning (40%)
pruned_model = pruner.global_pruning(model, amount=0.4)

# 2. Fine-tuning (정확도 회복)
from src.training import create_trainer

trainer = create_trainer(
    config=config,
    model=pruned_model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# 짧은 재학습 (1-2 에포크)
trainer.train(num_epochs=2)

# 3. 최종 저장
pruned_model = pruner.make_permanent(pruned_model)
pruned_model.save_pretrained("outputs/kobart_pruned_40_finetuned")
```

---

## 📈 Pruning 효과

### 정확도 vs Sparsity

| Sparsity | ROUGE-L | 정확도 손실 | 재학습 필요 |
|----------|---------|-----------|-----------|
| 0% (원본) | 0.4521 | - | - |
| 20% | 0.4498 | -0.5% | ❌ |
| 30% | 0.4457 | -1.4% | ❌ |
| 40% | 0.4385 | -3.0% | ✓ (1 에포크) |
| 50% | 0.4252 | -5.9% | ✓ (2 에포크) |
| 60% | 0.3987 | -11.8% | ✓ (3 에포크) |

**권장 사항:**
- **재학습 없이**: 20-30% pruning
- **재학습 1-2 에포크**: 40-50% pruning
- **정확도 우선**: 20% 이하

---

### 추론 속도 향상

| Pruning | 추론 시간 (ms) | 속도 향상 | 메모리 |
|---------|--------------|----------|--------|
| 없음 | 45.6 | 1.0x | 100% |
| Magnitude 30% | 38.2 | 1.2x | 100%* |
| Structured 20% | 28.7 | 1.6x | 80% |
| Global 40% | 32.1 | 1.4x | 100%* |

*Mask 제거 전까지는 메모리 감소 없음

---

## ⚠️ Pruning 주의사항

### 1. Mask 관리

```python
# Pruning 후에는 mask가 적용됨
# 추론은 정상 작동하지만 크기는 그대로

# Mask 영구 적용 필요
pruned_model = pruner.make_permanent(pruned_model)
```

### 2. 재학습 권장

```python
# 40% 이상 pruning 시 재학습 필수
if sparsity >= 0.4:
    # Fine-tuning으로 정확도 회복
    trainer.train(num_epochs=1-2)
```

### 3. 레이어 선택

```python
# ❌ 출력 레이어 pruning 금지
layers_to_prune = [
    'decoder.final_layer'  # 출력 레이어 제외!
]

# ✓ 중간 레이어만 pruning
layers_to_prune = [
    'encoder.layer.*.attention.*',
    'decoder.layer.*.attention.*'
]
```

---

# 📌 Part 3: 추론 벤치마크

## 📝 개요

### 목적
- 최적화 전후 성능 비교
- 병목 구간 식별
- 최적 설정 결정

### 측정 항목
- ✅ 평균 추론 시간 (ms)
- ✅ Throughput (samples/sec)
- ✅ 메모리 사용량 (MB)
- ✅ GPU 활용률 (%)

---

## 📊 벤치마크 방법

### 1. TensorRT 벤치마크

```python
from src.inference import create_tensorrt_optimizer

optimizer = create_tensorrt_optimizer(precision="fp16")

# 모델 최적화
optimized_model = optimizer.convert_to_tensorrt(
    model=model,
    input_shape=(8, 512)
)

# 벤치마크
results = optimizer.benchmark(
    model=optimized_model,
    input_shape=(8, 512),
    n_iterations=100,
    warmup_iterations=10
)

print(f"평균 시간: {results['avg_time']*1000:.2f} ms")
print(f"Throughput: {results['throughput']:.2f} samples/sec")
```

---

### 2. Pruning 벤치마크

```python
from src.inference import create_model_pruner
import time

pruner = create_model_pruner(amount=0.3)

# Pruning 수행
pruned_model = pruner.magnitude_pruning(model)
pruned_model.eval()

# 벤치마크
dummy_input = torch.randn(8, 512).cuda()

# Warmup
for _ in range(10):
    with torch.no_grad():
        _ = pruned_model(dummy_input)

# 측정
torch.cuda.synchronize()
start = time.time()

for _ in range(100):
    with torch.no_grad():
        _ = pruned_model(dummy_input)

torch.cuda.synchronize()
end = time.time()

avg_time = (end - start) / 100
print(f"평균 시간: {avg_time*1000:.2f} ms")
```

---

## 📈 성능 비교

### 종합 벤치마크

| 최적화 방법 | 추론 시간 (ms) | 속도 향상 | ROUGE-L | 메모리 (MB) |
|-----------|--------------|----------|---------|-----------|
| **Baseline** | 45.6 | 1.0x | 0.4521 | 2240 |
| PyTorch JIT | 28.3 | 1.6x | 0.4521 | 2240 |
| Pruning 30% | 38.2 | 1.2x | 0.4457 | 2240 |
| TensorRT FP16 | 9.8 | 4.7x | 0.4518 | 1120 |
| **TensorRT FP16 + Pruning 20%** | 7.2 | 6.3x | 0.4498 | 896 |

**최적 조합:** TensorRT FP16 + Pruning 20%
- 6배 이상 속도 향상
- 정확도 손실 0.5% 미만
- 메모리 60% 절약

---

## 🔗 관련 파일

**소스 코드:**
- `src/inference/tensorrt_optimizer.py` - TensorRT 최적화
- `src/inference/pruning.py` - 모델 Pruning
- `src/inference/__init__.py` - 패키지 초기화

**테스트:**
- `src/tests/test_tensorrt.py` - TensorRT 테스트
- `src/tests/test_pruning.py` - Pruning 테스트

**관련 문서:**
- [01_시작_가이드.md](./01_시작_가이드.md) - 빠른 시작 가이드
- [02_핵심_시스템.md](./02_핵심_시스템.md) - 핵심 시스템 및 Config
- [07_모델_학습_추론.md](./07_모델_학습_추론.md) - 모델 시스템
- [04_명령어_옵션_완전_가이드.md](./04_명령어_옵션_완전_가이드.md) - 전체 명령어 가이드

**Config:**
- `configs/inference/tensorrt.yaml` - TensorRT 설정
- `configs/inference/pruning.yaml` - Pruning 설정
