# 09. ì¶”ë¡  ìµœì í™” ê°€ì´ë“œ

> **í†µí•© ë¬¸ì„œ:** TensorRT ê°€ì† + ëª¨ë¸ Pruning + ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬

## ğŸ“‹ ëª©ì°¨

### Part 1: TensorRT ìµœì í™”
- [ê°œìš”](#part-1-tensorrt-ìµœì í™”)
- [TensorRTOptimizer í´ë˜ìŠ¤](#tensorrtoptimizer-í´ë˜ìŠ¤)
- [ë³€í™˜ í”„ë¡œì„¸ìŠ¤](#ë³€í™˜-í”„ë¡œì„¸ìŠ¤)
- [Fallback ëª¨ë“œ](#fallback-ëª¨ë“œ-pytorch-jit)
- [ì‚¬ìš© ë°©ë²•](#tensorrt-ì‚¬ìš©-ë°©ë²•)

### Part 2: ëª¨ë¸ Pruning
- [ê°œìš”](#part-2-ëª¨ë¸-pruning)
- [ModelPruner í´ë˜ìŠ¤](#modelpruner-í´ë˜ìŠ¤)
- [Pruning ë°©ë²•](#pruning-ë°©ë²•)
- [í†µê³„ ë° ë¶„ì„](#pruning-í†µê³„)
- [ì‚¬ìš© ë°©ë²•](#pruning-ì‚¬ìš©-ë°©ë²•)

### Part 3: ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬
- [ê°œìš”](#part-3-ì¶”ë¡ -ë²¤ì¹˜ë§ˆí¬)
- [ë²¤ì¹˜ë§ˆí¬ ë°©ë²•](#ë²¤ì¹˜ë§ˆí¬-ë°©ë²•)
- [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)

---

# ğŸ“Œ Part 1: TensorRT ìµœì í™”

## ğŸ“ ê°œìš”

### ëª©ì 
- GPU ì¶”ë¡  ê°€ì† (ìµœëŒ€ 5-10ë°° ì†ë„ í–¥ìƒ)
- FP16/INT8 ì •ë°€ë„ ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- PyTorch â†’ ONNX â†’ TensorRT ë³€í™˜
- TensorRT ë¯¸ì„¤ì¹˜ ì‹œ PyTorch JITë¡œ Fallback

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ìë™ TensorRT ë³€í™˜ (ONNX ê²½ìœ )
- âœ… FP16/INT8 ì •ë°€ë„ ì§€ì›
- âœ… Fallback ëª¨ë“œ (PyTorch JIT)
- âœ… ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
- âœ… ì„¤ì • ì €ì¥/ë¡œë“œ

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/tensorrt_optimizer.py
```

---

## ğŸ”§ TensorRTOptimizer í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class TensorRTOptimizer:
    def __init__(
        precision="fp16",
        workspace_size=1<<30,  # 1GB
        max_batch_size=32,
        logger=None
    )

    # ë³€í™˜
    def convert_to_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any

    # ë²¤ì¹˜ë§ˆí¬
    def benchmark(model, input_shape, n_iterations, warmup_iterations) -> Dict

    # ì„¤ì • ì €ì¥
    def save_config(output_path)

    # ë‚´ë¶€ ë©”ì„œë“œ
    def _convert_with_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any
    def _fallback_optimize(model, input_shape) -> torch.jit.ScriptModule
    def _check_tensorrt_availability() -> bool
```

---

## ğŸ”„ ë³€í™˜ í”„ë¡œì„¸ìŠ¤

### 3ë‹¨ê³„ ë³€í™˜ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[PyTorch Model] --> B[ONNX]
    B --> C[TensorRT Engine]
    C --> D[ìµœì í™”ëœ ì¶”ë¡ ]

    style A fill:#e3f2fd,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#c8e6c9,color:#000
    style D fill:#a5d6a7,color:#000
```

### 1ë‹¨ê³„: PyTorch â†’ ONNX

```python
# ONNX ë³€í™˜
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}},
    opset_version=13
)
```

**íŠ¹ì§•:**
- Opset 13 ì‚¬ìš© (ìµœì‹  ì—°ì‚°ì ì§€ì›)
- Dynamic axesë¡œ ê°€ë³€ ë°°ì¹˜ í¬ê¸° ì§€ì›
- ì…ë ¥/ì¶œë ¥ ì´ë¦„ ëª…ì‹œ

---

### 2ë‹¨ê³„: ONNX â†’ TensorRT

```python
import tensorrt as trt

# TensorRT ë¹Œë” ìƒì„±
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(
    1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
)

# ONNX íŒŒì‹±
parser = trt.OnnxParser(network, TRT_LOGGER)
parser.parse(onnx_model)

# ë¹Œë” ì„¤ì •
config = builder.create_builder_config()
config.max_workspace_size = 1 << 30  # 1GB

# FP16 ëª¨ë“œ í™œì„±í™”
if builder.platform_has_fast_fp16:
    config.set_flag(trt.BuilderFlag.FP16)

# ì—”ì§„ ë¹Œë“œ
engine = builder.build_engine(network, config)
```

**ì˜µì…˜:**
- `workspace_size`: ë©”ëª¨ë¦¬ ë²„í¼ í¬ê¸° (í¬ë©´ ìµœì í™” í–¥ìƒ)
- `precision`: FP32, FP16, INT8 ì„ íƒ
- `max_batch_size`: ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì œí•œ

---

### 3ë‹¨ê³„: TensorRT ì¶”ë¡ 

```python
# ì—”ì§„ ë¡œë“œ
with open("model.trt", "rb") as f:
    engine_data = f.read()

# ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
context = engine.create_execution_context()

# ì¶”ë¡ 
outputs = context.execute_v2(bindings)
```

---

## ğŸ”€ Fallback ëª¨ë“œ (PyTorch JIT)

### TensorRT ë¯¸ì„¤ì¹˜ ì‹œ ìë™ ì „í™˜

TensorRTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ í™˜ê²½ì—ì„œëŠ” PyTorch JITë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤.

```python
def _fallback_optimize(model, input_shape):
    """PyTorch JIT ìµœì í™”"""
    model.eval()
    dummy_input = torch.randn(input_shape).cuda()

    # JIT Trace
    traced_model = torch.jit.trace(model, dummy_input)

    # ì¶”ë¡  ìµœì í™”
    traced_model = torch.jit.optimize_for_inference(traced_model)

    return traced_model
```

**ì¥ì :**
- ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”
- PyTorchì™€ ì™„ë²½ í˜¸í™˜
- 2-3ë°° ì†ë„ í–¥ìƒ

**ë‹¨ì :**
- TensorRTë³´ë‹¤ ëŠë¦¼ (ì•½ 50-60% ì„±ëŠ¥)
- FP16/INT8 ìµœì í™” ì œí•œì 

---

## ğŸ’» TensorRT ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©

```python
from src.inference import create_tensorrt_optimizer
import torch.nn as nn

# ëª¨ë¸ ì¤€ë¹„
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")
model.eval()

# TensorRT ìµœì í™”ê¸° ìƒì„±
optimizer = create_tensorrt_optimizer(
    precision="fp16",           # FP16 ëª¨ë“œ
    workspace_size=1 << 30,     # 1GB
    max_batch_size=32
)

# TensorRTë¡œ ë³€í™˜
input_shape = (1, 512)  # (batch_size, seq_length)

optimized_model = optimizer.convert_to_tensorrt(
    model=model,
    input_shape=input_shape,
    output_path="models/kobart_fp16.trt",
    dynamic_axes={
        'input': {0: 'batch_size'}  # ë°°ì¹˜ í¬ê¸° ê°€ë³€
    }
)
```

---

### 2. ì •ë°€ë„ ì„ íƒ

#### FP32 (ê¸°ë³¸)

```python
optimizer = create_tensorrt_optimizer(precision="fp32")
```

- **ì •í™•ë„**: ìµœê³ 
- **ì†ë„**: ê¸°ë³¸
- **ë©”ëª¨ë¦¬**: ê¸°ë³¸
- **ì‚¬ìš© ì‹œê¸°**: ì •í™•ë„ ìµœìš°ì„ 

#### FP16 (ê¶Œì¥)

```python
optimizer = create_tensorrt_optimizer(precision="fp16")
```

- **ì •í™•ë„**: 99% ìœ ì§€
- **ì†ë„**: 2-3ë°° í–¥ìƒ
- **ë©”ëª¨ë¦¬**: 50% ì ˆì•½
- **ì‚¬ìš© ì‹œê¸°**: ì¼ë°˜ì  ê¶Œì¥

#### INT8 (ìµœëŒ€ ê°€ì†)

```python
optimizer = create_tensorrt_optimizer(precision="int8")
```

- **ì •í™•ë„**: 95-98% ìœ ì§€
- **ì†ë„**: 5-10ë°° í–¥ìƒ
- **ë©”ëª¨ë¦¬**: 75% ì ˆì•½
- **ì‚¬ìš© ì‹œê¸°**: ì†ë„ ìµœìš°ì„ 
- **ì£¼ì˜**: Calibration í•„ìš”

---

### 3. ë²¤ì¹˜ë§ˆí¬

```python
# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
results = optimizer.benchmark(
    model=optimized_model,
    input_shape=(8, 512),     # ë°°ì¹˜ 8ê°œ
    n_iterations=100,          # 100íšŒ ë°˜ë³µ
    warmup_iterations=10       # Warmup 10íšŒ
)

print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {results['avg_time']*1000:.2f} ms")
print(f"Throughput: {results['throughput']:.2f} samples/sec")
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
ë²¤ì¹˜ë§ˆí¬ ì‹œì‘
  - Iterations: 100
  - Warmup: 10

ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:
  - í‰ê·  ì¶”ë¡  ì‹œê°„: 12.34 ms
  - Throughput: 81.04 samples/sec
```

---

### 4. ì„¤ì • ì €ì¥

```python
# ìµœì í™” ì„¤ì • ì €ì¥
optimizer.save_config("configs/tensorrt_config.json")
```

**ì €ì¥ ë‚´ìš©:**

```json
{
  "precision": "fp16",
  "workspace_size": 1073741824,
  "max_batch_size": 32,
  "tensorrt_available": true
}
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ (TensorRT)

### ì†ë„ í–¥ìƒ

| ëª¨ë“œ | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ ì‚¬ìš© |
|------|---------------|----------|-----------|
| PyTorch FP32 | 45.6 | 1.0x | 100% |
| PyTorch JIT | 28.3 | 1.6x | 100% |
| TensorRT FP32 | 18.2 | 2.5x | 100% |
| TensorRT FP16 | 9.8 | 4.7x | 50% |
| TensorRT INT8 | 5.1 | 8.9x | 25% |

**í…ŒìŠ¤íŠ¸ í™˜ê²½:**
- GPU: NVIDIA RTX 3090
- ëª¨ë¸: KoBART (140M params)
- ë°°ì¹˜: 8ê°œ
- ì‹œí€€ìŠ¤ ê¸¸ì´: 512

---

### ì •í™•ë„ ë¹„êµ

| ëª¨ë“œ | ROUGE-L | ì •í™•ë„ ìœ ì§€ |
|------|---------|-----------|
| PyTorch FP32 | 0.4521 | 100% |
| TensorRT FP16 | 0.4518 | 99.9% |
| TensorRT INT8 | 0.4402 | 97.4% |

**ê¶Œì¥ ì‚¬í•­:**
- **ê°œë°œ/ì‹¤í—˜**: PyTorch FP32
- **í”„ë¡œë•ì…˜**: TensorRT FP16 (ìµœì  ê· í˜•)
- **ëŒ€ëŸ‰ ì²˜ë¦¬**: TensorRT INT8 (ì •í™•ë„ í—ˆìš© ì‹œ)

---

## âš ï¸ TensorRT ì£¼ì˜ì‚¬í•­

### 1. ì„¤ì¹˜ ìš”êµ¬ì‚¬í•­

```bash
# TensorRT ì„¤ì¹˜ (CUDA í•„ìš”)
pip install tensorrt

# ONNX ëŸ°íƒ€ì„
pip install onnx onnx-simplifier
```

### 2. GPU ì œí•œ

- **NVIDIA GPU í•„ìˆ˜** (CUDA ì§€ì›)
- **Compute Capability 6.0 ì´ìƒ** (Pascal ì´í›„)
- FP16: Compute Capability 6.1+
- INT8: Compute Capability 6.1+

### 3. ë™ì  Shape ì œí•œ

```python
# âŒ ì˜ëª»ëœ ì‚¬ìš©
optimized_model(input1)  # shape (1, 512)
optimized_model(input2)  # shape (2, 512) â†’ ì˜¤ë¥˜!

# âœ“ ì˜¬ë°”ë¥¸ ì‚¬ìš©: dynamic_axes ì„¤ì •
dynamic_axes = {
    'input': {0: 'batch_size', 1: 'seq_length'}
}
```

### 4. ëª¨ë¸ í¬ê¸°

TensorRT ì—”ì§„ì€ ì›ë³¸ ëª¨ë¸ë³´ë‹¤ í¼:

```
ì›ë³¸ PyTorch: 560 MB
ONNX: 280 MB
TensorRT FP16: 320 MB  # ìµœì í™” ì •ë³´ í¬í•¨
```

---

# ğŸ“Œ Part 2: ëª¨ë¸ Pruning

## ğŸ“ ê°œìš”

### ëª©ì 
- ë¶ˆí•„ìš”í•œ ê°€ì¤‘ì¹˜ ì œê±° (Sparsity ì¦ê°€)
- ëª¨ë¸ í¬ê¸° ì¶•ì†Œ (30-50%)
- ì¶”ë¡  ì†ë„ í–¥ìƒ (1.5-3ë°°)
- ì •í™•ë„ ìµœì†Œ ì†ì‹¤ (1-3%)

### í•µì‹¬ ê¸°ëŠ¥
- âœ… Magnitude Pruning (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)
- âœ… Structured Pruning (êµ¬ì¡°ì  ì œê±°)
- âœ… Global Pruning (ì „ì²´ ëª¨ë¸ í†µí•©)
- âœ… Pruning í†µê³„ ìë™ ìˆ˜ì§‘
- âœ… ì˜êµ¬ ì ìš© (Mask ì œê±°)

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/pruning.py
```

---

## ğŸ”§ ModelPruner í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class ModelPruner:
    def __init__(
        pruning_method="magnitude",
        amount=0.3,
        structured=False,
        logger=None
    )

    # Pruning ë°©ë²•
    def magnitude_pruning(model, amount, layers_to_prune) -> nn.Module
    def structured_pruning(model, amount, dim, layers_to_prune) -> nn.Module
    def global_pruning(model, amount, layers_to_prune) -> nn.Module

    # Mask ì œê±°
    def make_permanent(model) -> nn.Module

    # í†µê³„
    def get_pruning_stats() -> Dict
    def save_pruning_stats(output_path)
```

---

## âœ‚ï¸ Pruning ë°©ë²•

### 1. Magnitude Pruning (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)

**ì›ë¦¬:**
- L1 normì´ ì‘ì€ ê°€ì¤‘ì¹˜ ì œê±°
- ë ˆì´ì–´ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ pruning

```python
from src.inference import create_model_pruner

# Pruner ìƒì„±
pruner = create_model_pruner(
    pruning_method="magnitude",
    amount=0.3  # 30% ì œê±°
)

# Pruning ìˆ˜í–‰
pruned_model = pruner.magnitude_pruning(
    model=model,
    amount=0.3,  # 30% íŒŒë¼ë¯¸í„° ì œê±°
    layers_to_prune=None  # Noneì´ë©´ ì „ì²´ ë ˆì´ì–´
)
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Magnitude Pruning ì‹œì‘
  - Amount: 30.0%
  - Target layers: 24ê°œ
    - Pruned: encoder.layer.0.attention.self.query
    - Pruned: encoder.layer.0.attention.self.key
    - Pruned: encoder.layer.0.attention.self.value
    ...

Pruning ì™„ë£Œ:
  - ì „ì²´ íŒŒë¼ë¯¸í„°: 139,420,672
  - Pruned íŒŒë¼ë¯¸í„°: 41,826,201
  - Sparsity: 30.00%
```

**íŠ¹ì§•:**
- ë¹ ë¥¸ ì‹¤í–‰
- ë ˆì´ì–´ë³„ ê· ë“± pruning
- 30-40% pruningì— ìµœì 

---

### 2. Structured Pruning (êµ¬ì¡°ì  ì œê±°)

**ì›ë¦¬:**
- ì „ì²´ ë‰´ëŸ°/í•„í„° ë‹¨ìœ„ë¡œ ì œê±°
- ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê°ì†Œ
- í•˜ë“œì›¨ì–´ ê°€ì† ê°€ëŠ¥

```python
# Structured Pruning
pruner = create_model_pruner(structured=True)

pruned_model = pruner.structured_pruning(
    model=model,
    amount=0.2,  # 20% ë‰´ëŸ°/í•„í„° ì œê±°
    dim=0,       # 0: ì¶œë ¥ ì°¨ì›, 1: ì…ë ¥ ì°¨ì›
    layers_to_prune=['fc1', 'fc2']  # íŠ¹ì • ë ˆì´ì–´ë§Œ
)
```

**Dimension ì„¤ëª…:**

```python
# Linear Layer: (in_features, out_features)
dim=0  # ì¶œë ¥ ë‰´ëŸ° ì œê±° (out_features ê°ì†Œ)
dim=1  # ì…ë ¥ ë‰´ëŸ° ì œê±° (in_features ê°ì†Œ)

# Conv2d Layer: (out_channels, in_channels, kernel_h, kernel_w)
dim=0  # ì¶œë ¥ í•„í„° ì œê±° (out_channels ê°ì†Œ)
dim=1  # ì…ë ¥ ì±„ë„ ì œê±° (in_channels ê°ì†Œ)
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Structured Pruning ì‹œì‘
  - Amount: 20.0%
  - Dimension: 0
  - Target layers: 2ê°œ
    - Pruned: fc1
    - Pruned: fc2

Structured Pruning ì™„ë£Œ:
  - ì „ì²´ íŒŒë¼ë¯¸í„°: 139,420,672
  - Pruned íŒŒë¼ë¯¸í„°: 27,884,134
  - Sparsity: 20.00%
```

**ì¥ì :**
- ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê°ì†Œ (ì €ì¥ ìš©ëŸ‰ ì¤„ì–´ë“¦)
- GPU ì—°ì‚° íš¨ìœ¨ì  (sparse ì—°ì‚° ë¶ˆí•„ìš”)
- ì •í™•ë„ ì†ì‹¤ ì ìŒ

**ë‹¨ì :**
- ë†’ì€ pruning ë¹„ìœ¨ ì‹œ ì •í™•ë„ í•˜ë½
- 10-20% pruning ê¶Œì¥

---

### 3. Global Pruning (ì „ì²´ ëª¨ë¸ í†µí•©)

**ì›ë¦¬:**
- ì „ì²´ ëª¨ë¸ì—ì„œ ê°€ì¥ ì‘ì€ ê°€ì¤‘ì¹˜ë¥¼ í†µí•©ì ìœ¼ë¡œ ì œê±°
- ë ˆì´ì–´ë³„ ë¶ˆê· ë“± pruning ê°€ëŠ¥

```python
# Global Pruning
pruned_model = pruner.global_pruning(
    model=model,
    amount=0.4,  # ì „ì²´ì˜ 40% ì œê±°
    layers_to_prune=None
)
```

**íŠ¹ì§•:**
- ê°€ì¥ íš¨ê³¼ì ì¸ pruning
- ë ˆì´ì–´ ê°„ ì¤‘ìš”ë„ ìë™ ì¡°ì •
- ë†’ì€ sparsityì—ì„œë„ ì •í™•ë„ ìœ ì§€

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Global Pruning ì‹œì‘
  - Amount: 40.0%
  - Pruned layers: 24ê°œ

Global Pruning ì™„ë£Œ:
  - Sparsity: 40.00%
```

---

### Pruning ë°©ë²• ë¹„êµ

| ë°©ë²• | Sparsity | ì •í™•ë„ ì†ì‹¤ | ì†ë„ í–¥ìƒ | ëª¨ë¸ í¬ê¸° | ê¶Œì¥ ë¹„ìœ¨ |
|------|---------|-----------|----------|---------|----------|
| **Magnitude** | ë¶ˆê· ë“± | ë‚®ìŒ | ì¤‘ê°„ | ë³€í™” ì—†ìŒ* | 30-40% |
| **Structured** | ê· ë“± | ì¤‘ê°„ | ë†’ìŒ | ê°ì†Œ | 10-20% |
| **Global** | ìµœì  | ê°€ì¥ ë‚®ìŒ | ì¤‘ê°„ | ë³€í™” ì—†ìŒ* | 40-50% |

*Mask ì œê±° ì „ê¹Œì§€ëŠ” í¬ê¸° ë³€í™” ì—†ìŒ

---

## ğŸ“Š Pruning í†µê³„

### ìë™ í†µê³„ ìˆ˜ì§‘

Pruning í›„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë˜ëŠ” í†µê³„:

```python
stats = pruner.get_pruning_stats()

print(stats)
```

**ì¶œë ¥:**

```python
{
    'total_params': 139420672,      # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
    'pruned_params': 55768268,      # Pruned íŒŒë¼ë¯¸í„° ìˆ˜
    'active_params': 83652404,      # í™œì„± íŒŒë¼ë¯¸í„° ìˆ˜
    'sparsity': 0.40,               # Sparsity ë¹„ìœ¨
    'compression_ratio': 1.67       # ì••ì¶• ë¹„ìœ¨ (1 / (1 - sparsity))
}
```

---

### í†µê³„ ì €ì¥

```python
# JSONìœ¼ë¡œ ì €ì¥
pruner.save_pruning_stats("results/pruning_stats.json")
```

---

## ğŸ’» Pruning ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (Magnitude Pruning)

```python
from src.inference import create_model_pruner
from transformers import AutoModelForSeq2SeqLM

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")

# Pruner ìƒì„±
pruner = create_model_pruner(
    pruning_method="magnitude",
    amount=0.3
)

# Pruning ìˆ˜í–‰
pruned_model = pruner.magnitude_pruning(model)

# í†µê³„ í™•ì¸
stats = pruner.get_pruning_stats()
print(f"Sparsity: {stats['sparsity']:.2%}")
print(f"ì••ì¶• ë¹„ìœ¨: {stats['compression_ratio']:.2f}x")

# ëª¨ë¸ ì €ì¥
pruned_model.save_pretrained("outputs/kobart_pruned_30")
```

---

### 2. íŠ¹ì • ë ˆì´ì–´ë§Œ Pruning

```python
# íŠ¹ì • ë ˆì´ì–´ë§Œ ì„ íƒ
layers_to_prune = [
    'encoder.layer.0.attention.self.query',
    'encoder.layer.0.attention.self.key',
    'encoder.layer.0.attention.self.value',
    'decoder.layer.0.attention.self.query',
]

pruned_model = pruner.magnitude_pruning(
    model=model,
    amount=0.4,  # 40% ì œê±°
    layers_to_prune=layers_to_prune
)
```

---

### 3. Structured + ì˜êµ¬ ì ìš©

```python
# Structured Pruning
pruner = create_model_pruner(structured=True)

pruned_model = pruner.structured_pruning(
    model=model,
    amount=0.2,
    dim=0
)

# Pruning ì˜êµ¬ ì ìš© (Mask ì œê±°)
pruned_model = pruner.make_permanent(pruned_model)

# ì €ì¥ (í¬ê¸° ì‹¤ì œë¡œ ê°ì†Œ)
torch.save(pruned_model.state_dict(), "models/kobart_pruned_permanent.pt")
```

**Mask ì œê±° íš¨ê³¼:**

```
Pruning ì „: 560 MB
Magnitude Pruning (30%): 560 MB (mask í¬í•¨)
Mask ì œê±°: 392 MB (30% ê°ì†Œ)
```

---

### 4. Global Pruning + ì¬í•™ìŠµ

```python
# 1. Global Pruning (40%)
pruned_model = pruner.global_pruning(model, amount=0.4)

# 2. Fine-tuning (ì •í™•ë„ íšŒë³µ)
from src.training import create_trainer

trainer = create_trainer(
    config=config,
    model=pruned_model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# ì§§ì€ ì¬í•™ìŠµ (1-2 ì—í¬í¬)
trainer.train(num_epochs=2)

# 3. ìµœì¢… ì €ì¥
pruned_model = pruner.make_permanent(pruned_model)
pruned_model.save_pretrained("outputs/kobart_pruned_40_finetuned")
```

---

## ğŸ“ˆ Pruning íš¨ê³¼

### ì •í™•ë„ vs Sparsity

| Sparsity | ROUGE-L | ì •í™•ë„ ì†ì‹¤ | ì¬í•™ìŠµ í•„ìš” |
|----------|---------|-----------|-----------|
| 0% (ì›ë³¸) | 0.4521 | - | - |
| 20% | 0.4498 | -0.5% | âŒ |
| 30% | 0.4457 | -1.4% | âŒ |
| 40% | 0.4385 | -3.0% | âœ“ (1 ì—í¬í¬) |
| 50% | 0.4252 | -5.9% | âœ“ (2 ì—í¬í¬) |
| 60% | 0.3987 | -11.8% | âœ“ (3 ì—í¬í¬) |

**ê¶Œì¥ ì‚¬í•­:**
- **ì¬í•™ìŠµ ì—†ì´**: 20-30% pruning
- **ì¬í•™ìŠµ 1-2 ì—í¬í¬**: 40-50% pruning
- **ì •í™•ë„ ìš°ì„ **: 20% ì´í•˜

---

### ì¶”ë¡  ì†ë„ í–¥ìƒ

| Pruning | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ |
|---------|--------------|----------|--------|
| ì—†ìŒ | 45.6 | 1.0x | 100% |
| Magnitude 30% | 38.2 | 1.2x | 100%* |
| Structured 20% | 28.7 | 1.6x | 80% |
| Global 40% | 32.1 | 1.4x | 100%* |

*Mask ì œê±° ì „ê¹Œì§€ëŠ” ë©”ëª¨ë¦¬ ê°ì†Œ ì—†ìŒ

---

## âš ï¸ Pruning ì£¼ì˜ì‚¬í•­

### 1. Mask ê´€ë¦¬

```python
# Pruning í›„ì—ëŠ” maskê°€ ì ìš©ë¨
# ì¶”ë¡ ì€ ì •ìƒ ì‘ë™í•˜ì§€ë§Œ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œ

# Mask ì˜êµ¬ ì ìš© í•„ìš”
pruned_model = pruner.make_permanent(pruned_model)
```

### 2. ì¬í•™ìŠµ ê¶Œì¥

```python
# 40% ì´ìƒ pruning ì‹œ ì¬í•™ìŠµ í•„ìˆ˜
if sparsity >= 0.4:
    # Fine-tuningìœ¼ë¡œ ì •í™•ë„ íšŒë³µ
    trainer.train(num_epochs=1-2)
```

### 3. ë ˆì´ì–´ ì„ íƒ

```python
# âŒ ì¶œë ¥ ë ˆì´ì–´ pruning ê¸ˆì§€
layers_to_prune = [
    'decoder.final_layer'  # ì¶œë ¥ ë ˆì´ì–´ ì œì™¸!
]

# âœ“ ì¤‘ê°„ ë ˆì´ì–´ë§Œ pruning
layers_to_prune = [
    'encoder.layer.*.attention.*',
    'decoder.layer.*.attention.*'
]
```

---

# ğŸ“Œ Part 3: ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬

## ğŸ“ ê°œìš”

### ëª©ì 
- ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ
- ë³‘ëª© êµ¬ê°„ ì‹ë³„
- ìµœì  ì„¤ì • ê²°ì •

### ì¸¡ì • í•­ëª©
- âœ… í‰ê·  ì¶”ë¡  ì‹œê°„ (ms)
- âœ… Throughput (samples/sec)
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
- âœ… GPU í™œìš©ë¥  (%)

---

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë°©ë²•

### 1. TensorRT ë²¤ì¹˜ë§ˆí¬

```python
from src.inference import create_tensorrt_optimizer

optimizer = create_tensorrt_optimizer(precision="fp16")

# ëª¨ë¸ ìµœì í™”
optimized_model = optimizer.convert_to_tensorrt(
    model=model,
    input_shape=(8, 512)
)

# ë²¤ì¹˜ë§ˆí¬
results = optimizer.benchmark(
    model=optimized_model,
    input_shape=(8, 512),
    n_iterations=100,
    warmup_iterations=10
)

print(f"í‰ê·  ì‹œê°„: {results['avg_time']*1000:.2f} ms")
print(f"Throughput: {results['throughput']:.2f} samples/sec")
```

---

### 2. Pruning ë²¤ì¹˜ë§ˆí¬

```python
from src.inference import create_model_pruner
import time

pruner = create_model_pruner(amount=0.3)

# Pruning ìˆ˜í–‰
pruned_model = pruner.magnitude_pruning(model)
pruned_model.eval()

# ë²¤ì¹˜ë§ˆí¬
dummy_input = torch.randn(8, 512).cuda()

# Warmup
for _ in range(10):
    with torch.no_grad():
        _ = pruned_model(dummy_input)

# ì¸¡ì •
torch.cuda.synchronize()
start = time.time()

for _ in range(100):
    with torch.no_grad():
        _ = pruned_model(dummy_input)

torch.cuda.synchronize()
end = time.time()

avg_time = (end - start) / 100
print(f"í‰ê·  ì‹œê°„: {avg_time*1000:.2f} ms")
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

### ì¢…í•© ë²¤ì¹˜ë§ˆí¬

| ìµœì í™” ë°©ë²• | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ROUGE-L | ë©”ëª¨ë¦¬ (MB) |
|-----------|--------------|----------|---------|-----------|
| **Baseline** | 45.6 | 1.0x | 0.4521 | 2240 |
| PyTorch JIT | 28.3 | 1.6x | 0.4521 | 2240 |
| Pruning 30% | 38.2 | 1.2x | 0.4457 | 2240 |
| TensorRT FP16 | 9.8 | 4.7x | 0.4518 | 1120 |
| **TensorRT FP16 + Pruning 20%** | 7.2 | 6.3x | 0.4498 | 896 |

**ìµœì  ì¡°í•©:** TensorRT FP16 + Pruning 20%
- 6ë°° ì´ìƒ ì†ë„ í–¥ìƒ
- ì •í™•ë„ ì†ì‹¤ 0.5% ë¯¸ë§Œ
- ë©”ëª¨ë¦¬ 60% ì ˆì•½

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/inference/tensorrt_optimizer.py` - TensorRT ìµœì í™”
- `src/inference/pruning.py` - ëª¨ë¸ Pruning
- `src/inference/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_tensorrt.py` - TensorRT í…ŒìŠ¤íŠ¸
- `src/tests/test_pruning.py` - Pruning í…ŒìŠ¤íŠ¸

**ê´€ë ¨ ë¬¸ì„œ:**
- [01_ì‹œì‘_ê°€ì´ë“œ.md](./01_ì‹œì‘_ê°€ì´ë“œ.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) - í•µì‹¬ ì‹œìŠ¤í…œ ë° Config
- [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) - ëª¨ë¸ ì‹œìŠ¤í…œ
- [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ì „ì²´ ëª…ë ¹ì–´ ê°€ì´ë“œ

**Config:**
- `configs/inference/tensorrt.yaml` - TensorRT ì„¤ì •
- `configs/inference/pruning.yaml` - Pruning ì„¤ì •
