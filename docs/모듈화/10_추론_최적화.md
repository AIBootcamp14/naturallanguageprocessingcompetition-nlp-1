# 09. ì¶”ë¡  ìµœì í™” ê°€ì´ë“œ

> **í†µí•© ë¬¸ì„œ:** TensorRT ê°€ì† + ëª¨ë¸ Pruning + ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬

## ğŸ“‹ ëª©ì°¨

### Part 1: TensorRT ìµœì í™”
- [ê°œìš”](#part-1-tensorrt-ìµœì í™”)
- [TensorRTOptimizer í´ë˜ìŠ¤](#tensorrtoptimizer-í´ë˜ìŠ¤)
- [ë³€í™˜ í”„ë¡œì„¸ìŠ¤](#ë³€í™˜-í”„ë¡œì„¸ìŠ¤)
- [Fallback ëª¨ë“œ](#fallback-ëª¨ë“œ-pytorch-jit)
- [ì‚¬ìš© ë°©ë²•](#tensorrt-ì‚¬ìš©-ë°©ë²•)

### Part 2: ëª¨ë¸ Pruning
- [ê°œìš”](#part-2-ëª¨ë¸-pruning)
- [ModelPruner í´ë˜ìŠ¤](#modelpruner-í´ë˜ìŠ¤)
- [Pruning ë°©ë²•](#pruning-ë°©ë²•)
- [í†µê³„ ë° ë¶„ì„](#pruning-í†µê³„)
- [ì‚¬ìš© ë°©ë²•](#pruning-ì‚¬ìš©-ë°©ë²•)

### Part 3: ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬
- [ê°œìš”](#part-3-ì¶”ë¡ -ë²¤ì¹˜ë§ˆí¬)
- [ë²¤ì¹˜ë§ˆí¬ ë°©ë²•](#ë²¤ì¹˜ë§ˆí¬-ë°©ë²•)
- [ì„±ëŠ¥ ë¹„êµ](#ì„±ëŠ¥-ë¹„êµ)

---

# ğŸ“Œ Part 1: TensorRT ìµœì í™”

## ğŸ“ ê°œìš”

### ëª©ì 
- GPU ì¶”ë¡  ê°€ì† (ìµœëŒ€ 5-10ë°° ì†ë„ í–¥ìƒ)
- FP16/INT8 ì •ë°€ë„ ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- PyTorch â†’ ONNX â†’ TensorRT ë³€í™˜
- TensorRT ë¯¸ì„¤ì¹˜ ì‹œ PyTorch JITë¡œ Fallback

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ìë™ TensorRT ë³€í™˜ (ONNX ê²½ìœ )
- âœ… FP16/INT8 ì •ë°€ë„ ì§€ì›
- âœ… Fallback ëª¨ë“œ (PyTorch JIT)
- âœ… ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
- âœ… ì„¤ì • ì €ì¥/ë¡œë“œ

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/tensorrt_optimizer.py
```

---

## ğŸ”§ TensorRTOptimizer í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== TensorRT ìµœì í™” í´ë˜ìŠ¤ ==================== #
class TensorRTOptimizer:
    # ---------------------- ìƒì„±ì - ìµœì í™” ì„¤ì • ì´ˆê¸°í™” ---------------------- #
    def __init__(
        precision="fp16",                   # ì •ë°€ë„ ëª¨ë“œ (fp32/fp16/int8)
        workspace_size=1<<30,               # ì‘ì—… ê³µê°„ í¬ê¸° (1GB)
        max_batch_size=32,                  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
        logger=None                         # ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
    )

    # ---------------------- PyTorch ëª¨ë¸ì„ TensorRTë¡œ ë³€í™˜ ---------------------- #
    def convert_to_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any

    # ---------------------- ëª¨ë¸ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ---------------------- #
    def benchmark(model, input_shape, n_iterations, warmup_iterations) -> Dict

    # ---------------------- ìµœì í™” ì„¤ì •ì„ íŒŒì¼ë¡œ ì €ì¥ ---------------------- #
    def save_config(output_path)

    # ---------------------- ë‚´ë¶€ ë©”ì„œë“œ ---------------------- #
    def _convert_with_tensorrt(model, input_shape, output_path, dynamic_axes) -> Any   # TensorRT ë³€í™˜ ì‹¤í–‰
    def _fallback_optimize(model, input_shape) -> torch.jit.ScriptModule              # PyTorch JIT í´ë°±
    def _check_tensorrt_availability() -> bool                                        # TensorRT ì„¤ì¹˜ í™•ì¸
```

---

## ğŸ”„ ë³€í™˜ í”„ë¡œì„¸ìŠ¤

### 3ë‹¨ê³„ ë³€í™˜ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[PyTorch Model] --> B[ONNX]
    B --> C[TensorRT Engine]
    C --> D[ìµœì í™”ëœ ì¶”ë¡ ]

    style A fill:#e3f2fd,color:#000
    style B fill:#fff3e0,color:#000
    style C fill:#c8e6c9,color:#000
    style D fill:#a5d6a7,color:#000
```

### 1ë‹¨ê³„: PyTorch â†’ ONNX

```python
# ---------------------- PyTorch ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ---------------------- #
torch.onnx.export(
    model,                                          # ë³€í™˜í•  PyTorch ëª¨ë¸
    dummy_input,                                    # ë”ë¯¸ ì…ë ¥ í…ì„œ
    "model.onnx",                                   # ì €ì¥í•  ONNX íŒŒì¼ ê²½ë¡œ
    input_names=['input'],                          # ì…ë ¥ ë…¸ë“œ ì´ë¦„
    output_names=['output'],                        # ì¶œë ¥ ë…¸ë“œ ì´ë¦„
    dynamic_axes={'input': {0: 'batch_size'}},      # ë™ì  ì¶• ì„¤ì • (ë°°ì¹˜ í¬ê¸° ê°€ë³€)
    opset_version=13                                # ONNX opset ë²„ì „ (ìµœì‹  ì—°ì‚°ì ì§€ì›)
)
```

**íŠ¹ì§•:**
- Opset 13 ì‚¬ìš© (ìµœì‹  ì—°ì‚°ì ì§€ì›)
- Dynamic axesë¡œ ê°€ë³€ ë°°ì¹˜ í¬ê¸° ì§€ì›
- ì…ë ¥/ì¶œë ¥ ì´ë¦„ ëª…ì‹œ

---

### 2ë‹¨ê³„: ONNX â†’ TensorRT

```python
# ---------------------- TensorRT ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import tensorrt as trt

# ---------------------- TensorRT ë¹Œë” ìƒì„± ---------------------- #
builder = trt.Builder(TRT_LOGGER)                                           # TensorRT ë¹Œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
network = builder.create_network(                                           # ë„¤íŠ¸ì›Œí¬ ì •ì˜ ìƒì„±
    1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)              # Explicit Batch ëª¨ë“œ ì„¤ì •
)

# ---------------------- ONNX ëª¨ë¸ íŒŒì‹± ---------------------- #
parser = trt.OnnxParser(network, TRT_LOGGER)                                # ONNX íŒŒì„œ ìƒì„±
parser.parse(onnx_model)                                                    # ONNX ëª¨ë¸ íŒŒì‹± ë° ë„¤íŠ¸ì›Œí¬ êµ¬ì„±

# ---------------------- ë¹Œë” ì„¤ì • ---------------------- #
config = builder.create_builder_config()                                    # ë¹Œë” ì„¤ì • ê°ì²´ ìƒì„±
config.max_workspace_size = 1 << 30                                         # ìµœëŒ€ ì‘ì—… ê³µê°„ í¬ê¸° ì„¤ì • (1GB)

# ---------------------- FP16 ì •ë°€ë„ ëª¨ë“œ í™œì„±í™” ---------------------- #
if builder.platform_has_fast_fp16:                                          # FP16 ì§€ì› í™•ì¸
    config.set_flag(trt.BuilderFlag.FP16)                                   # FP16 ëª¨ë“œ í™œì„±í™”

# ---------------------- TensorRT ì—”ì§„ ë¹Œë“œ ---------------------- #
engine = builder.build_engine(network, config)                              # ìµœì í™”ëœ ì—”ì§„ ë¹Œë“œ
```

**ì˜µì…˜:**
- `workspace_size`: ë©”ëª¨ë¦¬ ë²„í¼ í¬ê¸° (í¬ë©´ ìµœì í™” í–¥ìƒ)
- `precision`: FP32, FP16, INT8 ì„ íƒ
- `max_batch_size`: ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ì œí•œ

---

### 3ë‹¨ê³„: TensorRT ì¶”ë¡ 

```python
# ---------------------- TensorRT ì—”ì§„ ë¡œë“œ ---------------------- #
with open("model.trt", "rb") as f:                  # TensorRT ì—”ì§„ íŒŒì¼ ì—´ê¸°
    engine_data = f.read()                          # ì—”ì§„ ë°ì´í„° ì½ê¸°

# ---------------------- ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ---------------------- #
context = engine.create_execution_context()         # ì¶”ë¡  ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±

# ---------------------- ì¶”ë¡  ì‹¤í–‰ ---------------------- #
outputs = context.execute_v2(bindings)              # ë°”ì¸ë”©ëœ ì…ë ¥/ì¶œë ¥ìœ¼ë¡œ ì¶”ë¡  ì‹¤í–‰
```

---

## ğŸ”€ Fallback ëª¨ë“œ (PyTorch JIT)

### TensorRT ë¯¸ì„¤ì¹˜ ì‹œ ìë™ ì „í™˜

TensorRTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ í™˜ê²½ì—ì„œëŠ” PyTorch JITë¡œ ìë™ ì „í™˜ë©ë‹ˆë‹¤.

```python
# ---------------------- PyTorch JIT ìµœì í™” (Fallback) ---------------------- #
def _fallback_optimize(model, input_shape):
    """PyTorch JIT ìµœì í™”"""
    # -------------- ëª¨ë¸ ì¤€ë¹„ -------------- #
    model.eval()                                                # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    dummy_input = torch.randn(input_shape).cuda()               # ë”ë¯¸ ì…ë ¥ í…ì„œ ìƒì„± (GPU)

    # -------------- JIT Trace ì‹¤í–‰ -------------- #
    traced_model = torch.jit.trace(model, dummy_input)          # ëª¨ë¸ì„ TorchScriptë¡œ ë³€í™˜

    # -------------- ì¶”ë¡  ìµœì í™” ì ìš© -------------- #
    traced_model = torch.jit.optimize_for_inference(traced_model)   # ì¶”ë¡ ì— ìµœì í™”ëœ ê·¸ë˜í”„ë¡œ ë³€í™˜

    return traced_model                                         # ìµœì í™”ëœ ëª¨ë¸ ë°˜í™˜
```

**ì¥ì :**
- ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”
- PyTorchì™€ ì™„ë²½ í˜¸í™˜
- 2-3ë°° ì†ë„ í–¥ìƒ

**ë‹¨ì :**
- TensorRTë³´ë‹¤ ëŠë¦¼ (ì•½ 50-60% ì„±ëŠ¥)
- FP16/INT8 ìµœì í™” ì œí•œì 

---

## ğŸ’» TensorRT ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©

```python
# ---------------------- í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import create_tensorrt_optimizer
import torch.nn as nn

# ---------------------- ëª¨ë¸ ì¤€ë¹„ ---------------------- #
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")  # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model.eval()                                                             # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

# ---------------------- TensorRT ìµœì í™”ê¸° ìƒì„± ---------------------- #
optimizer = create_tensorrt_optimizer(
    precision="fp16",           # FP16 ì •ë°€ë„ ëª¨ë“œ
    workspace_size=1 << 30,     # ì‘ì—… ê³µê°„ í¬ê¸° (1GB)
    max_batch_size=32           # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
)

# ---------------------- TensorRTë¡œ ë³€í™˜ ---------------------- #
input_shape = (1, 512)  # ì…ë ¥ í˜•íƒœ (ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´)

optimized_model = optimizer.convert_to_tensorrt(
    model=model,                                        # ë³€í™˜í•  ëª¨ë¸
    input_shape=input_shape,                            # ì…ë ¥ í˜•íƒœ
    output_path="models/kobart_fp16.trt",               # ì €ì¥ ê²½ë¡œ
    dynamic_axes={
        'input': {0: 'batch_size'}                      # ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
    }
)
```

---

### 2. ì •ë°€ë„ ì„ íƒ

#### FP32 (ê¸°ë³¸)

```python
# ---------------------- FP32 ì •ë°€ë„ ìµœì í™”ê¸° ìƒì„± ---------------------- #
optimizer = create_tensorrt_optimizer(precision="fp32")  # 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„
```

- **ì •í™•ë„**: ìµœê³ 
- **ì†ë„**: ê¸°ë³¸
- **ë©”ëª¨ë¦¬**: ê¸°ë³¸
- **ì‚¬ìš© ì‹œê¸°**: ì •í™•ë„ ìµœìš°ì„ 

#### FP16 (ê¶Œì¥)

```python
# ---------------------- FP16 ì •ë°€ë„ ìµœì í™”ê¸° ìƒì„± ---------------------- #
optimizer = create_tensorrt_optimizer(precision="fp16")  # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ (ê¶Œì¥)
```

- **ì •í™•ë„**: 99% ìœ ì§€
- **ì†ë„**: 2-3ë°° í–¥ìƒ
- **ë©”ëª¨ë¦¬**: 50% ì ˆì•½
- **ì‚¬ìš© ì‹œê¸°**: ì¼ë°˜ì  ê¶Œì¥

#### INT8 (ìµœëŒ€ ê°€ì†)

```python
# ---------------------- INT8 ì •ë°€ë„ ìµœì í™”ê¸° ìƒì„± ---------------------- #
optimizer = create_tensorrt_optimizer(precision="int8")  # 8ë¹„íŠ¸ ì •ìˆ˜ ì •ë°€ë„ (ìµœëŒ€ ê°€ì†)
```

- **ì •í™•ë„**: 95-98% ìœ ì§€
- **ì†ë„**: 5-10ë°° í–¥ìƒ
- **ë©”ëª¨ë¦¬**: 75% ì ˆì•½
- **ì‚¬ìš© ì‹œê¸°**: ì†ë„ ìµœìš°ì„ 
- **ì£¼ì˜**: Calibration í•„ìš”

---

### 3. ë²¤ì¹˜ë§ˆí¬

```python
# ---------------------- ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ---------------------- #
results = optimizer.benchmark(
    model=optimized_model,         # ë²¤ì¹˜ë§ˆí¬í•  ìµœì í™”ëœ ëª¨ë¸
    input_shape=(8, 512),          # ì…ë ¥ í˜•íƒœ (ë°°ì¹˜ 8ê°œ, ì‹œí€€ìŠ¤ 512)
    n_iterations=100,              # ë²¤ì¹˜ë§ˆí¬ ë°˜ë³µ íšŸìˆ˜
    warmup_iterations=10           # ì›Œë°ì—… ë°˜ë³µ íšŸìˆ˜
)

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {results['avg_time']*1000:.2f} ms")      # í‰ê·  ì¶”ë¡  ì‹œê°„ (ë°€ë¦¬ì´ˆ)
print(f"Throughput: {results['throughput']:.2f} samples/sec")    # ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
ë²¤ì¹˜ë§ˆí¬ ì‹œì‘
  - Iterations: 100
  - Warmup: 10

ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:
  - í‰ê·  ì¶”ë¡  ì‹œê°„: 12.34 ms
  - Throughput: 81.04 samples/sec
```

---

### 4. ì„¤ì • ì €ì¥

```python
# ---------------------- ìµœì í™” ì„¤ì • ì €ì¥ ---------------------- #
optimizer.save_config("configs/tensorrt_config.json")  # TensorRT ì„¤ì •ì„ JSON íŒŒì¼ë¡œ ì €ì¥
```

**ì €ì¥ ë‚´ìš©:**

```json
{
  "precision": "fp16",              // ì •ë°€ë„ ëª¨ë“œ
  "workspace_size": 1073741824,     // ì‘ì—… ê³µê°„ í¬ê¸° (ë°”ì´íŠ¸)
  "max_batch_size": 32,             // ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
  "tensorrt_available": true        // TensorRT ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
}
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ (TensorRT)

### ì†ë„ í–¥ìƒ

| ëª¨ë“œ | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ ì‚¬ìš© |
|------|---------------|----------|-----------|
| PyTorch FP32 | 45.6 | 1.0x | 100% |
| PyTorch JIT | 28.3 | 1.6x | 100% |
| TensorRT FP32 | 18.2 | 2.5x | 100% |
| TensorRT FP16 | 9.8 | 4.7x | 50% |
| TensorRT INT8 | 5.1 | 8.9x | 25% |

**í…ŒìŠ¤íŠ¸ í™˜ê²½:**
- GPU: NVIDIA RTX 3090
- ëª¨ë¸: KoBART (140M params)
- ë°°ì¹˜: 8ê°œ
- ì‹œí€€ìŠ¤ ê¸¸ì´: 512

---

### ì •í™•ë„ ë¹„êµ

| ëª¨ë“œ | ROUGE-L | ì •í™•ë„ ìœ ì§€ |
|------|---------|-----------|
| PyTorch FP32 | 0.4521 | 100% |
| TensorRT FP16 | 0.4518 | 99.9% |
| TensorRT INT8 | 0.4402 | 97.4% |

**ê¶Œì¥ ì‚¬í•­:**
- **ê°œë°œ/ì‹¤í—˜**: PyTorch FP32
- **í”„ë¡œë•ì…˜**: TensorRT FP16 (ìµœì  ê· í˜•)
- **ëŒ€ëŸ‰ ì²˜ë¦¬**: TensorRT INT8 (ì •í™•ë„ í—ˆìš© ì‹œ)

---

## âš ï¸ TensorRT ì£¼ì˜ì‚¬í•­

### 1. ì„¤ì¹˜ ìš”êµ¬ì‚¬í•­

```bash
# ---------------------- TensorRT ì„¤ì¹˜ (CUDA í•„ìš”) ---------------------- #
pip install tensorrt

# ---------------------- ONNX ëŸ°íƒ€ì„ ì„¤ì¹˜ ---------------------- #
pip install onnx onnx-simplifier
```

### 2. GPU ì œí•œ

- **NVIDIA GPU í•„ìˆ˜** (CUDA ì§€ì›)
- **Compute Capability 6.0 ì´ìƒ** (Pascal ì´í›„)
- FP16: Compute Capability 6.1+
- INT8: Compute Capability 6.1+

### 3. ë™ì  Shape ì œí•œ

```python
# ---------------------- ì˜ëª»ëœ ì‚¬ìš© ì˜ˆì‹œ ---------------------- #
optimized_model(input1)  # shape (1, 512)
optimized_model(input2)  # shape (2, 512) â†’ ì˜¤ë¥˜! (ë™ì  ì¶• ë¯¸ì„¤ì • ì‹œ)

# ---------------------- ì˜¬ë°”ë¥¸ ì‚¬ìš©: dynamic_axes ì„¤ì • ---------------------- #
dynamic_axes = {
    'input': {0: 'batch_size', 1: 'seq_length'}  # ë°°ì¹˜ í¬ê¸°ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
}
```

### 4. ëª¨ë¸ í¬ê¸°

TensorRT ì—”ì§„ì€ ì›ë³¸ ëª¨ë¸ë³´ë‹¤ í¼:

```
ì›ë³¸ PyTorch: 560 MB
ONNX: 280 MB
TensorRT FP16: 320 MB  # ìµœì í™” ì •ë³´ í¬í•¨
```

---

# ğŸ“Œ Part 2: ëª¨ë¸ Pruning

## ğŸ“ ê°œìš”

### ëª©ì 
- ë¶ˆí•„ìš”í•œ ê°€ì¤‘ì¹˜ ì œê±° (Sparsity ì¦ê°€)
- ëª¨ë¸ í¬ê¸° ì¶•ì†Œ (30-50%)
- ì¶”ë¡  ì†ë„ í–¥ìƒ (1.5-3ë°°)
- ì •í™•ë„ ìµœì†Œ ì†ì‹¤ (1-3%)

### í•µì‹¬ ê¸°ëŠ¥
- âœ… Magnitude Pruning (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)
- âœ… Structured Pruning (êµ¬ì¡°ì  ì œê±°)
- âœ… Global Pruning (ì „ì²´ ëª¨ë¸ í†µí•©)
- âœ… Pruning í†µê³„ ìë™ ìˆ˜ì§‘
- âœ… ì˜êµ¬ ì ìš© (Mask ì œê±°)

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/pruning.py
```

---

## ğŸ”§ ModelPruner í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== ëª¨ë¸ Pruning í´ë˜ìŠ¤ ==================== #
class ModelPruner:
    # ---------------------- ìƒì„±ì - Pruning ì„¤ì • ì´ˆê¸°í™” ---------------------- #
    def __init__(
        pruning_method="magnitude",         # Pruning ë°©ë²• (magnitude/structured/global)
        amount=0.3,                         # Pruning ë¹„ìœ¨ (0.0 ~ 1.0)
        structured=False,                   # êµ¬ì¡°ì  Pruning ì—¬ë¶€
        logger=None                         # ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
    )

    # ---------------------- Pruning ë°©ë²•ë“¤ ---------------------- #
    def magnitude_pruning(model, amount, layers_to_prune) -> nn.Module        # ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜ Pruning
    def structured_pruning(model, amount, dim, layers_to_prune) -> nn.Module  # êµ¬ì¡°ì  Pruning (ë‰´ëŸ°/í•„í„° ë‹¨ìœ„)
    def global_pruning(model, amount, layers_to_prune) -> nn.Module           # ì „ì—­ Pruning (ì „ì²´ ëª¨ë¸ í†µí•©)

    # ---------------------- Mask ì˜êµ¬ ì ìš© ---------------------- #
    def make_permanent(model) -> nn.Module                                    # Pruning maskë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì ìš©

    # ---------------------- í†µê³„ ìˆ˜ì§‘ ë° ì €ì¥ ---------------------- #
    def get_pruning_stats() -> Dict                                           # Pruning í†µê³„ ë°˜í™˜
    def save_pruning_stats(output_path)                                       # í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥
```

---

## âœ‚ï¸ Pruning ë°©ë²•

### 1. Magnitude Pruning (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)

**ì›ë¦¬:**
- L1 normì´ ì‘ì€ ê°€ì¤‘ì¹˜ ì œê±°
- ë ˆì´ì–´ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ pruning

```python
# ---------------------- í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import create_model_pruner

# ---------------------- Pruner ìƒì„± ---------------------- #
pruner = create_model_pruner(
    pruning_method="magnitude",         # Magnitude Pruning ë°©ë²• ì„ íƒ
    amount=0.3                          # 30% ê°€ì¤‘ì¹˜ ì œê±°
)

# ---------------------- Magnitude Pruning ìˆ˜í–‰ ---------------------- #
pruned_model = pruner.magnitude_pruning(
    model=model,                        # Pruningí•  ëª¨ë¸
    amount=0.3,                         # 30% íŒŒë¼ë¯¸í„° ì œê±°
    layers_to_prune=None                # Noneì´ë©´ ì „ì²´ ë ˆì´ì–´ ëŒ€ìƒ
)
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Magnitude Pruning ì‹œì‘
  - Amount: 30.0%
  - Target layers: 24ê°œ
    - Pruned: encoder.layer.0.attention.self.query
    - Pruned: encoder.layer.0.attention.self.key
    - Pruned: encoder.layer.0.attention.self.value
    ...

Pruning ì™„ë£Œ:
  - ì „ì²´ íŒŒë¼ë¯¸í„°: 139,420,672
  - Pruned íŒŒë¼ë¯¸í„°: 41,826,201
  - Sparsity: 30.00%
```

**íŠ¹ì§•:**
- ë¹ ë¥¸ ì‹¤í–‰
- ë ˆì´ì–´ë³„ ê· ë“± pruning
- 30-40% pruningì— ìµœì 

---

### 2. Structured Pruning (êµ¬ì¡°ì  ì œê±°)

**ì›ë¦¬:**
- ì „ì²´ ë‰´ëŸ°/í•„í„° ë‹¨ìœ„ë¡œ ì œê±°
- ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê°ì†Œ
- í•˜ë“œì›¨ì–´ ê°€ì† ê°€ëŠ¥

```python
# ---------------------- Structured Pruner ìƒì„± ---------------------- #
pruner = create_model_pruner(structured=True)               # êµ¬ì¡°ì  Pruning í™œì„±í™”

# ---------------------- Structured Pruning ìˆ˜í–‰ ---------------------- #
pruned_model = pruner.structured_pruning(
    model=model,                                            # Pruningí•  ëª¨ë¸
    amount=0.2,                                             # 20% ë‰´ëŸ°/í•„í„° ì œê±°
    dim=0,                                                  # 0: ì¶œë ¥ ì°¨ì›, 1: ì…ë ¥ ì°¨ì›
    layers_to_prune=['fc1', 'fc2']                          # íŠ¹ì • ë ˆì´ì–´ë§Œ ì„ íƒ
)
```

**Dimension ì„¤ëª…:**

```python
# ---------------------- Linear Layer ì°¨ì› ì„¤ì • ---------------------- #
# Linear Layer í˜•íƒœ: (in_features, out_features)
dim=0  # ì¶œë ¥ ë‰´ëŸ° ì œê±° (out_features ê°ì†Œ)
dim=1  # ì…ë ¥ ë‰´ëŸ° ì œê±° (in_features ê°ì†Œ)

# ---------------------- Conv2d Layer ì°¨ì› ì„¤ì • ---------------------- #
# Conv2d Layer í˜•íƒœ: (out_channels, in_channels, kernel_h, kernel_w)
dim=0  # ì¶œë ¥ í•„í„° ì œê±° (out_channels ê°ì†Œ)
dim=1  # ì…ë ¥ ì±„ë„ ì œê±° (in_channels ê°ì†Œ)
```

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Structured Pruning ì‹œì‘
  - Amount: 20.0%
  - Dimension: 0
  - Target layers: 2ê°œ
    - Pruned: fc1
    - Pruned: fc2

Structured Pruning ì™„ë£Œ:
  - ì „ì²´ íŒŒë¼ë¯¸í„°: 139,420,672
  - Pruned íŒŒë¼ë¯¸í„°: 27,884,134
  - Sparsity: 20.00%
```

**ì¥ì :**
- ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê°ì†Œ (ì €ì¥ ìš©ëŸ‰ ì¤„ì–´ë“¦)
- GPU ì—°ì‚° íš¨ìœ¨ì  (sparse ì—°ì‚° ë¶ˆí•„ìš”)
- ì •í™•ë„ ì†ì‹¤ ì ìŒ

**ë‹¨ì :**
- ë†’ì€ pruning ë¹„ìœ¨ ì‹œ ì •í™•ë„ í•˜ë½
- 10-20% pruning ê¶Œì¥

---

### 3. Global Pruning (ì „ì²´ ëª¨ë¸ í†µí•©)

**ì›ë¦¬:**
- ì „ì²´ ëª¨ë¸ì—ì„œ ê°€ì¥ ì‘ì€ ê°€ì¤‘ì¹˜ë¥¼ í†µí•©ì ìœ¼ë¡œ ì œê±°
- ë ˆì´ì–´ë³„ ë¶ˆê· ë“± pruning ê°€ëŠ¥

```python
# ---------------------- Global Pruning ìˆ˜í–‰ ---------------------- #
pruned_model = pruner.global_pruning(
    model=model,                        # Pruningí•  ëª¨ë¸
    amount=0.4,                         # ì „ì²´ ëª¨ë¸ì˜ 40% ê°€ì¤‘ì¹˜ ì œê±°
    layers_to_prune=None                # Noneì´ë©´ ì „ì²´ ë ˆì´ì–´ ëŒ€ìƒ
)
```

**íŠ¹ì§•:**
- ê°€ì¥ íš¨ê³¼ì ì¸ pruning
- ë ˆì´ì–´ ê°„ ì¤‘ìš”ë„ ìë™ ì¡°ì •
- ë†’ì€ sparsityì—ì„œë„ ì •í™•ë„ ìœ ì§€

**ì¶œë ¥ ì˜ˆì‹œ:**

```
Global Pruning ì‹œì‘
  - Amount: 40.0%
  - Pruned layers: 24ê°œ

Global Pruning ì™„ë£Œ:
  - Sparsity: 40.00%
```

---

### Pruning ë°©ë²• ë¹„êµ

| ë°©ë²• | Sparsity | ì •í™•ë„ ì†ì‹¤ | ì†ë„ í–¥ìƒ | ëª¨ë¸ í¬ê¸° | ê¶Œì¥ ë¹„ìœ¨ |
|------|---------|-----------|----------|---------|----------|
| **Magnitude** | ë¶ˆê· ë“± | ë‚®ìŒ | ì¤‘ê°„ | ë³€í™” ì—†ìŒ* | 30-40% |
| **Structured** | ê· ë“± | ì¤‘ê°„ | ë†’ìŒ | ê°ì†Œ | 10-20% |
| **Global** | ìµœì  | ê°€ì¥ ë‚®ìŒ | ì¤‘ê°„ | ë³€í™” ì—†ìŒ* | 40-50% |

*Mask ì œê±° ì „ê¹Œì§€ëŠ” í¬ê¸° ë³€í™” ì—†ìŒ

---

## ğŸ“Š Pruning í†µê³„

### ìë™ í†µê³„ ìˆ˜ì§‘

Pruning í›„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë˜ëŠ” í†µê³„:

```python
# ---------------------- Pruning í†µê³„ ì¡°íšŒ ---------------------- #
stats = pruner.get_pruning_stats()              # í†µê³„ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

print(stats)                                    # í†µê³„ ì¶œë ¥
```

**ì¶œë ¥:**

```python
{
    'total_params': 139420672,                  # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
    'pruned_params': 55768268,                  # Pruned íŒŒë¼ë¯¸í„° ìˆ˜
    'active_params': 83652404,                  # í™œì„± íŒŒë¼ë¯¸í„° ìˆ˜
    'sparsity': 0.40,                           # Sparsity ë¹„ìœ¨
    'compression_ratio': 1.67                   # ì••ì¶• ë¹„ìœ¨ (1 / (1 - sparsity))
}
```

---

### í†µê³„ ì €ì¥

```python
# ---------------------- Pruning í†µê³„ë¥¼ JSONìœ¼ë¡œ ì €ì¥ ---------------------- #
pruner.save_pruning_stats("results/pruning_stats.json")     # í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥
```

---

## ğŸ’» Pruning ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (Magnitude Pruning)

```python
# ---------------------- í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import create_model_pruner
from transformers import AutoModelForSeq2SeqLM

# ---------------------- ëª¨ë¸ ë¡œë“œ ---------------------- #
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")  # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ

# ---------------------- Pruner ìƒì„± ---------------------- #
pruner = create_model_pruner(
    pruning_method="magnitude",         # Magnitude Pruning ì„ íƒ
    amount=0.3                          # 30% Pruning
)

# ---------------------- Pruning ìˆ˜í–‰ ---------------------- #
pruned_model = pruner.magnitude_pruning(model)                          # Pruning ì‹¤í–‰

# ---------------------- í†µê³„ í™•ì¸ ---------------------- #
stats = pruner.get_pruning_stats()                                      # í†µê³„ ì¡°íšŒ
print(f"Sparsity: {stats['sparsity']:.2%}")                             # Sparsity ì¶œë ¥
print(f"ì••ì¶• ë¹„ìœ¨: {stats['compression_ratio']:.2f}x")                   # ì••ì¶• ë¹„ìœ¨ ì¶œë ¥

# ---------------------- ëª¨ë¸ ì €ì¥ ---------------------- #
pruned_model.save_pretrained("outputs/kobart_pruned_30")                # Pruned ëª¨ë¸ ì €ì¥
```

---

### 2. íŠ¹ì • ë ˆì´ì–´ë§Œ Pruning

```python
# ---------------------- íŠ¹ì • ë ˆì´ì–´ë§Œ ì„ íƒ ---------------------- #
layers_to_prune = [
    'encoder.layer.0.attention.self.query',         # Encoder Query ë ˆì´ì–´
    'encoder.layer.0.attention.self.key',           # Encoder Key ë ˆì´ì–´
    'encoder.layer.0.attention.self.value',         # Encoder Value ë ˆì´ì–´
    'decoder.layer.0.attention.self.query',         # Decoder Query ë ˆì´ì–´
]

# ---------------------- ì„ íƒí•œ ë ˆì´ì–´ë§Œ Pruning ---------------------- #
pruned_model = pruner.magnitude_pruning(
    model=model,                        # Pruningí•  ëª¨ë¸
    amount=0.4,                         # 40% ì œê±°
    layers_to_prune=layers_to_prune     # ì„ íƒí•œ ë ˆì´ì–´ ë¦¬ìŠ¤íŠ¸
)
```

---

### 3. Structured + ì˜êµ¬ ì ìš©

```python
# ---------------------- Structured Pruner ìƒì„± ---------------------- #
pruner = create_model_pruner(structured=True)                           # êµ¬ì¡°ì  Pruning í™œì„±í™”

# ---------------------- Structured Pruning ìˆ˜í–‰ ---------------------- #
pruned_model = pruner.structured_pruning(
    model=model,                        # Pruningí•  ëª¨ë¸
    amount=0.2,                         # 20% ì œê±°
    dim=0                               # ì¶œë ¥ ì°¨ì› Pruning
)

# ---------------------- Pruning ì˜êµ¬ ì ìš© (Mask ì œê±°) ---------------------- #
pruned_model = pruner.make_permanent(pruned_model)                      # Maskë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì œê±°

# ---------------------- ëª¨ë¸ ì €ì¥ (í¬ê¸° ì‹¤ì œë¡œ ê°ì†Œ) ---------------------- #
torch.save(pruned_model.state_dict(), "models/kobart_pruned_permanent.pt")  # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
```

**Mask ì œê±° íš¨ê³¼:**

```
Pruning ì „: 560 MB
Magnitude Pruning (30%): 560 MB (mask í¬í•¨)
Mask ì œê±°: 392 MB (30% ê°ì†Œ)
```

---

### 4. Global Pruning + ì¬í•™ìŠµ

```python
# ---------------------- 1ë‹¨ê³„: Global Pruning (40%) ---------------------- #
pruned_model = pruner.global_pruning(model, amount=0.4)                 # ì „ì—­ Pruning ì‹¤í–‰

# ---------------------- 2ë‹¨ê³„: Fine-tuning (ì •í™•ë„ íšŒë³µ) ---------------------- #
from src.training import create_trainer                                 # íŠ¸ë ˆì´ë„ˆ ì„í¬íŠ¸

trainer = create_trainer(
    config=config,                      # í•™ìŠµ ì„¤ì •
    model=pruned_model,                 # Pruned ëª¨ë¸
    tokenizer=tokenizer,                # í† í¬ë‚˜ì´ì €
    train_dataset=train_dataset,        # í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=eval_dataset           # ê²€ì¦ ë°ì´í„°ì…‹
)

# ì§§ì€ ì¬í•™ìŠµ (1-2 ì—í¬í¬)
trainer.train(num_epochs=2)                                             # 2 ì—í¬í¬ ì¬í•™ìŠµ

# ---------------------- 3ë‹¨ê³„: ìµœì¢… ì €ì¥ ---------------------- #
pruned_model = pruner.make_permanent(pruned_model)                      # Mask ì˜êµ¬ ì ìš©
pruned_model.save_pretrained("outputs/kobart_pruned_40_finetuned")     # ì¬í•™ìŠµëœ ëª¨ë¸ ì €ì¥
```

---

## ğŸ“ˆ Pruning íš¨ê³¼

### ì •í™•ë„ vs Sparsity

| Sparsity | ROUGE-L | ì •í™•ë„ ì†ì‹¤ | ì¬í•™ìŠµ í•„ìš” |
|----------|---------|-----------|-----------|
| 0% (ì›ë³¸) | 0.4521 | - | - |
| 20% | 0.4498 | -0.5% | âŒ |
| 30% | 0.4457 | -1.4% | âŒ |
| 40% | 0.4385 | -3.0% | âœ“ (1 ì—í¬í¬) |
| 50% | 0.4252 | -5.9% | âœ“ (2 ì—í¬í¬) |
| 60% | 0.3987 | -11.8% | âœ“ (3 ì—í¬í¬) |

**ê¶Œì¥ ì‚¬í•­:**
- **ì¬í•™ìŠµ ì—†ì´**: 20-30% pruning
- **ì¬í•™ìŠµ 1-2 ì—í¬í¬**: 40-50% pruning
- **ì •í™•ë„ ìš°ì„ **: 20% ì´í•˜

---

### ì¶”ë¡  ì†ë„ í–¥ìƒ

| Pruning | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ |
|---------|--------------|----------|--------|
| ì—†ìŒ | 45.6 | 1.0x | 100% |
| Magnitude 30% | 38.2 | 1.2x | 100%* |
| Structured 20% | 28.7 | 1.6x | 80% |
| Global 40% | 32.1 | 1.4x | 100%* |

*Mask ì œê±° ì „ê¹Œì§€ëŠ” ë©”ëª¨ë¦¬ ê°ì†Œ ì—†ìŒ

---

## âš ï¸ Pruning ì£¼ì˜ì‚¬í•­

### 1. Mask ê´€ë¦¬

```python
# ---------------------- Mask ê´€ë¦¬ ì£¼ì˜ì‚¬í•­ ---------------------- #
# Pruning í›„ì—ëŠ” maskê°€ ì ìš©ë¨
# ì¶”ë¡ ì€ ì •ìƒ ì‘ë™í•˜ì§€ë§Œ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œ

# ---------------------- Mask ì˜êµ¬ ì ìš© ---------------------- #
pruned_model = pruner.make_permanent(pruned_model)      # Maskë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ í¬ê¸° ê°ì†Œ
```

### 2. ì¬í•™ìŠµ ê¶Œì¥

```python
# ---------------------- ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ í™•ì¸ ---------------------- #
if sparsity >= 0.4:                                     # 40% ì´ìƒ Pruning ì‹œ
    # Fine-tuningìœ¼ë¡œ ì •í™•ë„ íšŒë³µ
    trainer.train(num_epochs=2)                         # 1-2 ì—í¬í¬ ì¬í•™ìŠµ
```

### 3. ë ˆì´ì–´ ì„ íƒ

```python
# ---------------------- ì˜ëª»ëœ ì˜ˆì‹œ: ì¶œë ¥ ë ˆì´ì–´ Pruning ê¸ˆì§€ ---------------------- #
layers_to_prune = [
    'decoder.final_layer'                           # ì¶œë ¥ ë ˆì´ì–´ëŠ” ì œì™¸í•´ì•¼ í•¨!
]

# ---------------------- ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ì¤‘ê°„ ë ˆì´ì–´ë§Œ Pruning ---------------------- #
layers_to_prune = [
    'encoder.layer.*.attention.*',                  # Encoder Attention ë ˆì´ì–´
    'decoder.layer.*.attention.*'                   # Decoder Attention ë ˆì´ì–´
]
```

---

# ğŸ“Œ Part 3: ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬

## ğŸ“ ê°œìš”

### ëª©ì 
- ìµœì í™” ì „í›„ ì„±ëŠ¥ ë¹„êµ
- ë³‘ëª© êµ¬ê°„ ì‹ë³„
- ìµœì  ì„¤ì • ê²°ì •

### ì¸¡ì • í•­ëª©
- âœ… í‰ê·  ì¶”ë¡  ì‹œê°„ (ms)
- âœ… Throughput (samples/sec)
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
- âœ… GPU í™œìš©ë¥  (%)

---

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë°©ë²•

### 1. TensorRT ë²¤ì¹˜ë§ˆí¬

```python
# ---------------------- í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import create_tensorrt_optimizer

# ---------------------- TensorRT ìµœì í™”ê¸° ìƒì„± ---------------------- #
optimizer = create_tensorrt_optimizer(precision="fp16")             # FP16 ëª¨ë“œ ìµœì í™”ê¸°

# ---------------------- ëª¨ë¸ ìµœì í™” ---------------------- #
optimized_model = optimizer.convert_to_tensorrt(
    model=model,                        # ìµœì í™”í•  ëª¨ë¸
    input_shape=(8, 512)                # ì…ë ¥ í˜•íƒœ (ë°°ì¹˜ 8, ì‹œí€€ìŠ¤ 512)
)

# ---------------------- ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ---------------------- #
results = optimizer.benchmark(
    model=optimized_model,              # ë²¤ì¹˜ë§ˆí¬í•  ëª¨ë¸
    input_shape=(8, 512),               # ì…ë ¥ í˜•íƒœ
    n_iterations=100,                   # ë°˜ë³µ íšŸìˆ˜
    warmup_iterations=10                # ì›Œë°ì—… íšŸìˆ˜
)

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(f"í‰ê·  ì‹œê°„: {results['avg_time']*1000:.2f} ms")              # í‰ê·  ì¶”ë¡  ì‹œê°„
print(f"Throughput: {results['throughput']:.2f} samples/sec")      # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰
```

---

### 2. Pruning ë²¤ì¹˜ë§ˆí¬

```python
# ---------------------- í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import create_model_pruner
import time

# ---------------------- Pruner ìƒì„± ë° Pruning ìˆ˜í–‰ ---------------------- #
pruner = create_model_pruner(amount=0.3)                    # 30% Pruning
pruned_model = pruner.magnitude_pruning(model)              # Magnitude Pruning ì‹¤í–‰
pruned_model.eval()                                         # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

# ---------------------- ë”ë¯¸ ì…ë ¥ ìƒì„± ---------------------- #
dummy_input = torch.randn(8, 512).cuda()                    # GPU ë”ë¯¸ ì…ë ¥ ìƒì„±

# ---------------------- Warmup ì‹¤í–‰ ---------------------- #
for _ in range(10):                                         # 10íšŒ ì›Œë°ì—…
    with torch.no_grad():                                   # Gradient ê³„ì‚° ë¹„í™œì„±í™”
        _ = pruned_model(dummy_input)                       # ì¶”ë¡  ì‹¤í–‰

# ---------------------- ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • ì‹œì‘ ---------------------- #
torch.cuda.synchronize()                                    # GPU ë™ê¸°í™”
start = time.time()                                         # ì‹œì‘ ì‹œê°„ ê¸°ë¡

for _ in range(100):                                        # 100íšŒ ë°˜ë³µ
    with torch.no_grad():                                   # Gradient ê³„ì‚° ë¹„í™œì„±í™”
        _ = pruned_model(dummy_input)                       # ì¶”ë¡  ì‹¤í–‰

torch.cuda.synchronize()                                    # GPU ë™ê¸°í™”
end = time.time()                                           # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡

# ---------------------- ê²°ê³¼ ì¶œë ¥ ---------------------- #
avg_time = (end - start) / 100                              # í‰ê·  ì‹œê°„ ê³„ì‚°
print(f"í‰ê·  ì‹œê°„: {avg_time*1000:.2f} ms")                 # í‰ê·  ì‹œê°„ ì¶œë ¥ (ë°€ë¦¬ì´ˆ)
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

### ì¢…í•© ë²¤ì¹˜ë§ˆí¬

| ìµœì í™” ë°©ë²• | ì¶”ë¡  ì‹œê°„ (ms) | ì†ë„ í–¥ìƒ | ROUGE-L | ë©”ëª¨ë¦¬ (MB) |
|-----------|--------------|----------|---------|-----------|
| **Baseline** | 45.6 | 1.0x | 0.4521 | 2240 |
| PyTorch JIT | 28.3 | 1.6x | 0.4521 | 2240 |
| Pruning 30% | 38.2 | 1.2x | 0.4457 | 2240 |
| TensorRT FP16 | 9.8 | 4.7x | 0.4518 | 1120 |
| **TensorRT FP16 + Pruning 20%** | 7.2 | 6.3x | 0.4498 | 896 |

**ìµœì  ì¡°í•©:** TensorRT FP16 + Pruning 20%
- 6ë°° ì´ìƒ ì†ë„ í–¥ìƒ
- ì •í™•ë„ ì†ì‹¤ 0.5% ë¯¸ë§Œ
- ë©”ëª¨ë¦¬ 60% ì ˆì•½

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/inference/tensorrt_optimizer.py` - TensorRT ìµœì í™”
- `src/inference/pruning.py` - ëª¨ë¸ Pruning
- `src/inference/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_tensorrt.py` - TensorRT í…ŒìŠ¤íŠ¸
- `src/tests/test_pruning.py` - Pruning í…ŒìŠ¤íŠ¸

**ê´€ë ¨ ë¬¸ì„œ:**
- [01_ì‹œì‘_ê°€ì´ë“œ.md](./01_ì‹œì‘_ê°€ì´ë“œ.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) - í•µì‹¬ ì‹œìŠ¤í…œ ë° Config
- [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) - ëª¨ë¸ ì‹œìŠ¤í…œ
- [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ì „ì²´ ëª…ë ¹ì–´ ê°€ì´ë“œ

**Config:**
- `configs/inference/tensorrt.yaml` - TensorRT ì„¤ì •
- `configs/inference/pruning.yaml` - Pruning ì„¤ì •
