# ì¶”ë¡  ì‹œìŠ¤í…œ ìƒì„¸ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [Predictor í´ë˜ìŠ¤](#predictor-í´ë˜ìŠ¤)
3. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
4. [ìƒì„± íŒŒë¼ë¯¸í„°](#ìƒì„±-íŒŒë¼ë¯¸í„°)
5. [ì œì¶œ íŒŒì¼ ìƒì„±](#ì œì¶œ-íŒŒì¼-ìƒì„±)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- í•™ìŠµëœ ëª¨ë¸ë¡œ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡
- ë°°ì¹˜ ì¶”ë¡  ë° ì§„í–‰ í‘œì‹œ
- ì œì¶œ íŒŒì¼ ìë™ ìƒì„±
- ìƒì„± íŒŒë¼ë¯¸í„° ìœ ì—°í•œ ì„¤ì •

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ë‹¨ì¼/ë°°ì¹˜ ì¶”ë¡  ì§€ì›
- âœ… DataFrame ì§ì ‘ ì²˜ë¦¬
- âœ… ì œì¶œ íŒŒì¼ ìë™ ìƒì„±
- âœ… ìƒì„± íŒŒë¼ë¯¸í„° ì˜¤ë²„ë¼ì´ë“œ
- âœ… Logger í†µí•© ì§€ì›

---

## ğŸ—ï¸ Predictor í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/inference/predictor.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class Predictor:
    def __init__(self, model, tokenizer, config=None, device=None, logger=None):
        """ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""

    def _setup_generation_config(self) -> Dict:
        """ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •"""

    def predict_single(self, dialogue: str, **generation_kwargs) -> str:
        """ë‹¨ì¼ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡"""

    def predict_batch(self, dialogues: List[str], batch_size=32,
                     show_progress=True, **generation_kwargs) -> List[str]:
        """ë°°ì¹˜ ëŒ€í™” ìš”ì•½ ì˜ˆì¸¡"""

    def predict_dataframe(self, df: pd.DataFrame, batch_size=32,
                          show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """DataFrameì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰"""

    def create_submission(self, test_df: pd.DataFrame, output_path: str,
                         batch_size=32, show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """ì œì¶œ íŒŒì¼ ìƒì„±"""
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ì¼ ì˜ˆì¸¡)

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# Predictor ìƒì„±
predictor = create_predictor(model, tokenizer)

# ë‹¨ì¼ ì˜ˆì¸¡
dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ë„¤ ì•ˆë…•í•˜ì„¸ìš”"
summary = predictor.predict_single(dialogue)

print(f"ì˜ˆì¸¡ ìš”ì•½: {summary}")
```

### 2. ë°°ì¹˜ ì˜ˆì¸¡

```python
dialogues = [
    "#Person1#: ì €ë… ë­ ë¨¹ì„ê¹Œ? #Person2#: ê¹€ì¹˜ì°Œê°œ ì–´ë•Œ?",
    "#Person1#: ë‚´ì¼ íšŒì˜ ëª‡ ì‹œì•¼? #Person2#: 3ì‹œë¡œ ì¡í˜”ì–´",
    "#Person1#: ë‚ ì”¨ ì¢‹ë„¤ #Person2#: ì‚°ì±… ê°€ì"
]

# ë°°ì¹˜ ì˜ˆì¸¡
summaries = predictor.predict_batch(
    dialogues,
    batch_size=32,
    show_progress=True
)

for dialogue, summary in zip(dialogues, summaries):
    print(f"ëŒ€í™”: {dialogue}")
    print(f"ìš”ì•½: {summary}\n")
```

### 3. DataFrame ì˜ˆì¸¡

```python
import pandas as pd

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# DataFrame ì˜ˆì¸¡
result_df = predictor.predict_dataframe(
    test_df,
    batch_size=32,
    show_progress=True
)

# ê²°ê³¼ í™•ì¸
print(result_df[['fname', 'dialogue', 'summary']].head())
```

### 4. ì œì¶œ íŒŒì¼ ìƒì„±

```python
# ì œì¶œ íŒŒì¼ ìë™ ìƒì„±
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32,
    show_progress=True
)

print(f"ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: submissions/submission.csv")
print(f"ìƒ˜í”Œ ìˆ˜: {len(submission_df)}")
```

---

## âš™ï¸ ìƒì„± íŒŒë¼ë¯¸í„°

### ê¸°ë³¸ ìƒì„± íŒŒë¼ë¯¸í„°

```python
default_config = {
    'max_length': 100,              # ìµœëŒ€ ìƒì„± ê¸¸ì´
    'num_beams': 4,                 # Beam search ë¹” ê°œìˆ˜
    'early_stopping': True,         # EOS í† í° ìƒì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
    'no_repeat_ngram_size': 2,      # n-gram ë°˜ë³µ ë°©ì§€
    'length_penalty': 1.0,          # ê¸¸ì´ í˜ë„í‹°
}
```

### Configì—ì„œ ìë™ ë¡œë“œ

```yaml
# configs/experiments/baseline_kobart.yaml
inference:
  generate_max_length: 100
  num_beams: 4
  early_stopping: true
  no_repeat_ngram_size: 2
  length_penalty: 1.0
```

### ëŸ°íƒ€ì„ ì˜¤ë²„ë¼ì´ë“œ

```python
# Config ê°’ ë¬´ì‹œí•˜ê³  íŒŒë¼ë¯¸í„° ì§€ì •
summaries = predictor.predict_batch(
    dialogues,
    batch_size=16,
    num_beams=8,            # ì˜¤ë²„ë¼ì´ë“œ
    max_length=150,         # ì˜¤ë²„ë¼ì´ë“œ
    temperature=0.8         # ì¶”ê°€ íŒŒë¼ë¯¸í„°
)
```

### ì£¼ìš” ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|-------|------|
| `max_length` | 100 | ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ |
| `num_beams` | 4 | Beam search ë¹” ê°œìˆ˜ (ë†’ì„ìˆ˜ë¡ í’ˆì§ˆâ†‘, ì†ë„â†“) |
| `early_stopping` | True | EOS í† í° ìƒì„± ì‹œ ì¦‰ì‹œ ì¢…ë£Œ |
| `no_repeat_ngram_size` | 2 | n-gram ë°˜ë³µ ë°©ì§€ í¬ê¸° |
| `length_penalty` | 1.0 | ê¸¸ì´ í˜ë„í‹° (>1: ê¸´ ë¬¸ì¥ ì„ í˜¸, <1: ì§§ì€ ë¬¸ì¥ ì„ í˜¸) |
| `temperature` | 1.0 | ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì , ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘) |
| `top_k` | 50 | Top-k ìƒ˜í”Œë§ |
| `top_p` | 1.0 | Nucleus ìƒ˜í”Œë§ |

---

## ğŸ“Š ì œì¶œ íŒŒì¼ ìƒì„±

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import pandas as pd
from src.inference import create_predictor

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# Predictor ìƒì„±
predictor = create_predictor(model, tokenizer)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# ì œì¶œ íŒŒì¼ ìƒì„±
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32,
    show_progress=True
)
```

### Loggerì™€ í•¨ê»˜ ì‚¬ìš©

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# Logger ì´ˆê¸°í™”
log_path = create_log_path("inference", "inference.log")
logger = Logger(log_path, print_also=True)
logger.start_redirect()

try:
    # Predictor ìƒì„± (Logger ì „ë‹¬)
    predictor = create_predictor(model, tokenizer, logger=logger)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission_df = predictor.create_submission(
        test_df=test_df,
        output_path="submissions/submission.csv",
        batch_size=32,
        show_progress=True
    )

finally:
    logger.stop_redirect()
    logger.close()
```

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘
============================================================

ìƒ˜í”Œ ìˆ˜: 2500
Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [02:15<00:00,  1.71s/it]

âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: submissions/submission.csv
============================================================
```

### ì œì¶œ íŒŒì¼ í˜•ì‹

```csv
fname,summary
test_001,ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤
test_002,íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤
test_003,ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤
...
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜
```
src/tests/test_predictor.py
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python src/tests/test_predictor.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 4ê°œ)

1. âœ… Predictor ìƒì„±
2. âœ… ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
3. âœ… ë‹¨ì¼ ì˜ˆì¸¡
4. âœ… í¸ì˜ í•¨ìˆ˜ (create_predictor)

---

## ğŸ¯ ì‹¤ì „ í™œìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ë¹„êµ

```python
checkpoints = [
    "outputs/baseline_kobart/checkpoint-500",
    "outputs/baseline_kobart/checkpoint-1000",
    "outputs/baseline_kobart/final_model"
]

for checkpoint_path in checkpoints:
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)

    # ì˜ˆì¸¡
    predictor = create_predictor(model, tokenizer)
    submission_df = predictor.create_submission(
        test_df=test_df,
        output_path=f"submissions/{Path(checkpoint_path).name}.csv",
        batch_size=32
    )

    print(f"âœ… {checkpoint_path} ì™„ë£Œ")
```

### ì˜ˆì‹œ 2: ë‹¤ì–‘í•œ ìƒì„± íŒŒë¼ë¯¸í„° ì‹¤í—˜

```python
params_list = [
    {"num_beams": 4, "max_length": 100},
    {"num_beams": 8, "max_length": 100},
    {"num_beams": 4, "max_length": 150},
]

for i, params in enumerate(params_list):
    summaries = predictor.predict_batch(
        dialogues,
        **params
    )

    print(f"\níŒŒë¼ë¯¸í„° ì„¸íŠ¸ {i+1}: {params}")
    print(f"ì˜ˆì¸¡ ì˜ˆì‹œ: {summaries[0]}")
```

### ì˜ˆì‹œ 3: ìƒ˜í”Œë³„ í’ˆì§ˆ í™•ì¸

```python
# ì˜ˆì¸¡ ìƒì„±
test_df = pd.read_csv("data/raw/test.csv")
result_df = predictor.predict_dataframe(test_df)

# ìš”ì•½ ê¸¸ì´ ë¶„í¬ í™•ì¸
result_df['summary_length'] = result_df['summary'].str.len()

print("ìš”ì•½ ê¸¸ì´ í†µê³„:")
print(result_df['summary_length'].describe())

# ê¸´ ìš”ì•½ ìƒ˜í”Œ í™•ì¸
long_summaries = result_df[result_df['summary_length'] > 200]
print(f"\nê¸´ ìš”ì•½ (>200ì): {len(long_summaries)}ê°œ")
print(long_summaries[['fname', 'summary', 'summary_length']].head())

# ì§§ì€ ìš”ì•½ ìƒ˜í”Œ í™•ì¸
short_summaries = result_df[result_df['summary_length'] < 20]
print(f"\nì§§ì€ ìš”ì•½ (<20ì): {len(short_summaries)}ê°œ")
print(short_summaries[['fname', 'summary', 'summary_length']].head())
```

---

## ğŸ“Œ ì£¼ì˜ì‚¬í•­

### 1. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

ë°°ì¹˜ í¬ê¸°ê°€ í¬ë©´ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë°œìƒ ê°€ëŠ¥:

```python
# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=16  # 32 â†’ 16
)
```

### 2. í† í¬ë‚˜ì´ì € ì¼ì¹˜

í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í† í¬ë‚˜ì´ì €ì™€ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# âŒ ì˜ëª»ëœ ë°©ë²•
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")  # í•™ìŠµ ì‹œì™€ ë‹¤ë¦„
```

### 3. ì§„í–‰ í‘œì‹œ ë¹„í™œì„±í™”

ìë™í™” ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ì§„í–‰ í‘œì‹œë¥¼ ë¹„í™œì„±í™”:

```python
summaries = predictor.predict_batch(
    dialogues,
    batch_size=32,
    show_progress=False  # ì§„í–‰ í‘œì‹œ ë„ê¸°
)
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/inference/predictor.py` - Predictor í´ë˜ìŠ¤
- `src/inference/__init__.py` - ì™¸ë¶€ API

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_predictor.py` - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**ìŠ¤í¬ë¦½íŠ¸:**
- `scripts/inference.py` - ì¶”ë¡  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `scripts/run_pipeline.py` - ì „ì²´ íŒŒì´í”„ë¼ì¸ (í•™ìŠµ + ì¶”ë¡ )
