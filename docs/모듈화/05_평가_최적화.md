# í‰ê°€ ë° ìµœì í™” ì‹œìŠ¤í…œ ê°€ì´ë“œ

> **í†µí•© ë¬¸ì„œ:** í‰ê°€ ì‹œìŠ¤í…œ + K-Fold êµì°¨ ê²€ì¦ + Optuna ìµœì í™”

## ğŸ“‹ ëª©ì°¨

### Part 1: í‰ê°€ ì‹œìŠ¤í…œ
- [ê°œìš”](#part-1-í‰ê°€-ì‹œìŠ¤í…œ)
- [RougeCalculator í´ë˜ìŠ¤](#rougecalculator-í´ë˜ìŠ¤)
- [ì‚¬ìš© ë°©ë²•](#í‰ê°€-ì‹œìŠ¤í…œ-ì‚¬ìš©-ë°©ë²•)
- [Multi-reference ì§€ì›](#multi-reference-ì§€ì›)
- [ë°°ì¹˜ ê³„ì‚°](#ë°°ì¹˜-ê³„ì‚°)
- [HuggingFace Trainer í†µí•©](#huggingface-trainer-í†µí•©)

### Part 2: K-Fold êµì°¨ ê²€ì¦
- [ê°œìš”](#part-2-k-fold-êµì°¨-ê²€ì¦)
- [KFoldSplitter](#kfoldsplitter)
- [ì‚¬ìš© ë°©ë²•](#êµì°¨-ê²€ì¦-ì‚¬ìš©-ë°©ë²•)
- [ì‹¤í–‰ ëª…ë ¹ì–´](#êµì°¨-ê²€ì¦-ì‹¤í–‰-ëª…ë ¹ì–´)

### Part 3: Optuna ìµœì í™”
- [ê°œìš”](#part-3-optuna-ìµœì í™”)
- [OptunaOptimizer í´ë˜ìŠ¤](#optunaoptimizer-í´ë˜ìŠ¤)
- [íƒìƒ‰ ê³µê°„ ì •ì˜](#íƒìƒ‰-ê³µê°„-ì •ì˜)
- [ìµœì í™” ì „ëµ](#ìµœì í™”-ì „ëµ)
- [ì‚¬ìš© ë°©ë²•](#optuna-ì‚¬ìš©-ë°©ë²•)
- [ì‹¤í–‰ ëª…ë ¹ì–´](#optuna-ì‹¤í–‰-ëª…ë ¹ì–´)

---

# ğŸ“Œ Part 1: í‰ê°€ ì‹œìŠ¤í…œ

## ğŸ“ ê°œìš”

### ëª©ì 
- ROUGE ì ìˆ˜ ìë™ ê³„ì‚° (ê²½ì§„ëŒ€íšŒ í‰ê°€ ê¸°ì¤€)
- Multi-reference í‰ê°€ ì§€ì›
- ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ ì •ë³´ ì œê³µ
- í•™ìŠµ/í‰ê°€ ì‹œ ìë™ í†µí•©

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ROUGE-1/2/L F1 ì ìˆ˜ ê³„ì‚°
- âœ… ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€) ìë™ ê³„ì‚°
- âœ… Multi-reference ì§€ì› (ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
- âœ… ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€)
- âœ… HuggingFace Trainerì™€ ìë™ í†µí•©

---

## ğŸ—ï¸ RougeCalculator í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/evaluation/metrics.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class RougeCalculator:
    def __init__(self, rouge_types=['rouge1', 'rouge2', 'rougeL'], use_stemmer=False):
        """ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”"""

    def calculate_single(self, prediction: str, reference: Union[str, List[str]]) -> Dict:
        """ë‹¨ì¼ ìƒ˜í”Œ ROUGE ê³„ì‚°"""

    def calculate_batch(self, predictions: List[str], references: List[str]) -> Dict:
        """ë°°ì¹˜ ìƒ˜í”Œ ROUGE í‰ê·  ê³„ì‚°"""

    def _empty_scores(self) -> Dict:
        """ë¹ˆ ì…ë ¥ì— ëŒ€í•œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜"""
```

---

## ğŸ’» í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ì¼ ìƒ˜í”Œ)

```python
from src.evaluation import RougeCalculator

# ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”
calculator = RougeCalculator()

# ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
reference = "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤"

scores = calculator.calculate_single(prediction, reference)

print(scores)
# ì¶œë ¥:
# {
#     'rouge1': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706},
#     'rouge2': {'precision': 0.5, 'recall': 0.4, 'fmeasure': 0.444},
#     'rougeL': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706}
# }
```

### 2. í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©

```python
from src.evaluation import calculate_rouge_scores

# ë‹¨ì¼ ìƒ˜í”Œ
scores = calculate_rouge_scores(
    predictions="ì˜ˆì¸¡ ìš”ì•½",
    references="ì •ë‹µ ìš”ì•½"
)

# ë°°ì¹˜ ìƒ˜í”Œ
predictions = ["ì˜ˆì¸¡1", "ì˜ˆì¸¡2", "ì˜ˆì¸¡3"]
references = ["ì •ë‹µ1", "ì •ë‹µ2", "ì •ë‹µ3"]

scores = calculate_rouge_scores(predictions, references)
```

### 3. ì ìˆ˜ í¬ë§·íŒ…

```python
from src.evaluation import calculate_rouge_scores, format_rouge_scores

scores = calculate_rouge_scores(predictions, references)
print(format_rouge_scores(scores))

# ì¶œë ¥:
# ROUGE1:
#   fmeasure: 0.7060
#   std: 0.1200
#   min: 0.5500
#   max: 0.8500
#
# ROUGE2:
#   fmeasure: 0.4440
#   ...
```

---

## ğŸ”„ Multi-reference ì§€ì›

### ê°œìš”

í•˜ë‚˜ì˜ ëŒ€í™”ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ì •ë‹µ ìš”ì•½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-reference í‰ê°€ëŠ” ê° ì •ë‹µì— ëŒ€í•´ ROUGEë¥¼ ê³„ì‚°í•œ í›„ **ìµœëŒ€ F1 ì ìˆ˜**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

### ì‚¬ìš© ë°©ë²•

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

# ë‹¨ì¼ ì˜ˆì¸¡, ë‹¤ì¤‘ ì •ë‹µ
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",
    "ì €ë…ì— ë§Œë‚˜ê¸°ë¡œ í–ˆë‹¤",
    "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
]

scores = calculator.calculate_single(prediction, references)
```

### ì²˜ë¦¬ ê³¼ì •

1. **ê° ì •ë‹µì— ëŒ€í•´ ROUGE ê³„ì‚°**
   ```python
   for ref in references:
       score = scorer.score(prediction, ref)
       all_scores.append(score)
   ```

2. **ìµœëŒ€ F1 ì ìˆ˜ ì„ íƒ**
   ```python
   max_score = max(all_scores, key=lambda x: x['rouge1'].fmeasure)
   ```

3. **ê²°ê³¼ ë°˜í™˜**
   ```python
   {
       'rouge1': {'precision': 1.0, 'recall': 1.0, 'fmeasure': 1.0},  # "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"ì™€ ì™„ì „ ì¼ì¹˜
       'rouge2': {...},
       'rougeL': {...}
   }
   ```

---

## ğŸ“Š ë°°ì¹˜ ê³„ì‚°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

predictions = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤",
    "íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤"
]

references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",
    "íšŒì˜ë¥¼ ì˜¤í›„ 3ì‹œì— í•˜ê¸°ë¡œ í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œë¥¼ ë¨¹ê¸°ë¡œ í–ˆë‹¤"
]

scores = calculator.calculate_batch(predictions, references)
```

### ì¶œë ¥ í˜•ì‹

```python
{
    'rouge1': {
        'fmeasure': 0.7060,      # í‰ê·  F1 ì ìˆ˜
        'std': 0.1200,           # í‘œì¤€í¸ì°¨
        'min': 0.5500,           # ìµœì†Œê°’
        'max': 0.8500            # ìµœëŒ€ê°’
    },
    'rouge2': {
        'fmeasure': 0.4440,
        'std': 0.0800,
        'min': 0.3000,
        'max': 0.6000
    },
    'rougeL': {
        'fmeasure': 0.7060,
        'std': 0.1200,
        'min': 0.5500,
        'max': 0.8500
    },
    'rouge_sum': {               # ROUGE-1 + ROUGE-2 + ROUGE-L
        'fmeasure': 1.8560,
        'std': 0.0,
        'min': 0.0,
        'max': 0.0
    }
}
```

### ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€)

ê²½ì§„ëŒ€íšŒì—ì„œëŠ” ROUGE-1, ROUGE-2, ROUGE-Lì˜ F1 ì ìˆ˜ í•©ê³„ë¥¼ ìµœì¢… í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
rouge_sum = rouge1_f1 + rouge2_f1 + rougeL_f1
# ì˜ˆ: 0.706 + 0.444 + 0.706 = 1.856
```

---

## ğŸ”— HuggingFace Trainer í†µí•©

### ModelTrainerì—ì„œ ìë™ ì‚¬ìš©

`src/training/trainer.py`ì˜ `ModelTrainer` í´ë˜ìŠ¤ëŠ” ìë™ìœ¼ë¡œ ROUGEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

```python
from src.training import create_trainer

trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ROUGE ê³„ì‚°
results = trainer.train()

# í‰ê°€ ê²°ê³¼
print(results['eval_metrics'])
# {
#     'eval_rouge1': 0.706,
#     'eval_rouge2': 0.444,
#     'eval_rougeL': 0.706,
#     'eval_rouge_sum': 1.856
# }
```

### compute_metrics í•¨ìˆ˜

Trainerì—ì„œ ì‚¬ìš©í•˜ëŠ” í‰ê°€ í•¨ìˆ˜:

```python
def compute_metrics(self, eval_preds) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE)"""
    predictions, labels = eval_preds

    # -100ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ë³€ê²½
    labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)

    # ë””ì½”ë”©
    decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ROUGE ê³„ì‚°
    scores = self.rouge_calculator.calculate_batch(
        decoded_preds,
        decoded_labels
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    result = {
        'rouge1': scores['rouge1']['fmeasure'],
        'rouge2': scores['rouge2']['fmeasure'],
        'rougeL': scores['rougeL']['fmeasure'],
        'rouge_sum': scores['rouge_sum']['fmeasure']
    }

    return result
```

---

# ğŸ“Œ Part 2: K-Fold êµì°¨ ê²€ì¦

## ğŸ“ ê°œìš”

### ëª©ì 
- K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
- Stratified K-Fold ì§€ì› (ì¸µí™” ì¶”ì¶œ)
- Fold ê²°ê³¼ ì§‘ê³„ ë° í†µê³„ ë¶„ì„

### í•µì‹¬ ê¸°ëŠ¥
- âœ… K-Fold ë¶„í• 
- âœ… Stratified K-Fold ë¶„í•  (ëŒ€í™” ê¸¸ì´/í† í”½ ê¸°ë°˜)
- âœ… Fold ê²°ê³¼ ì§‘ê³„
- âœ… ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

---

## ğŸ—ï¸ KFoldSplitter

### íŒŒì¼ ìœ„ì¹˜
```
src/validation/kfold.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class KFoldSplitter:
    def __init__(n_splits=5, shuffle=True, random_state=42, stratified=False)
    def split(data, stratify_column=None)
```

### ì£¼ìš” ê¸°ëŠ¥

#### 1. ê¸°ë³¸ K-Fold

```python
from src.validation.kfold import KFoldSplitter

splitter = KFoldSplitter(n_splits=5, shuffle=True, random_state=42)

folds = splitter.split(data)
# ê²°ê³¼: [(train_df1, val_df1), (train_df2, val_df2), ...]
```

**íŠ¹ì§•:**
- ë°ì´í„°ë¥¼ Kê°œ foldë¡œ ê· ë“± ë¶„í• 
- ê° foldì—ì„œ K-1ê°œëŠ” í•™ìŠµ, 1ê°œëŠ” ê²€ì¦
- ëª¨ë“  ë°ì´í„°ê°€ ì •í™•íˆ í•œ ë²ˆì”© ê²€ì¦ì— ì‚¬ìš©ë¨

**ì˜ˆì‹œ (100ê°œ ë°ì´í„°, 5-Fold):**
```
Fold 1: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 0-19)
Fold 2: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 20-39)
Fold 3: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 40-59)
Fold 4: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 60-79)
Fold 5: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 80-99)
```

---

#### 2. Stratified K-Fold (ì¸µí™” ì¶”ì¶œ)

```python
splitter = KFoldSplitter(
    n_splits=5,
    shuffle=True,
    random_state=42,
    stratified=True
)

# ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™”
folds = splitter.split(data, stratify_column='length')

# í† í”½ ê¸°ë°˜ ì¸µí™”
folds = splitter.split(data, stratify_column='topic')
```

**ì¸µí™” ê¸°ì¤€:**

1. **ëŒ€í™” ê¸¸ì´ (`stratify_column='length'`)**
   - ëŒ€í™” ê¸¸ì´ë¥¼ 4ë¶„ìœ„ë¡œ ë‚˜ëˆ”
   - ê° foldì— ëª¨ë“  ê¸¸ì´ ë²”ìœ„ì˜ ë°ì´í„° ê³ ë¥´ê²Œ ë¶„í¬

2. **í† í”½ (`stratify_column='topic'`)**
   - ë°ì´í„°ì— 'topic' ì»¬ëŸ¼ ì¡´ì¬ ì‹œ
   - ê° foldì— ëª¨ë“  í† í”½ì´ ê· ë“±í•˜ê²Œ ë¶„í¬

**íš¨ê³¼:**
- ê° foldê°€ ì „ì²´ ë°ì´í„° ë¶„í¬ë¥¼ ëŒ€í‘œ
- ê²€ì¦ ê²°ê³¼ì˜ ì•ˆì •ì„± í–¥ìƒ

---

## ğŸ’» êµì°¨ ê²€ì¦ ì‚¬ìš© ë°©ë²•

### 1. í¸ì˜ í•¨ìˆ˜ ì‚¬ìš© (ì¶”ì²œ)

```python
from src.validation.kfold import create_kfold_splits
import pandas as pd

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")

# 5-Fold ë¶„í• 
folds = create_kfold_splits(
    data=train_df,
    n_splits=5,
    stratified=False
)

# ê° foldë¡œ í•™ìŠµ ë° í‰ê°€
for fold_idx, (train_fold, val_fold) in enumerate(folds):
    print(f"\n=== Fold {fold_idx + 1}/{len(folds)} ===")
    print(f"í•™ìŠµ ë°ì´í„°: {len(train_fold)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_fold)}ê°œ")

    # ëª¨ë¸ í•™ìŠµ
    # model.train(train_fold)

    # ëª¨ë¸ í‰ê°€
    # metrics = model.evaluate(val_fold)
```

---

### 2. ì¸µí™” ì¶”ì¶œ ì‚¬ìš©

```python
from src.validation.kfold import create_kfold_splits

# ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™”
folds = create_kfold_splits(
    data=train_df,
    n_splits=5,
    stratified=True,
    stratify_column='length'  # ìë™ìœ¼ë¡œ ëŒ€í™” ê¸¸ì´ 4ë¶„ìœ„ ê³„ì‚°
)
```

---

### 3. Fold ê²°ê³¼ ì§‘ê³„

```python
from src.validation.kfold import aggregate_fold_results

# ê° fold í‰ê°€ ê²°ê³¼ ì €ì¥
fold_results = []

for fold_idx, (train_fold, val_fold) in enumerate(folds):
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    metrics = {
        'rouge1': 0.85,
        'rouge2': 0.75,
        'rougeL': 0.80
    }
    fold_results.append(metrics)

# ê²°ê³¼ ì§‘ê³„
aggregated = aggregate_fold_results(fold_results)

print(f"ROUGE-1: {aggregated['rouge1_mean']:.4f} (Â±{aggregated['rouge1_std']:.4f})")
print(f"  - Min: {aggregated['rouge1_min']:.4f}")
print(f"  - Max: {aggregated['rouge1_max']:.4f}")
```

**ì§‘ê³„ ê²°ê³¼:**
```
{
    'rouge1_mean': 0.8600,
    'rouge1_std': 0.0141,
    'rouge1_min': 0.8400,
    'rouge1_max': 0.8800,
    'rouge2_mean': 0.7600,
    'rouge2_std': 0.0141,
    ...
}
```

---

### 4. ì „ì²´ êµì°¨ ê²€ì¦ íŒŒì´í”„ë¼ì¸

```python
from src.config import load_config
from src.validation.kfold import create_kfold_splits, aggregate_fold_results
from src.models.model_loader import load_model_and_tokenizer
from src.data.preprocessor import create_dataset
from src.training.trainer import create_trainer
from src.evaluation.metrics import compute_metrics
import pandas as pd

# 1. Config ë¡œë“œ
config = load_config("baseline_kobart")

# 2. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(config.data.train_path)

# 3. K-Fold ë¶„í• 
folds = create_kfold_splits(
    data=train_df,
    n_splits=5,
    stratified=True,
    stratify_column='length'
)

# 4. ê° foldë¡œ í•™ìŠµ ë° í‰ê°€
fold_results = []

for fold_idx, (train_fold, val_fold) in enumerate(folds):
    print(f"\n{'='*60}")
    print(f"Fold {fold_idx + 1}/{len(folds)}")
    print(f"{'='*60}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    model, tokenizer = load_model_and_tokenizer(config)

    # Dataset ìƒì„±
    train_dataset = create_dataset(
        train_fold['dialogue'].tolist(),
        train_fold['summary'].tolist(),
        tokenizer,
        config
    )

    val_dataset = create_dataset(
        val_fold['dialogue'].tolist(),
        val_fold['summary'].tolist(),
        tokenizer,
        config
    )

    # Trainer ìƒì„±
    trainer = create_trainer(
        config,
        model,
        tokenizer,
        train_dataset,
        val_dataset
    )

    # í•™ìŠµ
    trainer.train()

    # í‰ê°€
    eval_results = trainer.evaluate()
    fold_results.append(eval_results)

# 5. ê²°ê³¼ ì§‘ê³„
aggregated = aggregate_fold_results(fold_results)

print(f"\n{'='*60}")
print("êµì°¨ ê²€ì¦ ìµœì¢… ê²°ê³¼")
print(f"{'='*60}")
print(f"ROUGE-1: {aggregated['rouge1_mean']:.4f} (Â±{aggregated['rouge1_std']:.4f})")
print(f"ROUGE-2: {aggregated['rouge2_mean']:.4f} (Â±{aggregated['rouge2_std']:.4f})")
print(f"ROUGE-L: {aggregated['rougeL_mean']:.4f} (Â±{aggregated['rougeL_std']:.4f})")
```

---

## ğŸ”§ êµì°¨ ê²€ì¦ ì‹¤í–‰ ëª…ë ¹ì–´

### Config ì„¤ì •

**íŒŒì¼:** `configs/experiments/baseline_kobart.yaml`

```yaml
validation:
  use_kfold: true
  n_splits: 5
  stratified: true
  stratify_column: 'length'  # ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™”
```

---

### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— í†µí•©

êµì°¨ ê²€ì¦ì„ ì§€ì›í•˜ëŠ” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ):

```bash
# K-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰
python scripts/train_with_cv.py --experiment baseline_kobart --n_splits 5
```

---

# ğŸ“Œ Part 3: Optuna ìµœì í™”

## ğŸ“ ê°œìš”

### ëª©ì 
- Bayesian Optimizationì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
- NLP íŠ¹í™” íƒìƒ‰ ê³µê°„ ì •ì˜ (LoRA, Generation íŒŒë¼ë¯¸í„° ë“±)
- ì¡°ê¸° ì¢…ë£Œë¥¼ í†µí•œ íš¨ìœ¨ì  íƒìƒ‰
- ROUGE ì ìˆ˜ ê¸°ë°˜ ìµœì í™”

### í•µì‹¬ ê¸°ëŠ¥
- âœ… TPE (Tree-structured Parzen Estimator) Sampler
- âœ… Median Prunerë¥¼ í†µí•œ ì¡°ê¸° ì¢…ë£Œ
- âœ… 15ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ íƒìƒ‰
- âœ… ìµœì  íŒŒë¼ë¯¸í„° ìë™ ì €ì¥
- âœ… ì‹œê°í™” ì§€ì› (Plotly)

---

## ğŸ”§ OptunaOptimizer í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/optimization/optuna_optimizer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class OptunaOptimizer:
    def __init__(config, train_dataset, val_dataset, n_trials, ...)
    def create_search_space(trial)
    def objective(trial)
    def optimize()
    def get_best_params()
    def get_best_value()
    def save_results(output_path)
    def plot_optimization_history(output_path)
```

### ì´ˆê¸°í™”

```python
from src.optimization import OptunaOptimizer
from src.data import load_and_preprocess_data

# ë°ì´í„° ë¡œë“œ
train_df, val_df = load_and_preprocess_data(train_path, split_ratio=0.9)

# Config ë¡œë“œ
from src.config import ConfigLoader
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ë°ì´í„°ì…‹ ìƒì„±
from src.data import DialogueSummarizationDataset
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),
    train_df['summary'].tolist(),
    tokenizer,
    config
)

val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),
    val_df['summary'].tolist(),
    tokenizer,
    config
)

# Optimizer ì´ˆê¸°í™”
optimizer = OptunaOptimizer(
    config=config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    n_trials=50,                    # 50íšŒ ì‹œë„
    timeout=None,                   # ë¬´ì œí•œ
    study_name="kobart_optuna",     # Study ì´ë¦„
    direction="maximize"            # ROUGE ìµœëŒ€í™”
)
```

---

## ğŸ” íƒìƒ‰ ê³µê°„ ì •ì˜

### 1. LoRA íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| lora_r | [8, 16, 32, 64] | LoRA rank |
| lora_alpha | [16, 32, 64, 128] | LoRA scaling factor |
| lora_dropout | 0.0 ~ 0.2 | LoRA dropout rate |

**ì½”ë“œ:**
```python
params['lora_r'] = trial.suggest_categorical('lora_r', [8, 16, 32, 64])
params['lora_alpha'] = trial.suggest_categorical('lora_alpha', [16, 32, 64, 128])
params['lora_dropout'] = trial.suggest_float('lora_dropout', 0.0, 0.2)
```

---

### 2. í•™ìŠµ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| learning_rate | 1e-6 ~ 1e-4 (log scale) | í•™ìŠµë¥  |
| batch_size | [8, 16, 32, 64] | ë°°ì¹˜ í¬ê¸° |
| num_epochs | 3 ~ 10 | ì—í¬í¬ ìˆ˜ |
| warmup_ratio | 0.0 ~ 0.2 | Warmup ë¹„ìœ¨ |
| weight_decay | 0.0 ~ 0.1 | Weight decay |

**ì½”ë“œ:**
```python
params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)
params['batch_size'] = trial.suggest_categorical('batch_size', [8, 16, 32, 64])
params['num_epochs'] = trial.suggest_int('num_epochs', 3, 10)
params['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)
params['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)
```

---

### 3. Scheduler

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| scheduler_type | [linear, cosine, cosine_with_restarts, polynomial] | Scheduler ì¢…ë¥˜ |

**ì½”ë“œ:**
```python
params['scheduler_type'] = trial.suggest_categorical(
    'scheduler_type',
    ['linear', 'cosine', 'cosine_with_restarts', 'polynomial']
)
```

---

### 4. Generation íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| temperature | 0.1 ~ 1.0 | ìƒì„± ì˜¨ë„ |
| top_p | 0.5 ~ 1.0 | Nucleus sampling |
| num_beams | [2, 4, 6, 8] | Beam search ë¹” ê°œìˆ˜ |
| length_penalty | 0.5 ~ 2.0 | ê¸¸ì´ íŒ¨ë„í‹° |

**ì½”ë“œ:**
```python
params['temperature'] = trial.suggest_float('temperature', 0.1, 1.0)
params['top_p'] = trial.suggest_float('top_p', 0.5, 1.0)
params['num_beams'] = trial.suggest_categorical('num_beams', [2, 4, 6, 8])
params['length_penalty'] = trial.suggest_float('length_penalty', 0.5, 2.0)
```

---

### 5. Dropout íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| hidden_dropout | 0.0 ~ 0.3 | Hidden layer dropout |
| attention_dropout | 0.0 ~ 0.3 | Attention dropout |

**ì½”ë“œ:**
```python
if config.model.get('hidden_dropout_prob') is not None:
    params['hidden_dropout'] = trial.suggest_float('hidden_dropout', 0.0, 0.3)
    params['attention_dropout'] = trial.suggest_float('attention_dropout', 0.0, 0.3)
```

---

## âš¡ ìµœì í™” ì „ëµ

### 1. Bayesian Optimization (TPE)

**íŠ¹ì§•:**
- Tree-structured Parzen Estimator
- ì´ì „ trial ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ íƒìƒ‰ ìœ„ì¹˜ ê²°ì •
- Random searchë³´ë‹¤ íš¨ìœ¨ì 

**ì„¤ì •:**
```python
from optuna.samplers import TPESampler

sampler = TPESampler(seed=42)
```

---

### 2. Median Pruner (ì¡°ê¸° ì¢…ë£Œ)

**íŠ¹ì§•:**
- ì¤‘ê°„ ê²°ê³¼ê°€ medianë³´ë‹¤ ë‚®ìœ¼ë©´ trial ì¢…ë£Œ
- ë¦¬ì†ŒìŠ¤ ì ˆì•½ (ë¶ˆí•„ìš”í•œ trial ì¡°ê¸° ì¤‘ë‹¨)

**ì„¤ì •:**
```python
from optuna.pruners import MedianPruner

pruner = MedianPruner(
    n_startup_trials=5,   # ì²˜ìŒ 5ê°œëŠ” pruning ì•ˆí•¨
    n_warmup_steps=3,     # 3 ì—í¬í¬ í›„ë¶€í„° ì²´í¬
    interval_steps=1      # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì²´í¬
)
```

**ë™ì‘ ë°©ì‹:**
```
Trial 0: [ì—í¬í¬1: 0.30] [ì—í¬í¬2: 0.32] [ì—í¬í¬3: 0.35] â†’ ê³„ì†
Trial 1: [ì—í¬í¬1: 0.28] [ì—í¬í¬2: 0.29] [ì—í¬í¬3: 0.30] â†’ ê³„ì†
Trial 2: [ì—í¬í¬1: 0.25] [ì—í¬í¬2: 0.26] [ì—í¬í¬3: 0.27] â†’ Pruned! (median=0.32ë³´ë‹¤ ë‚®ìŒ)
```

---

### 3. ëª©ì  í•¨ìˆ˜ (Objective Function)

**ëª©í‘œ:** ROUGE-L F1 ì ìˆ˜ ìµœëŒ€í™”

**íë¦„:**
1. Trialì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
2. Config ì—…ë°ì´íŠ¸
3. ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ
4. ê²€ì¦ ë°ì´í„° í‰ê°€
5. ROUGE-L F1 ë°˜í™˜

**ì½”ë“œ:**
```python
def objective(self, trial: optuna.Trial) -> float:
    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    params = self.create_search_space(trial)

    # 2. Config ì—…ë°ì´íŠ¸
    config.training.learning_rate = params['learning_rate']
    config.training.batch_size = params['batch_size']
    # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    # 3. ëª¨ë¸ í•™ìŠµ
    model_loader = ModelLoader(config)
    model, tokenizer = model_loader.load()

    trainer = ModelTrainer(...)
    trainer.train()

    # 4. í‰ê°€
    metrics = trainer.evaluate()
    rouge_l_f1 = metrics['rouge_l_f1']

    # 5. Pruning ì²´í¬
    trial.report(rouge_l_f1, step=config.training.num_epochs)
    if trial.should_prune():
        raise optuna.TrialPruned()

    return rouge_l_f1
```

---

## ğŸ’» Optuna ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ìµœì í™”

```python
from src.optimization import OptunaOptimizer
from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset

# Config ë¡œë“œ
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ë°ì´í„° ë¡œë“œ
train_df, val_df = load_and_preprocess_data("data/raw/train.csv", split_ratio=0.9)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model.name)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),
    train_df['summary'].tolist(),
    tokenizer,
    config
)

val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),
    val_df['summary'].tolist(),
    tokenizer,
    config
)

# Optimizer ì´ˆê¸°í™”
optimizer = OptunaOptimizer(
    config=config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    n_trials=50
)

# ìµœì í™” ì‹¤í–‰
study = optimizer.optimize()

# ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
best_params = optimizer.get_best_params()
best_value = optimizer.get_best_value()

print(f"ìµœì  ROUGE-L F1: {best_value:.4f}")
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
```

---

### 2. ê²°ê³¼ ì €ì¥

```python
# ê²°ê³¼ ì €ì¥
optimizer.save_results("outputs/optuna_results")

# ì €ì¥ë˜ëŠ” íŒŒì¼:
# - outputs/optuna_results/best_params.json
# - outputs/optuna_results/all_trials.csv
# - outputs/optuna_results/study_stats.json
```

**best_params.json ì˜ˆì‹œ:**
```json
{
  "best_params": {
    "learning_rate": 3.5e-05,
    "batch_size": 32,
    "num_epochs": 5,
    "lora_r": 16,
    "lora_alpha": 32,
    "temperature": 0.8,
    "num_beams": 6
  },
  "best_value": 0.4521,
  "n_trials": 50
}
```

---

## ğŸ”§ Optuna ì‹¤í–‰ ëª…ë ¹ì–´

### Optuna ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ)

**íŒŒì¼:** `scripts/optimize.py`

```python
import argparse
from pathlib import Path

from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset
from src.optimization import OptunaOptimizer
from transformers import AutoTokenizer


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", default="baseline_kobart", help="ì‹¤í—˜ config ì´ë¦„")
    parser.add_argument("--n_trials", type=int, default=50, help="Trial íšŸìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--output_dir", default="outputs/optuna_results", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()

    # Config ë¡œë“œ
    config_loader = ConfigLoader()
    config = config_loader.load(args.experiment)

    # ë°ì´í„° ë¡œë“œ
    train_df, val_df = load_and_preprocess_data("data/raw/train.csv", split_ratio=0.9)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = DialogueSummarizationDataset(
        train_df['dialogue'].tolist(),
        train_df['summary'].tolist(),
        tokenizer,
        config
    )

    val_dataset = DialogueSummarizationDataset(
        val_df['dialogue'].tolist(),
        val_df['summary'].tolist(),
        tokenizer,
        config
    )

    # Optimizer ì´ˆê¸°í™”
    optimizer = OptunaOptimizer(
        config=config,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        n_trials=args.n_trials,
        timeout=args.timeout,
        study_name=f"optuna_{args.experiment}"
    )

    # ìµœì í™” ì‹¤í–‰
    study = optimizer.optimize()

    # ê²°ê³¼ ì €ì¥
    optimizer.save_results(args.output_dir)

    # ì‹œê°í™”
    try:
        optimizer.plot_optimization_history(args.output_dir)
    except ImportError:
        print("plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

    print(f"\n{'='*60}")
    print(f"ìµœì í™” ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ìµœì  ROUGE-L F1: {optimizer.get_best_value():.4f}")
    print(f"ê²°ê³¼ ì €ì¥: {args.output_dir}")


if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
# ê¸°ë³¸ ì‹¤í–‰ (50 trials)
python scripts/optimize.py --experiment baseline_kobart

# Trial íšŸìˆ˜ ì¡°ì •
python scripts/optimize.py --experiment baseline_kobart --n_trials 100

# ì‹œê°„ ì œí•œ (12ì‹œê°„ = 43200ì´ˆ)
python scripts/optimize.py --experiment baseline_kobart --timeout 43200

# ê²°ê³¼ ë””ë ‰í† ë¦¬ ì§€ì •
python scripts/optimize.py --experiment baseline_kobart --output_dir outputs/kobart_optuna
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/evaluation/metrics.py` - RougeCalculator í´ë˜ìŠ¤
- `src/evaluation/__init__.py` - ì™¸ë¶€ API
- `src/validation/kfold.py` - K-Fold ì‹œìŠ¤í…œ
- `src/validation/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
- `src/optimization/optuna_optimizer.py` - Optuna optimizer
- `src/optimization/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_metrics.py` - ROUGE í…ŒìŠ¤íŠ¸
- `src/tests/test_kfold.py` - K-Fold í…ŒìŠ¤íŠ¸
- `src/tests/test_optuna.py` - Optuna í…ŒìŠ¤íŠ¸

**í†µí•©:**
- `src/training/trainer.py` - Trainerì—ì„œ ìë™ ì‚¬ìš©

**ë¬¸ì„œ:**
- `docs/ëª¨ë“ˆí™”/00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md` - ì‹œìŠ¤í…œ ê°œìš”
- `docs/ëª¨ë“ˆí™”/ì‹¤í–‰_ëª…ë ¹ì–´_ì´ì •ë¦¬.md` - ì‹¤í–‰ ëª…ë ¹ì–´
- `docs/PRD/13_Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°_ìµœì í™”.md` - PRD ë¬¸ì„œ

**Config:**
- `configs/base/default.yaml` - ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `configs/experiments/*.yaml` - ì‹¤í—˜ë³„ Config
