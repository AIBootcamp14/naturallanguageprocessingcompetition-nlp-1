# 데이터 증강 시스템 상세 가이드

## 📋 목차
1. [개요](#개요)
2. [DataAugmenter](#dataaugmenter)
3. [증강 방법](#증강-방법)
4. [사용 방법](#사용-방법)
5. [실행 명령어](#실행-명령어)

---

## 📝 개요

### 목적
- 학습 데이터 증강으로 모델 일반화 성능 향상
- 5가지 증강 방법 지원
- 유연한 조합 가능

### 핵심 기능
- ✅ Back-translation (한→영→한)
- ✅ Paraphrase 생성
- ✅ 대화 턴 섞기
- ✅ 동의어 치환
- ✅ 대화 샘플링

---

## 🏗️ DataAugmenter

### 파일 위치
```
src/data/augmentation.py
```

### 클래스 구조

```python
class DataAugmenter:
    def __init__(logger=None)
    def augment(dialogues, summaries, methods, samples_per_method)
    def back_translate(text)
    def paraphrase(text)
    def shuffle_turns(dialogue, preserve_ratio)
    def synonym_replacement(text, n)
    def sample_dialogue(dialogue, ratio)
```

---

## 🔄 증강 방법

### 1. Back-translation (역번역)

**원리:**
- 한국어 → 영어 → 한국어 번역
- Helsinki-NLP MarianMT 모델 사용

**코드:**
```python
from src.data.augmentation import BackTranslationAugmenter

augmenter = BackTranslationAugmenter()
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**예시:**
```
원본: "오늘 날씨가 정말 좋네요"
증강: "오늘 날씨가 매우 좋습니다"
```

**특징:**
- 의미는 유지하면서 표현 변경
- GPU 사용 권장 (모델 로딩 필요)
- 선택적 사용 (SentencePiece 라이브러리 필요)

---

### 2. Paraphrase (의역)

**원리:**
- 규칙 기반 동의어 치환
- 주요 표현 변형

**코드:**
```python
from src.data.augmentation import ParaphraseAugmenter

augmenter = ParaphraseAugmenter()
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**치환 규칙:**
```python
{
    "안녕하세요": ["안녕", "반갑습니다", "환영합니다"],
    "감사합니다": ["고맙습니다", "감사해요", "고마워요"],
    "죄송합니다": ["미안합니다", "죄송해요", "미안해요"],
    "네": ["예", "알겠습니다", "그렇습니다"],
    "아니요": ["아닙니다", "아니에요", "그렇지 않습니다"]
}
```

**예시:**
```
원본: "안녕하세요. 감사합니다."
증강: "반갑습니다. 고맙습니다."
```

---

### 3. Turn Shuffling (턴 섞기)

**원리:**
- 대화 턴 순서 무작위 섞기
- 처음/끝 일부 보존 (맥락 유지)

**코드:**
```python
from src.data.augmentation import ShuffleAugmenter

augmenter = ShuffleAugmenter(preserve_ratio=0.3)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**파라미터:**
- `preserve_ratio`: 보존할 턴 비율 (기본값: 0.3)

**예시:**
```
원본:
A: 안녕하세요
B: 안녕하세요
A: 오늘 날씨 좋네요
B: 네, 정말 좋아요
A: 점심 뭐 먹을까요?
B: 김치찌개 어때요?

증강:
A: 안녕하세요        # 처음 보존
B: 안녕하세요        # 처음 보존
A: 점심 뭐 먹을까요? # 중간 섞임
B: 김치찌개 어때요?  # 중간 섞임
A: 오늘 날씨 좋네요  # 중간 섞임
B: 네, 정말 좋아요   # 끝 보존
```

**특징:**
- 요약은 변경되지 않음
- 대화 순서에 민감하지 않은 경우 유용

---

### 4. Synonym Replacement (동의어 치환)

**원리:**
- 특정 단어를 동의어로 치환
- 한국어 동의어 사전 기반

**코드:**
```python
from src.data.augmentation import SynonymReplacementAugmenter

augmenter = SynonymReplacementAugmenter(replace_ratio=0.3)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**동의어 사전:**
```python
{
    "좋다": ["훌륭하다", "멋지다", "괜찮다"],
    "나쁘다": ["안좋다", "별로다", "형편없다"],
    "크다": ["거대하다", "넓다", "광대하다"],
    "작다": ["적다", "미미하다", "소소하다"],
    "빠르다": ["신속하다", "재빠르다", "날쌔다"],
    "느리다": ["더디다", "굼뜨다", "늦다"],
    "밥": ["식사", "음식", "끼니"],
    "먹다": ["섭취하다", "드시다", "식사하다"]
}
```

**예시:**
```
원본: "오늘 점심에 밥을 먹었다"
증강: "오늘 점심에 식사를 먹었다"
```

---

### 5. Dialogue Sampling (대화 샘플링)

**원리:**
- 긴 대화에서 일부 턴만 선택
- 처음과 끝 턴 항상 유지

**코드:**
```python
from src.data.augmentation import DialogueSamplingAugmenter

augmenter = DialogueSamplingAugmenter(sample_ratio=0.7)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**파라미터:**
- `sample_ratio`: 유지할 턴 비율 (기본값: 0.7)

**예시:**
```
원본 (8턴):
A: 1
B: 2
A: 3
B: 4
A: 5
B: 6
A: 7
B: 8

증강 (5턴, 70%):
A: 1       # 처음 유지
B: 2       # 랜덤 선택
A: 5       # 랜덤 선택
A: 7       # 랜덤 선택
B: 8       # 끝 유지
```

**특징:**
- 요약은 변경되지 않음
- 긴 대화 요약 학습에 유용

---

## 💻 사용 방법

### 1. 단일 방법 사용

```python
from src.data.augmentation import ShuffleAugmenter

augmenter = ShuffleAugmenter(preserve_ratio=0.3)

dialogue = "A: 안녕\nB: 안녕\nA: 날씨 좋네\nB: 네"
summary = "인사 및 날씨"

aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

---

### 2. DataAugmenter로 여러 방법 사용

```python
from src.data.augmentation import DataAugmenter

augmenter = DataAugmenter()

dialogues = ["대화1", "대화2", "대화3"]
summaries = ["요약1", "요약2", "요약3"]

# 3가지 방법 적용, 각 방법당 2개씩 생성
aug_dialogues, aug_summaries = augmenter.augment(
    dialogues,
    summaries,
    methods=['shuffle', 'synonym', 'sample'],
    samples_per_method=2
)

print(f"원본: {len(dialogues)}개")
print(f"증강 후: {len(aug_dialogues)}개")  # 3 + (3 × 3 × 2) = 21개
```

---

### 3. 편의 함수 사용 (augment_dataset)

```python
from src.data.augmentation import augment_dataset
import pandas as pd

# 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")

# 증강 실행
aug_dialogues, aug_summaries = augment_dataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    methods=['shuffle', 'sample'],
    n_aug=2  # 각 방법당 2개씩
)

print(f"원본: {len(train_df)}개")
print(f"증강 후: {len(aug_dialogues)}개")
# 12,457개 → 12,457 × 2 × 2 = 49,828개
```

---

### 4. 학습 스크립트에 통합

```python
from src.config import load_config
from src.data.augmentation import augment_dataset
from src.data.preprocessor import create_dataset
import pandas as pd

# 1. Config 로드
config = load_config("baseline_kobart")

# 2. 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")

# 3. 데이터 증강
if config.data.augmentation.enabled:
    aug_dialogues, aug_summaries = augment_dataset(
        dialogues=train_df['dialogue'].tolist(),
        summaries=train_df['summary'].tolist(),
        methods=config.data.augmentation.methods,
        n_aug=config.data.augmentation.n_aug
    )

    # 증강 데이터를 DataFrame에 추가
    aug_df = pd.DataFrame({
        'dialogue': aug_dialogues,
        'summary': aug_summaries
    })
    train_df = pd.concat([train_df, aug_df], ignore_index=True)

# 4. Dataset 생성
train_dataset = create_dataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    config=config
)
```

---

## 🔧 실행 명령어

### Config 설정

**파일:** `configs/experiments/baseline_kobart.yaml`

```yaml
data:
  augmentation:
    enabled: true
    methods:
      - shuffle
      - sample
    n_aug: 2  # 각 방법당 2개씩 생성
```

---

### 학습 시 자동 증강

학습 스크립트에서 Config 기반으로 자동 증강:

```bash
# Config에 augmentation 설정이 있으면 자동으로 증강됨
python scripts/train.py --experiment baseline_kobart
```

---

## 📂 Config 파일

### 증강 설정 예시

```yaml
data:
  train_path: "data/raw/train.csv"
  dev_path: "data/raw/dev.csv"

  augmentation:
    enabled: true
    methods:
      - shuffle        # 턴 섞기
      - sample         # 대화 샘플링
      - synonym        # 동의어 치환
    n_aug: 2           # 각 방법당 2개씩
```

**결과:**
- 원본: 12,457개
- 증강 후: 12,457 + (12,457 × 3 × 2) = **87,399개**

---

## 🧪 테스트

### 테스트 파일 위치
```
src/tests/test_augmentation.py
```

### 테스트 실행

```bash
python src/tests/test_augmentation.py
```

### 테스트 항목 (총 7개)

1. ✅ DataAugmenter 초기화
2. ✅ 역번역 증강 (선택적)
3. ✅ 의역 증강
4. ✅ 턴 섞기 증강
5. ✅ 동의어 치환 증강
6. ✅ 대화 샘플링 증강
7. ✅ augment_dataset 함수

**결과:** 7/7 테스트 통과 (100%)

---

## 📊 증강 효과

### 데이터 증가량

| 방법 수 | n_aug | 원본 | 증강 후 | 증가율 |
|---------|-------|------|---------|--------|
| 1개 | 1 | 12,457 | 24,914 | 2배 |
| 2개 | 1 | 12,457 | 37,371 | 3배 |
| 3개 | 1 | 12,457 | 49,828 | 4배 |
| 2개 | 2 | 12,457 | 62,285 | 5배 |
| 3개 | 2 | 12,457 | 87,399 | 7배 |

---

### 권장 조합

#### 1. 가벼운 증강 (2배)
```yaml
methods: [shuffle]
n_aug: 1
```
- 빠른 학습
- 메모리 효율적

#### 2. 중간 증강 (4배)
```yaml
methods: [shuffle, sample]
n_aug: 2
```
- 균형있는 성능 향상
- 학습 시간 적절

#### 3. 강력한 증강 (7배)
```yaml
methods: [shuffle, sample, synonym]
n_aug: 2
```
- 최대 성능 향상
- 긴 학습 시간 필요

---

## ⚠️ 주의사항

### 1. Back-translation
- Helsinki-NLP 모델 다운로드 필요 (약 300MB × 2)
- SentencePiece 라이브러리 필요
- GPU 권장 (느린 속도)

### 2. 메모리
- 증강 데이터는 메모리에 로드됨
- 7배 증강 시: 12,457개 → 87,399개
- 배치 크기 조정 필요할 수 있음

### 3. 학습 시간
- 데이터가 많아지면 학습 시간 증가
- 에포크 수 조정 고려

---

## 🎯 성능 목표

### 증강 전후 비교

| 모델 | 증강 전 | 증강 후 (4배) | 개선 |
|------|---------|---------------|------|
| KoBART | ROUGE 88-90 | **ROUGE 92-95** | +4-5 |

---

## 🔗 관련 파일

**소스 코드:**
- `src/data/augmentation.py` - 증강 시스템

**테스트:**
- `src/tests/test_augmentation.py` - 증강 테스트

**문서:**
- `docs/모듈화/00_전체_시스템_개요.md` - 시스템 개요
- `docs/모듈화/실행_명령어_총정리.md` - 실행 명령어
