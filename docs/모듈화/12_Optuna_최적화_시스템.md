# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œìŠ¤í…œ ìƒì„¸ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [OptunaOptimizer í´ë˜ìŠ¤](#optunaoptimizer-í´ë˜ìŠ¤)
3. [íƒìƒ‰ ê³µê°„ ì •ì˜](#íƒìƒ‰-ê³µê°„-ì •ì˜)
4. [ìµœì í™” ì „ëµ](#ìµœì í™”-ì „ëµ)
5. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
6. [ì‹¤í–‰ ëª…ë ¹ì–´](#ì‹¤í–‰-ëª…ë ¹ì–´)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- Bayesian Optimizationì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
- NLP íŠ¹í™” íƒìƒ‰ ê³µê°„ ì •ì˜ (LoRA, Generation íŒŒë¼ë¯¸í„° ë“±)
- ì¡°ê¸° ì¢…ë£Œë¥¼ í†µí•œ íš¨ìœ¨ì  íƒìƒ‰
- ROUGE ì ìˆ˜ ê¸°ë°˜ ìµœì í™”

### í•µì‹¬ ê¸°ëŠ¥
- âœ… TPE (Tree-structured Parzen Estimator) Sampler
- âœ… Median Prunerë¥¼ í†µí•œ ì¡°ê¸° ì¢…ë£Œ
- âœ… 15ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ íƒìƒ‰
- âœ… ìµœì  íŒŒë¼ë¯¸í„° ìë™ ì €ì¥
- âœ… ì‹œê°í™” ì§€ì› (Plotly)

---

## ğŸ”§ OptunaOptimizer í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/optimization/optuna_optimizer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class OptunaOptimizer:
    def __init__(config, train_dataset, val_dataset, n_trials, ...)
    def create_search_space(trial)
    def objective(trial)
    def optimize()
    def get_best_params()
    def get_best_value()
    def save_results(output_path)
    def plot_optimization_history(output_path)
```

### ì´ˆê¸°í™”

```python
from src.optimization import OptunaOptimizer
from src.data import load_and_preprocess_data

# ë°ì´í„° ë¡œë“œ
train_df, val_df = load_and_preprocess_data(train_path, split_ratio=0.9)

# Config ë¡œë“œ
from src.config import ConfigLoader
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ë°ì´í„°ì…‹ ìƒì„±
from src.data import DialogueSummarizationDataset
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),
    train_df['summary'].tolist(),
    tokenizer,
    config
)

val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),
    val_df['summary'].tolist(),
    tokenizer,
    config
)

# Optimizer ì´ˆê¸°í™”
optimizer = OptunaOptimizer(
    config=config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    n_trials=50,                    # 50íšŒ ì‹œë„
    timeout=None,                   # ë¬´ì œí•œ
    study_name="kobart_optuna",     # Study ì´ë¦„
    direction="maximize"            # ROUGE ìµœëŒ€í™”
)
```

---

## ğŸ” íƒìƒ‰ ê³µê°„ ì •ì˜

### 1. LoRA íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| lora_r | [8, 16, 32, 64] | LoRA rank |
| lora_alpha | [16, 32, 64, 128] | LoRA scaling factor |
| lora_dropout | 0.0 ~ 0.2 | LoRA dropout rate |

**ì½”ë“œ:**
```python
params['lora_r'] = trial.suggest_categorical('lora_r', [8, 16, 32, 64])
params['lora_alpha'] = trial.suggest_categorical('lora_alpha', [16, 32, 64, 128])
params['lora_dropout'] = trial.suggest_float('lora_dropout', 0.0, 0.2)
```

---

### 2. í•™ìŠµ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| learning_rate | 1e-6 ~ 1e-4 (log scale) | í•™ìŠµë¥  |
| batch_size | [8, 16, 32, 64] | ë°°ì¹˜ í¬ê¸° |
| num_epochs | 3 ~ 10 | ì—í¬í¬ ìˆ˜ |
| warmup_ratio | 0.0 ~ 0.2 | Warmup ë¹„ìœ¨ |
| weight_decay | 0.0 ~ 0.1 | Weight decay |

**ì½”ë“œ:**
```python
params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)
params['batch_size'] = trial.suggest_categorical('batch_size', [8, 16, 32, 64])
params['num_epochs'] = trial.suggest_int('num_epochs', 3, 10)
params['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)
params['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)
```

---

### 3. Scheduler

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| scheduler_type | [linear, cosine, cosine_with_restarts, polynomial] | Scheduler ì¢…ë¥˜ |

**ì½”ë“œ:**
```python
params['scheduler_type'] = trial.suggest_categorical(
    'scheduler_type',
    ['linear', 'cosine', 'cosine_with_restarts', 'polynomial']
)
```

---

### 4. Generation íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| temperature | 0.1 ~ 1.0 | ìƒì„± ì˜¨ë„ |
| top_p | 0.5 ~ 1.0 | Nucleus sampling |
| num_beams | [2, 4, 6, 8] | Beam search ë¹” ê°œìˆ˜ |
| length_penalty | 0.5 ~ 2.0 | ê¸¸ì´ íŒ¨ë„í‹° |

**ì½”ë“œ:**
```python
params['temperature'] = trial.suggest_float('temperature', 0.1, 1.0)
params['top_p'] = trial.suggest_float('top_p', 0.5, 1.0)
params['num_beams'] = trial.suggest_categorical('num_beams', [2, 4, 6, 8])
params['length_penalty'] = trial.suggest_float('length_penalty', 0.5, 2.0)
```

---

### 5. Dropout íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… |
|---------|----------|------|
| hidden_dropout | 0.0 ~ 0.3 | Hidden layer dropout |
| attention_dropout | 0.0 ~ 0.3 | Attention dropout |

**ì½”ë“œ:**
```python
if config.model.get('hidden_dropout_prob') is not None:
    params['hidden_dropout'] = trial.suggest_float('hidden_dropout', 0.0, 0.3)
    params['attention_dropout'] = trial.suggest_float('attention_dropout', 0.0, 0.3)
```

---

## âš¡ ìµœì í™” ì „ëµ

### 1. Bayesian Optimization (TPE)

**íŠ¹ì§•:**
- Tree-structured Parzen Estimator
- ì´ì „ trial ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ íƒìƒ‰ ìœ„ì¹˜ ê²°ì •
- Random searchë³´ë‹¤ íš¨ìœ¨ì 

**ì„¤ì •:**
```python
from optuna.samplers import TPESampler

sampler = TPESampler(seed=42)
```

---

### 2. Median Pruner (ì¡°ê¸° ì¢…ë£Œ)

**íŠ¹ì§•:**
- ì¤‘ê°„ ê²°ê³¼ê°€ medianë³´ë‹¤ ë‚®ìœ¼ë©´ trial ì¢…ë£Œ
- ë¦¬ì†ŒìŠ¤ ì ˆì•½ (ë¶ˆí•„ìš”í•œ trial ì¡°ê¸° ì¤‘ë‹¨)

**ì„¤ì •:**
```python
from optuna.pruners import MedianPruner

pruner = MedianPruner(
    n_startup_trials=5,   # ì²˜ìŒ 5ê°œëŠ” pruning ì•ˆí•¨
    n_warmup_steps=3,     # 3 ì—í¬í¬ í›„ë¶€í„° ì²´í¬
    interval_steps=1      # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì²´í¬
)
```

**ë™ì‘ ë°©ì‹:**
```
Trial 0: [ì—í¬í¬1: 0.30] [ì—í¬í¬2: 0.32] [ì—í¬í¬3: 0.35] â†’ ê³„ì†
Trial 1: [ì—í¬í¬1: 0.28] [ì—í¬í¬2: 0.29] [ì—í¬í¬3: 0.30] â†’ ê³„ì†
Trial 2: [ì—í¬í¬1: 0.25] [ì—í¬í¬2: 0.26] [ì—í¬í¬3: 0.27] â†’ Pruned! (median=0.32ë³´ë‹¤ ë‚®ìŒ)
```

---

### 3. ëª©ì  í•¨ìˆ˜ (Objective Function)

**ëª©í‘œ:** ROUGE-L F1 ì ìˆ˜ ìµœëŒ€í™”

**íë¦„:**
1. Trialì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
2. Config ì—…ë°ì´íŠ¸
3. ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ
4. ê²€ì¦ ë°ì´í„° í‰ê°€
5. ROUGE-L F1 ë°˜í™˜

**ì½”ë“œ:**
```python
def objective(self, trial: optuna.Trial) -> float:
    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    params = self.create_search_space(trial)

    # 2. Config ì—…ë°ì´íŠ¸
    config.training.learning_rate = params['learning_rate']
    config.training.batch_size = params['batch_size']
    # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    # 3. ëª¨ë¸ í•™ìŠµ
    model_loader = ModelLoader(config)
    model, tokenizer = model_loader.load()

    trainer = ModelTrainer(...)
    trainer.train()

    # 4. í‰ê°€
    metrics = trainer.evaluate()
    rouge_l_f1 = metrics['rouge_l_f1']

    # 5. Pruning ì²´í¬
    trial.report(rouge_l_f1, step=config.training.num_epochs)
    if trial.should_prune():
        raise optuna.TrialPruned()

    return rouge_l_f1
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ìµœì í™”

```python
from src.optimization import OptunaOptimizer
from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset

# Config ë¡œë“œ
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ë°ì´í„° ë¡œë“œ
train_df, val_df = load_and_preprocess_data("data/raw/train.csv", split_ratio=0.9)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model.name)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),
    train_df['summary'].tolist(),
    tokenizer,
    config
)

val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),
    val_df['summary'].tolist(),
    tokenizer,
    config
)

# Optimizer ì´ˆê¸°í™”
optimizer = OptunaOptimizer(
    config=config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    n_trials=50
)

# ìµœì í™” ì‹¤í–‰
study = optimizer.optimize()

# ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸
best_params = optimizer.get_best_params()
best_value = optimizer.get_best_value()

print(f"ìµœì  ROUGE-L F1: {best_value:.4f}")
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
```

---

### 2. ê²°ê³¼ ì €ì¥

```python
# ê²°ê³¼ ì €ì¥
optimizer.save_results("outputs/optuna_results")

# ì €ì¥ë˜ëŠ” íŒŒì¼:
# - outputs/optuna_results/best_params.json
# - outputs/optuna_results/all_trials.csv
# - outputs/optuna_results/study_stats.json
```

**best_params.json ì˜ˆì‹œ:**
```json
{
  "best_params": {
    "learning_rate": 3.5e-05,
    "batch_size": 32,
    "num_epochs": 5,
    "lora_r": 16,
    "lora_alpha": 32,
    "temperature": 0.8,
    "num_beams": 6
  },
  "best_value": 0.4521,
  "n_trials": 50
}
```

**study_stats.json ì˜ˆì‹œ:**
```json
{
  "study_name": "kobart_optuna",
  "n_trials": 50,
  "n_completed": 42,
  "n_pruned": 6,
  "n_failed": 2,
  "best_value": 0.4521,
  "best_trial_number": 37
}
```

---

### 3. ì‹œê°í™”

```python
# ì‹œê°í™” ìƒì„± (requires plotly)
optimizer.plot_optimization_history("outputs/optuna_plots")

# ìƒì„±ë˜ëŠ” íŒŒì¼:
# - optimization_history.html (ìµœì í™” íˆìŠ¤í† ë¦¬)
# - param_importances.html (íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„)
# - parallel_coordinate.html (ë³‘ë ¬ ì¢Œí‘œ í”Œë¡¯)
```

---

### 4. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì¬í•™ìŠµ

```python
# ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ
import json
with open("outputs/optuna_results/best_params.json", 'r') as f:
    data = json.load(f)
    best_params = data['best_params']

# Config ì—…ë°ì´íŠ¸
config.training.learning_rate = best_params['learning_rate']
config.training.batch_size = best_params['batch_size']
config.training.num_epochs = best_params['num_epochs']
config.generation.temperature = best_params['temperature']
config.generation.num_beams = best_params['num_beams']

if 'lora_r' in best_params:
    config.lora.r = best_params['lora_r']
    config.lora.alpha = best_params['lora_alpha']

# ìµœì¢… í•™ìŠµ
from src.training import ModelTrainer
trainer = ModelTrainer(...)
trainer.train()
```

---

## ğŸ”§ ì‹¤í–‰ ëª…ë ¹ì–´

### Optuna ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ)

**íŒŒì¼:** `scripts/optimize.py`

```python
import argparse
from pathlib import Path

from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset
from src.optimization import OptunaOptimizer
from transformers import AutoTokenizer


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", default="baseline_kobart", help="ì‹¤í—˜ config ì´ë¦„")
    parser.add_argument("--n_trials", type=int, default=50, help="Trial íšŸìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--output_dir", default="outputs/optuna_results", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()

    # Config ë¡œë“œ
    config_loader = ConfigLoader()
    config = config_loader.load(args.experiment)

    # ë°ì´í„° ë¡œë“œ
    train_df, val_df = load_and_preprocess_data("data/raw/train.csv", split_ratio=0.9)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = DialogueSummarizationDataset(
        train_df['dialogue'].tolist(),
        train_df['summary'].tolist(),
        tokenizer,
        config
    )

    val_dataset = DialogueSummarizationDataset(
        val_df['dialogue'].tolist(),
        val_df['summary'].tolist(),
        tokenizer,
        config
    )

    # Optimizer ì´ˆê¸°í™”
    optimizer = OptunaOptimizer(
        config=config,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        n_trials=args.n_trials,
        timeout=args.timeout,
        study_name=f"optuna_{args.experiment}"
    )

    # ìµœì í™” ì‹¤í–‰
    study = optimizer.optimize()

    # ê²°ê³¼ ì €ì¥
    optimizer.save_results(args.output_dir)

    # ì‹œê°í™”
    try:
        optimizer.plot_optimization_history(args.output_dir)
    except ImportError:
        print("plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

    print(f"\n{'='*60}")
    print(f"ìµœì í™” ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ìµœì  ROUGE-L F1: {optimizer.get_best_value():.4f}")
    print(f"ê²°ê³¼ ì €ì¥: {args.output_dir}")


if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
# ê¸°ë³¸ ì‹¤í–‰ (50 trials)
python scripts/optimize.py --experiment baseline_kobart

# Trial íšŸìˆ˜ ì¡°ì •
python scripts/optimize.py --experiment baseline_kobart --n_trials 100

# ì‹œê°„ ì œí•œ (12ì‹œê°„ = 43200ì´ˆ)
python scripts/optimize.py --experiment baseline_kobart --timeout 43200

# ê²°ê³¼ ë””ë ‰í† ë¦¬ ì§€ì •
python scripts/optimize.py --experiment baseline_kobart --output_dir outputs/kobart_optuna
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜
```
src/tests/test_optuna.py
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python src/tests/test_optuna.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 7ê°œ)

1. âœ… OptunaOptimizer ì´ˆê¸°í™”
2. âœ… íƒìƒ‰ ê³µê°„ ìƒì„±
3. âœ… íƒìƒ‰ ê³µê°„ ë²”ìœ„ ê²€ì¦
4. âœ… Sampler ë° Pruner ì„¤ì •
5. âœ… Best params ë©”ì„œë“œ
6. âœ… ê²°ê³¼ ì €ì¥
7. âœ… create_optuna_optimizer í•¨ìˆ˜

**ê²°ê³¼:** 7/7 í…ŒìŠ¤íŠ¸ í†µê³¼ (100%)

**ì°¸ê³ :** ì‹¤ì œ optimize() í…ŒìŠ¤íŠ¸ëŠ” ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì´ í•„ìš”

---

## ğŸ“Š ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„

### í•˜ë“œì›¨ì–´ë³„ ì˜ˆìƒ ì‹œê°„ (KoBART ê¸°ì¤€)

| í•˜ë“œì›¨ì–´ | Trialë‹¹ ì‹œê°„ | 50 trials | 100 trials |
|---------|-------------|-----------|------------|
| A6000 (48GB) | 20-30ë¶„ | 16-25ì‹œê°„ | 33-50ì‹œê°„ |
| A100 (80GB) | 15-20ë¶„ | 12-16ì‹œê°„ | 25-33ì‹œê°„ |
| V100 (32GB) | 30-40ë¶„ | 25-33ì‹œê°„ | 50-66ì‹œê°„ |

**íŒ:**
- ì´ˆë°˜ 10-20 trialsë¡œ ê²½í–¥ íŒŒì•… í›„ ê²°ì •
- Pruningì´ íš¨ê³¼ì ì´ë©´ ì‹œê°„ ë‹¨ì¶• (ì•½ 20-30%)
- ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸ ê¶Œì¥

---

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### 1. Study ì €ì¥ì†Œ (RDB)

**SQLite ì‚¬ìš©:**
```python
optimizer = OptunaOptimizer(
    config=config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    n_trials=50,
    storage="sqlite:///optuna_study.db",  # SQLite DBì— ì €ì¥
    study_name="kobart_optuna"
)
```

**PostgreSQL ì‚¬ìš©:**
```python
storage = "postgresql://user:password@localhost:5432/optuna"
optimizer = OptunaOptimizer(..., storage=storage)
```

**ì¥ì :**
- ì¤‘ë‹¨ í›„ ì¬ê°œ ê°€ëŠ¥
- ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œ ìµœì í™”
- ê²°ê³¼ ì˜êµ¬ ë³´ì¡´

---

### 2. ë‹¤ì¤‘ ëª©ì  ìµœì í™”

**ROUGE-1, ROUGE-2, ROUGE-L ë™ì‹œ ìµœì í™”:**

```python
def objective(trial):
    # ... í•™ìŠµ ë° í‰ê°€

    # ë‹¤ì¤‘ ëª©ì  ë°˜í™˜
    return metrics['rouge_1_f1'], metrics['rouge_2_f1'], metrics['rouge_l_f1']

# Multi-objective study
study = optuna.create_study(
    directions=["maximize", "maximize", "maximize"]
)
```

---

### 3. ì¡°ê±´ë¶€ íƒìƒ‰ ê³µê°„

**ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ íƒìƒ‰ ê³µê°„:**

```python
def create_search_space(self, trial):
    params = {}

    # ê³µí†µ íŒŒë¼ë¯¸í„°
    params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)

    # KoBART ì „ìš©
    if 'bart' in self.config.model.name.lower():
        params['encoder_layers'] = trial.suggest_int('encoder_layers', 6, 12)
        params['decoder_layers'] = trial.suggest_int('decoder_layers', 6, 12)

    # LLM ì „ìš©
    elif 'llama' in self.config.model.name.lower() or 'qwen' in self.config.model.name.lower():
        params['lora_r'] = trial.suggest_categorical('lora_r', [8, 16, 32, 64])
        params['lora_alpha'] = trial.suggest_categorical('lora_alpha', [16, 32, 64, 128])

    return params
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

**ë¬¸ì œ:** Trialë§ˆë‹¤ ëª¨ë¸ ë¡œë“œë¡œ ë©”ëª¨ë¦¬ ëˆ„ì 

**í•´ê²°:**
```python
def objective(trial):
    try:
        # ... í•™ìŠµ
        return rouge_l_f1
    finally:
        # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
        import torch
        torch.cuda.empty_cache()

        # ëª¨ë¸ ì‚­ì œ
        del model
        del trainer
```

---

### 2. WandB ë¹„í™œì„±í™”

**ì´ìœ :** ìˆ˜ì‹­ ê°œ trialì´ WandBì— ë¡œê·¸ë˜ë©´ ê´€ë¦¬ ì–´ë ¤ì›€

**ì„¤ì •:**
```python
# objective í•¨ìˆ˜ ë‚´ë¶€
config.logging.use_wandb = False
```

---

### 3. ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€

**ë„ˆë¬´ ê³µê²©ì ì¸ Pruning:**
```python
pruner = MedianPruner(
    n_startup_trials=2,   # ë„ˆë¬´ ì ìŒ
    n_warmup_steps=1      # ë„ˆë¬´ ë¹ ë¦„
)
# â†’ ì¢‹ì€ trialë„ ì¡°ê¸° ì¢…ë£Œë  ìˆ˜ ìˆìŒ
```

**ê¶Œì¥ ì„¤ì •:**
```python
pruner = MedianPruner(
    n_startup_trials=5,   # ì¶©ë¶„í•œ ì´ˆê¸° trial
    n_warmup_steps=3      # ì¶©ë¶„í•œ warmup
)
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/optimization/optuna_optimizer.py` - Optuna optimizer
- `src/optimization/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_optuna.py` - Optuna í…ŒìŠ¤íŠ¸

**ë¬¸ì„œ:**
- `docs/PRD/13_Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°_ìµœì í™”.md` - PRD ë¬¸ì„œ
- `docs/ëª¨ë“ˆí™”/00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md` - ì‹œìŠ¤í…œ ê°œìš”

**Config:**
- `configs/base/default.yaml` - ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `configs/experiments/*.yaml` - ì‹¤í—˜ë³„ Config

---

## ğŸ“š ì°¸ê³  ìë£Œ

**Optuna ê³µì‹ ë¬¸ì„œ:**
- https://optuna.readthedocs.io/
- Sampler: https://optuna.readthedocs.io/en/stable/reference/samplers.html
- Pruner: https://optuna.readthedocs.io/en/stable/reference/pruners.html

**TPE ë…¼ë¬¸:**
- Bergstra et al. (2011) "Algorithms for Hyper-Parameter Optimization"

**ì‹¤ì „ íŒ:**
- https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html
