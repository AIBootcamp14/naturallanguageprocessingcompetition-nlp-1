# KoBART ë‹¨ì¼ ëª¨ë¸ ìµœê°• ì„±ëŠ¥ ì „ëµ (ë§‰íŒ ìŠ¤í¼íŠ¸)

> **ëª©ì **: ê²½ì§„ëŒ€íšŒ ë§‰íŒ í•˜ë£¨ ë‚¨ì€ ìƒí™©ì—ì„œ KoBART ë‹¨ì¼ ëª¨ë¸ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
> **ì‘ì„±ì¼**: 2025-10-14
> **ì „ëµ**: ë¹ ë¥¸ í•™ìŠµ + ë°ì´í„°/í”„ë¡¬í”„íŠ¸/ì¶”ë¡  ê³ ë„í™”

---

## ğŸ“‹ ëª©ì°¨

1. [ì „ëµ ê°œìš”](#1-ì „ëµ-ê°œìš”)
2. [í•´ê²°ëœ ë¬¸ì œì  í™•ì¸](#2-í•´ê²°ëœ-ë¬¸ì œì -í™•ì¸)
3. [ìµœê°• ì„±ëŠ¥ ëª…ë ¹ì–´ ì¡°í•©](#3-ìµœê°•-ì„±ëŠ¥-ëª…ë ¹ì–´-ì¡°í•©)
4. [ì¶”ë¡  ì‹œ ì„±ëŠ¥ í–¥ìƒ ì „ëµ](#4-ì¶”ë¡ -ì‹œ-ì„±ëŠ¥-í–¥ìƒ-ì „ëµ)
5. [ë¹ ë¥¸ ì‹¤í–‰ ëª…ë ¹ì–´](#5-ë¹ ë¥¸-ì‹¤í–‰-ëª…ë ¹ì–´)
6. [ì˜ˆìƒ ì„±ëŠ¥ ë° ì‹œê°„](#6-ì˜ˆìƒ-ì„±ëŠ¥-ë°-ì‹œê°„)

---

## 1. ì „ëµ ê°œìš”

### 1.1 í•µì‹¬ ì „ëµ

```mermaid
graph TB
    A[KoBART ë‹¨ì¼ ëª¨ë¸] --> B[ê³ ë„í™” í•™ìŠµ]
    B --> C[ë°ì´í„° ì¦ê°• 50%]
    B --> D[K-Fold êµì°¨ê²€ì¦]
    B --> E[Optuna ìµœì í™”]

    A --> F[ê³ ë„í™” ì¶”ë¡ ]
    F --> G[Solar API ì•™ìƒë¸”]
    F --> H[ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •]
    F --> I[í›„ì²˜ë¦¬ ê°•í™”]

    C --> J[ìµœê³  ì„±ëŠ¥]
    D --> J
    E --> J
    G --> J
    H --> J
    I --> J

    style A fill:#81c784,stroke:#2e7d32,color:#000
    style B fill:#a5d6a7,stroke:#388e3c,color:#000
    style F fill:#a5d6a7,stroke:#388e3c,color:#000
    style J fill:#66bb6a,stroke:#1b5e20,color:#fff
```

### 1.2 ì‹œê°„ ë°°ë¶„ ì „ëµ

| ë‹¨ê³„ | ì†Œìš” ì‹œê°„ | ë¹„ê³  |
|------|----------|------|
| **ì „ëµ 1: ì ˆëŒ€ ìµœê³ ** | 12-15ì‹œê°„ | Optuna 20 trials (íš¨ìœ¨ì  íƒìƒ‰, 100â†’20) |
| **ì „ëµ 2: ê· í˜•** | 3-4ì‹œê°„ | K-Fold 5 + Epoch 7 + GradAcc 10 (15â†’7 epochs) |
| **ì „ëµ 3: ë¹ ë¥¸ ê³ ì„±ëŠ¥** | 1.5-2ì‹œê°„ | K-Fold 3 + Epoch 7 + GradAcc 10 (10â†’7 epochs) |
| **ì „ëµ 4: ì´ˆê³ ì†** | 30-45ë¶„ | Single + Epoch 5 + GradAcc 10 |

---

## 2. í•´ê²°ëœ ë¬¸ì œì  í™•ì¸

### 2.1 docs/issues/ í•´ê²° ì‚¬í•­

| ë¬¸ì œ | í•´ê²° ë°©ë²• | ëª…ë ¹ì–´ ë°˜ì˜ |
|------|----------|------------|
| âœ… gradient_accumulation_steps ê³¼ë‹¤ | ê¸°ë³¸ê°’ 1ë¡œ ì„¤ì • ì™„ë£Œ | `--gradient_accumulation_steps 10` (ìµœê³  ì„±ëŠ¥ìš©) |
| âœ… ë°ì´í„° ì¦ê°• 30% â†’ 50% | ê¸°ë³¸ê°’ 0.5 ì„¤ì • | `--augmentation_ratio 0.5` |
| âœ… ì—­ë²ˆì—­ ìš°ì„  ì‚¬ìš© | back_translation ì¶”ê°€ | `--augmentation_methods back_translation paraphrase` |
| âœ… ë¬¸ì¥ ëŠê¹€ (99.6% í•´ê²°) | í›„ì²˜ë¦¬ í•¨ìˆ˜ ê°•í™” | ìë™ ì ìš© |
| âœ… Decoder-only padding | left-padding ì„¤ì • | ìë™ ì ìš© |
| âœ… max_new_tokens | 100ìœ¼ë¡œ ìµœì í™” | `--max_new_tokens 100` (í•œêµ­ì–´ ìš”ì•½ ìµœì ) |
| âœ… Full Fine-tuning | ì˜µì…˜ ì¶”ê°€ | `--use_full_finetuning` (ì„ íƒ) |

### 2.2 ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ

**gradient_accumulation_steps**:
- ê¸°ë³¸ê°’ 1: ë¹ ë¥¸ í•™ìŠµ (ì¼ë°˜ì )
- ì„±ëŠ¥ìš© 10: íš¨ê³¼ì  ë°°ì¹˜ 160 (16Ã—10) â†’ ë§¤ìš° ì•ˆì •ì  í•™ìŠµ, ì¼ë°˜í™” ëŠ¥ë ¥ ëŒ€í­ í–¥ìƒ

**k_folds**:
- 2-3: ë¹ ë¥¸ ì‹¤í–‰
- 5-10: ê³¼ì í•© ë°©ì§€, êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì  ì„±ëŠ¥

**epochs**:
- 2-5: ë¹ ë¥¸ ì‹¤í–‰
- 20-30: ì¶©ë¶„í•œ í•™ìŠµ (Early Stopping í•„ìˆ˜)

**optuna**:
- ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰
- learning_rate, batch_size, gradient_accumulation_steps ë“± ìµœì í™”

---

## 3. ìµœê°• ì„±ëŠ¥ ëª…ë ¹ì–´ ì¡°í•©

### 3.1 ì „ëµ 1: ì ˆëŒ€ ìµœê³  ì„±ëŠ¥ (Optuna + K-Fold + Full)

#### ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

```mermaid
graph TB
    subgraph Input["ì…ë ¥ ê³„ì¸µ"]
        A[ëª…ë ¹ì–´ ì‹¤í–‰<br/>--mode optuna] --> B[Config ë¡œë“œ<br/>kobart.yaml]
        A1[í•™ìŠµ ë°ì´í„°<br/>train.csv] --> C[ë°ì´í„° ë¡œë“œ]
    end

    subgraph DataProcess["ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"]
        C --> D[ë°ì´í„° ì¦ê°• 50%<br/>back_translation + paraphrase]
        D --> E[Train/Eval ë¶„í• ]
    end

    subgraph Optimization["Optuna ìµœì í™” ê³„ì¸µ"]
        B --> F[OptunaOptimizer ì´ˆê¸°í™”<br/>100 trials]
        E --> F
        F --> G[Trial 1~100 ë°˜ë³µ]
        G --> H{ê° Trialë§ˆë‹¤}
        H --> I[í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§<br/>learning_rate, epochs, warmup_ratio<br/>weight_decay, scheduler_type<br/>num_beams, length_penalty]
        I --> J[ëª¨ë¸ ë¡œë“œ<br/>digit82/kobart-summarization]
        J --> K[Dataset ìƒì„±<br/>encoder_max_len=512<br/>decoder_max_len=128]
        K --> L[Trainer ìƒì„±<br/>Seq2SeqTrainer]
        L --> M[í•™ìŠµ ì‹¤í–‰<br/>Epoch 30 + Early Stopping]
        M --> N[í‰ê°€ ROUGE-L F1]
        N --> O{ROUGE-L F1<br/>ìµœê³  ì ìˆ˜?}
        O -->|Yes| P[ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥]
        O -->|No| Q[ë‹¤ìŒ Trial]
        Q --> G
        P --> G
    end

    subgraph Results["ê²°ê³¼ ì €ì¥ ê³„ì¸µ"]
        G --> R[ìµœì í™” ì™„ë£Œ]
        R --> S[best_params.json ì €ì¥<br/>learning_rate, epochs, etc.]
        R --> T[all_trials.csv ì €ì¥<br/>100ê°œ trial ê²°ê³¼]
        R --> U[study_stats.json ì €ì¥<br/>ì™„ë£Œ/Pruned/ì‹¤íŒ¨ í†µê³„]
        R --> V[ì‹œê°í™” ìƒì„±<br/>optimization_history.html<br/>param_importances.html]
    end

    subgraph Warning["ì¤‘ìš” ì •ë³´"]
        W[OptunaëŠ” ìµœì  íŒŒë¼ë¯¸í„°ë§Œ ì°¾ìŒ<br/>K-FoldëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ<br/>ë³„ë„ë¡œ kfold ëª¨ë“œ ì‹¤í–‰ í•„ìš”]
    end

    style Input fill:#e1f5ff,stroke:#01579b,color:#000
    style DataProcess fill:#fff3e0,stroke:#e65100,color:#000
    style Optimization fill:#e8f5e9,stroke:#1b5e20,color:#000
    style Results fill:#c8e6c9,stroke:#2e7d32,color:#000
    style Warning fill:#ffebee,stroke:#c62828,color:#000

    style A fill:#90caf9,stroke:#1976d2,color:#000
    style A1 fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#90caf9,stroke:#1976d2,color:#000
    style C fill:#ffcc80,stroke:#f57c00,color:#000
    style D fill:#ffcc80,stroke:#f57c00,color:#000
    style E fill:#ffcc80,stroke:#f57c00,color:#000
    style F fill:#81c784,stroke:#388e3c,color:#000
    style G fill:#81c784,stroke:#388e3c,color:#000
    style H fill:#81c784,stroke:#388e3c,color:#000
    style I fill:#a5d6a7,stroke:#388e3c,color:#000
    style J fill:#a5d6a7,stroke:#388e3c,color:#000
    style K fill:#ffcc80,stroke:#f57c00,color:#000
    style L fill:#81c784,stroke:#388e3c,color:#000
    style M fill:#81c784,stroke:#388e3c,color:#000
    style N fill:#ffab91,stroke:#e64a19,color:#000
    style O fill:#fff59d,stroke:#f9a825,color:#000
    style P fill:#66bb6a,stroke:#2e7d32,color:#fff
    style Q fill:#90caf9,stroke:#1976d2,color:#000
    style R fill:#66bb6a,stroke:#2e7d32,color:#fff
    style S fill:#ce93d8,stroke:#7b1fa2,color:#000
    style T fill:#ce93d8,stroke:#7b1fa2,color:#000
    style U fill:#ce93d8,stroke:#7b1fa2,color:#000
    style V fill:#ce93d8,stroke:#7b1fa2,color:#000
    style W fill:#ef9a9a,stroke:#c62828,color:#000
```

#### ì‹œë‚˜ë¦¬ì˜¤
1. Optunaë¡œ 100íšŒ ì‹œí–‰í•˜ì—¬ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
2. ì°¾ì€ íŒŒë¼ë¯¸í„°ë¡œ K-Fold 5ë¡œ êµì°¨ê²€ì¦ (ë³„ë„ ì‹¤í–‰)
3. Epoch 30 + Early Stoppingìœ¼ë¡œ ì¶©ë¶„í•œ í•™ìŠµ
4. ë°ì´í„° ì¦ê°• 50% (back_translation + paraphrase)
5. Full Fine-tuning (LoRA ëŒ€ì‹  ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ)

#### ëª…ë ¹ì–´ ì˜µì…˜ ì„¤ëª…

| ì˜µì…˜ | ê°’ | ê¸°ëŠ¥/ì„±ëŠ¥ | ê·¼ê±° |
|------|-----|----------|------|
| `--mode` | optuna | Optuna ìµœì í™” ëª¨ë“œ | ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰ |
| `--models` | kobart | KoBART ë‹¨ì¼ ëª¨ë¸ | ì†ë„(99ì´ˆ) Ã— ì„±ëŠ¥(1.048) ìµœê³  |
| `--optuna_trials` | 20 | Optuna ì‹œí–‰ íšŸìˆ˜ | 20íšŒ íš¨ìœ¨ì  íƒìƒ‰ (100â†’20, Trial 11ì—ì„œ ìµœì ê°’ ë°œê²¬) |
| `--epochs` | 7 | í•™ìŠµ ì—í­ | Optuna ìµœì ê°’ (30â†’7, ì‹œê°„ 76.7% ë‹¨ì¶•) |
| `--batch_size` | 16 | ë°°ì¹˜ í¬ê¸° | GPU ë©”ëª¨ë¦¬ ìµœì  í™œìš© |
| `--gradient_accumulation_steps` | 10 | ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  | íš¨ê³¼ì  ë°°ì¹˜ 160 (16Ã—10) |
| `--learning_rate` | 9.14e-5 | í•™ìŠµë¥  | Optuna ìµœì ê°’ (5e-5â†’9.14e-5, ì•½ 1.8ë°°) |
| `--warmup_ratio` | 0.00136 | Warmup ë¹„ìœ¨ | Optuna ìµœì ê°’ (0.1â†’0.00136, ê±°ì˜ ë¶ˆí•„ìš”) |
| `--weight_decay` | 0.0995 | ê°€ì¤‘ì¹˜ ê°ì‡  | Optuna ìµœì ê°’ (0.01â†’0.0995, ì•½ 10ë°°) |
| `--max_grad_norm` | 1.0 | ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ | í•™ìŠµ ì•ˆì •í™” |
| `--label_smoothing` | 0.1 | ë ˆì´ë¸” ìŠ¤ë¬´ë”© | ê³¼ì í•© ë°©ì§€ |
| `--use_augmentation` | - | ë°ì´í„° ì¦ê°• í™œì„±í™” | ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒ |
| `--augmentation_ratio` | 0.5 | ì¦ê°• ë¹„ìœ¨ 50% | ë©˜í†  ê¶Œì¥ |
| `--augmentation_methods` | back_translation paraphrase | ì¦ê°• ë°©ë²• | ì—­ë²ˆì—­(ìš°ìˆ˜) + ì˜ì—­(ê´œì°®ìŒ) |
| `--k_folds` | 5 | K-Fold êµì°¨ê²€ì¦ | ê³¼ì í•© ë°©ì§€, ì•ˆì •ì  ì„±ëŠ¥ |
| `--fold_seed` | 42 | Fold ì‹œë“œ | ì¬í˜„ ê°€ëŠ¥ì„± |
| `--max_new_tokens` | 100 | ìƒì„± ìµœëŒ€ í† í° | í•œêµ­ì–´ ìš”ì•½ ìµœì  ê¸¸ì´ |
| `--min_new_tokens` | 30 | ìƒì„± ìµœì†Œ í† í° | ë„ˆë¬´ ì§§ì€ ìš”ì•½ ë°©ì§€ |
| `--num_beams` | 4 | Beam Search | Optuna ìµœì ê°’ (5â†’4, ì†ë„â†‘ í’ˆì§ˆ ìœ ì§€) |
| `--repetition_penalty` | 1.5 | ë°˜ë³µ ì–µì œ | ë°˜ë³µ ë¬¸ì¥ ê°•ë ¥ ë°©ì§€ |
| `--length_penalty` | 0.938 | ê¸¸ì´ í˜ë„í‹° | Optuna ìµœì ê°’ (1.0â†’0.938, ì•½ê°„ ì§§ê²Œ) |
| `--no_repeat_ngram_size` | 3 | N-gram ë°˜ë³µ ê¸ˆì§€ | 3-gram ë°˜ë³µ ë°©ì§€ |
| `--use_solar_api` | - | Solar API í†µí•© | ê³ í’ˆì§ˆ ë²ˆì—­/ìš”ì•½ ë³´ì • |
| `--use_pretrained_correction` | - | HuggingFace ë³´ì • í™œì„±í™” | ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • (PRD 04, 12) |
| `--correction_models` | gogamza/kobart-base-v2 digit82/kobart-summarization | HF ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ | ë³´ì •ìš© ì‚¬ì „í•™ìŠµ ëª¨ë¸ |
| `--correction_strategy` | quality_based | ë³´ì • ì „ëµ | í’ˆì§ˆ ê¸°ë°˜ ì„ íƒ (ìµœì ) |
| `--correction_threshold` | 0.3 | í’ˆì§ˆ ì„ê³„ê°’ | ë‚®ì„ìˆ˜ë¡ ì—„ê²©í•œ ë³´ì • |
| `--save_visualizations` | - | ì‹œê°í™” ì €ì¥ | í•™ìŠµ ê³¼ì • ë¶„ì„ |
| `--experiment_name` | kobart_ultimate | ì‹¤í—˜ ì´ë¦„ | ê²°ê³¼ ì¶”ì  |
| `--seed` | 42 | ëœë¤ ì‹œë“œ | ì¬í˜„ ê°€ëŠ¥ì„± |

#### ì‚¬ìš© ê¸°ìˆ /ì „ëµ
- âœ… **Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: ìë™ìœ¼ë¡œ ìµœì  ê°’ íƒìƒ‰
- âœ… **K-Fold êµì°¨ê²€ì¦ (5-Fold)**: ê³¼ì í•© ë°©ì§€, ì•ˆì •ì  ì¼ë°˜í™”
- âœ… **ë°ì´í„° ì¦ê°• 50%**: ì—­ë²ˆì—­ + ì˜ì—­ìœ¼ë¡œ ë°ì´í„° í’ë¶€í™”
- âœ… **Gradient Accumulation (10)**: íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° 160
- âœ… **Early Stopping**: ê³¼ì í•© ë°©ì§€
- âœ… **Label Smoothing**: ê³¼ì‹  ë°©ì§€
- âœ… **Solar API í†µí•©**: ì¶”ë¡  ì‹œ ê³ í’ˆì§ˆ ë³´ì •
- âœ… **HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •**: quality_based ì „ëµìœ¼ë¡œ ìš”ì•½ í’ˆì§ˆ í–¥ìƒ
- âœ… **ê°•í™”ëœ í›„ì²˜ë¦¬**: 99.6% ì™„ì „í•œ ë¬¸ì¥ ìƒì„±

#### ìµœê³  ì„±ëŠ¥ ëª…ë ¹ì–´

```bash
# ==================== ì „ëµ 1: ì ˆëŒ€ ìµœê³  ì„±ëŠ¥ (Optuna ìµœì í™” ë°˜ì˜) ==================== #
# âœ… ì²´í¬í¬ì¸íŠ¸ Resume ì§€ì›: ì¤‘ë‹¨ ì‹œ --resume ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
python scripts/train.py \
  --mode optuna \
  --models kobart \
  --optuna_trials 20 \
  --optuna_timeout 10800 \
  --epochs 7 \
  --batch_size 16 \
  --gradient_accumulation_steps 10 \
  --learning_rate 9.14e-5 \
  --warmup_ratio 0.00136 \
  --weight_decay 0.0995 \
  --scheduler_type cosine \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --use_augmentation \
  --augmentation_ratio 0.5 \
  --augmentation_methods back_translation paraphrase \
  --k_folds 5 \
  --fold_seed 42 \
  --max_new_tokens 100 \
  --min_new_tokens 30 \
  --num_beams 4 \
  --repetition_penalty 1.5 \
  --length_penalty 0.938 \
  --no_repeat_ngram_size 3 \
  --use_solar_api \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --save_visualizations \
  --experiment_name kobart_ultimate \
  --seed 42 \
  --resume  # âœ… ì¤‘ë‹¨ í›„ ì´ì–´ì„œ ì‹¤í–‰ (ì™„ë£Œëœ Trial ìë™ ê±´ë„ˆë›°ê¸°)

# ì˜ˆìƒ ì‹œê°„: 12-15ì‹œê°„ (Optuna 20 trials, ê¸°ì¡´ ëŒ€ë¹„ 50% ë‹¨ì¶•)
# ì˜ˆìƒ ROUGE Sum: 1.30-1.42 (í˜„ì¬ 1.048 â†’ +24-35%, ìµœì í™” ë°˜ì˜ + HuggingFace ë³´ì •)
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: Trial ì™„ë£Œë§ˆë‹¤ ìë™ ì €ì¥, ì¤‘ë‹¨ ì‹œ ì™„ë£Œëœ Trialë¶€í„° Resume ê°€ëŠ¥
```

---

### 3.2 ì „ëµ 2: ê· í˜•ì¡íŒ ê³ ì„±ëŠ¥ (K-Fold + ì¤‘ê°„ Epoch)

#### ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

```mermaid
graph TB
    subgraph Input["ì…ë ¥ ê³„ì¸µ"]
        A[ëª…ë ¹ì–´ ì‹¤í–‰<br/>--mode kfold] --> B[Config ë¡œë“œ<br/>kobart.yaml]
        A1[í•™ìŠµ ë°ì´í„°<br/>train.csv] --> C[ë°ì´í„° ë¡œë“œ]
    end

    subgraph DataProcess["ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"]
        C --> D[ë°ì´í„° ì¦ê°• 50%<br/>back_translation + paraphrase]
        D --> E[K-Fold ë¶„í• <br/>5-Fold, seed=42]
    end

    subgraph Training["K-Fold í•™ìŠµ ê³„ì¸µ (Fold 1~5 ë°˜ë³µ)"]
        B --> F[Fold 1/5 ì‹œì‘]
        E --> F
        F --> G[Train/Val ë¶„í• ]
        G --> H[ëª¨ë¸ ë¡œë“œ<br/>digit82/kobart-summarization]
        H --> I[Dataset ìƒì„±<br/>encoder_max_len=512<br/>decoder_max_len=128]
        I --> J[Trainer ìƒì„±<br/>batch_size=16<br/>grad_acc_steps=10<br/>effective_batch=160]
        J --> K[í•™ìŠµ ì‹¤í–‰<br/>Epoch 15 + Early Stopping]
        K --> L[í‰ê°€ ROUGE]
        L --> M[ì²´í¬í¬ì¸íŠ¸ ì €ì¥<br/>fold_1/checkpoint-best]
        M --> N{ë‹¤ìŒ Fold?}
        N -->|Yes| O[Fold 2/5 ì‹œì‘]
        O --> G
        N -->|No| P[ì•™ìƒë¸” ì¤€ë¹„]
    end

    subgraph Ensemble["ì•™ìƒë¸” ê³„ì¸µ"]
        P --> Q[Fold 1~5 ëª¨ë¸ ë¡œë“œ]
        Q --> R[Test ë°ì´í„° ì¶”ë¡ <br/>ê° Fold ì˜ˆì¸¡]
        R --> S[Soft Voting<br/>í‰ê·  í™•ë¥  ê¸°ë°˜ ì„ íƒ]
    end

    subgraph Inference["ì¶”ë¡  ê³ ë„í™” ê³„ì¸µ"]
        S --> T[HuggingFace ë³´ì •<br/>gogamza/kobart-base-v2<br/>digit82/kobart-summarization<br/>quality_based ì „ëµ]
        T --> U[Solar API ì•™ìƒë¸”<br/>solar-1-mini-chat<br/>ë°°ì¹˜ ì²˜ë¦¬]
        U --> V[í›„ì²˜ë¦¬<br/>99.6% ì™„ì „í•œ ë¬¸ì¥]
    end

    subgraph Results["ê²°ê³¼ ì €ì¥ ê³„ì¸µ"]
        V --> W[submission.csv ìƒì„±<br/>ID, summary]
        L --> X[ë¡œê·¸ ì €ì¥<br/>train.log, metrics.json]
    end

    style Input fill:#e1f5ff,stroke:#01579b,color:#000
    style DataProcess fill:#fff3e0,stroke:#e65100,color:#000
    style Training fill:#e8f5e9,stroke:#1b5e20,color:#000
    style Ensemble fill:#f3e5f5,stroke:#4a148c,color:#000
    style Inference fill:#e0f7fa,stroke:#006064,color:#000
    style Results fill:#f3e5f5,stroke:#4a148c,color:#000

    style A fill:#90caf9,stroke:#1976d2,color:#000
    style A1 fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#90caf9,stroke:#1976d2,color:#000
    style C fill:#ffcc80,stroke:#f57c00,color:#000
    style D fill:#ffcc80,stroke:#f57c00,color:#000
    style E fill:#ffcc80,stroke:#f57c00,color:#000
    style F fill:#81c784,stroke:#388e3c,color:#000
    style G fill:#ffcc80,stroke:#f57c00,color:#000
    style H fill:#a5d6a7,stroke:#388e3c,color:#000
    style I fill:#ffcc80,stroke:#f57c00,color:#000
    style J fill:#81c784,stroke:#388e3c,color:#000
    style K fill:#81c784,stroke:#388e3c,color:#000
    style L fill:#ffab91,stroke:#e64a19,color:#000
    style M fill:#ce93d8,stroke:#7b1fa2,color:#000
    style N fill:#fff59d,stroke:#f9a825,color:#000
    style O fill:#81c784,stroke:#388e3c,color:#000
    style P fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style Q fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style R fill:#ce93d8,stroke:#7b1fa2,color:#000
    style S fill:#ce93d8,stroke:#7b1fa2,color:#000
    style T fill:#81d4fa,stroke:#0288d1,color:#000
    style U fill:#81d4fa,stroke:#0288d1,color:#000
    style V fill:#ffcc80,stroke:#f57c00,color:#000
    style W fill:#66bb6a,stroke:#2e7d32,color:#fff
    style X fill:#ce93d8,stroke:#7b1fa2,color:#000
```

#### ì‹œë‚˜ë¦¬ì˜¤
1. K-Fold 5ë¡œ êµì°¨ê²€ì¦
2. Epoch 15 + Early Stopping
3. ë°ì´í„° ì¦ê°• 50%
4. Gradient Accumulation 10

#### ëª…ë ¹ì–´ ì˜µì…˜ ì„¤ëª…

| ì˜µì…˜ | ê°’ | ë³€ê²½ ì´ìœ  | ì „ëµ 1 ëŒ€ë¹„ |
|------|-----|----------|------------|
| `--mode` | kfold | K-Fold êµì°¨ê²€ì¦ | Optuna ì œì™¸ (ì‹œê°„ ë‹¨ì¶•) |
| `--epochs` | 7 | Optuna ìµœì ê°’ ì ìš© | 7 ìœ ì§€ (ìµœì  í•™ìŠµëŸ‰) |
| `--gradient_accumulation_steps` | 10 | ìµœê³  ë°°ì¹˜ íš¨ê³¼ | ë™ì¼ ìœ ì§€ (ì„±ëŠ¥ ìš°ì„ ) |

#### ê· í˜• ì„±ëŠ¥ ëª…ë ¹ì–´

```bash
# ==================== ì „ëµ 2: ê· í˜•ì¡íŒ ê³ ì„±ëŠ¥ (Optuna ìµœì í™” ë°˜ì˜) ==================== #
# âœ… ì²´í¬í¬ì¸íŠ¸ Resume ì§€ì›: ì¤‘ë‹¨ ì‹œ --resume ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
python scripts/train.py \
  --mode kfold \
  --models kobart \
  --epochs 7 \
  --batch_size 16 \
  --gradient_accumulation_steps 10 \
  --learning_rate 9.14e-5 \
  --warmup_ratio 0.00136 \
  --weight_decay 0.0995 \
  --scheduler_type cosine \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --use_augmentation \
  --augmentation_ratio 0.5 \
  --augmentation_methods back_translation paraphrase \
  --k_folds 5 \
  --fold_seed 42 \
  --max_new_tokens 100 \
  --min_new_tokens 30 \
  --num_beams 4 \
  --repetition_penalty 1.5 \
  --length_penalty 0.938 \
  --no_repeat_ngram_size 3 \
  --use_solar_api \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --experiment_name kobart_balanced \
  --seed 42 \
  --resume  # âœ… ì¤‘ë‹¨ í›„ ì´ì–´ì„œ ì‹¤í–‰ (ì™„ë£Œëœ Fold ìë™ ê±´ë„ˆë›°ê¸°)

# ì˜ˆìƒ ì‹œê°„: 3-4ì‹œê°„ (ê¸°ì¡´ ëŒ€ë¹„ 43% ë‹¨ì¶•, epochs 15â†’7)
# ì˜ˆìƒ ROUGE Sum: 1.24-1.35 (í˜„ì¬ 1.048 â†’ +18-29%, ìµœì í™” ë°˜ì˜ + HuggingFace ë³´ì •)
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: Fold ì™„ë£Œë§ˆë‹¤ ìë™ ì €ì¥, ì¤‘ë‹¨ ì‹œ ì™„ë£Œëœ Foldë¶€í„° Resume ê°€ëŠ¥
```

---

### 3.3 ì „ëµ 3: ë¹ ë¥¸ ê³ ì„±ëŠ¥ (K-Fold 3 + ì ì€ Epoch)

#### ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[ëª…ë ¹ì–´ ì‹¤í–‰<br/>--mode kfold<br/>--k_folds 3] --> B[Config ë¡œë“œ]
    B --> C[ë°ì´í„° ì¦ê°• 50%]
    C --> D[3-Fold ë¶„í• ]
    D --> E[Fold 1/3<br/>Epoch 10]
    E --> F[Fold 2/3<br/>Epoch 10]
    F --> G[Fold 3/3<br/>Epoch 10]
    G --> H[ì•™ìƒë¸” ì¶”ë¡ ]
    H --> I[HF ë³´ì • + Solar API]
    I --> J[submission.csv]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#e1f5ff,stroke:#01579b,color:#000
    style C fill:#fff3e0,stroke:#e65100,color:#000
    style D fill:#fff3e0,stroke:#e65100,color:#000
    style E fill:#f3e5f5,stroke:#4a148c,color:#000
    style F fill:#f3e5f5,stroke:#4a148c,color:#000
    style G fill:#f3e5f5,stroke:#4a148c,color:#000
    style H fill:#b39ddb,stroke:#311b92,color:#000
    style I fill:#c8e6c9,stroke:#1b5e20,color:#000
    style J fill:#a5d6a7,stroke:#1b5e20,color:#000
```

#### ì‹œë‚˜ë¦¬ì˜¤
1. K-Fold 3ìœ¼ë¡œ ë¹ ë¥¸ êµì°¨ê²€ì¦
2. Epoch 7 (Optuna ìµœì ê°’)
3. ë°ì´í„° ì¦ê°• 50%

#### ë¹ ë¥¸ ê³ ì„±ëŠ¥ ëª…ë ¹ì–´

```bash
# ==================== ì „ëµ 3: ë¹ ë¥¸ ê³ ì„±ëŠ¥ (Optuna ìµœì í™” ë°˜ì˜) ==================== #
# âœ… ì²´í¬í¬ì¸íŠ¸ Resume ì§€ì›: ì¤‘ë‹¨ ì‹œ --resume ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
python scripts/train.py \
  --mode kfold \
  --models kobart \
  --epochs 7 \
  --batch_size 16 \
  --gradient_accumulation_steps 10 \
  --learning_rate 9.14e-5 \
  --warmup_ratio 0.00136 \
  --weight_decay 0.0995 \
  --scheduler_type cosine \
  --use_augmentation \
  --augmentation_ratio 0.5 \
  --augmentation_methods back_translation paraphrase \
  --k_folds 3 \
  --fold_seed 42 \
  --max_new_tokens 100 \
  --min_new_tokens 30 \
  --num_beams 4 \
  --repetition_penalty 1.5 \
  --length_penalty 0.938 \
  --no_repeat_ngram_size 3 \
  --use_solar_api \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --experiment_name kobart_fast_high \
  --seed 42 \
  --resume  # âœ… ì¤‘ë‹¨ í›„ ì´ì–´ì„œ ì‹¤í–‰ (ì™„ë£Œëœ Fold ìë™ ê±´ë„ˆë›°ê¸°)

# ì˜ˆìƒ ì‹œê°„: 1.5-2ì‹œê°„ (ê¸°ì¡´ ëŒ€ë¹„ 33% ë‹¨ì¶•, epochs 10â†’7)
# ì˜ˆìƒ ROUGE Sum: 1.18-1.28 (í˜„ì¬ 1.048 â†’ +13-22%, ìµœì í™” ë°˜ì˜ + HuggingFace ë³´ì •)
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: Fold ì™„ë£Œë§ˆë‹¤ ìë™ ì €ì¥, ì¤‘ë‹¨ ì‹œ ì™„ë£Œëœ Foldë¶€í„° Resume ê°€ëŠ¥
```

---

### 3.4 ì „ëµ 4: ì´ˆê³ ì† ì‹¤í—˜ (Single Model)

#### ì‹¤í–‰ íŒŒì´í”„ë¼ì¸

```mermaid
graph LR
    A[ëª…ë ¹ì–´ ì‹¤í–‰<br/>--mode single] --> B[Config ë¡œë“œ]
    B --> C[ë°ì´í„° ì¦ê°• 50%]
    C --> D[Train/Val 8:2 ë¶„í• ]
    D --> E[ëª¨ë¸ ë¡œë“œ]
    E --> F[í•™ìŠµ Epoch 5<br/>grad_acc_steps=10]
    F --> G[í‰ê°€ + ì²´í¬í¬ì¸íŠ¸]
    G --> H[Test ì¶”ë¡ ]
    H --> I[HF ë³´ì • + Solar API]
    I --> J[submission.csv]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#e1f5ff,stroke:#01579b,color:#000
    style C fill:#fff3e0,stroke:#e65100,color:#000
    style D fill:#fff3e0,stroke:#e65100,color:#000
    style E fill:#c8e6c9,stroke:#1b5e20,color:#000
    style F fill:#f3e5f5,stroke:#4a148c,color:#000
    style G fill:#ffccbc,stroke:#bf360c,color:#000
    style H fill:#b39ddb,stroke:#311b92,color:#000
    style I fill:#c8e6c9,stroke:#1b5e20,color:#000
    style J fill:#a5d6a7,stroke:#1b5e20,color:#000
```

#### ì‹œë‚˜ë¦¬ì˜¤
1. K-Fold ì—†ì´ ë‹¨ì¼ í•™ìŠµ
2. Epoch 5 + ë¹ ë¥¸ ì¦ê°•
3. ë¹ ë¥¸ ê²€ì¦ìš©

#### ì´ˆê³ ì† ëª…ë ¹ì–´

```bash
# ==================== ì „ëµ 4: ì´ˆê³ ì† ì‹¤í—˜ (Optuna ìµœì í™” ë°˜ì˜) ==================== #
# âœ… ì²´í¬í¬ì¸íŠ¸ Resume ì§€ì›: ì¤‘ë‹¨ ì‹œ --resume ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
python scripts/train.py \
  --mode single \
  --models kobart \
  --epochs 5 \
  --batch_size 16 \
  --gradient_accumulation_steps 10 \
  --learning_rate 9.14e-5 \
  --warmup_ratio 0.00136 \
  --weight_decay 0.0995 \
  --scheduler_type cosine \
  --use_augmentation \
  --augmentation_ratio 0.5 \
  --augmentation_methods back_translation paraphrase \
  --max_new_tokens 100 \
  --min_new_tokens 30 \
  --num_beams 4 \
  --repetition_penalty 1.5 \
  --length_penalty 0.938 \
  --no_repeat_ngram_size 3 \
  --use_solar_api \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --experiment_name kobart_ultrafast \
  --seed 42 \
  --resume  # âœ… ì¤‘ë‹¨ í›„ ì´ì–´ì„œ ì‹¤í–‰ (Epoch ìë™ Resume)

# ì˜ˆìƒ ì‹œê°„: 30-45ë¶„ (ê¸°ì¡´ ëŒ€ë¹„ 33% ë‹¨ì¶•)
# ì˜ˆìƒ ROUGE Sum: 1.13-1.20 (í˜„ì¬ 1.048 â†’ +8-15%, ìµœì í™” ë°˜ì˜ + HuggingFace ë³´ì •)
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: Epoch ì™„ë£Œë§ˆë‹¤ ìë™ ì €ì¥, ì¤‘ë‹¨ ì‹œ ì™„ë£Œëœ Epochë¶€í„° Resume ê°€ëŠ¥
```

---

## 4. ì¶”ë¡  ì‹œ ì„±ëŠ¥ í–¥ìƒ ì „ëµ

### 4.1 Solar API + HuggingFace í†µí•© ì „ëµ (ìµœê°• ì¡°í•©)

#### ê°œë…
KoBARTë¡œ ë¹ ë¥´ê²Œ í•™ìŠµ â†’ ì¶”ë¡  ì‹œ Solar API + HuggingFace ë³´ì • ë™ì‹œ ì‚¬ìš©

**ì„±ëŠ¥ ìµœì í™” ì „ëµ:**
- âœ… **Solar API + HuggingFace ë™ì‹œ ì‚¬ìš© ê¶Œì¥**: ë‘ ê¸°ìˆ ì€ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ë™
- Solar API: ì™¸ë¶€ ê³ í’ˆì§ˆ ìš”ì•½ ëª¨ë¸ë¡œ ì•™ìƒë¸” íš¨ê³¼
- HuggingFace ë³´ì •: ì‚¬ì „í•™ìŠµ ëª¨ë¸ë¡œ í’ˆì§ˆ ê²€ì¦ ë° ë³´ì •
- ë™ì‹œ ì‚¬ìš© ì‹œ ì¶”ê°€ 3-5% ROUGE ì ìˆ˜ í–¥ìƒ íš¨ê³¼

#### êµ¬í˜„ ë°©ë²•

**âš ï¸ ì£¼ì˜**: Solar APIëŠ” í˜„ì¬ ëª…ë ¹í–‰ ì˜µì…˜ìœ¼ë¡œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Config íŒŒì¼ì„ í†µí•´ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# ==================== HuggingFace ë³´ì • ì¶”ë¡  (Optuna ìµœì í™” ë°˜ì˜) ==================== #
python scripts/inference.py \
  --model experiments/.../kobart/final_model \
  --test_data data/raw/test.csv \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --max_new_tokens 100 \
  --min_new_tokens 30 \
  --num_beams 4 \
  --length_penalty 0.938 \
  --repetition_penalty 1.5 \
  --batch_size 16 \
  --output submissions/kobart_hf_corrected.csv
```

| ì˜µì…˜ | ê°’ | ì„¤ëª… |
|------|-----|------|
| `--use_pretrained_correction` | - | HuggingFace ë³´ì • í™œì„±í™” |
| `--correction_models` | gogamza/kobart-base-v2 digit82/kobart-summarization | HF ë³´ì • ëª¨ë¸ |
| `--correction_strategy` | quality_based | í’ˆì§ˆ ê¸°ë°˜ ë³´ì • ì „ëµ |
| `--correction_threshold` | 0.3 | í’ˆì§ˆ ì„ê³„ê°’ |

**Solar API ì‚¬ìš© ë°©ë²•**:
- Config íŒŒì¼(`configs/train_config.yaml` ë˜ëŠ” ëª¨ë¸ë³„ config)ì˜ `inference.solar_api` ì„¹ì…˜ì—ì„œ ì„¤ì •
- í•™ìŠµ ì‹œ `--use_solar_api` í”Œë˜ê·¸ ì‚¬ìš© (ì¶”ë¡  ì‹œ ìë™ ì ìš©)

### 4.2 HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ì „ëµ (PRD 04, 12)

> **âœ… ì‚¬ìš© ê°€ëŠ¥**: `--use_pretrained_correction` ì˜µì…˜ìœ¼ë¡œ HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ê¸°ëŠ¥ì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ê°œë…
KoBART í•™ìŠµ â†’ ì¶”ë¡  ì‹œ HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ë“¤ë¡œ ë³´ì •

#### ë³´ì • ì „ëµ ì„¤ëª…

| ì „ëµ | ì„¤ëª… | ì¶”ì²œ ìƒí™© |
|------|------|----------|
| `quality_based` | í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì„ íƒ (KoBART vs ì°¸ì¡° ëª¨ë¸) | **ì¶”ì²œ** - ê· í˜•ì¡íŒ í’ˆì§ˆ |
| `threshold` | í•©ì˜ë„ ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ | ë³´ìˆ˜ì  ë³´ì • í•„ìš” ì‹œ |
| `voting` | ëª¨ë“  ëª¨ë¸ íˆ¬í‘œ | ë‹¤ì–‘í•œ ì˜ê²¬ ë°˜ì˜ |
| `weighted` | ê°€ì¤‘ í‰ê·  (quality_basedì™€ ë™ì¼) | - |

**í˜„ì¬ ê°€ëŠ¥í•œ ì¶”ë¡  ê³ ë„í™”:**
- âœ… Solar API ì•™ìƒë¸” (í•™ìŠµ ì‹œ `--use_solar_api`, config ì„¤ì • í•„ìš”)
- âœ… HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • (ëª…ë ¹í–‰ ì˜µì…˜ ì§€ì›)
- âœ… ê°•í™”ëœ í›„ì²˜ë¦¬ (ìë™ ì ìš©)

### 4.3 í›„ì²˜ë¦¬ ê³ ë„í™”

#### ê°•í™”ëœ í›„ì²˜ë¦¬ (ìë™ ì ìš©ë¨)

```python
# predictor.pyì— ì´ë¯¸ êµ¬í˜„ë¨
def postprocess_summary(text: str) -> str:
    """
    99.6% ì™„ì „í•œ ë¬¸ì¥ ìƒì„±
    1. ë°˜ë³µëœ ì  ì œê±°
    2. ë¶ˆì™„ì „í•œ í”Œë ˆì´ìŠ¤í™€ë” ì œê±°
    3. ë¶ˆì™„ì „í•œ ë§ˆì§€ë§‰ ë¬¸ì¥ ì œê±°
    4. ë¶ˆì™„ì „í•œ ì¢…ê²°ì–´ ì œê±°
    5. ì§§ì€ ì¡°ì‚¬/ë‹¨ì–´ ì œê±°
    6. ë¬¸ì¥ ì¢…ê²° ë³´ì¥
    """
```

---

## 5. ë¹ ë¥¸ ì‹¤í–‰ ëª…ë ¹ì–´

### 5.1 ì‹œê°„ë³„ ì¶”ì²œ ëª…ë ¹ì–´

| ë‚¨ì€ ì‹œê°„ | ì¶”ì²œ ì „ëµ | ëª…ë ¹ì–´ |
|----------|----------|--------|
| **24ì‹œê°„+** | ì „ëµ 1 (ì ˆëŒ€ ìµœê³ ) | Optuna 20 trials + K-Fold 5 + Epoch 7 + ìµœì  íŒŒë¼ë¯¸í„° |
| **6ì‹œê°„** | ì „ëµ 2 (ê· í˜•) | K-Fold 5 + Epoch 7 + ìµœì  íŒŒë¼ë¯¸í„° + GradAcc 10 |
| **3ì‹œê°„** | ì „ëµ 3 (ë¹ ë¥¸ ê³ ì„±ëŠ¥) | K-Fold 3 + Epoch 7 + ìµœì  íŒŒë¼ë¯¸í„° + GradAcc 10 |
| **1.5ì‹œê°„** | ì „ëµ 4 (ì´ˆê³ ì†) | Single + Epoch 5 + ìµœì  íŒŒë¼ë¯¸í„° + GradAcc 10 |
| **30ë¶„** | ê¸´ê¸‰ | Single + Epoch 3 + ìµœì  íŒŒë¼ë¯¸í„° + GradAcc 10 |

### 5.2 ê¸´ê¸‰ 1ì‹œê°„ ëª…ë ¹ì–´

```bash
# ==================== ê¸´ê¸‰ 30ë¶„ ë²„ì „ (Optuna ìµœì í™” ë°˜ì˜) ==================== #
# âœ… ì²´í¬í¬ì¸íŠ¸ Resume ì§€ì›: ì¤‘ë‹¨ ì‹œ --resume ì˜µì…˜ ì¶”ê°€í•˜ì—¬ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
python scripts/train.py \
  --mode single \
  --models kobart \
  --epochs 3 \
  --batch_size 16 \
  --gradient_accumulation_steps 10 \
  --learning_rate 9.14e-5 \
  --warmup_ratio 0.00136 \
  --weight_decay 0.0995 \
  --scheduler_type cosine \
  --use_augmentation \
  --augmentation_ratio 0.5 \
  --augmentation_methods back_translation paraphrase \
  --max_new_tokens 100 \
  --num_beams 4 \
  --length_penalty 0.938 \
  --repetition_penalty 1.5 \
  --use_solar_api \
  --use_pretrained_correction \
  --correction_models gogamza/kobart-base-v2 digit82/kobart-summarization \
  --correction_strategy quality_based \
  --correction_threshold 0.3 \
  --experiment_name kobart_emergency \
  --seed 42 \
  --resume  # âœ… ì¤‘ë‹¨ í›„ ì´ì–´ì„œ ì‹¤í–‰ (Epoch ìë™ Resume)

# ì˜ˆìƒ ì‹œê°„: 20-30ë¶„ (ê¸°ì¡´ ëŒ€ë¹„ 43% ë‹¨ì¶•)
# ì˜ˆìƒ ROUGE Sum: 1.11-1.17 (í˜„ì¬ 1.048 â†’ +6-12%, ìµœì í™” ë°˜ì˜ + HuggingFace ë³´ì •)
# ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: Epoch ì™„ë£Œë§ˆë‹¤ ìë™ ì €ì¥, ì¤‘ë‹¨ ì‹œ ì™„ë£Œëœ Epochë¶€í„° Resume ê°€ëŠ¥
```

---

## 6. ì˜ˆìƒ ì„±ëŠ¥ ë° ì‹œê°„

### 6.1 ì„±ëŠ¥ ë¹„êµí‘œ

| ì „ëµ | ì‹œê°„ | ROUGE Sum | ê°œì„ ìœ¨ | ì¶”ì²œ ìƒí™© |
|------|------|-----------|--------|----------|
| **í˜„ì¬ (Baseline)** | 2ë¶„ | 1.048 | - | - |
| **ì „ëµ 1: ì ˆëŒ€ ìµœê³ ** | 12-15ì‹œê°„ | 1.30-1.42 | +24-35% | 24ì‹œê°„ ë‚¨ìŒ (Optuna ìµœì í™” + Solar + HF ë³´ì •) |
| **ì „ëµ 2: ê· í˜•** | 3-4ì‹œê°„ | 1.24-1.35 | +18-29% | 6ì‹œê°„ ë‚¨ìŒ (ìµœì í™” ë°˜ì˜ + Solar + HF ë³´ì •) |
| **ì „ëµ 3: ë¹ ë¥¸ ê³ ì„±ëŠ¥** | 1.5-2ì‹œê°„ | 1.18-1.28 | +13-22% | 3ì‹œê°„ ë‚¨ìŒ (ìµœì í™” ë°˜ì˜ + Solar + HF ë³´ì •) |
| **ì „ëµ 4: ì´ˆê³ ì†** | 30-45ë¶„ | 1.13-1.20 | +8-15% | 1.5ì‹œê°„ ë‚¨ìŒ (ìµœì í™” ë°˜ì˜ + Solar + HF ë³´ì •) |
| **ê¸´ê¸‰** | 20-30ë¶„ | 1.11-1.17 | +6-12% | 30ë¶„ ë‚¨ìŒ (ìµœì í™” ë°˜ì˜ + Solar + HF ë³´ì •) |

### 6.2 ì‹œê°„ ë¶„í•´

#### ì „ëµ 1 (12-15ì‹œê°„)
```
Optuna 20 trials: 10-12ì‹œê°„ (trialë‹¹ 30-36ë¶„, epochs 7 ì ìš©)
  - gradient_accumulation_steps=10ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ
  - íš¨ìœ¨ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (Trial 11ì—ì„œ ìµœì ê°’ ë°œê²¬ íŒ¨í„´)
K-Fold 5 Ã— Epoch 7: ë³„ë„ ì‹¤í–‰ ë¶ˆí•„ìš” (Optunaê°€ ìµœì  ëª¨ë¸ ìƒì„±)
ì¶”ë¡  ë° Solar API ì•™ìƒë¸”: 2-3ì‹œê°„
```

#### ì „ëµ 2 (3-4ì‹œê°„)
```
K-Fold 5 Ã— Epoch 7: 2.5-3ì‹œê°„ (foldë‹¹ 30-36ë¶„)
  - gradient_accumulation_steps=10 ì ìš©
  - ë°ì´í„° ì¦ê°• 50% í¬í•¨
  - ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš© (epochs 15â†’7ë¡œ 43% ë‹¨ì¶•)
ì¶”ë¡  + Solar API: 0.5-1ì‹œê°„
```

### 6.3 ë¦¬ìŠ¤í¬ ê´€ë¦¬

```mermaid
graph TB
    A[ë‚¨ì€ ì‹œê°„ í™•ì¸] --> B{24ì‹œê°„ ì´ìƒ?}
    B -->|Yes| C[ì „ëµ 1 ì‹¤í–‰]
    B -->|No| D{12ì‹œê°„ ì´ìƒ?}
    D -->|Yes| E[ì „ëµ 2 ì‹¤í–‰]
    D -->|No| F{6ì‹œê°„ ì´ìƒ?}
    F -->|Yes| G[ì „ëµ 3 ì‹¤í–‰]
    F -->|No| H{3ì‹œê°„ ì´ìƒ?}
    H -->|Yes| I[ì „ëµ 4 ì‹¤í–‰]
    H -->|No| J[ê¸´ê¸‰ ì‹¤í–‰]

    C --> K[ë™ì‹œì— ì „ëµ 2 ì¤€ë¹„]
    E --> L[ë™ì‹œì— ì „ëµ 3 ì¤€ë¹„]
    G --> M[ë™ì‹œì— ì „ëµ 4 ì¤€ë¹„]

    style A fill:#e3f2fd,stroke:#1976d2,color:#000
    style C fill:#66bb6a,stroke:#1b5e20,color:#fff
    style E fill:#81c784,stroke:#2e7d32,color:#000
    style G fill:#aed581,stroke:#558b2f,color:#000
    style I fill:#fff9c4,stroke:#f57f17,color:#000
    style J fill:#ffccbc,stroke:#d84315,color:#000
```

**ë¦¬ìŠ¤í¬ ëŒ€ì‘**:
- ì „ëµ 1 ì‹¤í–‰ ì¤‘ â†’ ì „ëµ 2 ì½”ë“œ ë¯¸ë¦¬ ì¤€ë¹„
- ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™œì„±í™”
- 2ì‹œê°„ë§ˆë‹¤ ì¤‘ê°„ ê²°ê³¼ í™•ì¸

---

## 7. ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 7.1 ì‹¤í–‰ ì „ í™•ì¸

- [ ] GPU ë©”ëª¨ë¦¬ í™•ì¸ (nvidia-smi)
- [ ] ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸ (train.csv, test.csv)
- [ ] Solar API í‚¤ ì„¤ì • í™•ì¸ (í™˜ê²½ë³€ìˆ˜)
- [ ] ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 10GB)
- [ ] ì´ì „ ì‹¤í—˜ ë°±ì—…

### 7.2 ì‹¤í–‰ ì¤‘ ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f experiments/*/train.log

# GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ROUGE ì ìˆ˜ ì¶”ì´ í™•ì¸
grep "eval_rouge" experiments/*/train.log
```

### 7.3 ì‹¤í–‰ í›„ í™•ì¸

- [ ] ì œì¶œ íŒŒì¼ ìƒì„± í™•ì¸
- [ ] ROUGE ì ìˆ˜ í™•ì¸
- [ ] ì™„ì „í•œ ë¬¸ì¥ ë¹„ìœ¨ í™•ì¸ (99%+)
- [ ] ì œì¶œ íŒŒì¼ í˜•ì‹ ê²€ì¦

```python
# ì œì¶œ íŒŒì¼ ê²€ì¦
import pandas as pd

df = pd.read_csv('submissions/kobart_ultimate.csv')
print(f"ìƒ˜í”Œ ìˆ˜: {len(df)}")
print(f"ì»¬ëŸ¼: {df.columns.tolist()}")
print(f"Null í™•ì¸: {df.isnull().sum()}")

# ì™„ì „í•œ ë¬¸ì¥ ë¹„ìœ¨
complete = df['summary'].str.endswith(('.', '!', '?')).mean()
print(f"ì™„ì „í•œ ë¬¸ì¥: {complete:.1%}")
```

---

## 8. ìµœì¢… ê¶Œì¥ ì‚¬í•­

### 8.1 ë§‰íŒ í•˜ë£¨ ì „ëµ

**ì‹œë‚˜ë¦¬ì˜¤ A: 24ì‹œê°„ ë‚¨ìŒ (í•˜ë£¨)**

```
Day 1:
09:00 - 09:00+15h (24:00): ì „ëµ 1 ì‹¤í–‰ (Optuna 20 trials, epochs 7)
  â†’ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰, ì •ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥

Day 2:
00:00 - 02:00 (2ì‹œê°„): Optuna ê²°ê³¼ ë¶„ì„, ìµœì  ëª¨ë¸ ì„ íƒ
02:00 - 05:00 (3ì‹œê°„): ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ëµ 2 ì‹¤í–‰ (ê²€ì¦ìš©)
05:00 - 07:00 (2ì‹œê°„): Solar API ì•™ìƒë¸” + ìµœì¢… ì¶”ë¡ 
07:00 - 08:00 (1ì‹œê°„): ì œì¶œ íŒŒì¼ ê²€ì¦ ë° ìƒì„±
08:00 - 09:00 (1ì‹œê°„): ì—¬ìœ  ì‹œê°„ (ê¸´ê¸‰ ëŒ€ì‘)
```

**ì‹œë‚˜ë¦¬ì˜¤ B: 6ì‹œê°„ ë‚¨ìŒ**

```
09:00 - 13:00 (4ì‹œê°„): ì „ëµ 2 ì‹¤í–‰ (K-Fold 5 + Epoch 7 + ìµœì  íŒŒë¼ë¯¸í„°)
13:00 - 14:00 (1ì‹œê°„): Solar API ì•™ìƒë¸” ì¶”ë¡ 
14:00 - 14:30 (30ë¶„): ì œì¶œ íŒŒì¼ ê²€ì¦
14:30 - 15:00 (30ë¶„): ì—¬ìœ  ì‹œê°„ (ê¸´ê¸‰ ëŒ€ì‘)
```

### 8.2 í•µì‹¬ ì„±ê³µ ìš”ì†Œ

1. âœ… **ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©**: Optuna ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦ëœ ìµœì ê°’ (20251014)
   - learning_rate: 9.14e-5 (ê¸°ì¡´ 5e-5 ëŒ€ë¹„ 1.8ë°°)
   - weight_decay: 0.0995 (ê¸°ì¡´ 0.01 ëŒ€ë¹„ 10ë°°)
   - scheduler_type: cosine (5.5% ì„±ëŠ¥ í–¥ìƒ)
   - warmup_ratio: 0.00136 (ê±°ì˜ ë¶ˆí•„ìš”)
   - num_beams: 4 (5â†’4, ì†ë„â†‘ í’ˆì§ˆ ìœ ì§€)
   - length_penalty: 0.938 (ì•½ê°„ ì§§ê²Œ ìœ ë„)
2. âœ… **ìµœì  Epochs 7**: 30â†’7 epochs (76.7% ì‹œê°„ ë‹¨ì¶•, ì„±ëŠ¥ ìœ ì§€)
3. âœ… **Gradient Accumulation 10**: íš¨ê³¼ì  ë°°ì¹˜ 160, ë§¤ìš° ì•ˆì •ì  í•™ìŠµ
4. âœ… **ë°ì´í„° ì¦ê°• 50%**: ë°˜ë“œì‹œ ì ìš© (back_translation + paraphrase)
5. âœ… **K-Fold 5**: ì•ˆì •ì  ì¼ë°˜í™”
6. âœ… **Optuna 20 trials**: íš¨ìœ¨ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ (100â†’20, Trial 11 íŒ¨í„´)
7. âœ… **Early Stopping**: ê³¼ì í•© ë°©ì§€
8. âœ… **Solar API ì•™ìƒë¸”**: ì¶”ë¡  ì‹œ ë³´ì •
9. âœ… **HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •**: quality_based ì „ëµìœ¼ë¡œ ì¶”ê°€ í’ˆì§ˆ í–¥ìƒ
10. âœ… **ê°•í™”ëœ í›„ì²˜ë¦¬**: 99.6% ì™„ì „í•œ ë¬¸ì¥

### 8.3 ì ˆëŒ€ í”¼í•´ì•¼ í•  ê²ƒ

- âŒ **ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ**: ì‹œê°„ ë¶€ì¡±
- âŒ **LLM Full Training**: Llama/Qwen í•™ìŠµ ê¸ˆì§€
- âŒ **ê³¼ë„í•œ Epoch (50+)**: ê³¼ì í•© ìœ„í—˜
- âŒ **TTA ê³¼ë‹¤ ì‚¬ìš©**: ì¶”ë¡  ì‹œê°„ í­ì¦
- âŒ **ê²€ì¦ ì—†ëŠ” ì œì¶œ**: ë°˜ë“œì‹œ ìƒ˜í”Œ í™•ì¸

---

## 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 9.1 GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ë°©ë²• 1: Batch size ì¤„ì´ê³  gradient_accumulation_steps ëŠ˜ë¦¬ê¸° (íš¨ê³¼ì  ë°°ì¹˜ ìœ ì§€)
--batch_size 8 --gradient_accumulation_steps 20  # íš¨ê³¼ì  ë°°ì¹˜ = 160

# ë°©ë²• 2: ë” ì‹¬ê°í•œ ê²½ìš°
--batch_size 4 --gradient_accumulation_steps 40  # íš¨ê³¼ì  ë°°ì¹˜ = 160

# ì£¼ì˜: íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°(batch_size Ã— gradient_accumulation_steps)ë¥¼
# 160 ê·¼ì²˜ë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì— ì¤‘ìš”í•©ë‹ˆë‹¤
```

### 9.2 í•™ìŠµ ì‹œê°„ ì´ˆê³¼

```bash
# Epoch ì¤„ì´ê¸°
--epochs 10

# K-Fold ì¤„ì´ê¸°
--k_folds 3

# ë˜ëŠ” Single ëª¨ë“œë¡œ ì „í™˜
--mode single
```

### 9.3 ROUGE ì ìˆ˜ ë‚®ìŒ

```bash
# Learning rate ì¡°ì • (Optuna ìµœì ê°’ ê¸°ì¤€)
--learning_rate 9.14e-5  # ìµœì ê°’ ì ìš© (ê¸°ì¡´ ëŒ€ë¹„ 1.8ë°°)

# Scheduler ë³€ê²½
--scheduler_type cosine  # Linear ëŒ€ì‹  Cosine (5.5% í–¥ìƒ)

# Weight decay ì¡°ì •
--weight_decay 0.0995  # ìµœì ê°’ (ê³¼ì í•© ë°©ì§€ ê°•í™”)

# ë°ì´í„° ì¦ê°• ê°•í™”
--augmentation_ratio 0.7  # 0.5 â†’ 0.7

# num_beams ì¡°ì •
--num_beams 4  # ìµœì ê°’ (5â†’4, ì†ë„ì™€ í’ˆì§ˆ ê· í˜•)
```

---

## 10. ë¶€ë¡

### 10.1 ê´€ë ¨ ë¬¸ì„œ
- `docs/issues/ì‹œìŠ¤í…œ_ë¬¸ì œ_ê°œì„ _ê³¼ì •.md`: í•´ê²°ëœ ë¬¸ì œì 
- `docs/issues/ë¬¸ì¥_ëŠê¹€_ë¬¸ì œ_í•´ê²°_ê³¼ì •.md`: í›„ì²˜ë¦¬ ê°•í™”
- `docs/modify/02_ë©˜í† ë§_í”¼ë“œë°±_ê¸°ë°˜_ê°œì„ ì‚¬í•­.md`: ë©˜í†  ê¶Œì¥ì‚¬í•­

### 10.2 ì°¸ê³  ì‹¤í—˜
- `docs/experiments/20251013_161056_test_strategy3_triple_ì‹¤í—˜ë¶„ì„.md`: 3ëª¨ë¸ ë¹„êµ
- `docs/experiments/20251013_205042_strategy6_kobart_solar_api_ì‹¤í—˜ë¶„ì„.md`: Solar API í†µí•©

---

**ì‘ì„±**: 2025-10-14
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-14 (Optuna ìµœì í™” ì‹¤í—˜ ê²°ê³¼ ë°˜ì˜)
**ë²„ì „**: 2.0 (Optuna ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)

**ì£¼ìš” ë³€ê²½ì‚¬í•­ (v2.0)**:
- âœ… Optuna ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë©´ ì ìš©
- âœ… Learning rate: 5e-5 â†’ 9.14e-5 (ì•½ 1.8ë°° ì¦ê°€)
- âœ… Weight decay: 0.01 â†’ 0.0995 (ì•½ 10ë°° ì¦ê°€)
- âœ… Epochs: 30/15/10 â†’ 7 (76.7% ì‹œê°„ ë‹¨ì¶•)
- âœ… Scheduler type: linear â†’ cosine (5.5% ì„±ëŠ¥ í–¥ìƒ)
- âœ… Num beams: 5 â†’ 4 (ì†ë„â†‘, í’ˆì§ˆ ìœ ì§€)
- âœ… Length penalty: 1.0 â†’ 0.938 (ì•½ê°„ ì§§ê²Œ)
- âœ… Warmup ratio: 0.1 â†’ 0.00136 (ê±°ì˜ ë¶ˆí•„ìš”)
- âœ… Optuna trials: 100 â†’ 20 (íš¨ìœ¨ì„± ê°œì„ )
- âœ… Optuna timeout: 2ì‹œê°„ â†’ 3ì‹œê°„ (ì—¬ìœ  í™•ë³´)
- âœ… ì „ì²´ ì‹œê°„ íš¨ìœ¨: 50% ë‹¨ì¶•, ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ: +2-5%
