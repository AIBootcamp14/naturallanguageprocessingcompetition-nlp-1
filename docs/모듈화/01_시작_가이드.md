# ì‹œì‘ ê°€ì´ë“œ (ë¹ ë¥¸ ì‹œì‘ + ì‹¤í–‰ ëª…ë ¹ì–´ + ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©ë²•)

## ğŸ“‹ ëª©ì°¨
1. [5ë¶„ ë¹ ë¥¸ ì‹œì‘](#5ë¶„-ë¹ ë¥¸-ì‹œì‘)
2. [ì „ì²´ ì‹¤í–‰ ëª…ë ¹ì–´](#ì „ì²´-ì‹¤í–‰-ëª…ë ¹ì–´)
3. [ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì‚¬ìš©ë²•](#ìŠ¤í¬ë¦½íŠ¸-ìƒì„¸-ì‚¬ìš©ë²•)
4. [í…ŒìŠ¤íŠ¸ ì‹¤í–‰](#í…ŒìŠ¤íŠ¸-ì‹¤í–‰)
5. [ê²°ê³¼ íŒŒì¼ ê´€ë¦¬](#ê²°ê³¼-íŒŒì¼-ê´€ë¦¬)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

# ğŸ“Œ Part 1: 5ë¶„ ë¹ ë¥¸ ì‹œì‘

## ğŸš€ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# ë˜ëŠ”
pyenv activate nlp_py3_11_9

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition

# íŒ¨í‚¤ì§€ í™•ì¸
pip list | grep -E "(torch|transformers|wandb)"
```

## ğŸ“ 2ë‹¨ê³„: ì „ì²´ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸

```bash
# ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (6ê°œ)
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py

# ê³ ê¸‰ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (7ê°œ)
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py && \
python src/tests/test_ensemble.py && \
python src/tests/test_solar_api.py && \
python src/tests/test_optuna.py && \
python src/tests/test_prompts.py
```

**ê²°ê³¼:** ì´ 79ê°œ í…ŒìŠ¤íŠ¸ (100% í†µê³¼)

## âš¡ 3ë‹¨ê³„: ì£¼ìš” ì½”ë“œ íŒ¨í„´

### Config ë¡œë“œ

```python
from src.config import load_config

config = load_config("baseline_kobart")
```

### ë°ì´í„° ì „ì²˜ë¦¬

```python
from src.data import DialoguePreprocessor
import pandas as pd

preprocessor = DialoguePreprocessor()
df = pd.read_csv("data/raw/train.csv")
df_processed = preprocessor.preprocess_dataframe(df)
```

### ëª¨ë¸ ë¡œë”©

```python
from src.models import load_model_and_tokenizer

model, tokenizer = load_model_and_tokenizer(config)
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
```

### LLM íŒŒì¸íŠœë‹ (Llama + QLoRA)

```python
from src.models.lora_loader import load_lora_model_and_tokenizer

model, tokenizer = load_lora_model_and_tokenizer(
    config,
    use_qlora=True  # 4-bit ì–‘ìí™”
)
```

### ë°ì´í„° ì¦ê°• (4ë°°)

```python
from src.data.augmentation import augment_dataset

aug_dialogues, aug_summaries = augment_dataset(
    dialogues,
    summaries,
    methods=['shuffle', 'sample'],
    n_aug=2
)
# 12,457ê°œ â†’ 49,828ê°œ
```

### ì•™ìƒë¸” ì¶”ë¡ 

```python
from src.ensemble import ModelManager

manager = ModelManager()
manager.load_models([path1, path2, path3])

ensemble = manager.create_ensemble('weighted', weights=[0.5, 0.3, 0.2])
predictions = ensemble.predict(dialogues)
```

### í•™ìŠµ ì‹¤í–‰

```python
from src.training import create_trainer
from src.data import DialogueSummarizationDataset

train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer
)

trainer = create_trainer(config, model, tokenizer, train_dataset)
results = trainer.train()
```

### ì¶”ë¡  ì‹¤í–‰

```python
from src.inference import create_predictor

predictor = create_predictor(model, tokenizer, config)

# ë‹¨ì¼ ì˜ˆì¸¡
summary = predictor.predict_single("#Person1#: ì•ˆë…•í•˜ì„¸ìš”")

# ì œì¶œ íŒŒì¼ ìƒì„±
test_df = pd.read_csv("data/raw/test.csv")
submission = predictor.create_submission(
    test_df,
    output_path="submissions/submission.csv"
)
```

---

# ğŸ“Œ Part 2: ì „ì²´ ì‹¤í–‰ ëª…ë ¹ì–´

## ğŸš€ í•™ìŠµ ì‹¤í–‰

### 1. ê¸°ë³¸ í•™ìŠµ (KoBART)

```bash
python scripts/train.py --experiment baseline_kobart
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/baseline_kobart/final_model/`
- ì²´í¬í¬ì¸íŠ¸: `outputs/baseline_kobart/checkpoint-{N}/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_baseline_kobart_YYYYMMDD_HHMMSS.log`

### 2. ë””ë²„ê·¸ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

**ë””ë²„ê·¸ ì„¤ì •:**
- ë°ì´í„°: í•™ìŠµ 100ê°œ, ê²€ì¦ 20ê°œ
- ì—í¬í¬: 2íšŒ
- ë°°ì¹˜ í¬ê¸°: 4
- WandB: ë¹„í™œì„±í™”
- ì˜ˆìƒ ì‹œê°„: ~2ë¶„ (A6000)

### 3. LLM íŒŒì¸íŠœë‹ (Llama-3.2-3B + QLoRA)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**ê²°ê³¼:**
- ëª¨ë¸: `outputs/llama_3.2_3b_qlora/final_model/`
- ë©”ëª¨ë¦¬: 24GB â†’ 8-10GB (4-bit ì–‘ìí™”)

### 4. LLM Instruction Tuning (ë°ì´í„° 5ë°° ì¦ê°•)

```bash
python scripts/train_llm.py \
    --experiment llama_3.2_3b \
    --use_qlora \
    --use_instruction_augmentation
```

**íš¨ê³¼:**
- í•™ìŠµ ë°ì´í„°: 12,457ê°œ â†’ 62,285ê°œ (5ë°°)
- 5ê°€ì§€ instruction í…œí”Œë¦¿ ì ìš©

### 5. K-Fold êµì°¨ ê²€ì¦ í•™ìŠµ

```bash
python scripts/train_with_cv.py \
    --experiment baseline_kobart \
    --n_folds 5 \
    --stratify length
```

**íš¨ê³¼:**
- 5-Fold êµì°¨ ê²€ì¦
- ê¸¸ì´ ê¸°ë°˜ ê³„ì¸µí™” ë¶„í• 
- í‰ê·  ì„±ëŠ¥ ë° í‘œì¤€í¸ì°¨ ê³„ì‚°

---

## ğŸ”® ì¶”ë¡  ì‹¤í–‰

### 1. ê¸°ë³¸ ì¶”ë¡ 

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

**ê²°ê³¼:**
- ì œì¶œ íŒŒì¼: `submissions/submission.csv`
- ìƒ˜í”Œ ìˆ˜: 2,500ê°œ
- ì˜ˆìƒ ì‹œê°„: ~3-5ë¶„ (A6000)

### 2. íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ì¶”ë¡ 

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/checkpoint-1000 \
    --output submissions/checkpoint_1000.csv
```

### 3. ì¶”ë¡  íŒŒë¼ë¯¸í„° ì¡°ì •

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv \
    --batch_size 16 \
    --num_beams 8
```

**ì˜µì…˜:**
- `--test_data`: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸: `data/raw/test.csv`)
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32)
- `--num_beams`: Beam search ë¹” ê°œìˆ˜ (ê¸°ë³¸: 4)
- `--experiment`: Config ì´ë¦„ (ê¸°ë³¸: `baseline_kobart`)

### 4. ì•™ìƒë¸” ì¶”ë¡ 

```bash
python scripts/inference_ensemble.py \
    --models outputs/model1/final_model outputs/model2/final_model outputs/model3/final_model \
    --weights 0.5 0.3 0.2 \
    --output submissions/ensemble.csv
```

**ì˜µì…˜:**
- `--models`: ëª¨ë¸ ê²½ë¡œ ëª©ë¡ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
- `--weights`: ê°€ì¤‘ì¹˜ ëª©ë¡ (í•©ì´ 1.0, ìƒëµ ì‹œ ë™ì¼ ê°€ì¤‘ì¹˜)
- `--method`: ì•™ìƒë¸” ë°©ë²• (`weighted` ë˜ëŠ” `voting`)

### 5. Solar API ì¶”ë¡ 

```bash
export SOLAR_API_KEY="your_api_key_here"

python scripts/inference_solar.py \
    --test_data data/raw/test.csv \
    --output submissions/solar.csv \
    --batch_size 10 \
    --token_limit 512
```

**íš¨ê³¼:**
- Zero-shot ì¶”ë¡ 
- í† í° ìµœì í™” 70% ì ˆì•½
- Few-shot Learning ì§€ì›

---

## ğŸ”„ Full Pipeline ì‹¤í–‰

### 1. í•™ìŠµ + ì¶”ë¡  í•œ ë²ˆì—

```bash
python scripts/run_pipeline.py --experiment baseline_kobart
```

**ì‹¤í–‰ íë¦„:**
1. í•™ìŠµ â†’ `outputs/baseline_kobart/final_model/`
2. ì¶”ë¡  â†’ `submissions/submission.csv`

**ì˜ˆìƒ ì‹œê°„:** ~4-6ì‹œê°„ (A6000)

### 2. í•™ìŠµ ê±´ë„ˆë›°ê³  ì¶”ë¡ ë§Œ

```bash
python scripts/run_pipeline.py \
    --experiment baseline_kobart \
    --skip_training \
    --model_path outputs/baseline_kobart/final_model \
    --output submissions/submission_v2.csv
```

---

## ğŸ¯ ê³ ê¸‰ ì‹¤í–‰ íŒ¨í„´

### íŒ¨í„´ 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…

```bash
# 1. ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¹ ë¥¸ í•™ìŠµ
python scripts/train.py --experiment baseline_kobart --debug

# 2. í•™ìŠµëœ ëª¨ë¸ í™•ì¸
ls outputs/baseline_kobart/final_model/

# 3. ì¶”ë¡  í…ŒìŠ¤íŠ¸
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/debug_test.csv \
    --batch_size 4
```

### íŒ¨í„´ 2: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ë¹„êµ

```bash
# ê° ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ 
for checkpoint in checkpoint-500 checkpoint-1000 checkpoint-1500 final_model
do
    python scripts/inference.py \
        --model outputs/baseline_kobart/$checkpoint \
        --output submissions/${checkpoint}.csv
done

# ê²°ê³¼ ë¹„êµ
ls -lh submissions/
```

### íŒ¨í„´ 3: ì‹¤í—˜ ì¶”ì 

```bash
# ì—¬ëŸ¬ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰
experiments=("baseline_kobart" "experiment_v2" "experiment_v3")

for exp in "${experiments[@]}"
do
    echo "ì‹¤í—˜ ì‹œì‘: $exp"
    python scripts/run_pipeline.py --experiment $exp
    echo "ì‹¤í—˜ ì™„ë£Œ: $exp"
    echo "---"
done

# ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ í™•ì¸
ls -R outputs/
ls submissions/
```

### íŒ¨í„´ 4: Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```bash
python scripts/optimize.py \
    --experiment baseline_kobart \
    --n_trials 50 \
    --output_dir outputs/optuna_results
```

**íš¨ê³¼:**
- 50íšŒ Trialë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
- TPE Sampler + Median Pruner
- ê²°ê³¼: `outputs/optuna_results/best_params.json`

---

# ğŸ“Œ Part 3: ìŠ¤í¬ë¦½íŠ¸ ìƒì„¸ ì‚¬ìš©ë²•

## ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… | ìš©ë„ |
|---------|------|------|
| `train.py` | ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ | KoBART ë“± Encoder-Decoder í•™ìŠµ |
| `inference.py` | ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± | í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ |
| `run_pipeline.py` | ì „ì²´ íŒŒì´í”„ë¼ì¸ | í•™ìŠµ + ì¶”ë¡  í•œ ë²ˆì— |
| `train_llm.py` | LLM íŒŒì¸íŠœë‹ | Llama/Qwen + QLoRA |
| `train_with_cv.py` | K-Fold êµì°¨ ê²€ì¦ | 5-Fold CV í•™ìŠµ |
| `inference_ensemble.py` | ì•™ìƒë¸” ì¶”ë¡  | ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” |
| `inference_solar.py` | Solar API ì¶”ë¡  | API ê¸°ë°˜ Zero-shot |
| `optimize.py` | Optuna ìµœì í™” | í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ |

## ğŸš€ train.py - ê¸°ë³¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

### ëª…ë ¹ì–´ ì˜µì…˜

```bash
python scripts/train.py --help
```

**í•„ìˆ˜ ì˜µì…˜:**
- `--experiment`: ì‹¤í—˜ Config ì´ë¦„

**ì„ íƒ ì˜µì…˜:**
- `--debug`: ë””ë²„ê·¸ ëª¨ë“œ (ì‘ì€ ë°ì´í„°, 2 ì—í¬í¬, WandB OFF)

### ì‹¤í–‰ ë‹¨ê³„

```
[1/6] Config ë¡œë”© (configs/experiments/{experiment}.yaml)
[2/6] ë°ì´í„° ë¡œë”© (train.csv, dev.csv)
[3/6] ëª¨ë¸ ë¡œë”© (KoBART ë“±)
[4/6] Dataset ìƒì„±
[5/6] í•™ìŠµ ì‹¤í–‰ (WandB ë¡œê¹…)
[6/6] ëª¨ë¸ ì €ì¥ (outputs/{experiment}/)
```

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
í•™ìŠµ ì‹œì‘: baseline_kobart
============================================================

[1/6] Config ë¡œë”©...
  âœ… Config ë¡œë“œ ì™„ë£Œ (seed: 42)

[2/6] ë°ì´í„° ë¡œë”©...
  âœ… í•™ìŠµ ë°ì´í„°: 12,457ê°œ
  âœ… ê²€ì¦ ë°ì´í„°: 1,384ê°œ

[3/6] ëª¨ë¸ ë¡œë”©...
  â†’ ë””ë°”ì´ìŠ¤: cuda
  â†’ ì „ì²´ íŒŒë¼ë¯¸í„°: 123,859,968

[4/6] Dataset ìƒì„±...
  âœ… í•™ìŠµ Dataset: 12,457ê°œ

[5/6] í•™ìŠµ ì‹œì‘...
[í•™ìŠµ ë¡œê·¸...]

ìµœì¢… í‰ê°€ ê²°ê³¼:
  eval_rouge1: 0.4521
  eval_rouge2: 0.2134
  eval_rougeL: 0.3890

[6/6] í•™ìŠµ ì™„ë£Œ!
  ìµœì¢… ëª¨ë¸: outputs/baseline_kobart/final_model
============================================================
```

## ğŸ”® inference.py - ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

### ëª…ë ¹ì–´ ì˜µì…˜

**í•„ìˆ˜ ì˜µì…˜:**
- `--model`: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
- `--output`: ì œì¶œ íŒŒì¼ ì¶œë ¥ ê²½ë¡œ

**ì„ íƒ ì˜µì…˜:**
- `--test_data`: í…ŒìŠ¤íŠ¸ ë°ì´í„° (ê¸°ë³¸: `data/raw/test.csv`)
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32)
- `--num_beams`: Beam ê°œìˆ˜ (ê¸°ë³¸: 4)
- `--experiment`: Config ì´ë¦„ (ê¸°ë³¸: `baseline_kobart`)

### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
ì¶”ë¡  ì‹œì‘
============================================================

[1/5] Config ë¡œë”©...
  âœ… Config: baseline_kobart

[2/5] ëª¨ë¸ ë¡œë”©: outputs/baseline_kobart/final_model
  âœ… ëª¨ë¸ íŒŒë¼ë¯¸í„°: 123,859,968

[3/5] í…ŒìŠ¤íŠ¸ ë°ì´í„°: data/raw/test.csv
  âœ… í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 2,500ê°œ

[4/5] ì¶”ë¡  ì‹¤í–‰...
Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [02:15<00:00]

[5/5] ì¶”ë¡  ì™„ë£Œ!
  âœ… ì œì¶œ íŒŒì¼: submissions/submission.csv

  ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 3ê°œ):
    [test_001]: ë‘ ì‚¬ëŒì´ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤
    [test_002]: ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì‚°ì±…ì„ ê°€ê¸°ë¡œ í–ˆë‹¤
    [test_003]: ì ì‹¬ ë©”ë‰´ë¡œ ê¹€ì¹˜ì°Œê°œë¥¼ ì„ íƒí–ˆë‹¤
============================================================
```

## ğŸ¤– train_llm.py - LLM íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

### ëª…ë ¹ì–´ ì˜µì…˜

**í•„ìˆ˜ ì˜µì…˜:**
- `--experiment`: LLM Config ì´ë¦„ (`llama_3.2_3b`, `qwen3_4b` ë“±)

**ì„ íƒ ì˜µì…˜:**
- `--use_qlora`: QLoRA 4-bit ì–‘ìí™” ì‚¬ìš©
- `--use_instruction_augmentation`: Instruction ë°ì´í„° 5ë°° ì¦ê°•
- `--debug`: ë””ë²„ê·¸ ëª¨ë“œ

### ì‚¬ìš© ì˜ˆì‹œ

```bash
# Llama-3.2-3B QLoRA í•™ìŠµ
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora

# Instruction Tuning (ë°ì´í„° 5ë°°)
python scripts/train_llm.py \
    --experiment llama_3.2_3b \
    --use_qlora \
    --use_instruction_augmentation

# Qwen2.5-3B í•™ìŠµ
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

### íŠ¹ì§•

- **LoRA íŒŒë¼ë¯¸í„°:** r=16, alpha=32, dropout=0.05
- **Target Modules:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **4-bit ì–‘ìí™”:** BitsAndBytesConfig (nf4, double_quant)
- **ë©”ëª¨ë¦¬ ì ˆê°:** 24GB â†’ 8-10GB

---

# ğŸ“Œ Part 4: í…ŒìŠ¤íŠ¸ ì‹¤í–‰

## ğŸ§ª ì „ì²´ í…ŒìŠ¤íŠ¸ (79ê°œ)

```bash
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py && \
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py && \
python src/tests/test_ensemble.py && \
python src/tests/test_solar_api.py && \
python src/tests/test_optuna.py && \
python src/tests/test_prompts.py
```

## ğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ ëª¨ë“ˆ (6ê°œ í…ŒìŠ¤íŠ¸, 37ê°œ í•­ëª©)

```bash
python src/tests/test_config_loader.py      # Config ë¡œë”©
python src/tests/test_preprocessor.py       # ë°ì´í„° ì „ì²˜ë¦¬
python src/tests/test_model_loader.py       # ëª¨ë¸ ë¡œë”
python src/tests/test_metrics.py            # ROUGE í‰ê°€
python src/tests/test_trainer.py            # í•™ìŠµ ì‹œìŠ¤í…œ
python src/tests/test_predictor.py          # ì¶”ë¡  ì‹œìŠ¤í…œ
```

### ê³ ê¸‰ ëª¨ë“ˆ (7ê°œ í…ŒìŠ¤íŠ¸, 42ê°œ í•­ëª©)

```bash
python src/tests/test_lora_loader.py        # LLM LoRA (4ê°œ)
python src/tests/test_augmentation.py       # ë°ì´í„° ì¦ê°• (7ê°œ)
python src/tests/test_kfold.py              # K-Fold CV (6ê°œ)
python src/tests/test_ensemble.py           # ì•™ìƒë¸” (6ê°œ)
python src/tests/test_solar_api.py          # Solar API (7ê°œ)
python src/tests/test_optuna.py             # Optuna (7ê°œ)
python src/tests/test_prompts.py            # í”„ë¡¬í”„íŠ¸ (9ê°œ)
```

---

# ğŸ“Œ Part 5: ê²°ê³¼ íŒŒì¼ ê´€ë¦¬

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
natural-language-processing-competition/
â”œâ”€â”€ logs/                        # ë¡œê·¸ íŒŒì¼
â”‚   â””â”€â”€ YYYYMMDD/
â”‚       â”œâ”€â”€ train/               # í•™ìŠµ ë¡œê·¸
â”‚       â””â”€â”€ inference/           # ì¶”ë¡  ë¡œê·¸
â”‚
â”œâ”€â”€ outputs/                     # ëª¨ë¸ ì¶œë ¥
â”‚   â””â”€â”€ {experiment}/
â”‚       â”œâ”€â”€ checkpoint-{N}/      # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚       â”œâ”€â”€ final_model/         # ìµœì¢… ëª¨ë¸
â”‚       â””â”€â”€ logs/                # Trainer ë¡œê·¸
â”‚
â”œâ”€â”€ submissions/                 # ì œì¶œ íŒŒì¼
â”‚   â”œâ”€â”€ submission.csv
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ cache/                       # ìºì‹œ (Solar API ë“±)
    â””â”€â”€ solar/
```

## ğŸ“ ë¡œê·¸ íŒŒì¼

**ê²½ë¡œ ê·œì¹™:** `logs/YYYYMMDD/{log_type}/{filename}.log`

**ì˜ˆì‹œ:**
- í•™ìŠµ: `logs/20251011/train/train_baseline_kobart_20251011_150300.log`
- ì¶”ë¡ : `logs/20251011/inference/inference_final_model_20251011_150500.log`

**ë‚´ìš©:**
- ì‹¤í–‰ ì‹œì‘/ì¢…ë£Œ ì‹œê°
- GPU ì •ë³´ (ì´ë¦„, ë©”ëª¨ë¦¬, tier)
- Config ì„¤ì •ê°’
- í•™ìŠµ/ì¶”ë¡  ì§„í–‰ ìƒí™©
- ìµœì¢… í‰ê°€ ê²°ê³¼
- ì—ëŸ¬ ë©”ì‹œì§€

## ğŸ’¾ ëª¨ë¸ íŒŒì¼

**ê²½ë¡œ:** `outputs/{experiment_name}/`

**êµ¬ì¡°:**
```
outputs/baseline_kobart/
â”œâ”€â”€ checkpoint-500/              # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â””â”€â”€ training_args.bin
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ final_model/                 # ìµœì¢… ëª¨ë¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ vocab.txt
â””â”€â”€ logs/                        # Trainer ë¡œê·¸
```

**ìš©ëŸ‰:**
- KoBART: ì•½ 500MB/ì²´í¬í¬ì¸íŠ¸
- ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€ (save_total_limit: 3)

## ğŸ“Š ì œì¶œ íŒŒì¼

**ê²½ë¡œ:** `submissions/{filename}.csv`

**í˜•ì‹:**
```csv
fname,summary
test_001,ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤
test_002,íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤
...
```

**ìƒ˜í”Œ ìˆ˜:** 2,500ê°œ (test.csv ê¸°ì¤€)

---

# ğŸ“Œ Part 6: ë¬¸ì œ í•´ê²°

## âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ë¬¸ì œ:** `CUDA out of memory`

**í•´ê²° ë°©ë²•:**

### ë°©ë²• 1: ë””ë²„ê·¸ ëª¨ë“œ

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

### ë°©ë²• 2: Config ìˆ˜ì •

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  batch_size: 16  # 50 â†’ 16
  gradient_accumulation_steps: 4  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
```

### ë°©ë²• 3: QLoRA ì‚¬ìš© (LLM)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**íš¨ê³¼:** 24GB â†’ 8-10GB

---

## âŒ WandB ë¡œê·¸ì¸ í•„ìš”

**ë¬¸ì œ:** `wandb: ERROR Please log in`

**í•´ê²° ë°©ë²•:**

### ë°©ë²• 1: ë¡œê·¸ì¸

```bash
wandb login
```

### ë°©ë²• 2: ë””ë²„ê·¸ ëª¨ë“œ (ìë™ ë¹„í™œì„±í™”)

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

### ë°©ë²• 3: Config ìˆ˜ì •

```yaml
# configs/experiments/baseline_kobart.yaml
wandb:
  enabled: false
```

---

## âŒ ëª¨ë¸ ê²½ë¡œ ì°¾ì„ ìˆ˜ ì—†ìŒ

**ë¬¸ì œ:** `Model not found: outputs/baseline_kobart/final_model`

**í•´ê²° ë°©ë²•:**

### ê²½ë¡œ í™•ì¸

```bash
ls -la outputs/baseline_kobart/
```

### ì •í™•í•œ ê²½ë¡œ ì‚¬ìš©

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

---

## âŒ ë¡œê·¸ íŒŒì¼ ì°¾ê¸°

**ë¬¸ì œ:** ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜ë¥¼ ëª¨ë¦„

**í•´ê²° ë°©ë²•:**

### ì˜¤ëŠ˜ ë‚ ì§œ ë¡œê·¸ í™•ì¸

```bash
ls logs/$(date +%Y%m%d)/
```

### ìµœì‹  í•™ìŠµ ë¡œê·¸

```bash
ls -lt logs/$(date +%Y%m%d)/train/ | head
```

### ë¡œê·¸ ë‚´ìš© ë³´ê¸°

```bash
tail -100 logs/$(date +%Y%m%d)/train/train_baseline_kobart_*.log
```

---

## âŒ í† í¬ë‚˜ì´ì € íŠ¹ìˆ˜ í† í° ê²½ê³ 

**ë¬¸ì œ:** `Token indices sequence length is longer than...`

**í•´ê²° ë°©ë²•:**

```python
# íŠ¹ìˆ˜ í† í° ëª…ì‹œì  ì¶”ê°€
special_tokens = ['#Person1#', '#Person2#']
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
model.resize_token_embeddings(len(tokenizer))
```

---

## ğŸ“Š ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (A6000 ê¸°ì¤€)

### í•™ìŠµ

| ëª¨ë“œ | ë°ì´í„° | ì—í¬í¬ | ë°°ì¹˜ | ì‹œê°„ |
|------|--------|--------|------|------|
| ë””ë²„ê·¸ | 100ê°œ | 2 | 4 | ~2ë¶„ |
| ì „ì²´ (KoBART) | 12,457ê°œ | 20 | 50 | ~4-6ì‹œê°„ |
| LLM (Llama 3B) | 12,457ê°œ | 3 | 16 | ~8-12ì‹œê°„ |

### ì¶”ë¡ 

| ë°ì´í„° | ë°°ì¹˜ | ì‹œê°„ |
|--------|------|------|
| 2,500ê°œ | 32 | ~3-5ë¶„ |
| 2,500ê°œ | 16 | ~5-7ë¶„ |

### Full Pipeline

| ëª¨ë“œ | ì‹œê°„ |
|------|------|
| í•™ìŠµ + ì¶”ë¡  | ~4-6ì‹œê°„ |
| ì¶”ë¡ ë§Œ | ~3-5ë¶„ |

---

## ğŸ“ Config íŒŒì¼ ìœ„ì¹˜

### ì‹¤í—˜ Config

```
configs/experiments/
â”œâ”€â”€ baseline_kobart.yaml
â”œâ”€â”€ llama_3.2_3b.yaml
â””â”€â”€ qwen3_4b.yaml
```

### Base Config

```
configs/base/
â”œâ”€â”€ default.yaml                 # ì „ì²´ ê¸°ë³¸ ì„¤ì •
â”œâ”€â”€ encoder_decoder.yaml         # Encoder-Decoder
â””â”€â”€ causal_lm.yaml               # Causal LM (LLM)
```

### ëª¨ë¸ Config

```
configs/models/
â”œâ”€â”€ kobart.yaml
â”œâ”€â”€ llama_3.2_3b.yaml
â””â”€â”€ qwen3_4b.yaml
```

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) | ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë° ê³µí†µ ì¸í”„ë¼ |
| [03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) | ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°• |
| [04_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./04_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) | ëª¨ë¸ ë¡œë”©, í•™ìŠµ, ì¶”ë¡ , LLM |
| [05_í‰ê°€_ìµœì í™”.md](./05_í‰ê°€_ìµœì í™”.md) | í‰ê°€, êµì°¨ ê²€ì¦, Optuna |
| [06_ì•™ìƒë¸”_API.md](./06_ì•™ìƒë¸”_API.md) | ì•™ìƒë¸”, Solar API, í”„ë¡¬í”„íŠ¸ |
