# 시작 가이드 (빠른 시작 + 실행 명령어 + 스크립트 사용법)

## 📋 목차
1. [5분 빠른 시작](#5분-빠른-시작)
2. [전체 실행 명령어](#전체-실행-명령어)
3. [스크립트 상세 사용법](#스크립트-상세-사용법)
4. [테스트 실행](#테스트-실행)
5. [결과 파일 관리](#결과-파일-관리)
6. [문제 해결](#문제-해결)

---

# 📌 Part 1: 5분 빠른 시작

## 🚀 1단계: 환경 설정

```bash
# 가상환경 활성화
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# 또는
pyenv activate nlp_py3_11_9

# 프로젝트 루트로 이동
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition

# 패키지 확인
pip list | grep -E "(torch|transformers|wandb)"
```

## 📝 2단계: 전체 모듈 테스트

```bash
# 기본 모듈 테스트 (6개)
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py

# 고급 모듈 테스트 (7개)
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py && \
python src/tests/test_ensemble.py && \
python src/tests/test_solar_api.py && \
python src/tests/test_optuna.py && \
python src/tests/test_prompts.py
```

**결과:** 총 79개 테스트 (100% 통과)

## ⚡ 3단계: 주요 코드 패턴

### Config 로드

```python
from src.config import load_config

config = load_config("baseline_kobart")
```

### 데이터 전처리

```python
from src.data import DialoguePreprocessor
import pandas as pd

preprocessor = DialoguePreprocessor()
df = pd.read_csv("data/raw/train.csv")
df_processed = preprocessor.preprocess_dataframe(df)
```

### 모델 로딩

```python
from src.models import load_model_and_tokenizer

model, tokenizer = load_model_and_tokenizer(config)
print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### LLM 파인튜닝 (Llama + QLoRA)

```python
from src.models.lora_loader import load_lora_model_and_tokenizer

model, tokenizer = load_lora_model_and_tokenizer(
    config,
    use_qlora=True  # 4-bit 양자화
)
```

### 데이터 증강 (4배)

```python
from src.data.augmentation import augment_dataset

aug_dialogues, aug_summaries = augment_dataset(
    dialogues,
    summaries,
    methods=['shuffle', 'sample'],
    n_aug=2
)
# 12,457개 → 49,828개
```

### 앙상블 추론

```python
from src.ensemble import ModelManager

manager = ModelManager()
manager.load_models([path1, path2, path3])

ensemble = manager.create_ensemble('weighted', weights=[0.5, 0.3, 0.2])
predictions = ensemble.predict(dialogues)
```

### 학습 실행

```python
from src.training import create_trainer
from src.data import DialogueSummarizationDataset

train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer
)

trainer = create_trainer(config, model, tokenizer, train_dataset)
results = trainer.train()
```

### 추론 실행

```python
from src.inference import create_predictor

predictor = create_predictor(model, tokenizer, config)

# 단일 예측
summary = predictor.predict_single("#Person1#: 안녕하세요")

# 제출 파일 생성
test_df = pd.read_csv("data/raw/test.csv")
submission = predictor.create_submission(
    test_df,
    output_path="submissions/submission.csv"
)
```

---

# 📌 Part 2: 전체 실행 명령어

## 🚀 학습 실행

### 1. 기본 학습 (KoBART)

```bash
python scripts/train.py --experiment baseline_kobart
```

**결과 파일:**
- 모델: `outputs/baseline_kobart/final_model/`
- 체크포인트: `outputs/baseline_kobart/checkpoint-{N}/`
- 로그: `logs/YYYYMMDD/train/train_baseline_kobart_YYYYMMDD_HHMMSS.log`

### 2. 디버그 모드 (빠른 테스트)

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

**디버그 설정:**
- 데이터: 학습 100개, 검증 20개
- 에포크: 2회
- 배치 크기: 4
- WandB: 비활성화
- 예상 시간: ~2분 (A6000)

### 3. LLM 파인튜닝 (Llama-3.2-3B + QLoRA)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**결과:**
- 모델: `outputs/llama_3.2_3b_qlora/final_model/`
- 메모리: 24GB → 8-10GB (4-bit 양자화)

### 4. LLM Instruction Tuning (데이터 5배 증강)

```bash
python scripts/train_llm.py \
    --experiment llama_3.2_3b \
    --use_qlora \
    --use_instruction_augmentation
```

**효과:**
- 학습 데이터: 12,457개 → 62,285개 (5배)
- 5가지 instruction 템플릿 적용

### 5. K-Fold 교차 검증 학습

```bash
python scripts/train_with_cv.py \
    --experiment baseline_kobart \
    --n_folds 5 \
    --stratify length
```

**효과:**
- 5-Fold 교차 검증
- 길이 기반 계층화 분할
- 평균 성능 및 표준편차 계산

---

## 🔮 추론 실행

### 1. 기본 추론

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

**결과:**
- 제출 파일: `submissions/submission.csv`
- 샘플 수: 2,500개
- 예상 시간: ~3-5분 (A6000)

### 2. 특정 체크포인트 추론

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/checkpoint-1000 \
    --output submissions/checkpoint_1000.csv
```

### 3. 추론 파라미터 조정

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv \
    --batch_size 16 \
    --num_beams 8
```

**옵션:**
- `--test_data`: 테스트 데이터 경로 (기본: `data/raw/test.csv`)
- `--batch_size`: 배치 크기 (기본: 32)
- `--num_beams`: Beam search 빔 개수 (기본: 4)
- `--experiment`: Config 이름 (기본: `baseline_kobart`)

### 4. 앙상블 추론

```bash
python scripts/inference_ensemble.py \
    --models outputs/model1/final_model outputs/model2/final_model outputs/model3/final_model \
    --weights 0.5 0.3 0.2 \
    --output submissions/ensemble.csv
```

**옵션:**
- `--models`: 모델 경로 목록 (공백으로 구분)
- `--weights`: 가중치 목록 (합이 1.0, 생략 시 동일 가중치)
- `--method`: 앙상블 방법 (`weighted` 또는 `voting`)

### 5. Solar API 추론

```bash
export SOLAR_API_KEY="your_api_key_here"

python scripts/inference_solar.py \
    --test_data data/raw/test.csv \
    --output submissions/solar.csv \
    --batch_size 10 \
    --token_limit 512
```

**효과:**
- Zero-shot 추론
- 토큰 최적화 70% 절약
- Few-shot Learning 지원

---

## 🔄 Full Pipeline 실행

### 1. 학습 + 추론 한 번에

```bash
python scripts/run_pipeline.py --experiment baseline_kobart
```

**실행 흐름:**
1. 학습 → `outputs/baseline_kobart/final_model/`
2. 추론 → `submissions/submission.csv`

**예상 시간:** ~4-6시간 (A6000)

### 2. 학습 건너뛰고 추론만

```bash
python scripts/run_pipeline.py \
    --experiment baseline_kobart \
    --skip_training \
    --model_path outputs/baseline_kobart/final_model \
    --output submissions/submission_v2.csv
```

---

## 🎯 고급 실행 패턴

### 패턴 1: 빠른 프로토타입

```bash
# 1. 디버그 모드로 빠른 학습
python scripts/train.py --experiment baseline_kobart --debug

# 2. 학습된 모델 확인
ls outputs/baseline_kobart/final_model/

# 3. 추론 테스트
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/debug_test.csv \
    --batch_size 4
```

### 패턴 2: 여러 체크포인트 비교

```bash
# 각 체크포인트로 추론
for checkpoint in checkpoint-500 checkpoint-1000 checkpoint-1500 final_model
do
    python scripts/inference.py \
        --model outputs/baseline_kobart/$checkpoint \
        --output submissions/${checkpoint}.csv
done

# 결과 비교
ls -lh submissions/
```

### 패턴 3: 실험 추적

```bash
# 여러 실험 순차 실행
experiments=("baseline_kobart" "experiment_v2" "experiment_v3")

for exp in "${experiments[@]}"
do
    echo "실험 시작: $exp"
    python scripts/run_pipeline.py --experiment $exp
    echo "실험 완료: $exp"
    echo "---"
done

# 모든 실험 결과 확인
ls -R outputs/
ls submissions/
```

### 패턴 4: Optuna 하이퍼파라미터 최적화

```bash
python scripts/optimize.py \
    --experiment baseline_kobart \
    --n_trials 50 \
    --output_dir outputs/optuna_results
```

**효과:**
- 50회 Trial로 최적 하이퍼파라미터 탐색
- TPE Sampler + Median Pruner
- 결과: `outputs/optuna_results/best_params.json`

---

# 📌 Part 3: 스크립트 상세 사용법

## 📝 사용 가능한 스크립트

| 스크립트 | 설명 | 용도 |
|---------|------|------|
| `train.py` | 기본 모델 학습 | KoBART 등 Encoder-Decoder 학습 |
| `inference.py` | 추론 및 제출 파일 생성 | 학습된 모델로 예측 |
| `run_pipeline.py` | 전체 파이프라인 | 학습 + 추론 한 번에 |
| `train_llm.py` | LLM 파인튜닝 | Llama/Qwen + QLoRA |
| `train_with_cv.py` | K-Fold 교차 검증 | 5-Fold CV 학습 |
| `inference_ensemble.py` | 앙상블 추론 | 다중 모델 앙상블 |
| `inference_solar.py` | Solar API 추론 | API 기반 Zero-shot |
| `optimize.py` | Optuna 최적화 | 하이퍼파라미터 자동 튜닝 |

## 🚀 train.py - 기본 학습 스크립트

### 명령어 옵션

```bash
python scripts/train.py --help
```

**필수 옵션:**
- `--experiment`: 실험 Config 이름

**선택 옵션:**
- `--debug`: 디버그 모드 (작은 데이터, 2 에포크, WandB OFF)

### 실행 단계

```
[1/6] Config 로딩 (configs/experiments/{experiment}.yaml)
[2/6] 데이터 로딩 (train.csv, dev.csv)
[3/6] 모델 로딩 (KoBART 등)
[4/6] Dataset 생성
[5/6] 학습 실행 (WandB 로깅)
[6/6] 모델 저장 (outputs/{experiment}/)
```

### 출력 예시

```
============================================================
학습 시작: baseline_kobart
============================================================

[1/6] Config 로딩...
  ✅ Config 로드 완료 (seed: 42)

[2/6] 데이터 로딩...
  ✅ 학습 데이터: 12,457개
  ✅ 검증 데이터: 1,384개

[3/6] 모델 로딩...
  → 디바이스: cuda
  → 전체 파라미터: 123,859,968

[4/6] Dataset 생성...
  ✅ 학습 Dataset: 12,457개

[5/6] 학습 시작...
[학습 로그...]

최종 평가 결과:
  eval_rouge1: 0.4521
  eval_rouge2: 0.2134
  eval_rougeL: 0.3890

[6/6] 학습 완료!
  최종 모델: outputs/baseline_kobart/final_model
============================================================
```

## 🔮 inference.py - 추론 스크립트

### 명령어 옵션

**필수 옵션:**
- `--model`: 모델 체크포인트 경로
- `--output`: 제출 파일 출력 경로

**선택 옵션:**
- `--test_data`: 테스트 데이터 (기본: `data/raw/test.csv`)
- `--batch_size`: 배치 크기 (기본: 32)
- `--num_beams`: Beam 개수 (기본: 4)
- `--experiment`: Config 이름 (기본: `baseline_kobart`)

### 출력 예시

```
============================================================
추론 시작
============================================================

[1/5] Config 로딩...
  ✅ Config: baseline_kobart

[2/5] 모델 로딩: outputs/baseline_kobart/final_model
  ✅ 모델 파라미터: 123,859,968

[3/5] 테스트 데이터: data/raw/test.csv
  ✅ 테스트 샘플: 2,500개

[4/5] 추론 실행...
Predicting: 100%|██████████| 79/79 [02:15<00:00]

[5/5] 추론 완료!
  ✅ 제출 파일: submissions/submission.csv

  샘플 예측 결과 (처음 3개):
    [test_001]: 두 사람이 인사를 나누었다
    [test_002]: 날씨가 좋아서 산책을 가기로 했다
    [test_003]: 점심 메뉴로 김치찌개를 선택했다
============================================================
```

## 🤖 train_llm.py - LLM 파인튜닝 스크립트

### 명령어 옵션

**필수 옵션:**
- `--experiment`: LLM Config 이름 (`llama_3.2_3b`, `qwen3_4b` 등)

**선택 옵션:**
- `--use_qlora`: QLoRA 4-bit 양자화 사용
- `--use_instruction_augmentation`: Instruction 데이터 5배 증강
- `--debug`: 디버그 모드

### 사용 예시

```bash
# Llama-3.2-3B QLoRA 학습
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora

# Instruction Tuning (데이터 5배)
python scripts/train_llm.py \
    --experiment llama_3.2_3b \
    --use_qlora \
    --use_instruction_augmentation

# Qwen2.5-3B 학습
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

### 특징

- **LoRA 파라미터:** r=16, alpha=32, dropout=0.05
- **Target Modules:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **4-bit 양자화:** BitsAndBytesConfig (nf4, double_quant)
- **메모리 절감:** 24GB → 8-10GB

---

# 📌 Part 4: 테스트 실행

## 🧪 전체 테스트 (79개)

```bash
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py && \
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py && \
python src/tests/test_ensemble.py && \
python src/tests/test_solar_api.py && \
python src/tests/test_optuna.py && \
python src/tests/test_prompts.py
```

## 📋 개별 테스트

### 기본 모듈 (6개 테스트, 37개 항목)

```bash
python src/tests/test_config_loader.py      # Config 로딩
python src/tests/test_preprocessor.py       # 데이터 전처리
python src/tests/test_model_loader.py       # 모델 로더
python src/tests/test_metrics.py            # ROUGE 평가
python src/tests/test_trainer.py            # 학습 시스템
python src/tests/test_predictor.py          # 추론 시스템
```

### 고급 모듈 (7개 테스트, 42개 항목)

```bash
python src/tests/test_lora_loader.py        # LLM LoRA (4개)
python src/tests/test_augmentation.py       # 데이터 증강 (7개)
python src/tests/test_kfold.py              # K-Fold CV (6개)
python src/tests/test_ensemble.py           # 앙상블 (6개)
python src/tests/test_solar_api.py          # Solar API (7개)
python src/tests/test_optuna.py             # Optuna (7개)
python src/tests/test_prompts.py            # 프롬프트 (9개)
```

---

# 📌 Part 5: 결과 파일 관리

## 📂 디렉토리 구조

```
natural-language-processing-competition/
├── logs/                        # 로그 파일
│   └── YYYYMMDD/
│       ├── train/               # 학습 로그
│       └── inference/           # 추론 로그
│
├── outputs/                     # 모델 출력
│   └── {experiment}/
│       ├── checkpoint-{N}/      # 중간 체크포인트
│       ├── final_model/         # 최종 모델
│       └── logs/                # Trainer 로그
│
├── submissions/                 # 제출 파일
│   ├── submission.csv
│   └── ...
│
└── cache/                       # 캐시 (Solar API 등)
    └── solar/
```

## 📝 로그 파일

**경로 규칙:** `logs/YYYYMMDD/{log_type}/{filename}.log`

**예시:**
- 학습: `logs/20251011/train/train_baseline_kobart_20251011_150300.log`
- 추론: `logs/20251011/inference/inference_final_model_20251011_150500.log`

**내용:**
- 실행 시작/종료 시각
- GPU 정보 (이름, 메모리, tier)
- Config 설정값
- 학습/추론 진행 상황
- 최종 평가 결과
- 에러 메시지

## 💾 모델 파일

**경로:** `outputs/{experiment_name}/`

**구조:**
```
outputs/baseline_kobart/
├── checkpoint-500/              # 중간 체크포인트
│   ├── config.json
│   ├── pytorch_model.bin
│   ├── trainer_state.json
│   └── training_args.bin
├── checkpoint-1000/
├── final_model/                 # 최종 모델
│   ├── config.json
│   ├── pytorch_model.bin
│   ├── tokenizer_config.json
│   ├── special_tokens_map.json
│   └── vocab.txt
└── logs/                        # Trainer 로그
```

**용량:**
- KoBART: 약 500MB/체크포인트
- 최대 3개 체크포인트 유지 (save_total_limit: 3)

## 📊 제출 파일

**경로:** `submissions/{filename}.csv`

**형식:**
```csv
fname,summary
test_001,두 사람이 저녁 약속을 잡았다
test_002,회의 시간을 3시로 정했다
...
```

**샘플 수:** 2,500개 (test.csv 기준)

---

# 📌 Part 6: 문제 해결

## ❌ GPU 메모리 부족

**문제:** `CUDA out of memory`

**해결 방법:**

### 방법 1: 디버그 모드

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

### 방법 2: Config 수정

```yaml
# configs/experiments/baseline_kobart.yaml
training:
  batch_size: 16  # 50 → 16
  gradient_accumulation_steps: 4  # 그래디언트 누적
```

### 방법 3: QLoRA 사용 (LLM)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**효과:** 24GB → 8-10GB

---

## ❌ WandB 로그인 필요

**문제:** `wandb: ERROR Please log in`

**해결 방법:**

### 방법 1: 로그인

```bash
wandb login
```

### 방법 2: 디버그 모드 (자동 비활성화)

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

### 방법 3: Config 수정

```yaml
# configs/experiments/baseline_kobart.yaml
wandb:
  enabled: false
```

---

## ❌ 모델 경로 찾을 수 없음

**문제:** `Model not found: outputs/baseline_kobart/final_model`

**해결 방법:**

### 경로 확인

```bash
ls -la outputs/baseline_kobart/
```

### 정확한 경로 사용

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

---

## ❌ 로그 파일 찾기

**문제:** 로그 파일 위치를 모름

**해결 방법:**

### 오늘 날짜 로그 확인

```bash
ls logs/$(date +%Y%m%d)/
```

### 최신 학습 로그

```bash
ls -lt logs/$(date +%Y%m%d)/train/ | head
```

### 로그 내용 보기

```bash
tail -100 logs/$(date +%Y%m%d)/train/train_baseline_kobart_*.log
```

---

## ❌ 토크나이저 특수 토큰 경고

**문제:** `Token indices sequence length is longer than...`

**해결 방법:**

```python
# 특수 토큰 명시적 추가
special_tokens = ['#Person1#', '#Person2#']
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})
model.resize_token_embeddings(len(tokenizer))
```

---

## 📊 예상 실행 시간 (A6000 기준)

### 학습

| 모드 | 데이터 | 에포크 | 배치 | 시간 |
|------|--------|--------|------|------|
| 디버그 | 100개 | 2 | 4 | ~2분 |
| 전체 (KoBART) | 12,457개 | 20 | 50 | ~4-6시간 |
| LLM (Llama 3B) | 12,457개 | 3 | 16 | ~8-12시간 |

### 추론

| 데이터 | 배치 | 시간 |
|--------|------|------|
| 2,500개 | 32 | ~3-5분 |
| 2,500개 | 16 | ~5-7분 |

### Full Pipeline

| 모드 | 시간 |
|------|------|
| 학습 + 추론 | ~4-6시간 |
| 추론만 | ~3-5분 |

---

## 📝 Config 파일 위치

### 실험 Config

```
configs/experiments/
├── baseline_kobart.yaml
├── llama_3.2_3b.yaml
└── qwen3_4b.yaml
```

### Base Config

```
configs/base/
├── default.yaml                 # 전체 기본 설정
├── encoder_decoder.yaml         # Encoder-Decoder
└── causal_lm.yaml               # Causal LM (LLM)
```

### 모델 Config

```
configs/models/
├── kobart.yaml
├── llama_3.2_3b.yaml
└── qwen3_4b.yaml
```

---

## 🔗 다음 단계

| 문서 | 설명 |
|------|------|
| [02_핵심_시스템.md](./02_핵심_시스템.md) | 시스템 아키텍처 및 공통 인프라 |
| [03_데이터_파이프라인.md](./03_데이터_파이프라인.md) | 데이터 처리 및 증강 |
| [04_모델_학습_추론.md](./04_모델_학습_추론.md) | 모델 로딩, 학습, 추론, LLM |
| [05_평가_최적화.md](./05_평가_최적화.md) | 평가, 교차 검증, Optuna |
| [06_앙상블_API.md](./06_앙상블_API.md) | 앙상블, Solar API, 프롬프트 |
