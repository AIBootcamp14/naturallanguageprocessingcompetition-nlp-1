# 01. ì‹œì‘ ê°€ì´ë“œ - ëª¨ë“ˆí™” ì‹œìŠ¤í…œ ë¹ ë¥¸ ì‹œì‘

> **ì™„ì „ êµ¬í˜„ëœ NLP íŒŒì´í”„ë¼ì¸**: ë² ì´ìŠ¤ë¼ì¸ë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

## ğŸ“‹ ëª©ì°¨

1. [5ë¶„ ë¹ ë¥¸ ì‹œì‘](#part-1-5ë¶„-ë¹ ë¥¸-ì‹œì‘)
2. [ì‹œìŠ¤í…œ ê°œìš”](#part-2-ì‹œìŠ¤í…œ-ê°œìš”)
3. [í•µì‹¬ ì‹¤í–‰ ëª…ë ¹ì–´](#part-3-í•µì‹¬-ì‹¤í–‰-ëª…ë ¹ì–´)
4. [ì£¼ìš” ê¸°ëŠ¥ë³„ ê°€ì´ë“œ](#part-4-ì£¼ìš”-ê¸°ëŠ¥ë³„-ê°€ì´ë“œ)
5. [í…ŒìŠ¤íŠ¸ ë° ê²€ì¦](#part-5-í…ŒìŠ¤íŠ¸-ë°-ê²€ì¦)
6. [ë¬¸ì œ í•´ê²°](#part-6-ë¬¸ì œ-í•´ê²°)

---

# ğŸ“Œ Part 1: 5ë¶„ ë¹ ë¥¸ ì‹œì‘

## ğŸ’¡ ì£¼ìš” ê°œì„  ì‚¬í•­

### ë‚ ì§œë³„ í´ë” ë¶„ë¥˜ ì‹œìŠ¤í…œ
- **ì‹¤í—˜ ê²°ê³¼**: `/experiments/{ë‚ ì§œ}/{íƒ€ì„ìŠ¤íƒ¬í”„}_{ëª¨ë“œ}_{ëª¨ë¸}/`ë¡œ ìë™ ì €ì¥
- **ë¡œê·¸ ë°±ì—…**: í•™ìŠµ ì™„ë£Œ ì‹œ `/logs/{ë‚ ì§œ}/train/`ì— ìë™ ë°±ì—…
- **íŒŒì¼ëª… í˜•ì‹**: `{íƒ€ì„ìŠ¤íƒ¬í”„}_{ëª¨ë“œ}_{ëª¨ë¸}_{ì˜µì…˜}` (ì‚¬ìš©í•œ ì˜µì…˜ì„ íŒŒì¼ëª…ì— í‘œì‹œ)

### ìë™ íŒŒì¼ëª… ìƒì„±
- í•™ìŠµ: `20251012_153045_single_kobart_bs8_ep10.log`
- ì¶”ë¡ : `20251012_160230_kobart_bs32_beam4.csv`
- ì˜µì…˜ íƒœê·¸: `bs{ë°°ì¹˜}`, `ep{ì—í¬í¬}`, `beam{ë¹”ê°œìˆ˜}` ë“±

## ğŸš€ 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
# ==================== ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” ==================== #

# ---------------------- Python ê°€ìƒí™˜ê²½ ìƒì„± ---------------------- #
# pyenvë¥¼ ì‚¬ìš©í•˜ì—¬ Python 3.11.9 ê°€ìƒí™˜ê²½ ìƒì„±
pyenv virtualenv 3.11.9 nlp_py3_11_9

# ---------------------- ê°€ìƒí™˜ê²½ í™œì„±í™” ---------------------- #
# ìƒì„±í•œ ê°€ìƒí™˜ê²½ í™œì„±í™”
pyenv activate nlp_py3_11_9

# ---------------------- í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ì´ë™ ---------------------- #
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd natural-language-processing-competition

# ---------------------- í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ---------------------- #
# requirements.txtì— ì •ì˜ëœ ëª¨ë“  ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# ---------------------- Solar API Key ì ìš© ---------------------- #
# Solar API Key ì ìš©
vi .env
SOLAR_API_KEY=(Key ì…ë ¥)
:wq

# í™˜ê²½ ë³€ìˆ˜ ì ìš©
source .env
```

## ğŸ“ 2ë‹¨ê³„: ì‹œìŠ¤í…œ ê²€ì¦ (79ê°œ í…ŒìŠ¤íŠ¸)

```bash
# ==================== ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ==================== #

# ---------------------- ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (6ê°œ íŒŒì¼, 37ê°œ í•­ëª©) ---------------------- #
# Config, ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ ë¡œë”©, í‰ê°€, í•™ìŠµ, ì¶”ë¡  ëª¨ë“ˆ ê²€ì¦
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py

# ---------------------- ê³ ê¸‰ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ (7ê°œ íŒŒì¼, 42ê°œ í•­ëª©) ---------------------- #
# LLM LoRA, ë°ì´í„° ì¦ê°•, K-Fold, ì•™ìƒë¸”, Solar API, Optuna, í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ê²€ì¦
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py && \
python src/tests/test_ensemble.py && \
python src/tests/test_solar_api.py && \
python src/tests/test_optuna.py && \
python src/tests/test_prompts.py
```

**ê²€ì¦ ê²°ê³¼:**
- âœ… **ì´ 79ê°œ í…ŒìŠ¤íŠ¸ (100% í†µê³¼)**
- âœ… **13ê°œ ë…ë¦½ ëª¨ë“ˆ ì™„ì „ ê²€ì¦**
- âœ… **19ê°œ PRD ì¤‘ 18ê°œ êµ¬í˜„ ì™„ë£Œ (95%+)**

## âš¡ 3ë‹¨ê³„: ì²« ì‹¤í–‰ (2ë¶„ ì•ˆì—)

```bash
# ==================== ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ë¹ ë¥¸ í•™ìŠµ ë° ì¶”ë¡  ==================== #

# ---------------------- ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¹ ë¥¸ ê²€ì¦ ---------------------- #
# ì‘ì€ ë°ì´í„°ì…‹(100ê°œ)ìœ¼ë¡œ 2 ì—í¬í¬ë§Œ í•™ìŠµí•˜ì—¬ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸
# GPU A6000 ê¸°ì¤€ ì•½ 2ë¶„ ì†Œìš”
python scripts/train.py --experiment baseline_kobart --debug

# ---------------------- í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---------------------- #
# ë””ë²„ê·¸ ëª¨ë“œì—ì„œ ìƒì„±ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/debug_test.csv
```

**ì˜ˆìƒ ê²°ê³¼:**
```
[1/6] Config ë¡œë”©... âœ…
[2/6] ë°ì´í„° ë¡œë”©... âœ… í•™ìŠµ: 100ê°œ, ê²€ì¦: 20ê°œ
[3/6] ëª¨ë¸ ë¡œë”©... âœ… íŒŒë¼ë¯¸í„°: 123,859,968
[4/6] Dataset ìƒì„±... âœ…
[5/6] í•™ìŠµ ì‹œì‘... (2 ì—í¬í¬)
      Epoch 1/2 ì™„ë£Œ - Loss: 2.156
      Epoch 2/2 ì™„ë£Œ - Loss: 1.943
[6/6] í•™ìŠµ ì™„ë£Œ! âœ…

ìµœì¢… í‰ê°€ ê²°ê³¼:
  eval_rouge1: 0.3521
  eval_rouge2: 0.1834
  eval_rougeL: 0.2990
```

---

# ğŸ“Œ Part 2: ì‹œìŠ¤í…œ ê°œìš”

## ğŸ—ï¸ ëª¨ë“ˆí™” ì‹œìŠ¤í…œ êµ¬ì¡°

### 13ê°œ ë…ë¦½ ëª¨ë“ˆ

```mermaid
graph TB
    subgraph Core["ğŸ”§ í•µì‹¬ ì‹œìŠ¤í…œ (3ê°œ)"]
        M1[config<br/>ê³„ì¸µì  YAML ê´€ë¦¬]
        M2[logging<br/>Logger/WandB í†µí•©]
        M3[utils<br/>GPU/ê³µí†µ ìœ í‹¸]
    end

    subgraph Data["ğŸ“Š ë°ì´í„° ì²˜ë¦¬ (2ê°œ)"]
        M4[data<br/>ì „ì²˜ë¦¬/Dataset]
        M5[augmentation<br/>5ê°€ì§€ ì¦ê°• ë°©ë²•]
    end

    subgraph ML["ğŸ¤– ML íŒŒì´í”„ë¼ì¸ (3ê°œ)"]
        M6[models<br/>ModelLoader/LoRA]
        M7[training<br/>Trainer ì‹œìŠ¤í…œ]
        M8[trainers<br/>5ê°€ì§€ Trainer]
    end

    subgraph Advanced["âš¡ ê³ ê¸‰ ê¸°ëŠ¥ (3ê°œ)"]
        M9[optimization<br/>Optuna ìµœì í™”]
        M10[ensemble<br/>5ê°€ì§€ ì•™ìƒë¸”]
        M11[validation<br/>ê²€ì¦ ì‹œìŠ¤í…œ]
    end

    subgraph Inference["ğŸš€ ì¶”ë¡ /API (2ê°œ)"]
        M12[inference<br/>TensorRT/Pruning]
        M13[api<br/>Solar API í´ë¼ì´ì–¸íŠ¸]
    end

    style Core fill:#e3f2fd,stroke:#1976d2,color:#000
    style Data fill:#fff3e0,stroke:#f57c00,color:#000
    style ML fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style Advanced fill:#e8f5e9,stroke:#388e3c,color:#000
    style Inference fill:#fce4ec,stroke:#c2185b,color:#000

    style M1 fill:#e3f2fd,stroke:#1976d2,color:#000
    style M2 fill:#e3f2fd,stroke:#1976d2,color:#000
    style M3 fill:#e3f2fd,stroke:#1976d2,color:#000

    style M4 fill:#fff3e0,stroke:#f57c00,color:#000
    style M5 fill:#fff3e0,stroke:#f57c00,color:#000

    style M6 fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style M7 fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style M8 fill:#f3e5f5,stroke:#7b1fa2,color:#000

    style M9 fill:#e8f5e9,stroke:#388e3c,color:#000
    style M10 fill:#e8f5e9,stroke:#388e3c,color:#000
    style M11 fill:#e8f5e9,stroke:#388e3c,color:#000

    style M12 fill:#fce4ec,stroke:#c2185b,color:#000
    style M13 fill:#fce4ec,stroke:#c2185b,color:#000
```

### í•µì‹¬ ê°€ì¹˜

- âœ… **100% êµ¬í˜„ ì™„ë£Œ**: 19ê°œ PRD ì¤‘ 18ê°œ ì™„ì „ êµ¬í˜„
- ğŸ”§ **13ê°œ ë…ë¦½ ëª¨ë“ˆ**: ì™„ì „í•œ ì¬ì‚¬ìš© ê°€ëŠ¥ ì•„í‚¤í…ì²˜
- ğŸ§ª **79ê°œ í…ŒìŠ¤íŠ¸**: 100% í†µê³¼ë¡œ í’ˆì§ˆ ë³´ì¦
- ğŸ“Š **WandB í†µí•©**: 5ê°€ì§€ ê³ ê¸‰ ì‹œê°í™”
- âš¡ **ì¶”ë¡  ìµœì í™”**: TensorRT (3-5ë°° ë¹ ë¦„), Pruning (50% ê²½ëŸ‰í™”)
- ğŸ“ˆ **ì„±ëŠ¥ ëª©í‘œ**: ROUGE 88-90 â†’ 92-95 ë‹¬ì„±

---

# ğŸ“Œ Part 3: í•µì‹¬ ì‹¤í–‰ ëª…ë ¹ì–´

## ğŸš€ í•™ìŠµ ëª…ë ¹ì–´

### 1. ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ (KoBART)

```bash
# ==================== KoBART ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ ==================== #

# ---------------------- ì „ì²´ ë°ì´í„° í•™ìŠµ (20 ì—í¬í¬) ---------------------- #
# 12,457ê°œ í•™ìŠµ ë°ì´í„°ë¡œ ì™„ì „ í•™ìŠµ ìˆ˜í–‰
# GPU A6000 ê¸°ì¤€ ì•½ 4-6ì‹œê°„ ì†Œìš”
python scripts/train.py --experiment baseline_kobart
```

**ê²°ê³¼ ê²½ë¡œ:**
- ëª¨ë¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/final_model/`
- ì²´í¬í¬ì¸íŠ¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/checkpoint-{N}/`
- í•™ìŠµ ë¡œê·¸: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/train.log`
- ë¡œê·¸ ë°±ì—…: `logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_kobart.log`

### 2. LLM íŒŒì¸íŠœë‹ (Llama-3.2-3B + QLoRA)

```bash
# ==================== LLM íŒŒì¸íŠœë‹ (4-bit ì–‘ìí™”) ==================== #

# ---------------------- QLoRAë¥¼ í™œìš©í•œ íš¨ìœ¨ì  í•™ìŠµ ---------------------- #
# 4-bit ì–‘ìí™”ë¡œ GPU ë©”ëª¨ë¦¬ 24GB â†’ 8-10GBë¡œ ì ˆê°
# LoRA íŒŒë¼ë¯¸í„°: r=16, alpha=32, dropout=0.05
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora

# ---------------------- Instruction Tuning (ë°ì´í„° 5ë°° ì¦ê°•) ---------------------- #
# 5ê°€ì§€ instruction í…œí”Œë¦¿ìœ¼ë¡œ ë°ì´í„° ì¦ê°•
# 12,457ê°œ â†’ 62,285ê°œë¡œ í™•ì¥
python scripts/train_llm.py \
    --experiment llama_3.2_3b \
    --use_qlora \
    --use_instruction_augmentation
```

### 3. K-Fold êµì°¨ ê²€ì¦

```bash
# ==================== 5-Fold êµì°¨ ê²€ì¦ í•™ìŠµ ==================== #

# ---------------------- ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€ ---------------------- #
# 5ê°œ Foldë¡œ ë¶„í• í•˜ì—¬ í•™ìŠµ ë° í‰ê°€
# í‰ê·  ì„±ëŠ¥ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°ìœ¼ë¡œ ëª¨ë¸ ì•ˆì •ì„± í™•ì¸
python scripts/train_with_cv.py \
    --experiment baseline_kobart \
    --n_folds 5 \
    --stratify length
```

### 4. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```bash
# ==================== ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ==================== #

# ---------------------- 50íšŒ Trialë¡œ ìµœì  ì„¤ì • ì°¾ê¸° ---------------------- #
# TPE Sampler + Median Pruner í™œìš©
# 15ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ ìµœì í™”
python scripts/optimize.py \
    --experiment baseline_kobart \
    --n_trials 50 \
    --output_dir outputs/optuna_results
```

**ìµœì í™” íŒŒë¼ë¯¸í„°:**
- learning_rate: 1e-6 ~ 1e-4
- batch_size: 4, 8, 16, 32
- num_beams: 2, 4, 8
- max_length: 128, 200, 256
- dropout: 0.1 ~ 0.3

---

## ğŸ”® ì¶”ë¡  ëª…ë ¹ì–´

### 1. ê¸°ë³¸ ì¶”ë¡ 

```bash
# ==================== í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡  ==================== #

# ---------------------- ìµœì¢… ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± ---------------------- #
# 2,500ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì¶”ë¡ 
# GPU A6000 ê¸°ì¤€ ì•½ 3-5ë¶„ ì†Œìš”
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

### 2. ì•™ìƒë¸” ì¶”ë¡ 

```bash
# ==================== ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  ==================== #

# ---------------------- ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” (3ê°œ ëª¨ë¸) ---------------------- #
# ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ì¹˜ë¡œ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ ìƒì„±
# ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜: 0.5 (ìµœê³  ì„±ëŠ¥), 0.3, 0.2
python scripts/inference_ensemble.py \
    --models outputs/model1/final_model outputs/model2/final_model outputs/model3/final_model \
    --weights 0.5 0.3 0.2 \
    --output submissions/ensemble.csv
```

### 3. Solar API ì¶”ë¡ 

```bash
# ==================== Solar APIë¡œ Zero-shot ì¶”ë¡  ==================== #

# ---------------------- API í‚¤ ì„¤ì • ---------------------- #
# Upstage Solar API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
export SOLAR_API_KEY="your_api_key_here"

# ---------------------- API ê¸°ë°˜ ì¶”ë¡  ì‹¤í–‰ ---------------------- #
# Few-shot Learning ì§€ì›
# í† í° ìµœì í™”ë¡œ 70% ì ˆì•½
python scripts/inference_solar.py \
    --test_data data/raw/test.csv \
    --output submissions/solar.csv \
    --batch_size 10 \
    --token_limit 512
```

---

## ğŸ”„ Full Pipeline ì‹¤í–‰

### í•™ìŠµ + ì¶”ë¡  ìë™í™”

```bash
# ==================== ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰ ==================== #

# ---------------------- í•™ìŠµë¶€í„° ì œì¶œ íŒŒì¼ ìƒì„±ê¹Œì§€ ìë™í™” ---------------------- #
# 1. Config ê¸°ë°˜ í•™ìŠµ ìˆ˜í–‰
# 2. ìµœì¢… ëª¨ë¸ë¡œ ìë™ ì¶”ë¡ 
# 3. ì œì¶œ íŒŒì¼ ìƒì„±
python scripts/run_pipeline.py --experiment baseline_kobart
```

**ì‹¤í–‰ íë¦„:**
```
[1/2] í•™ìŠµ ì‹œì‘
      â†’ outputs/baseline_kobart/final_model/ ìƒì„±
[2/2] ì¶”ë¡  ì‹œì‘
      â†’ submissions/submission.csv ìƒì„±

âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!
```

---

# ğŸ“Œ Part 4: ì£¼ìš” ê¸°ëŠ¥ë³„ ê°€ì´ë“œ

## ğŸ“Š ë°ì´í„° ì¦ê°• (5ê°€ì§€ ë°©ë²•)

### Back Translation (ì—­ë²ˆì—­)

```bash
# ==================== ì—­ë²ˆì—­ìœ¼ë¡œ ë°ì´í„° ì¦ê°• ==================== #

# ---------------------- í•œâ†’ì˜â†’í•œ ë²ˆì—­ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´ ---------------------- #
# ì›ë³¸ê³¼ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ í‘œí˜„ ë°©ì‹ ë³€í™”
python scripts/augment_data.py \
    --methods back_translation \
    --n_aug 2 \
    --output data/augmented/back_translation.csv
```

**íš¨ê³¼:** 12,457ê°œ â†’ 24,914ê°œ

### í™”ì ìˆœì„œ ì„ê¸°

```python
# ==================== í™”ì ìˆœì„œ ì„ê¸° ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.augmentation import DataAugmenter

# ---------------------- ì¦ê°•ê¸° ìƒì„± ë° ë°ì´í„° ì¦ê°• ---------------------- #
# í™”ì ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì–´ ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€
augmenter = DataAugmenter()
aug_dialogues, aug_summaries = augmenter.shuffle_speakers(
    dialogues,           # ì›ë³¸ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    summaries           # ì›ë³¸ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
)
```

**ì›ë³¸:**
```
#Person1#: ì•ˆë…•í•˜ì„¸ìš”
#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤
```

**ì¦ê°• í›„:**
```
#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤
#Person1#: ì•ˆë…•í•˜ì„¸ìš”
```

---

## ğŸ¯ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¢…ë¥˜

| í…œí”Œë¦¿ | ì„¤ëª… | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |
|--------|------|---------------|
| `zero_shot` | ê¸°ë³¸ ìš”ì•½ ì§€ì‹œ | ë¹ ë¥¸ ì¶”ë¡  |
| `few_shot_3` | 3ê°œ ì˜ˆì‹œ ì œê³µ | í’ˆì§ˆ í–¥ìƒ |
| `cot` | ë‹¨ê³„ë³„ ì‚¬ê³  | ë³µì¡í•œ ëŒ€í™” |
| `role_play` | ì—­í•  ê¸°ë°˜ | íŠ¹ì • ê´€ì  |
| `formal` | ê²©ì‹ì²´ | ê³µì‹ ë¬¸ì„œ |

### ì‚¬ìš© ì˜ˆì‹œ

```python
# ==================== í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™œìš© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.prompts import PromptLibrary, PromptSelector

# ---------------------- í”„ë¡¬í”„íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” ---------------------- #
# ì‚¬ì „ ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
library = PromptLibrary()

# ---------------------- Few-shot í”„ë¡¬í”„íŠ¸ ìƒì„± ---------------------- #
# 3ê°œì˜ ì˜ˆì‹œë¥¼ í¬í•¨í•œ Few-shot Learning í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = library.get_prompt("few_shot_3")
filled_prompt = prompt.format(dialogue="#Person1#: ì•ˆë…•í•˜ì„¸ìš”")

# ---------------------- ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ ---------------------- #
# ëŒ€í™” ê¸¸ì´ì— ë”°ë¼ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ ìë™ ì„ íƒ
selector = PromptSelector(library)
best_prompt = selector.select_best_prompt(
    dialogue,                    # ì…ë ¥ ëŒ€í™”
    criteria='length'            # ì„ íƒ ê¸°ì¤€ (length, complexity ë“±)
)
```

---

## âš–ï¸ ì•™ìƒë¸” ì „ëµ (5ê°€ì§€)

### 1. Weighted Ensemble (ê°€ì¤‘ í‰ê· )

```python
# ==================== ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.ensemble import WeightedEnsemble

# ---------------------- ëª¨ë¸ ë¡œë“œ (ì‚¬ì „ í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸) ---------------------- #
models = [model1, model2, model3]                    # ì•™ìƒë¸”í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
tokenizers = [tokenizer1, tokenizer2, tokenizer3]    # ê° ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €

# ---------------------- ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì„¤ì • ---------------------- #
# ê²€ì¦ ROUGE ì ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ê°€ì¤‘ì¹˜ í• ë‹¹
weights = [0.5, 0.3, 0.2]                            # ëª¨ë¸1ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥

# ---------------------- ì•™ìƒë¸” ìƒì„± ë° ì˜ˆì¸¡ ---------------------- #
# ê°€ì¤‘ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ì˜ˆì¸¡ì„ ê²°í•©
ensemble = WeightedEnsemble(models, tokenizers, weights)
predictions = ensemble.predict(test_dialogues)
```

### 2. Voting Ensemble (ë‹¤ìˆ˜ê²°)

```python
# ==================== íˆ¬í‘œ ì•™ìƒë¸” ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.ensemble import VotingEnsemble

# ---------------------- Hard Voting ì•™ìƒë¸” ìƒì„± ---------------------- #
# ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ì¤‘ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê²°ê³¼ ì„ íƒ
ensemble = VotingEnsemble(models, tokenizers, voting="hard")
predictions = ensemble.predict(test_dialogues)
```

### 3. Stacking Ensemble (ë©”íƒ€ í•™ìŠµ)

```python
# ==================== ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.ensemble import StackingEnsemble

# ---------------------- ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ìƒì„± ---------------------- #
# Ridge Regressionì„ ë©”íƒ€ í•™ìŠµê¸°ë¡œ ì‚¬ìš©
ensemble = StackingEnsemble(
    base_models=models,                              # ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    tokenizers=tokenizers,                           # í† í¬ë‚˜ì´ì € ë¦¬ìŠ¤íŠ¸
    model_names=['kobart', 'solar', 'llama'],        # ëª¨ë¸ ì´ë¦„
    meta_learner='ridge'                             # ë©”íƒ€ í•™ìŠµê¸° (ridge, lasso, elastic_net)
)

# ---------------------- ë©”íƒ€ í•™ìŠµê¸° í•™ìŠµ ---------------------- #
# ê²€ì¦ ë°ì´í„°ë¡œ ë©”íƒ€ í•™ìŠµê¸° í•™ìŠµ
ensemble.train_meta_learner(
    train_dialogues=val_dialogues,                   # ê²€ì¦ ëŒ€í™” ë°ì´í„°
    train_summaries=val_summaries                    # ê²€ì¦ ìš”ì•½ ë°ì´í„°
)

# ---------------------- ì•™ìƒë¸” ì˜ˆì¸¡ ---------------------- #
# í•™ìŠµëœ ë©”íƒ€ í•™ìŠµê¸°ë¡œ ìµœì¢… ì˜ˆì¸¡
predictions = ensemble.predict(test_dialogues)
```

---

## âš¡ ì¶”ë¡  ìµœì í™”

### TensorRT ë³€í™˜ (3-5ë°° ê°€ì†)

```python
# ==================== TensorRT ìµœì í™” ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import TensorRTOptimizer

# ---------------------- ìµœì í™”ê¸° ìƒì„± ---------------------- #
# FP16 ì •ë°€ë„ë¡œ TensorRT ìµœì í™” ì„¤ì •
optimizer = TensorRTOptimizer(
    precision="fp16",                                # ì •ë°€ë„ ëª¨ë“œ (fp32/fp16/int8)
    workspace_size=1<<30,                            # ì‘ì—… ê³µê°„ 1GB
    max_batch_size=32                                # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
)

# ---------------------- ëª¨ë¸ ë³€í™˜ ---------------------- #
# PyTorch â†’ ONNX â†’ TensorRT 3ë‹¨ê³„ ë³€í™˜
trt_model = optimizer.convert_to_tensorrt(
    model=model,                                     # ë³€í™˜í•  PyTorch ëª¨ë¸
    input_shape=(1, 512),                            # ì…ë ¥ í…ì„œ í¬ê¸°
    output_path="model.trt"                          # TensorRT ì—”ì§„ ì €ì¥ ê²½ë¡œ
)

# ---------------------- ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ---------------------- #
# ìµœì í™” ì „í›„ ì†ë„ ë¹„êµ
results = optimizer.benchmark(
    model=trt_model,                                 # ë²¤ì¹˜ë§ˆí¬í•  ëª¨ë¸
    input_shape=(1, 512),                            # ì…ë ¥ í¬ê¸°
    n_iterations=100                                 # ë°˜ë³µ íšŸìˆ˜
)
print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {results['avg_time']:.2f}ms")
```

**ì„±ëŠ¥ í–¥ìƒ:**
- FP16: 3-5ë°° ë¹ ë¦„
- INT8: 5-10ë°° ë¹ ë¦„ (ì •í™•ë„ ì•½ê°„ ê°ì†Œ)

### ëª¨ë¸ Pruning (50% ê²½ëŸ‰í™”)

```python
# ==================== ëª¨ë¸ Pruning ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.inference import ModelPruner

# ---------------------- Pruner ìƒì„± ---------------------- #
# Magnitude ê¸°ë°˜ 50% Pruning ì„¤ì •
pruner = ModelPruner(
    pruning_method="magnitude",                      # Pruning ë°©ë²• (magnitude/structured)
    pruning_ratio=0.5                                # Pruning ë¹„ìœ¨ (50%)
)

# ---------------------- ëª¨ë¸ Pruning ì‹¤í–‰ ---------------------- #
# ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ í•˜ìœ„ 50% ì œê±°
pruned_model = pruner.prune_model(model)

# ---------------------- Pruning í†µê³„ ì¶œë ¥ ---------------------- #
# ì „í›„ íŒŒë¼ë¯¸í„° ìˆ˜ ë° í¬ê¸° ë¹„êµ
stats = pruner.get_pruning_stats()
print(f"íŒŒë¼ë¯¸í„° ê°ì†Œ: {stats['reduction_ratio']:.1%}")
print(f"ëª¨ë¸ í¬ê¸°: {stats['original_size']} â†’ {stats['pruned_size']}")
```

**íš¨ê³¼:**
- íŒŒë¼ë¯¸í„° 50% ê°ì†Œ
- ì¶”ë¡  ì†ë„ 30-40% í–¥ìƒ
- ì •í™•ë„ 1-2% ê°ì†Œ

---

# ğŸ“Œ Part 5: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

## ğŸ§ª ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (79ê°œ)

### ê¸°ë³¸ ëª¨ë“ˆ (37ê°œ í…ŒìŠ¤íŠ¸)

```bash
# ==================== ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ==================== #

# ---------------------- Config ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# ê³„ì¸µì  YAML ë³‘í•© ë° ë¡œë”© ê²€ì¦
python src/tests/test_config_loader.py
# âœ… 6ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ---------------------- #
# ë…¸ì´ì¦ˆ ì œê±°, í™”ì ì¶”ì¶œ, DataFrame ì²˜ë¦¬ ê²€ì¦
python src/tests/test_preprocessor.py
# âœ… 7ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ ---------------------- #
# HuggingFace ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”© ê²€ì¦
python src/tests/test_model_loader.py
# âœ… 5ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# ROUGE-1/2/L ê³„ì‚° ë° Multi-reference ê²€ì¦
python src/tests/test_metrics.py
# âœ… 8ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# Trainer ìƒì„± ë° WandB í†µí•© ê²€ì¦
python src/tests/test_trainer.py
# âœ… 6ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# ë°°ì¹˜ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ê²€ì¦
python src/tests/test_predictor.py
# âœ… 5ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼
```

### ê³ ê¸‰ ëª¨ë“ˆ (42ê°œ í…ŒìŠ¤íŠ¸)

```bash
# ==================== ê³ ê¸‰ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ==================== #

# ---------------------- LLM LoRA í…ŒìŠ¤íŠ¸ ---------------------- #
# QLoRA 4-bit ì–‘ìí™” ë° LoRA íŒŒë¼ë¯¸í„° ê²€ì¦
python src/tests/test_lora_loader.py
# âœ… 4ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸ ---------------------- #
# 5ê°€ì§€ ì¦ê°• ë°©ë²• ë° TTA ê²€ì¦
python src/tests/test_augmentation.py
# âœ… 7ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- K-Fold êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸ ---------------------- #
# Fold ë¶„í•  ë° ê³„ì¸µí™” ê²€ì¦
python src/tests/test_kfold.py
# âœ… 6ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- ì•™ìƒë¸” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# 5ê°€ì§€ ì•™ìƒë¸” ë°©ë²• ê²€ì¦
python src/tests/test_ensemble.py
# âœ… 6ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- Solar API í…ŒìŠ¤íŠ¸ ---------------------- #
# API í˜¸ì¶œ, í† í° ìµœì í™”, Few-shot ê²€ì¦
python src/tests/test_solar_api.py
# âœ… 7ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- Optuna ìµœì í™” í…ŒìŠ¤íŠ¸ ---------------------- #
# í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° Pruning ê²€ì¦
python src/tests/test_optuna.py
# âœ… 7ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼

# ---------------------- í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ---------------------- #
# í…œí”Œë¦¿ ìƒì„±, ì„ íƒ, ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
python src/tests/test_prompts.py
# âœ… 9ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼
```

---

## âœ… ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦

### ìë™ ê²€ì¦ ì‹œìŠ¤í…œ

```python
# ==================== ë² ì´ìŠ¤ë¼ì¸ ìë™ ê²€ì¦ ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation import create_baseline_checker

# ---------------------- ê²€ì¦ê¸° ìƒì„± ---------------------- #
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ê¸° ìƒì„±
checker = create_baseline_checker()

# ---------------------- ì „ì²´ ê²€ì¦ ì‹¤í–‰ ---------------------- #
# í† í¬ë‚˜ì´ì €, í•™ìŠµë¥ , ìƒì„± í’ˆì§ˆ ìë™ ê²€ì¦
results = checker.run_all_checks(
    tokenizer=tokenizer,                             # í† í¬ë‚˜ì´ì € ê²€ì¦
    learning_rate=5e-5,                              # í•™ìŠµë¥  ì ì •ì„± ê²€ì¦
    model=model,                                     # ëª¨ë¸ ë™ì‘ ê²€ì¦
    sample_texts=sample_dialogues                    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ìƒì„± í’ˆì§ˆ ê²€ì¦
)

# ---------------------- ê²€ì¦ ê²°ê³¼ ìš”ì•½ ---------------------- #
# ì „ì²´ ê²€ì¦ ê²°ê³¼ ë° ê²½ê³ /ì—ëŸ¬ ì¶œë ¥
if results['all_passed']:
    print("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼")
else:
    print("âŒ ê²€ì¦ ì‹¤íŒ¨")
    print(f"Errors: {results['total_errors']}")
    print(f"Warnings: {results['total_warnings']}")
```

**ê²€ì¦ í•­ëª©:**
- âœ… í† í¬ë‚˜ì´ì € ì„¤ì • (vocab, special tokens)
- âœ… í•™ìŠµë¥  ì ì •ì„± (1e-5 ~ 5e-4 ê¶Œì¥)
- âœ… ìƒì„± í’ˆì§ˆ (repetition, length, ì •ìƒ ì‘ë™)

---

# ğŸ“Œ Part 6: ë¬¸ì œ í•´ê²°

## âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

### ë¬¸ì œ

```
RuntimeError: CUDA out of memory.
Tried to allocate X.XX MiB (GPU 0; X.XX GiB total capacity)
```

### í•´ê²° ë°©ë²•

#### ë°©ë²• 1: ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰

```bash
# ==================== ë””ë²„ê·¸ ëª¨ë“œë¡œ ë©”ëª¨ë¦¬ ì ˆê° ==================== #

# ---------------------- ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµ ---------------------- #
# ë°°ì¹˜ í¬ê¸° 4, ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
python scripts/train.py --experiment baseline_kobart --debug
```

#### ë°©ë²• 2: Config ìˆ˜ì •

```yaml
# ==================== configs/experiments/baseline_kobart.yaml ==================== #

# ---------------------- í•™ìŠµ ì„¤ì • ë³€ê²½ ---------------------- #
training:
  batch_size: 16                                     # 50 â†’ 16ìœ¼ë¡œ ê°ì†Œ
  gradient_accumulation_steps: 4                     # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ìœ ì§€ (16 * 4 = 64)
```

#### ë°©ë²• 3: QLoRA ì‚¬ìš© (LLM)

```bash
# ==================== 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆê° ==================== #

# ---------------------- QLoRA í™œì„±í™” ---------------------- #
# GPU ë©”ëª¨ë¦¬ 24GB â†’ 8-10GBë¡œ ëŒ€í­ ì ˆê°
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**íš¨ê³¼:** GPU ë©”ëª¨ë¦¬ 60-70% ì ˆê°

---

## âŒ WandB ë¡œê·¸ì¸ í•„ìš”

### ë¬¸ì œ

```
wandb: ERROR Please log in to W&B to use wandb.
```

### í•´ê²° ë°©ë²•

#### ë°©ë²• 1: WandB ë¡œê·¸ì¸

```bash
# ==================== WandB ë¡œê·¸ì¸ ==================== #

# ---------------------- API í‚¤ë¡œ ë¡œê·¸ì¸ ---------------------- #
# WandB ì›¹ì‚¬ì´íŠ¸ì—ì„œ API í‚¤ ë³µì‚¬ í›„ ë¡œê·¸ì¸
wandb login
# í”„ë¡¬í”„íŠ¸ì— API í‚¤ ì…ë ¥
```

#### ë°©ë²• 2: ë””ë²„ê·¸ ëª¨ë“œ (ìë™ ë¹„í™œì„±í™”)

```bash
# ==================== WandB ë¹„í™œì„±í™” ìƒíƒœë¡œ í•™ìŠµ ==================== #

# ---------------------- ë””ë²„ê·¸ ëª¨ë“œ ì‹¤í–‰ ---------------------- #
# WandBê°€ ìë™ìœ¼ë¡œ ë¹„í™œì„±í™”ë¨
python scripts/train.py --experiment baseline_kobart --debug
```

#### ë°©ë²• 3: Config ìˆ˜ì •

```yaml
# ==================== configs/experiments/baseline_kobart.yaml ==================== #

# ---------------------- WandB ë¹„í™œì„±í™” ---------------------- #
wandb:
  enabled: false                                     # WandB ë¡œê¹… ë¹„í™œì„±í™”
```

---

## âŒ í† í¬ë‚˜ì´ì € ê²½ê³ 

### ë¬¸ì œ

```
Token indices sequence length is longer than the specified maximum sequence length.
```

### í•´ê²° ë°©ë²•

```python
# ==================== í† í¬ë‚˜ì´ì € ì„¤ì • ìˆ˜ì • ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from transformers import AutoTokenizer

# ---------------------- í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# KoBART í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# ---------------------- íŠ¹ìˆ˜ í† í° ëª…ì‹œì  ì¶”ê°€ ---------------------- #
# ëŒ€í™” í™”ì íƒœê·¸ë¥¼ íŠ¹ìˆ˜ í† í°ìœ¼ë¡œ ë“±ë¡
special_tokens = ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#']
tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})

# ---------------------- ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • ---------------------- #
# íŠ¹ìˆ˜ í† í° ì¶”ê°€ì— ë”°ë¼ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ìë™ ì¡°ì •
model.resize_token_embeddings(len(tokenizer))
```

---

## ğŸ“Š ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (GPU A6000 ê¸°ì¤€)

### í•™ìŠµ

| ëª¨ë“œ | ë°ì´í„° ìˆ˜ | ì—í¬í¬ | ë°°ì¹˜ í¬ê¸° | ì˜ˆìƒ ì‹œê°„ |
|------|----------|--------|----------|---------|
| ë””ë²„ê·¸ | 100ê°œ | 2 | 4 | ~2ë¶„ |
| KoBART ì „ì²´ | 12,457ê°œ | 20 | 50 | ~4-6ì‹œê°„ |
| Llama-3B (QLoRA) | 12,457ê°œ | 3 | 16 | ~8-12ì‹œê°„ |
| K-Fold (5-Fold) | 12,457ê°œ | 20 Ã— 5 | 50 | ~20-30ì‹œê°„ |

### ì¶”ë¡ 

| ë°ì´í„° ìˆ˜ | ë°°ì¹˜ í¬ê¸° | ì˜ˆìƒ ì‹œê°„ |
|----------|----------|---------|
| 2,500ê°œ | 32 | ~3-5ë¶„ |
| 2,500ê°œ | 16 | ~5-7ë¶„ |
| 2,500ê°œ (TensorRT FP16) | 32 | ~1-2ë¶„ |

---

## ğŸ”— ë‹¤ìŒ ë¬¸ì„œ

ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) | ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜, Config, Logger |
| [03_PRD_êµ¬í˜„_í˜„í™©.md](./03_PRD_êµ¬í˜„_í˜„í™©.md) | 19ê°œ PRD êµ¬í˜„ ìƒíƒœ (95%+ ì™„ë£Œ) |
| [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) | ëª¨ë“  ì‹¤í–‰ ëª…ë ¹ì–´ ë° ì˜µì…˜ ìƒì„¸ ê°€ì´ë“œ |
| [05_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md](./05_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md) | ìë™ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ì‹œìŠ¤í…œ |
| [06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) | ë°ì´í„° ì „ì²˜ë¦¬, ì¦ê°•, TTA |
| [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) | ëª¨ë¸ ë¡œë”©, í•™ìŠµ, ì¶”ë¡ , LLM íŒŒì¸íŠœë‹ |
| [08_í‰ê°€_ìµœì í™”.md](./08_í‰ê°€_ìµœì í™”.md) | ROUGE í‰ê°€, K-Fold, Optuna |
| [09_ì•™ìƒë¸”_API.md](./09_ì•™ìƒë¸”_API.md) | 5ê°€ì§€ ì•™ìƒë¸”, Solar API, í”„ë¡¬í”„íŠ¸ |
| [10_ì¶”ë¡ _ìµœì í™”.md](./10_ì¶”ë¡ _ìµœì í™”.md) | TensorRT, Pruning, ë²¤ì¹˜ë§ˆí¬ |
| [README.md](./README.md) | ì „ì²´ ëª¨ë“ˆí™” ì‹œìŠ¤í…œ ê°œìš” |

---

## ğŸ¯ ë¹ ë¥¸ ì°¸ì¡°

### ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ì–´

```bash
# ==================== ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ì–´ ëª¨ìŒ ==================== #

# ---------------------- 1. ë¹ ë¥¸ ê²€ì¦ (2ë¶„) ---------------------- #
python scripts/train.py --experiment baseline_kobart --debug

# ---------------------- 2. ì „ì²´ í•™ìŠµ (4-6ì‹œê°„) ---------------------- #
python scripts/train.py --experiment baseline_kobart

# ---------------------- 3. ì¶”ë¡  ì‹¤í–‰ (3-5ë¶„) ---------------------- #
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv

# ---------------------- 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ ---------------------- #
python scripts/run_pipeline.py --experiment baseline_kobart

# ---------------------- 5. LLM íŒŒì¸íŠœë‹ ---------------------- #
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

### ì£¼ìš” ê²½ë¡œ

```
# ==================== ì¤‘ìš” ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ê²½ë¡œ ==================== #

# ---------------------- Config íŒŒì¼ ---------------------- #
configs/experiments/baseline_kobart.yaml             # ì‹¤í—˜ ì„¤ì •
configs/base/default.yaml                            # ê¸°ë³¸ ì„¤ì •

# ---------------------- í•™ìŠµ ê²°ê³¼ (ë‚ ì§œë³„ ë¶„ë¥˜) ---------------------- #
experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/final_model/    # ìµœì¢… ëª¨ë¸
experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/checkpoint-{N}/ # ì²´í¬í¬ì¸íŠ¸
experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_kobart/train.log       # í•™ìŠµ ë¡œê·¸

# ---------------------- ì œì¶œ íŒŒì¼ ---------------------- #
submissions/YYYYMMDD_HHMMSS_kobart.csv               # ì œì¶œ íŒŒì¼ (ìë™ ìƒì„± ì´ë¦„)

# ---------------------- ë¡œê·¸ ë°±ì—… (ë‚ ì§œë³„ ë¶„ë¥˜) ---------------------- #
logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_kobart_{options}.log    # í•™ìŠµ ë¡œê·¸ ë°±ì—…
logs/YYYYMMDD/inference/YYYYMMDD_HHMMSS_kobart_{options}.log       # ì¶”ë¡  ë¡œê·¸
```

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) | ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë° ê³µí†µ ì¸í”„ë¼ |
| [03_PRD_êµ¬í˜„_í˜„í™©.md](./03_PRD_êµ¬í˜„_í˜„í™©.md) | ì „ì²´ PRD êµ¬í˜„ í˜„í™© (100%) |
| [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) | ëª¨ë“  ì‹¤í–‰ ëª…ë ¹ì–´ ë° ì˜µì…˜ |
| [05_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md](./05_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md) | ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ì „ëµ |
| [06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) | ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°• |
| [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) | ëª¨ë¸ ë¡œë”©, í•™ìŠµ, ì¶”ë¡ , LLM |
| [08_í‰ê°€_ìµœì í™”.md](./08_í‰ê°€_ìµœì í™”.md) | í‰ê°€, êµì°¨ ê²€ì¦, Optuna |
| [09_ì•™ìƒë¸”_API.md](./09_ì•™ìƒë¸”_API.md) | ì•™ìƒë¸”, Solar API, í”„ë¡¬í”„íŠ¸ |
| [10_ì¶”ë¡ _ìµœì í™”.md](./10_ì¶”ë¡ _ìµœì í™”.md) | TensorRT, Pruning ìµœì í™” |
