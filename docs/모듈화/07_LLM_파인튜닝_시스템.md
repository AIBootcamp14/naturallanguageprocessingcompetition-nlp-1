# LLM íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ìƒì„¸ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [LoRA Loader](#lora-loader)
3. [LLM Dataset](#llm-dataset)
4. [LLM Trainer](#llm-trainer)
5. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
6. [ì‹¤í–‰ ëª…ë ¹ì–´](#ì‹¤í–‰-ëª…ë ¹ì–´)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- Causal LM (Llama, Qwen) íŒŒì¸íŠœë‹
- QLoRA 4-bit ì–‘ìí™” ì§€ì›
- LoRA (Low-Rank Adaptation) ì§€ì›
- Instruction/Chat Format ì§€ì›

### í•µì‹¬ ê¸°ëŠ¥
- âœ… QLoRA 4-bit ì–‘ìí™”
- âœ… LoRA íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  í•™ìŠµ
- âœ… Chat template í† í° ìë™ ì¶”ê°€
- âœ… Prompt truncation ë°©ì§€
- âœ… Instruction Tuning ë°ì´í„° ì¦ê°•

---

## ğŸ—ï¸ LoRA Loader

### íŒŒì¼ ìœ„ì¹˜
```
src/models/lora_loader.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LoRALoader:
    def __init__(self, config, logger=None)
    def load_model_and_tokenizer(use_lora=True, use_qlora=False)
    def _load_tokenizer(checkpoint)
    def _create_bnb_config()
    def _load_causal_lm(checkpoint, bnb_config)
    def _add_chat_tokens(model, tokenizer)
    def _apply_lora(model)
    def _configure_tokenizer(tokenizer)
```

### ì£¼ìš” ê¸°ëŠ¥

#### 1. QLoRA 4-bit ì–‘ìí™”
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # Llama: bf16, Qwen: fp16
)
```

#### 2. LoRA ì„¤ì •
```python
lora_config = LoraConfig(
    r=16,                    # LoRA rank
    lora_alpha=32,           # alpha = r * 2
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"       # MLP
    ],
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM
)
```

#### 3. Chat Template í† í° ì¶”ê°€
```python
# Llama ëª¨ë¸
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# Qwen ëª¨ë¸
chat_tokens = ["<|im_start|>", "<|im_end|>"]

tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
model.resize_token_embeddings(len(tokenizer))
```

#### 4. Prompt Truncation ë°©ì§€
```python
# Left padding/truncation (Causal LM í•„ìˆ˜)
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
```

---

## ğŸ“Š LLM Dataset

### íŒŒì¼ ìœ„ì¹˜
```
src/data/llm_dataset.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LLMSummarizationDataset(Dataset):
    def __init__(dialogues, summaries, tokenizer,
                 encoder_max_len=1024, decoder_max_len=200,
                 format_type="instruction")
    def __getitem__(idx)
    def _format_instruction(dialogue, summary)
    def _format_chat(dialogue, summary)

class InstructionAugmentedDataset(Dataset):
    # 5ê°€ì§€ instruction í…œí”Œë¦¿ìœ¼ë¡œ ë°ì´í„° ì¦ê°•
```

### ë°ì´í„° í¬ë§·

#### Instruction Format
```
### Instruction:
ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

### Input:
{dialogue}

### Response:
{summary}
```

#### Chat Format (Llama)
```
<|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:
{dialogue}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
{summary}<|eot_id|>
```

---

## ğŸš€ LLM Trainer

### íŒŒì¼ ìœ„ì¹˜
```
src/training/llm_trainer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class LLMTrainer:
    def __init__(config, model, tokenizer, train_dataset, eval_dataset)
    def train()
    def evaluate()
    def _create_training_args()
    def _create_trainer()
```

### í•™ìŠµ ì„¤ì • (QLoRA ìµœì í™”)

```python
training_args = TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=8,      # effective batch=64
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    weight_decay=0.1,
    max_grad_norm=1.2,

    # QLoRA ìµœì í™”
    optim="paged_adamw_32bit",
    bf16=True,                          # Llama: bf16
    gradient_checkpointing=True,

    # í‰ê°€ ë° ì €ì¥
    eval_strategy='epoch',
    save_strategy='epoch',
    metric_for_best_model='eval_loss',  # Causal LMì€ loss ì‚¬ìš©
    greater_is_better=False
)
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. Config ê¸°ë°˜ í•™ìŠµ

```python
from src.config import load_config
from src.models.lora_loader import load_lora_model_and_tokenizer
from src.data.llm_dataset import create_llm_dataset
from src.training.llm_trainer import create_llm_trainer
import pandas as pd

# 1. Config ë¡œë“œ
config = load_config("llama_3.2_3b")

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model, tokenizer = load_lora_model_and_tokenizer(
    config,
    use_lora=True,
    use_qlora=True
)

# 3. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")
eval_df = pd.read_csv("data/raw/dev.csv")

# 4. Dataset ìƒì„±
train_dataset = create_llm_dataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=1024,  # Prompt truncation ë°©ì§€
    decoder_max_len=200,
    format_type="chat"
)

# 5. Trainer ìƒì„± ë° í•™ìŠµ
trainer = create_llm_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# 6. í•™ìŠµ ì‹¤í–‰
results = trainer.train()
```

---

## ğŸ”§ ì‹¤í–‰ ëª…ë ¹ì–´

### 1. ê¸°ë³¸ LLM í•™ìŠµ (Llama-3.2-3B + QLoRA)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/llama_3.2_3b_qlora/final_model/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_llm_llama_3.2_3b_YYYYMMDD_HHMMSS.log`

### 2. Qwen ëª¨ë¸ í•™ìŠµ

```bash
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/qwen3_4b_qlora/final_model/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_llm_qwen3_4b_YYYYMMDD_HHMMSS.log`

### 3. Instruction Tuning (ë°ì´í„° 5ë°° ì¦ê°•)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**íš¨ê³¼:**
- í•™ìŠµ ë°ì´í„°: 12,457ê°œ â†’ 62,285ê°œ (5ë°°)
- 5ê°€ì§€ instruction í…œí”Œë¦¿ ì ìš©

### 4. ë””ë²„ê·¸ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •:**
- ë°ì´í„°: í•™ìŠµ 50ê°œ, ê²€ì¦ 10ê°œ
- ì—í¬í¬: 1íšŒ
- ë°°ì¹˜ í¬ê¸°: 2
- WandB: ë¹„í™œì„±í™”

---

## ğŸ“‚ Config íŒŒì¼

### Causal LM ê¸°ë³¸ ì„¤ì •
**íŒŒì¼:** `configs/base/causal_lm.yaml`

```yaml
model:
  type: causal_lm
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"

lora:
  r: 16
  alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  dropout: 0.05
  use_qlora: true

tokenizer:
  encoder_max_len: 1024  # Prompt truncation ë°©ì§€
  decoder_max_len: 200

training:
  epochs: 3
  batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.1
  max_grad_norm: 1.2
  gradient_checkpointing: true
```

### Llama ëª¨ë¸ ì„¤ì •
**íŒŒì¼:** `configs/models/llama_3.2_3b.yaml`

```yaml
_base_: ../base/causal_lm.yaml

model:
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"
  size: "3B"
  dtype: "bf16"
  chat_template: "llama"

dataset:
  format_type: "chat"

experiment:
  name: "llama_3.2_3b_qlora"
  tags:
    - "llama"
    - "3b"
    - "qlora"
```

### Qwen ëª¨ë¸ ì„¤ì •
**íŒŒì¼:** `configs/models/qwen3_4b.yaml`

```yaml
_base_: ../base/causal_lm.yaml

model:
  checkpoint: "Qwen/Qwen3-4B-Instruct-2507"
  size: "4B"
  dtype: "fp16"
  chat_template: "qwen"

training:
  batch_size: 6
  gradient_accumulation_steps: 10

dataset:
  format_type: "chat"

experiment:
  name: "qwen3_4b_qlora"
  tags:
    - "qwen"
    - "4b"
    - "qlora"
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜
```
src/tests/test_lora_loader.py
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python src/tests/test_lora_loader.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 4ê°œ)

1. âœ… LoRALoader ì´ˆê¸°í™”
2. âœ… í† í¬ë‚˜ì´ì € ë¡œë”©
3. âœ… LoRA Config ìƒì„±
4. âœ… í¸ì˜ í•¨ìˆ˜

**ê²°ê³¼:** 4/4 í…ŒìŠ¤íŠ¸ í†µê³¼ (100%)

---

## ğŸ“Š ì˜ˆìƒ ë¦¬ì†ŒìŠ¤

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | QLoRA (4-bit) | Full Fine-tuning |
|------|----------|---------------|------------------|
| Llama-3.2-3B | 3B | **8GB** | 24GB |
| Qwen3-4B | 4B | **10GB** | 32GB |
| Llama-3-8B | 8B | **16GB** | 64GB |

### í•™ìŠµ ì‹œê°„ (A6000 ê¸°ì¤€)

| ëª¨ë¸ | ì—í¬í¬ | ë°ì´í„° | ì˜ˆìƒ ì‹œê°„ |
|------|--------|--------|-----------|
| Llama-3.2-3B | 3 | 12,457ê°œ | 6-8ì‹œê°„ |
| Qwen3-4B | 3 | 12,457ê°œ | 8-10ì‹œê°„ |

---

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ

### Zero-shot ì„±ëŠ¥ (ê²€ì¦ë¨)

| ëª¨ë¸ | ROUGE-1 | ROUGE-2 | ROUGE-L | **ROUGE Sum** |
|------|---------|---------|---------|---------------|
| Llama-3.2-Korean-3B | 26.96 | 11.08 | 24.22 | **49.52** (1ìœ„) |
| Qwen3-4B | 24.22 | 9.23 | 21.79 | **45.02** (4ìœ„) |

### íŒŒì¸íŠœë‹ ëª©í‘œ

| ëª¨ë¸ | Zero-shot | íŒŒì¸íŠœë‹ ëª©í‘œ | ê°œì„  ëª©í‘œ |
|------|-----------|---------------|-----------|
| Llama-3.2-Korean-3B | 49.52 | **95+** | +45 í¬ì¸íŠ¸ |
| Qwen3-4B | 45.02 | **95+** | +50 í¬ì¸íŠ¸ |

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/models/lora_loader.py` - LoRA Loader
- `src/data/llm_dataset.py` - LLM Dataset
- `src/training/llm_trainer.py` - LLM Trainer

**Config:**
- `configs/base/causal_lm.yaml` - Causal LM ê¸°ë³¸ ì„¤ì •
- `configs/models/llama_3.2_3b.yaml` - Llama ëª¨ë¸
- `configs/models/qwen3_4b.yaml` - Qwen ëª¨ë¸

**ìŠ¤í¬ë¦½íŠ¸:**
- `scripts/train_llm.py` - LLM í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_lora_loader.py` - LoRA Loader í…ŒìŠ¤íŠ¸
