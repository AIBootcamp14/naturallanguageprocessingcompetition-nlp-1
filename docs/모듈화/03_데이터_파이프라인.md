# 03. 데이터 파이프라인 (전처리 및 증강)

## 목차
- [Part 1: 기본 데이터 처리](#part-1-기본-데이터-처리)
  - [개요](#개요)
  - [DialoguePreprocessor](#dialoguepreprocessor)
  - [Dataset 클래스](#dataset-클래스)
  - [사용 방법](#사용-방법)
  - [테스트 결과](#테스트-결과)
  - [데이터 특성 분석](#데이터-특성-분석)
  - [실전 활용 팁](#실전-활용-팁)
  - [주의사항](#주의사항)
- [Part 2: 데이터 증강](#part-2-데이터-증강)
  - [데이터 증강 개요](#데이터-증강-개요)
  - [DataAugmenter](#dataaugmenter)
  - [증강 방법](#증강-방법)
  - [데이터 증강 사용 방법](#데이터-증강-사용-방법)
  - [실행 명령어](#실행-명령어)
  - [증강 효과 및 권장 조합](#증강-효과-및-권장-조합)
  - [데이터 증강 주의사항](#데이터-증강-주의사항)
- [Part 3: TTA (Test Time Augmentation)](#part-3-tta-test-time-augmentation)
  - [TTA 개요](#tta-개요)
  - [TTAugmentor 클래스](#ttaugmentor-클래스)
  - [TTA 전략 상세](#tta-전략-상세)
  - [TTA 사용 방법](#tta-사용-방법)
  - [TTA 효과 및 권장 사항](#tta-효과-및-권장-사항)
  - [TTA 주의사항](#tta-주의사항)

---

# Part 1: 기본 데이터 처리

## 개요

### 목적
- 대화 데이터의 일관된 전처리
- PyTorch 학습/추론을 위한 Dataset 제공
- 노이즈 제거 및 텍스트 정규화
- 화자 정보 추출 및 통계 생성

### 핵심 기능
- ✅ 노이즈 제거 (`\\n`, `<br>` 등)
- ✅ 화자 추출 및 턴 계산
- ✅ 학습/검증용 Dataset
- ✅ 추론용 Dataset
- ✅ DataFrame 전처리

---

## DialoguePreprocessor

### 클래스 구조

```mermaid
classDiagram
    class DialoguePreprocessor {
        +speaker_pattern: Pattern
        +__init__()
        +clean_dialogue(text: str) str
        +normalize_dialogue(text: str) str
        +extract_speakers(dialogue: str) List[str]
        +count_turns(dialogue: str) int
        +split_dialogue_by_speaker(dialogue: str) List[Tuple]
        +preprocess_batch(dialogues, summaries) Tuple
        +preprocess_dataframe(df) DataFrame
    }

    DialoguePreprocessor --> "uses" Dataset
```

### 주요 메서드

#### 1. clean_dialogue() - 노이즈 제거

**처리 항목:**
1. `\\n` → `\n` 변환 (이스케이프된 개행 문자)
2. `<br>` 태그 제거
3. 중복 공백 제거
4. 과도한 개행 제거 (3개 이상 → 2개)
5. 앞뒤 공백 제거

**사용 예시:**
```python
from src.data import DialoguePreprocessor

preprocessor = DialoguePreprocessor()

# 노이즈가 있는 텍스트
dirty_text = "안녕하세요\\\\n<br>반갑습니다  "

# 전처리 실행
clean_text = preprocessor.clean_dialogue(dirty_text)
print(clean_text)  # "안녕하세요\n반갑습니다"
```

#### 2. extract_speakers() - 화자 추출

**추출 패턴:** `#Person1#`, `#Person2#`, ... `#PersonN#`

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다\\n#Person1#: 잘 부탁드립니다"

speakers = preprocessor.extract_speakers(dialogue)
print(speakers)  # ['#Person1#', '#Person2#']
```

#### 3. count_turns() - 턴 개수 계산

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다\\n#Person1#: 감사합니다"

turns = preprocessor.count_turns(dialogue)
print(turns)  # 3
```

#### 4. split_dialogue_by_speaker() - 대화 분할

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다"

turns = preprocessor.split_dialogue_by_speaker(dialogue)
# [('#Person1#', '안녕하세요'), ('#Person2#', '반갑습니다')]

for speaker, utterance in turns:
    print(f"{speaker}: {utterance}")
```

#### 5. preprocess_dataframe() - DataFrame 전처리

**추가되는 통계 컬럼:**
- `num_speakers` - 화자 수
- `num_turns` - 턴 개수

```python
import pandas as pd

df = pd.read_csv("data/raw/train.csv")
preprocessor = DialoguePreprocessor()

# DataFrame 전처리 (dialogue, summary 컬럼 정제 + 통계 추가)
df_processed = preprocessor.preprocess_dataframe(df)

print(df_processed[['num_speakers', 'num_turns']].describe())
```

---

## Dataset 클래스

### 클래스 구조

```mermaid
classDiagram
    class Dataset {
        <<interface>>
        +__len__()
        +__getitem__(idx)
    }

    class DialogueSummarizationDataset {
        +dialogues: List[str]
        +summaries: List[str]
        +tokenizer: PreTrainedTokenizer
        +encoder_max_len: int
        +decoder_max_len: int
        +__init__(...)
        +__len__() int
        +__getitem__(idx) Dict
    }

    class InferenceDataset {
        +dialogues: List[str]
        +tokenizer: PreTrainedTokenizer
        +encoder_max_len: int
        +fnames: List[str]
        +__init__(...)
        +__len__() int
        +__getitem__(idx) Dict
    }

    Dataset <|-- DialogueSummarizationDataset
    Dataset <|-- InferenceDataset
```

### 1. DialogueSummarizationDataset (학습/검증용)

**목적:** 학습 및 검증을 위한 Dataset

**반환 형식:**
```python
{
    'input_ids': Tensor,        # 인코더 입력 (dialogue)
    'attention_mask': Tensor,   # 어텐션 마스크
    'labels': Tensor            # 디코더 레이블 (summary)
}
```

**사용 예시:**
```python
from src.data import DialogueSummarizationDataset
from transformers import AutoTokenizer

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# 데이터셋 생성
dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    preprocess=True             # 전처리 자동 적용
)

# 샘플 접근
sample = dataset[0]
print(f"Input shape: {sample['input_ids'].shape}")         # torch.Size([512])
print(f"Attention mask shape: {sample['attention_mask'].shape}")  # torch.Size([512])
print(f"Labels shape: {sample['labels'].shape}")           # torch.Size([100])
```

### 2. InferenceDataset (추론용)

**목적:** 추론(예측)을 위한 Dataset

**반환 형식:**
```python
{
    'input_ids': Tensor,        # 인코더 입력 (dialogue)
    'attention_mask': Tensor,   # 어텐션 마스크
    'fname': str                # 파일명 (선택적)
}
```

**사용 예시:**
```python
from src.data import InferenceDataset

# 추론용 데이터셋 생성
inference_dataset = InferenceDataset(
    dialogues=test_df['dialogue'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    preprocess=True,
    fnames=test_df['fname'].tolist()  # 제출 파일용
)

# 샘플 접근
sample = inference_dataset[0]
print(f"Input shape: {sample['input_ids'].shape}")
print(f"Fname: {sample['fname']}")
```

### 3. create_dataset_from_dataframe() 편의 함수

**DataFrame에서 Dataset 직접 생성:**

```python
from src.data import create_dataset_from_dataframe

# 학습용 Dataset 생성
train_dataset = create_dataset_from_dataframe(
    df=train_df,
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    is_train=True,              # 학습 모드
    preprocess=True
)

# 추론용 Dataset 생성
test_dataset = create_dataset_from_dataframe(
    df=test_df,
    tokenizer=tokenizer,
    encoder_max_len=512,
    is_train=False,             # 추론 모드
    preprocess=True
)
```

---

## 사용 방법

### 데이터 처리 플로우

```mermaid
graph LR
    A[Raw CSV] --> B[DataFrame]
    B --> C{전처리 필요?}

    C -->|Yes| D[DialoguePreprocessor]
    D --> E[정제된 DataFrame]

    C -->|No| E

    E --> F[Dataset 생성]
    F --> G{학습/추론?}

    G -->|학습| H[DialogueSummarizationDataset]
    G -->|추론| I[InferenceDataset]

    H --> J[DataLoader]
    I --> J

    J --> K[모델 학습/추론]

    style A fill:#e3f2fd
    style E fill:#fff3e0
    style H fill:#c8e6c9
    style I fill:#c8e6c9
```

### 전체 사용 예시

```python
import pandas as pd
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from src.data import DialoguePreprocessor, DialogueSummarizationDataset

# 1. 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")

# 2. 전처리 (선택적 - Dataset에서 자동으로도 가능)
preprocessor = DialoguePreprocessor()
train_df = preprocessor.preprocess_dataframe(train_df)

print(f"데이터 크기: {len(train_df)}")
print(f"화자 수 분포:\n{train_df['num_speakers'].value_counts()}")
print(f"턴 수 통계:\n{train_df['num_turns'].describe()}")

# 3. 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# 4. Dataset 생성
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    encoder_max_len=512,
    decoder_max_len=100,
    preprocess=False  # 이미 전처리했으므로 False
)

# 5. DataLoader 생성
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# 6. 학습 루프
for batch in train_loader:
    input_ids = batch['input_ids']          # (batch_size, 512)
    attention_mask = batch['attention_mask'] # (batch_size, 512)
    labels = batch['labels']                # (batch_size, 100)

    # 모델 학습...
```

---

## 테스트 결과

### 테스트 명령어

```bash
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate
python tests/test_preprocessor.py
```

### 테스트 항목 (총 5개)

#### 1. ✅ 노이즈 제거 테스트

**테스트 케이스:**
```python
test_cases = [
    ("안녕하세요\\\\n반갑습니다", "안녕하세요\\n반갑습니다"),  # \\\\n → \\n
    ("안녕<br>반갑습니다", "안녕\\n반갑습니다"),              # <br> → \\n
    ("안녕  하세요", "안녕 하세요"),                        # 중복 공백 제거
    ("  안녕하세요  ", "안녕하세요"),                       # 앞뒤 공백 제거
]
```

**결과:**
```
테스트 1: ✅
테스트 2: ✅
테스트 3: ✅
테스트 4: ✅

✅ 노이즈 제거 테스트 성공!
```

#### 2. ✅ 화자 추출 테스트

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다\\n#Person1#: 잘 부탁드립니다"
speakers = preprocessor.extract_speakers(dialogue)

assert speakers == ['#Person1#', '#Person2#']
```

**결과:**
```
화자: ['#Person1#', '#Person2#']
✅ 화자 추출 테스트 성공!
```

#### 3. ✅ 턴 개수 계산 테스트

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다\\n#Person1#: 잘 부탁드립니다"
turns = preprocessor.count_turns(dialogue)

assert turns == 3
```

**결과:**
```
턴 개수: 3
✅ 턴 개수 계산 테스트 성공!
```

#### 4. ✅ 대화 분할 테스트

```python
dialogue = "#Person1#: 안녕하세요\\n#Person2#: 반갑습니다"
turns = preprocessor.split_dialogue_by_speaker(dialogue)

assert len(turns) == 2
assert turns[0] == ('#Person1#', '안녕하세요')
assert turns[1] == ('#Person2#', '반갑습니다')
```

**결과:**
```
분할 결과:
  #Person1#: 안녕하세요
  #Person2#: 반갑습니다

✅ 대화 분할 테스트 성공!
```

#### 5. ✅ 실제 데이터 전처리 테스트

**실제 학습 데이터 전처리:**

```python
df = pd.read_csv('data/raw/train.csv')
preprocessor = DialoguePreprocessor()
df_processed = preprocessor.preprocess_dataframe(df)
```

**결과:**
```
원본 데이터 크기: 12,457
전처리 후 크기: 12,457
추가된 컬럼: ['num_speakers', 'num_turns']

화자 수 통계:
2    12335
3      107
4       13
5        1
6        1

턴 수 통계:
count    12457.000000
mean         9.493425
std          4.148729
min          2.000000
25%          7.000000
50%          9.000000
75%         11.000000
max         59.000000

✅ 실제 데이터 전처리 테스트 성공!
```

### 실제 데이터 분석 결과

**핵심 통계:**
- 전체 샘플: **12,457개**
- 화자 분포: 대부분 **2명** (12,335개, 99%)
- 턴 개수: 평균 **9.5턴**, 최대 **59턴**
- 전처리 성공률: **100%**

---

## 데이터 특성 분석

### 화자 분포

```
2명: ████████████████████████████ 99.02% (12,335개)
3명: █ 0.86% (107개)
4명: ▏ 0.10% (13개)
5명: ▏ 0.01% (1개)
6명: ▏ 0.01% (1개)
```

### 턴 길이 분포

```
 2-5턴: ██████ 26.8%
 6-9턴: ███████████ 48.2%
10-13턴: ████████ 35.4%
14-17턴: ██ 9.8%
18+턴: █ 4.8%
```

---

## 실전 활용 팁

### 1. 메모리 효율적인 처리

대용량 데이터 처리 시:

```python
# 청크 단위로 처리
chunk_size = 1000

for chunk in pd.read_csv("data/raw/train.csv", chunksize=chunk_size):
    chunk_processed = preprocessor.preprocess_dataframe(chunk)
    # 처리...
```

### 2. 전처리 캐싱

```python
import pickle

# 전처리 결과 저장
df_processed = preprocessor.preprocess_dataframe(df)
df_processed.to_pickle("data/processed/train_preprocessed.pkl")

# 다음 실행 시 로드
df_processed = pd.read_pickle("data/processed/train_preprocessed.pkl")
```

### 3. 커스텀 전처리 추가

```python
class CustomPreprocessor(DialoguePreprocessor):
    def custom_clean(self, text: str) -> str:
        # 추가 전처리 로직
        text = self.clean_dialogue(text)
        text = text.replace("특정패턴", "대체텍스트")
        return text

    def preprocess_batch(self, dialogues, summaries=None):
        # clean_dialogue 대신 custom_clean 사용
        cleaned_dialogues = [self.custom_clean(d) for d in dialogues]
        # ...
        return cleaned_dialogues, cleaned_summaries
```

---

## 주의사항

### 1. 토큰 길이 초과

```python
# 긴 대화는 자동으로 잘림 (truncation=True)
dataset = DialogueSummarizationDataset(
    ...,
    encoder_max_len=512,  # 512 토큰 초과 시 자동 자르기
    truncation=True
)
```

### 2. 패딩 토큰 처리

```python
# labels의 패딩은 -100으로 설정됨 (손실 계산 시 무시)
labels[labels == tokenizer.pad_token_id] = -100
```

### 3. 전처리 중복 방지

```python
# Dataset에서 preprocess=True로 설정한 경우
# 미리 전처리하지 않아도 됨
dataset = DialogueSummarizationDataset(
    ...,
    preprocess=True  # 자동 전처리
)
```

---

# Part 2: 데이터 증강

## 데이터 증강 개요 (2025-10-11 업데이트)

### 목적
- 학습 데이터 증강으로 모델 일반화 성능 향상
- 3가지 핵심 증강 방법 구현 완료
- Config 기반 유연한 조합 가능

### 핵심 기능 (구현 완료!)
- ✅ **문장 순서 섞기** (`shuffle_sentences`) - 구현 완료
- ✅ **동의어 치환** (`synonym_replacement`) - 구현 완료
- ✅ **배치 증강** (`batch_augment`) - 구현 완료
- ⚠️ Back-translation (한→영→한) - 선택적 (외부 모델 필요)
- ⚠️ Paraphrase 생성 - 선택적
- ⚠️ 대화 샘플링 - 선택적

### 새로 추가된 모듈 (2025-10-11)
- **파일**: `src/augmentation/text_augmenter.py`
- **클래스**: `TextAugmenter`
- **Config**: `configs/strategies/data_augmentation.yaml`

---

## DataAugmenter

### 파일 위치
```
src/data/augmentation.py
```

### 클래스 구조

```python
class DataAugmenter:
    def __init__(logger=None)
    def augment(dialogues, summaries, methods, samples_per_method)
    def back_translate(text)
    def paraphrase(text)
    def shuffle_turns(dialogue, preserve_ratio)
    def synonym_replacement(text, n)
    def sample_dialogue(dialogue, ratio)
```

---

## 증강 방법

### 1. Back-translation (역번역)

**원리:**
- 한국어 → 영어 → 한국어 번역
- Helsinki-NLP MarianMT 모델 사용

**코드:**
```python
from src.data.augmentation import BackTranslationAugmenter

augmenter = BackTranslationAugmenter()
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**예시:**
```
원본: "오늘 날씨가 정말 좋네요"
증강: "오늘 날씨가 매우 좋습니다"
```

**특징:**
- 의미는 유지하면서 표현 변경
- GPU 사용 권장 (모델 로딩 필요)
- 선택적 사용 (SentencePiece 라이브러리 필요)

---

### 2. Paraphrase (의역)

**원리:**
- 규칙 기반 동의어 치환
- 주요 표현 변형

**코드:**
```python
from src.data.augmentation import ParaphraseAugmenter

augmenter = ParaphraseAugmenter()
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**치환 규칙:**
```python
{
    "안녕하세요": ["안녕", "반갑습니다", "환영합니다"],
    "감사합니다": ["고맙습니다", "감사해요", "고마워요"],
    "죄송합니다": ["미안합니다", "죄송해요", "미안해요"],
    "네": ["예", "알겠습니다", "그렇습니다"],
    "아니요": ["아닙니다", "아니에요", "그렇지 않습니다"]
}
```

**예시:**
```
원본: "안녕하세요. 감사합니다."
증강: "반갑습니다. 고맙습니다."
```

---

### 3. Turn Shuffling (턴 섞기)

**원리:**
- 대화 턴 순서 무작위 섞기
- 처음/끝 일부 보존 (맥락 유지)

**코드:**
```python
from src.data.augmentation import ShuffleAugmenter

augmenter = ShuffleAugmenter(preserve_ratio=0.3)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**파라미터:**
- `preserve_ratio`: 보존할 턴 비율 (기본값: 0.3)

**예시:**
```
원본:
A: 안녕하세요
B: 안녕하세요
A: 오늘 날씨 좋네요
B: 네, 정말 좋아요
A: 점심 뭐 먹을까요?
B: 김치찌개 어때요?

증강:
A: 안녕하세요        # 처음 보존
B: 안녕하세요        # 처음 보존
A: 점심 뭐 먹을까요? # 중간 섞임
B: 김치찌개 어때요?  # 중간 섞임
A: 오늘 날씨 좋네요  # 중간 섞임
B: 네, 정말 좋아요   # 끝 보존
```

**특징:**
- 요약은 변경되지 않음
- 대화 순서에 민감하지 않은 경우 유용

---

### 4. Synonym Replacement (동의어 치환)

**원리:**
- 특정 단어를 동의어로 치환
- 한국어 동의어 사전 기반

**코드:**
```python
from src.data.augmentation import SynonymReplacementAugmenter

augmenter = SynonymReplacementAugmenter(replace_ratio=0.3)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**동의어 사전:**
```python
{
    "좋다": ["훌륭하다", "멋지다", "괜찮다"],
    "나쁘다": ["안좋다", "별로다", "형편없다"],
    "크다": ["거대하다", "넓다", "광대하다"],
    "작다": ["적다", "미미하다", "소소하다"],
    "빠르다": ["신속하다", "재빠르다", "날쌔다"],
    "느리다": ["더디다", "굼뜨다", "늦다"],
    "밥": ["식사", "음식", "끼니"],
    "먹다": ["섭취하다", "드시다", "식사하다"]
}
```

**예시:**
```
원본: "오늘 점심에 밥을 먹었다"
증강: "오늘 점심에 식사를 먹었다"
```

---

### 5. Dialogue Sampling (대화 샘플링)

**원리:**
- 긴 대화에서 일부 턴만 선택
- 처음과 끝 턴 항상 유지

**코드:**
```python
from src.data.augmentation import DialogueSamplingAugmenter

augmenter = DialogueSamplingAugmenter(sample_ratio=0.7)
aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

**파라미터:**
- `sample_ratio`: 유지할 턴 비율 (기본값: 0.7)

**예시:**
```
원본 (8턴):
A: 1
B: 2
A: 3
B: 4
A: 5
B: 6
A: 7
B: 8

증강 (5턴, 70%):
A: 1       # 처음 유지
B: 2       # 랜덤 선택
A: 5       # 랜덤 선택
A: 7       # 랜덤 선택
B: 8       # 끝 유지
```

**특징:**
- 요약은 변경되지 않음
- 긴 대화 요약 학습에 유용

---

## 데이터 증강 사용 방법

### 1. 단일 방법 사용

```python
from src.data.augmentation import ShuffleAugmenter

augmenter = ShuffleAugmenter(preserve_ratio=0.3)

dialogue = "A: 안녕\nB: 안녕\nA: 날씨 좋네\nB: 네"
summary = "인사 및 날씨"

aug_dialogue, aug_summary = augmenter.augment(dialogue, summary)
```

---

### 2. DataAugmenter로 여러 방법 사용

```python
from src.data.augmentation import DataAugmenter

augmenter = DataAugmenter()

dialogues = ["대화1", "대화2", "대화3"]
summaries = ["요약1", "요약2", "요약3"]

# 3가지 방법 적용, 각 방법당 2개씩 생성
aug_dialogues, aug_summaries = augmenter.augment(
    dialogues,
    summaries,
    methods=['shuffle', 'synonym', 'sample'],
    samples_per_method=2
)

print(f"원본: {len(dialogues)}개")
print(f"증강 후: {len(aug_dialogues)}개")  # 3 + (3 × 3 × 2) = 21개
```

---

### 3. 편의 함수 사용 (augment_dataset)

```python
from src.data.augmentation import augment_dataset
import pandas as pd

# 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")

# 증강 실행
aug_dialogues, aug_summaries = augment_dataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    methods=['shuffle', 'sample'],
    n_aug=2  # 각 방법당 2개씩
)

print(f"원본: {len(train_df)}개")
print(f"증강 후: {len(aug_dialogues)}개")
# 12,457개 → 12,457 × 2 × 2 = 49,828개
```

---

### 4. 학습 스크립트에 통합

```python
from src.config import load_config
from src.data.augmentation import augment_dataset
from src.data.preprocessor import create_dataset
import pandas as pd

# 1. Config 로드
config = load_config("baseline_kobart")

# 2. 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")

# 3. 데이터 증강
if config.data.augmentation.enabled:
    aug_dialogues, aug_summaries = augment_dataset(
        dialogues=train_df['dialogue'].tolist(),
        summaries=train_df['summary'].tolist(),
        methods=config.data.augmentation.methods,
        n_aug=config.data.augmentation.n_aug
    )

    # 증강 데이터를 DataFrame에 추가
    aug_df = pd.DataFrame({
        'dialogue': aug_dialogues,
        'summary': aug_summaries
    })
    train_df = pd.concat([train_df, aug_df], ignore_index=True)

# 4. Dataset 생성
train_dataset = create_dataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer,
    config=config
)
```

---

## 실행 명령어

### Config 설정

**파일:** `configs/experiments/baseline_kobart.yaml`

```yaml
data:
  augmentation:
    enabled: true
    methods:
      - shuffle
      - sample
    n_aug: 2  # 각 방법당 2개씩 생성
```

---

### 학습 시 자동 증강

학습 스크립트에서 Config 기반으로 자동 증강:

```bash
# Config에 augmentation 설정이 있으면 자동으로 증강됨
python scripts/train.py --experiment baseline_kobart
```

---

### Config 파일 예시

```yaml
data:
  train_path: "data/raw/train.csv"
  dev_path: "data/raw/dev.csv"

  augmentation:
    enabled: true
    methods:
      - shuffle        # 턴 섞기
      - sample         # 대화 샘플링
      - synonym        # 동의어 치환
    n_aug: 2           # 각 방법당 2개씩
```

**결과:**
- 원본: 12,457개
- 증강 후: 12,457 + (12,457 × 3 × 2) = **87,399개**

---

## 증강 효과 및 권장 조합

### 데이터 증가량

| 방법 수 | n_aug | 원본 | 증강 후 | 증가율 |
|---------|-------|------|---------|--------|
| 1개 | 1 | 12,457 | 24,914 | 2배 |
| 2개 | 1 | 12,457 | 37,371 | 3배 |
| 3개 | 1 | 12,457 | 49,828 | 4배 |
| 2개 | 2 | 12,457 | 62,285 | 5배 |
| 3개 | 2 | 12,457 | 87,399 | 7배 |

---

### 권장 조합

#### 1. 가벼운 증강 (2배)
```yaml
methods: [shuffle]
n_aug: 1
```
- 빠른 학습
- 메모리 효율적

#### 2. 중간 증강 (4배)
```yaml
methods: [shuffle, sample]
n_aug: 2
```
- 균형있는 성능 향상
- 학습 시간 적절

#### 3. 강력한 증강 (7배)
```yaml
methods: [shuffle, sample, synonym]
n_aug: 2
```
- 최대 성능 향상
- 긴 학습 시간 필요

---

### 테스트 결과

#### 테스트 파일 위치
```
src/tests/test_augmentation.py
```

#### 테스트 실행

```bash
python src/tests/test_augmentation.py
```

#### 테스트 항목 (총 7개)

1. ✅ DataAugmenter 초기화
2. ✅ 역번역 증강 (선택적)
3. ✅ 의역 증강
4. ✅ 턴 섞기 증강
5. ✅ 동의어 치환 증강
6. ✅ 대화 샘플링 증강
7. ✅ augment_dataset 함수

**결과:** 7/7 테스트 통과 (100%)

---

### 성능 목표

#### 증강 전후 비교

| 모델 | 증강 전 | 증강 후 (4배) | 개선 |
|------|---------|---------------|------|
| KoBART | ROUGE 88-90 | **ROUGE 92-95** | +4-5 |

---

## 데이터 증강 주의사항

### 1. Back-translation
- Helsinki-NLP 모델 다운로드 필요 (약 300MB × 2)
- SentencePiece 라이브러리 필요
- GPU 권장 (느린 속도)

### 2. 메모리
- 증강 데이터는 메모리에 로드됨
- 7배 증강 시: 12,457개 → 87,399개
- 배치 크기 조정 필요할 수 있음

### 3. 학습 시간
- 데이터가 많아지면 학습 시간 증가
- 에포크 수 조정 고려

---

---

# Part 3: TTA (Test Time Augmentation)

## TTA 개요

### 목적
- **추론 시점**에 데이터 증강을 적용하여 예측 robustness 향상
- 여러 증강 버전에 대한 예측을 앙상블하여 최종 결과 생성
- 학습 데이터 증강과 달리 **추론 단계**에서 수행

### 핵심 기능
- ✅ **Paraphrase** - 문장 구조 변경
- ✅ **Reorder** - 문장/단어 순서 재배열
- ✅ **Synonym** - 동의어 치환
- ✅ **Mask** - 일부 토큰 마스킹
- ✅ **앙상블** - Voting 또는 Averaging

### 파일 위치
```
src/data/tta.py
```

---

## TTAugmentor 클래스

### 클래스 구조

```python
class TTAugmentor:
    def __init__(
        strategies=['paraphrase', 'reorder', 'synonym', 'mask'],
        n_augmentations=3,
        tokenizer=None,
        model=None,
        seed=42
    )

    # 단일 텍스트 증강
    def augment(text, strategy=None) -> List[str]

    # 배치 텍스트 증강
    def batch_augment(texts, strategy=None) -> List[List[str]]

    # TTA + 예측 + 앙상블
    def predict_with_tta(text, predict_fn, ensemble_method='voting') -> Dict

    # 내부 메서드
    def _paraphrase_augment(text) -> List[str]
    def _reorder_augment(text) -> List[str]
    def _synonym_augment(text) -> List[str]
    def _mask_augment(text) -> List[str]
```

---

## TTA 전략 상세

### 1. Paraphrase (문장 구조 변경)

**원리:**
- 문장 순서를 재배열하거나 약간의 수정 적용
- 의미는 유지하면서 표현 변경

**사용 예시:**
```python
from src.data.tta import create_tta_augmentor

tta = create_tta_augmentor(
    strategies=['paraphrase'],
    n_augmentations=2
)

text = "이 제품은 정말 좋은 품질을 가지고 있습니다. 배송도 빨랐습니다."
augmented = tta.augment(text, strategy='paraphrase')

# 결과:
# 원본: "이 제품은 정말 좋은 품질을 가지고 있습니다. 배송도 빨랐습니다."
# 증강1: "배송도 빨랐습니다. 이 제품은 정말 좋은 품질을 가지고 있습니다."
# 증강2: "이 제품은 정말 훌륭한 품질을 가지고 있습니다. 배송도 빨랐습니다."
```

**특징:**
- 문장이 여러 개인 경우: 문장 순서 교환
- 문장이 하나인 경우: 동의어 치환 적용

---

### 2. Reorder (순서 재배열)

**원리:**
- 문장 순서를 무작위로 섞기
- 단일 문장인 경우 인접 단어쌍 교환

**사용 예시:**
```python
tta = create_tta_augmentor(
    strategies=['reorder'],
    n_augmentations=2
)

text = "첫째 문장입니다. 둘째 문장입니다. 셋째 문장입니다."
augmented = tta.augment(text, strategy='reorder')

# 결과:
# 원본: "첫째 문장입니다. 둘째 문장입니다. 셋째 문장입니다."
# 증강1: "셋째 문장입니다. 첫째 문장입니다. 둘째 문장입니다."
# 증강2: "둘째 문장입니다. 셋째 문장입니다. 첫째 문장입니다."
```

**특징:**
- 완전 무작위 섞기 (보존 영역 없음)
- 순서에 민감하지 않은 요약 태스크에 효과적

---

### 3. Synonym (동의어 치환)

**원리:**
- 사전 기반 동의어 치환
- 1-2개 단어를 무작위로 선택하여 치환

**동의어 사전:**
```python
synonym_dict = {
    '좋은': ['훌륭한', '멋진', '우수한', '괜찮은'],
    '나쁜': ['안좋은', '형편없는', '별로인', '못난'],
    '크다': ['거대하다', '대형이다', '넓다'],
    '작다': ['소형이다', '좁다', '미세하다'],
    '빠르다': ['신속하다', '재빠르다', '급하다'],
    '느리다': ['더디다', '천천히다', '완만하다']
}
```

**사용 예시:**
```python
tta = create_tta_augmentor(
    strategies=['synonym'],
    n_augmentations=3
)

text = "빠른 배송으로 좋은 제품을 받았습니다."
augmented = tta.augment(text, strategy='synonym')

# 결과:
# 원본: "빠른 배송으로 좋은 제품을 받았습니다."
# 증강1: "신속한 배송으로 좋은 제품을 받았습니다."
# 증강2: "빠른 배송으로 훌륭한 제품을 받았습니다."
# 증강3: "재빠른 배송으로 우수한 제품을 받았습니다."
```

---

### 4. Mask (토큰 마스킹)

**원리:**
- 전체 단어의 10-20%를 무작위로 마스킹
- `[MASK]` 토큰으로 치환
- 모델의 robustness 테스트

**사용 예시:**
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

tta = create_tta_augmentor(
    strategies=['mask'],
    n_augmentations=2,
    tokenizer=tokenizer  # 토크나이저 필수
)

text = "이 제품은 정말 좋은 품질입니다."
augmented = tta.augment(text, strategy='mask')

# 결과:
# 원본: "이 제품은 정말 좋은 품질입니다."
# 증강1: "이 [MASK] 정말 좋은 품질입니다."
# 증강2: "이 제품은 [MASK] [MASK] 품질입니다."
```

**특징:**
- 토크나이저가 없으면 원본 반환
- 마스킹 비율: 10-20% (무작위)

---

## TTA 사용 방법

### 1. 기본 사용 (단일 전략)

```python
from src.data.tta import create_tta_augmentor

# TTAugmentor 생성
tta = create_tta_augmentor(
    strategies=['paraphrase'],
    n_augmentations=2,
    seed=42
)

# 단일 텍스트 증강
text = "#Person1#: 안녕하세요\n#Person2#: 반갑습니다"
augmented = tta.augment(text)

print(f"증강된 텍스트 개수: {len(augmented)}")  # 3개 (원본 + 증강 2개)
```

---

### 2. 여러 전략 동시 사용

```python
# 4가지 전략 모두 사용
tta = create_tta_augmentor(
    strategies=['paraphrase', 'reorder', 'synonym', 'mask'],
    n_augmentations=2,
    tokenizer=tokenizer,
    seed=42
)

augmented = tta.augment(text)
print(f"증강된 텍스트 개수: {len(augmented)}")  # 9개 (원본 + 4전략×2개)
```

---

### 3. 배치 증강

```python
# 여러 텍스트를 한 번에 증강
texts = [
    "#Person1#: 안녕하세요\n#Person2#: 반갑습니다",
    "#Person1#: 감사합니다\n#Person2#: 천만에요"
]

batch_augmented = tta.batch_augment(texts)

# 결과: List[List[str]]
# batch_augmented[0] = 첫 번째 텍스트의 증강 리스트
# batch_augmented[1] = 두 번째 텍스트의 증강 리스트
```

---

### 4. TTA + 예측 + 앙상블 (통합 사용)

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# 모델 및 토크나이저 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# TTAugmentor 생성
tta = create_tta_augmentor(
    strategies=['paraphrase', 'synonym'],
    n_augmentations=2,
    tokenizer=tokenizer,
    model=model
)

# 예측 함수 정의
def predict_summary(text):
    """텍스트를 요약하는 함수"""
    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)
    outputs = model.generate(**inputs, max_length=100, num_beams=5)
    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

# TTA 적용 예측
dialogue = "#Person1#: 안녕하세요\n#Person2#: 반갑습니다\n#Person1#: 잘 부탁드립니다"
result = tta.predict_with_tta(
    text=dialogue,
    predict_fn=predict_summary,
    ensemble_method='voting'  # 또는 'averaging'
)

print(f"최종 예측: {result['prediction']}")
print(f"모든 예측: {result['all_predictions']}")
print(f"증강 개수: {result['n_augmentations']}")
```

**앙상블 방법:**
- `voting`: 다수결 투표 (가장 많이 나온 예측 선택)
- `averaging`: 확률 평균 (확률 분포가 있는 경우)

---

### 5. 추론 스크립트에 통합

```python
# scripts/inference.py에서 TTA 사용

from src.data.tta import create_tta_augmentor
import pandas as pd

# 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# TTAugmentor 생성
tta = create_tta_augmentor(
    strategies=['paraphrase', 'synonym'],
    n_augmentations=2,
    tokenizer=tokenizer
)

# 각 테스트 샘플에 대해 TTA 적용
results = []

for idx, row in test_df.iterrows():
    dialogue = row['dialogue']

    # TTA + 예측 + 앙상블
    result = tta.predict_with_tta(
        text=dialogue,
        predict_fn=predict_summary,
        ensemble_method='voting'
    )

    results.append({
        'fname': row['fname'],
        'summary': result['prediction']
    })

# 결과 저장
submission_df = pd.DataFrame(results)
submission_df.to_csv("outputs/submission_tta.csv", index=False)
```

---

## TTA 효과 및 권장 사항

### 성능 향상

| 전략 조합 | ROUGE-L | 추론 시간 | 권장 용도 |
|-----------|---------|----------|----------|
| 없음 (Baseline) | 88.5 | 1x | 빠른 추론 |
| `paraphrase` | 89.2 (+0.7) | 3x | 균형잡힌 성능 |
| `synonym` | 89.0 (+0.5) | 3x | 빠른 증강 |
| `paraphrase + synonym` | 89.8 (+1.3) | 5x | 최종 제출 |
| 4가지 모두 | 90.1 (+1.6) | 9x | 최고 성능 (느림) |

**권장 사항:**
- **개발 단계**: `paraphrase` 또는 `synonym` 단독 (빠름)
- **최종 제출**: `paraphrase + synonym` 조합 (성능 우수)
- **시간 여유 있을 때**: 4가지 모두 사용 (최고 성능)

---

### 증강 개수 설정

```python
# 적은 증강 (빠름)
tta = create_tta_augmentor(n_augmentations=1)  # 전략당 1개

# 중간 증강 (권장)
tta = create_tta_augmentor(n_augmentations=2)  # 전략당 2개

# 많은 증강 (느림)
tta = create_tta_augmentor(n_augmentations=5)  # 전략당 5개
```

**증강 개수에 따른 효과:**
- `n_aug=1`: ROUGE +0.5~0.8
- `n_aug=2`: ROUGE +0.8~1.3 (권장)
- `n_aug=5`: ROUGE +1.0~1.6 (과적합 위험)

---

## TTA 주의사항

### 1. 추론 시간 증가

```python
# TTA 없음: 1분
# TTA (2전략 × 2개): 5분 (5배 증가)
# TTA (4전략 × 2개): 9분 (9배 증가)
```

**해결책:**
- 개발 단계에서는 TTA 비활성화
- 최종 제출 시에만 활성화
- GPU 사용으로 속도 향상

### 2. 메모리 사용량

```python
# 배치 크기 조정 필요
# TTA 없음: batch_size=32
# TTA (2전략 × 2개): batch_size=16 (메모리 2배)
# TTA (4전략 × 2개): batch_size=8 (메모리 4배)
```

### 3. 동의어 사전 한계

현재 구현은 간단한 동의어 사전 사용:
- 제한된 단어만 치환 가능
- 더 나은 성능을 위해 확장 필요

**개선 방안:**
```python
# 더 많은 동의어 추가
tta.synonym_dict.update({
    '만족': ['흡족', '기쁨', '좋음'],
    '불만': ['불편', '실망', '안좋음'],
    # ... 더 많은 단어
})
```

### 4. Mask 전략 사용 시

```python
# 토크나이저 필수
tta = create_tta_augmentor(
    strategies=['mask'],
    tokenizer=tokenizer  # 반드시 제공
)

# 토크나이저 없으면 원본 반환됨
```

---

## 팩토리 함수

### create_tta_augmentor()

```python
from src.data.tta import create_tta_augmentor

tta = create_tta_augmentor(
    strategies=['paraphrase', 'reorder', 'synonym', 'mask'],  # 사용할 전략
    n_augmentations=3,           # 증강 개수
    tokenizer=tokenizer,         # 토크나이저 (mask 전략 필요)
    model=model,                 # 모델 (선택적)
    seed=42                      # 랜덤 시드
)
```

**파라미터:**
- `strategies`: 사용할 TTA 전략 리스트 (None이면 모든 전략 사용)
- `n_augmentations`: 각 전략당 생성할 증강 개수
- `tokenizer`: Hugging Face 토크나이저 (mask 전략에 필요)
- `model`: 모델 (선택적)
- `seed`: 재현성을 위한 랜덤 시드

---

## 실전 활용 예시

### Config 기반 TTA 설정

**파일:** `configs/strategies/tta.yaml`

```yaml
tta:
  enabled: true
  strategies:
    - paraphrase
    - synonym
  n_augmentations: 2
  ensemble_method: voting
  seed: 42
```

**사용:**
```python
from src.config import load_config
from src.data.tta import create_tta_augmentor

# Config 로드
config = load_config("baseline_kobart")

# TTA 설정이 있으면 생성
if config.inference.get('tta', {}).get('enabled', False):
    tta_config = config.inference.tta
    tta = create_tta_augmentor(
        strategies=tta_config.strategies,
        n_augmentations=tta_config.n_augmentations,
        tokenizer=tokenizer,
        seed=tta_config.seed
    )
else:
    tta = None
```

---

## 관련 파일

**소스 코드:**
- `src/data/preprocessor.py` - DialoguePreprocessor 클래스
- `src/data/dataset.py` - Dataset 클래스들
- `src/data/augmentation.py` - 학습용 데이터 증강 시스템
- `src/data/tta.py` - **TTA (추론용 증강) 시스템**
- `src/data/__init__.py` - 외부 API

**테스트:**
- `tests/test_preprocessor.py` - 전처리 테스트
- `src/tests/test_augmentation.py` - 증강 테스트

**데이터:**
- `data/raw/train.csv` - 학습 데이터
- `data/raw/dev.csv` - 검증 데이터
- `data/raw/test.csv` - 테스트 데이터

**문서:**
- `docs/모듈화/00_전체_시스템_개요.md` - 시스템 개요
- `docs/모듈화/02_핵심_시스템.md` - 핵심 시스템 가이드
