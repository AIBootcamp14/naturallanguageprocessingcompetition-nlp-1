# 02. í•µì‹¬ ì‹œìŠ¤í…œ (ì „ì²´ ì•„í‚¤í…ì²˜, Config, Logger)

## ëª©ì°¨
- [Part 1: ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#part-1-ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
  - [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
  - [ëª¨ë“ˆ êµ¬ì¡°](#ëª¨ë“ˆ-êµ¬ì¡°)
  - [ë°ì´í„° í”Œë¡œìš°](#ë°ì´í„°-í”Œë¡œìš°)
  - [ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •](#ì„¤ì¹˜-ë°-í™˜ê²½-ì„¤ì •)
  - [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
  - [Config ì‹œìŠ¤í…œ ê°œìš”](#config-ì‹œìŠ¤í…œ-ê°œìš”)
  - [í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½](#í…ŒìŠ¤íŠ¸-ê²°ê³¼-ìš”ì•½)
- [Part 2: Config ì‹œìŠ¤í…œ](#part-2-config-ì‹œìŠ¤í…œ)
  - [ê°œìš”](#ê°œìš”)
  - [Config êµ¬ì¡°](#config-êµ¬ì¡°)
  - [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
  - [Config íŒŒì¼ ì‘ì„±](#config-íŒŒì¼-ì‘ì„±)
  - [í…ŒìŠ¤íŠ¸ ê²°ê³¼](#í…ŒìŠ¤íŠ¸-ê²°ê³¼)
  - [ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ](#ì‹¤ì „-ì‚¬ìš©-ì˜ˆì‹œ)
  - [ì£¼ì˜ì‚¬í•­](#ì£¼ì˜ì‚¬í•­)
- [Part 3: Logger ì‹œìŠ¤í…œ](#part-3-logger-ì‹œìŠ¤í…œ)
  - [Logger ê°œìš”](#logger-ê°œìš”)
  - [Logger ì‚¬ìš©ë²•](#logger-ì‚¬ìš©ë²•)
  - [ëª¨ë“ˆë³„ Logger í†µí•©](#ëª¨ë“ˆë³„-logger-í†µí•©)
  - [GPU ìœ í‹¸ë¦¬í‹° í†µí•©](#gpu-ìœ í‹¸ë¦¬í‹°-í†µí•©)
  - [í†µí•© ì˜ˆì‹œ](#í†µí•©-ì˜ˆì‹œ)

---

# Part 1: ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "ì…ë ¥ ê³„ì¸µ"
        A[Config YAML íŒŒì¼ë“¤] --> B[ConfigLoader]
        C[Raw Data CSV] --> D[DialoguePreprocessor]
    end

    subgraph "ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"
        B --> E[í†µí•© Config]
        D --> F[ì „ì²˜ë¦¬ëœ ë°ì´í„°]
        F --> G[PyTorch Dataset]
    end

    subgraph "ëª¨ë¸ ê³„ì¸µ"
        E --> H[ModelLoader]
        H --> I[ì‚¬ì „í•™ìŠµ ëª¨ë¸]
        H --> J[Tokenizer]
    end

    subgraph "í•™ìŠµ ê³„ì¸µ"
        G --> K[Trainer]
        I --> K
        J --> K
        E --> K
        K --> L[í•™ìŠµëœ ëª¨ë¸]
        K --> M[WandB ë¡œê·¸]
    end

    subgraph "í‰ê°€ ê³„ì¸µ"
        K --> N[RougeCalculator]
        N --> O[ROUGE ì ìˆ˜]
    end

    subgraph "ì¶”ë¡  ê³„ì¸µ"
        L --> P[Predictor]
        J --> P
        P --> Q[ì˜ˆì¸¡ ê²°ê³¼]
        Q --> R[ì œì¶œ íŒŒì¼ CSV]
    end

    style A fill:#e1f5ff
    style C fill:#e1f5ff
    style L fill:#c8e6c9
    style R fill:#c8e6c9
    style M fill:#fff9c4
```

---

## ëª¨ë“ˆ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
src/
â”œâ”€â”€ config/              # Config ê´€ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loader.py       # ê³„ì¸µì  Config ë³‘í•©
â”‚
â”œâ”€â”€ data/               # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessor.py # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
â”‚   â””â”€â”€ dataset.py      # PyTorch Dataset í´ë˜ìŠ¤
â”‚
â”œâ”€â”€ models/             # ëª¨ë¸ ë¡œë”©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_loader.py # HuggingFace ëª¨ë¸ ë¡œë”
â”‚
â”œâ”€â”€ evaluation/         # í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ metrics.py      # ROUGE ê³„ì‚°
â”‚
â”œâ”€â”€ training/           # í•™ìŠµ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py      # Seq2SeqTrainer ë˜í¼
â”‚
â””â”€â”€ inference/          # ì¶”ë¡  ì‹œìŠ¤í…œ
    â”œâ”€â”€ __init__.py
    â””â”€â”€ predictor.py    # ë°°ì¹˜ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
```

### ëª¨ë“ˆë³„ ì—­í• 

| ëª¨ë“ˆ | ì£¼ìš” í´ë˜ìŠ¤ | í•µì‹¬ ê¸°ëŠ¥ |
|------|------------|----------|
| **config** | `ConfigLoader` | YAML ê¸°ë°˜ ê³„ì¸µì  ì„¤ì • ê´€ë¦¬ |
| **data** | `DialoguePreprocessor`<br>`DialogueSummarizationDataset`<br>`InferenceDataset` | ë°ì´í„° ì „ì²˜ë¦¬<br>í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹<br>ì¶”ë¡  ë°ì´í„°ì…‹ |
| **models** | `ModelLoader` | ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”©<br>íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ |
| **evaluation** | `RougeCalculator` | ROUGE-1/2/L ê³„ì‚°<br>Multi-reference ì§€ì› |
| **training** | `ModelTrainer` | í•™ìŠµ íŒŒì´í”„ë¼ì¸<br>WandB ë¡œê¹… í†µí•© |
| **inference** | `Predictor` | ë°°ì¹˜ ì¶”ë¡ <br>ì œì¶œ íŒŒì¼ ìƒì„± |

---

## ë°ì´í„° í”Œë¡œìš°

### í•™ìŠµ íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Config
    participant Data
    participant Model
    participant Trainer
    participant WandB

    User->>Config: load_config("baseline_kobart")
    Config-->>User: í†µí•© Config ê°ì²´

    User->>Model: load_model_and_tokenizer(config)
    Model-->>User: model, tokenizer

    User->>Data: DialogueSummarizationDataset(...)
    Data-->>User: train_dataset, eval_dataset

    User->>Trainer: ModelTrainer(config, model, ...)
    Trainer->>WandB: init_run()

    User->>Trainer: trainer.train()

    loop ê° ì—í¬í¬
        Trainer->>Model: í•™ìŠµ ìŠ¤í…
        Trainer->>Data: ê²€ì¦ ë°ì´í„° ë¡œë“œ
        Trainer->>Model: í‰ê°€ ìŠ¤í…
        Trainer->>Trainer: ROUGE ê³„ì‚°
        Trainer->>WandB: log_metrics()
        Trainer->>Trainer: ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    end

    Trainer-->>User: í•™ìŠµ ì™„ë£Œ (ìµœì¢… ëª¨ë¸)
```

### ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Model
    participant Data
    participant Predictor
    participant File

    User->>Model: í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    Model-->>User: model, tokenizer

    User->>Predictor: Predictor(model, tokenizer, config)

    User->>Data: test.csv ì½ê¸°
    Data-->>User: test_df

    User->>Predictor: create_submission(test_df, output_path)

    loop ê° ë°°ì¹˜
        Predictor->>Data: ë°°ì¹˜ ë°ì´í„° ë¡œë“œ
        Predictor->>Model: generate()
        Model-->>Predictor: ìƒì„±ëœ ìš”ì•½
    end

    Predictor->>File: submission.csv ì €ì¥
    Predictor-->>User: submission_df
```

---

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ í™œì„±í™”

```bash
# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# ë˜ëŠ” pyenv ëª…ë ¹ì–´ ì‚¬ìš©
pyenv activate nlp_py3_11_9
```

### 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

**ì£¼ìš” íŒ¨í‚¤ì§€:**
- `torch==2.8.0` - PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `transformers==4.57.0` - HuggingFace Transformers
- `pandas==2.3.3` - ë°ì´í„° ì²˜ë¦¬
- `omegaconf==2.3.0` - Config ê´€ë¦¬
- `rouge-score==0.1.2` - ROUGE í‰ê°€
- `wandb==0.22.2` - ì‹¤í—˜ ë¡œê¹…

### 3. í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
tree -L 2 src/
tree -L 2 configs/
tree -L 2 tests/
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. Config ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰

```python
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# 1. Config ë¡œë“œ
config = load_config("baseline_kobart")

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# 3. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer
)

# 4. Trainer ìƒì„± ë° í•™ìŠµ
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset
)

# 5. í•™ìŠµ ì‹¤í–‰
results = trainer.train()
```

### 2. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±

```python
from src.models import load_model_and_tokenizer
from src.inference import create_predictor
import pandas as pd

# 1. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)
# ë˜ëŠ” ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
# model = AutoModelForSeq2SeqLM.from_pretrained("outputs/best_model")

# 2. Predictor ìƒì„±
predictor = create_predictor(
    model=model,
    tokenizer=tokenizer,
    config=config
)

# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# 4. ì œì¶œ íŒŒì¼ ìƒì„±
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32
)
```

### 3. ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ ê° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

python tests/test_config_loader.py
python tests/test_preprocessor.py
python tests/test_model_loader.py
python tests/test_metrics.py
python tests/test_trainer.py
python tests/test_predictor.py
```

---

## Config ì‹œìŠ¤í…œ ê°œìš”

### Config ê³„ì¸µ êµ¬ì¡°

```mermaid
graph LR
    A[base/default.yaml] --> F[ìµœì¢… Config]
    B[base/encoder_decoder.yaml] --> F
    C[models/kobart.yaml] --> F
    D[experiments/baseline_kobart.yaml] --> F

    F --> G[í•™ìŠµ ì‹¤í–‰]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style F fill:#c8e6c9
```

### Config ë³‘í•© ìš°ì„ ìˆœìœ„

1. `base/default.yaml` - ê¸°ë³¸ ì„¤ì •
2. `base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì…ë³„ ì„¤ì •
3. `models/{model_name}.yaml` - íŠ¹ì • ëª¨ë¸ ì„¤ì •
4. `experiments/{experiment_name}.yaml` - ì‹¤í—˜ë³„ ì„¤ì • (ìµœìš°ì„ )

**ì˜ˆì‹œ:**
```yaml
# base/default.yaml
training:
  batch_size: 8
  learning_rate: 5e-5

# experiments/baseline_kobart.yaml
training:
  batch_size: 50        # ì˜¤ë²„ë¼ì´ë“œë¨
  learning_rate: 1e-5   # ì˜¤ë²„ë¼ì´ë“œë¨
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½

### ì „ì²´ í…ŒìŠ¤íŠ¸ í˜„í™©

| ëª¨ë“ˆ | í…ŒìŠ¤íŠ¸ íŒŒì¼ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|------|------------|-----------|------|
| Config | `test_config_loader.py` | 6ê°œ | âœ… í†µê³¼ |
| Data | `test_preprocessor.py` | 5ê°œ | âœ… í†µê³¼ |
| Models | `test_model_loader.py` | 5ê°œ | âœ… í†µê³¼ |
| Evaluation | `test_metrics.py` | 6ê°œ | âœ… í†µê³¼ |
| Training | `test_trainer.py` | 5ê°œ | âœ… í†µê³¼ |
| Inference | `test_predictor.py` | 6ê°œ | âœ… í†µê³¼ |
| **ì „ì²´** | **6ê°œ íŒŒì¼** | **33ê°œ** | **âœ… ëª¨ë‘ í†µê³¼** |

### ì£¼ìš” ê²€ì¦ í•­ëª©

âœ… **Config Loader**
- ê³„ì¸µì  YAML ë³‘í•© ì •ìƒ ë™ì‘
- ì‹¤í—˜ë³„ Config ì˜¤ë²„ë¼ì´ë“œ ì •ìƒ ì‘ë™

âœ… **Data Preprocessing**
- 12,457ê°œ ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ
- ë…¸ì´ì¦ˆ ì œê±°, í™”ì ì¶”ì¶œ, í„´ ê³„ì‚° ì •ìƒ ë™ì‘

âœ… **Model Loader**
- KoBART ëª¨ë¸ (123M íŒŒë¼ë¯¸í„°) ë¡œë”© ì„±ê³µ
- GPU ìë™ ê°ì§€ ë° ë°°ì¹˜ ì •ìƒ ë™ì‘

âœ… **Metrics**
- ROUGE-1/2/L ê³„ì‚° ì •ìƒ ë™ì‘
- Multi-reference í‰ê°€ ì§€ì› í™•ì¸

âœ… **Trainer**
- Seq2SeqTrainer ë˜í•‘ ì •ìƒ ë™ì‘
- WandB ë¡œê¹… í†µí•© í™•ì¸

âœ… **Inference**
- ë°°ì¹˜ ì¶”ë¡  ì •ìƒ ë™ì‘
- ì œì¶œ íŒŒì¼ ìƒì„± ì •ìƒ ë™ì‘

---

# Part 2: Config ì‹œìŠ¤í…œ

## ê°œìš”

### ëª©ì 
- ì‹¤í—˜ ì„¤ì •ì˜ ì²´ê³„ì  ê´€ë¦¬
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í™˜ê²½
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²„ì „ ê´€ë¦¬
- ë‹¤ì–‘í•œ ì‹¤í—˜ ì„¤ì •ì˜ ë¹ ë¥¸ ì „í™˜

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ê³„ì¸µì  YAML ë³‘í•©
- âœ… ì‹¤í—˜ë³„ Config ì˜¤ë²„ë¼ì´ë“œ
- âœ… OmegaConf ê¸°ë°˜ íƒ€ì… ì•ˆì „ì„±
- âœ… ëˆ„ë½ëœ íŒŒì¼ ìë™ ì²˜ë¦¬

---

## Config êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
configs/
â”œâ”€â”€ base/                           # ê¸°ë³¸ ì„¤ì •
â”‚   â”œâ”€â”€ default.yaml               # ì „ì²´ ê¸°ë³¸ê°’
â”‚   â””â”€â”€ encoder_decoder.yaml       # ëª¨ë¸ íƒ€ì…ë³„ ê¸°ë³¸ê°’
â”‚
â”œâ”€â”€ models/                         # ëª¨ë¸ë³„ ì„¤ì •
â”‚   â”œâ”€â”€ kobart.yaml                # KoBART ì„¤ì •
â”‚   â”œâ”€â”€ t5.yaml                    # T5 ì„¤ì • (ì˜ˆì‹œ)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ experiments/                    # ì‹¤í—˜ë³„ ì„¤ì •
    â”œâ”€â”€ baseline_kobart.yaml       # ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜
    â”œâ”€â”€ finetuned_kobart.yaml      # íŒŒì¸íŠœë‹ ì‹¤í—˜ (ì˜ˆì‹œ)
    â””â”€â”€ ...
```

### Config ë³‘í•© í”Œë¡œìš°

```mermaid
graph TD
    A[ì‹¤í—˜ ì‹œì‘] --> B{ì‹¤í—˜ ì´ë¦„ ì§€ì •}
    B --> C[1. base/default.yaml ë¡œë“œ]
    C --> D[2. base/encoder_decoder.yaml ë¡œë“œ]
    D --> E[3. models/kobart.yaml ë¡œë“œ]
    E --> F[4. experiments/baseline_kobart.yaml ë¡œë“œ]

    F --> G[OmegaConfë¡œ ë³‘í•©]
    G --> H[ìµœì¢… í†µí•© Config]

    H --> I[í•™ìŠµ/ì¶”ë¡  ì‹œìŠ¤í…œìœ¼ë¡œ ì „ë‹¬]

    style C fill:#e3f2fd
    style D fill:#e3f2fd
    style E fill:#fff3e0
    style F fill:#f3e5f5
    style H fill:#c8e6c9
```

### ë³‘í•© ìš°ì„ ìˆœìœ„

**ë‚®ìŒ â†’ ë†’ìŒ ìˆœì„œ:**
1. `base/default.yaml` - ì „ì²´ ê¸°ë³¸ ì„¤ì •
2. `base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì… ì„¤ì •
3. `models/{model_name}.yaml` - íŠ¹ì • ëª¨ë¸ ì„¤ì •
4. `experiments/{experiment_name}.yaml` - **ì‹¤í—˜ ì„¤ì • (ìµœìš°ì„ )**

**ì˜ˆì‹œ:**
```yaml
# base/default.yaml
training:
  batch_size: 8
  learning_rate: 5e-5
  epochs: 10

# experiments/baseline_kobart.yaml
training:
  batch_size: 50        # ì˜¤ë²„ë¼ì´ë“œ
  learning_rate: 1e-5   # ì˜¤ë²„ë¼ì´ë“œ
  # epochs: 10ì€ default ê°’ ì‚¬ìš©
```

**ë³‘í•© ê²°ê³¼:**
```yaml
training:
  batch_size: 50        # experimentsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
  learning_rate: 1e-5   # experimentsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
  epochs: 10            # default ê°’ ìœ ì§€
```

---

## ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.config import load_config

# ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ Config ë¡œë“œ
config = load_config("baseline_kobart")

# Config ê°’ ì ‘ê·¼
print(config.training.batch_size)     # 50
print(config.model.checkpoint)        # "digit82/kobart-summarization"
print(config.experiment.name)         # "baseline_kobart"
```

### 2. Config ê°’ í™•ì¸

```python
from omegaconf import OmegaConf

# Config ì „ì²´ ì¶œë ¥
print(OmegaConf.to_yaml(config))

# íŠ¹ì • ì„¹ì…˜ë§Œ ì¶œë ¥
print(OmegaConf.to_yaml(config.training))
print(OmegaConf.to_yaml(config.model))
```

### 3. Config ê°’ ìˆ˜ì • (ëŸ°íƒ€ì„)

```python
# ê°’ ë³€ê²½
config.training.batch_size = 32
config.training.learning_rate = 2e-5

# ìƒˆë¡œìš´ í‚¤ ì¶”ê°€
config.custom_param = "value"
```

### 4. ConfigLoader ì§ì ‘ ì‚¬ìš©

```python
from src.config import ConfigLoader

# ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
loader = ConfigLoader(config_dir="configs")

# ìˆ˜ë™ìœ¼ë¡œ ë³‘í•©
config = loader.merge_configs("baseline_kobart")

# íŠ¹ì • íŒŒì¼ë§Œ ë¡œë“œ
base_config = loader.load_base_config()
model_config = loader.load_model_config("kobart")
```

---

## Config íŒŒì¼ ì‘ì„±

### base/default.yaml êµ¬ì¡°

```yaml
# ==================== ê¸°ë³¸ ì„¤ì • ==================== #

# ì‹¤í—˜ ì •ë³´
experiment:
  name: "default"
  seed: 42
  deterministic: true

# ê²½ë¡œ ì„¤ì •
paths:
  train_data: "data/raw/train.csv"
  dev_data: "data/raw/dev.csv"
  test_data: "data/raw/test.csv"
  output_dir: "outputs"

# í•™ìŠµ ì„¤ì •
training:
  output_dir: "outputs"
  epochs: 10
  batch_size: 8
  learning_rate: 5e-5
  device: "cuda"

# í‰ê°€ ì„¤ì •
evaluation:
  metric: "rouge"
  rouge_types:
    - "rouge1"
    - "rouge2"
    - "rougeL"
```

### base/encoder_decoder.yaml êµ¬ì¡°

```yaml
# ==================== Encoder-Decoder ê³µí†µ ì„¤ì • ==================== #

model:
  type: "encoder_decoder"
  architecture: "bart"

# í† í¬ë‚˜ì´ì € ì„¤ì •
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    # ... ì¶”ê°€ íŠ¹ìˆ˜ í† í°

# ì¶”ë¡  ì„¤ì •
inference:
  batch_size: 32
  num_beams: 4
  early_stopping: true
  generate_max_length: 100
  no_repeat_ngram_size: 2
```

### models/kobart.yaml êµ¬ì¡°

```yaml
# ==================== KoBART ëª¨ë¸ ì„¤ì • ==================== #

model:
  name: "kobart"
  checkpoint: "digit82/kobart-summarization"

# KoBART íŠ¹í™” ì„¤ì •
# (í•„ìš”ì‹œ ì¶”ê°€)
```

### experiments/baseline_kobart.yaml êµ¬ì¡°

```yaml
# ==================== ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜ ì„¤ì • ==================== #

experiment:
  name: "baseline_kobart"
  description: "ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸ ì¬í˜„ ì‹¤í—˜"
  tags:
    - "baseline"
    - "kobart"

# WandB ì„¤ì •
wandb:
  enabled: true
  project: "nlp-competition"
  entity: "ieyeppo"

# ëª¨ë¸ ì„¤ì •
model:
  name: "kobart"

# í•™ìŠµ ì„¤ì • (ë² ì´ìŠ¤ë¼ì¸ ì˜¤ë²„ë¼ì´ë“œ)
training:
  epochs: 20
  batch_size: 50
  learning_rate: 1e-5

# ê²½ë¡œ ì„¤ì •
paths:
  output_dir: "outputs/baseline_kobart"
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼

### í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_config_loader.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 6ê°œ)

#### 1. âœ… ê¸°ë³¸ Config ë¡œë“œ
```python
def test_load_base_config():
    """base/default.yaml ë¡œë“œ í™•ì¸"""
    loader = ConfigLoader()
    config = loader.load_base_config()

    assert config.experiment.seed == 42
    assert config.paths.train_data == "data/raw/train.csv"
```

**ê²°ê³¼:**
```
âœ… ê¸°ë³¸ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  experiment.seed: 42
  paths.train_data: data/raw/train.csv
```

#### 2. âœ… ëª¨ë¸ íƒ€ì… Config ë¡œë“œ
```python
def test_load_model_type_config():
    """base/encoder_decoder.yaml ë¡œë“œ í™•ì¸"""
    loader = ConfigLoader()
    config = loader.load_model_type_config("encoder_decoder")

    assert config.model.type == "encoder_decoder"
    assert config.tokenizer.encoder_max_len == 512
```

**ê²°ê³¼:**
```
âœ… ëª¨ë¸ íƒ€ì… Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  model.type: encoder_decoder
  tokenizer.encoder_max_len: 512
```

#### 3. âœ… ëª¨ë¸ë³„ Config ë¡œë“œ
```python
def test_load_model_config():
    """models/kobart.yaml ë¡œë“œ í™•ì¸"""
    loader = ConfigLoader()
    config = loader.load_model_config("kobart")

    assert config.model.name == "kobart"
    assert "digit82" in config.model.checkpoint
```

**ê²°ê³¼:**
```
âœ… ëª¨ë¸ë³„ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  model.name: kobart
  model.checkpoint: digit82/kobart-summarization
```

#### 4. âœ… ì‹¤í—˜ Config ë¡œë“œ
```python
def test_load_experiment_config():
    """experiments/baseline_kobart.yaml ë¡œë“œ í™•ì¸"""
    loader = ConfigLoader()
    config = loader.load_experiment_config("baseline_kobart")

    assert config.experiment.name == "baseline_kobart"
    assert config.wandb.enabled == True
```

**ê²°ê³¼:**
```
âœ… ì‹¤í—˜ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  experiment.name: baseline_kobart
  wandb.enabled: True
```

#### 5. âœ… ê³„ì¸µì  Config ë³‘í•©
```python
def test_merge_configs():
    """ì „ì²´ Config ë³‘í•© í™•ì¸"""
    loader = ConfigLoader()
    config = loader.merge_configs("baseline_kobart")

    # ë³‘í•© ìš°ì„ ìˆœìœ„ í™•ì¸
    assert config.training.batch_size == 50      # experimentì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    assert config.training.epochs == 20          # experimentì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    assert config.tokenizer.encoder_max_len == 512  # base ê°’ ìœ ì§€
```

**ê²°ê³¼:**
```
âœ… ê³„ì¸µì  Config ë³‘í•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  training.batch_size: 50 (ì˜¤ë²„ë¼ì´ë“œë¨)
  training.epochs: 20 (ì˜¤ë²„ë¼ì´ë“œë¨)
  tokenizer.encoder_max_len: 512 (base ê°’ ìœ ì§€)
```

#### 6. âœ… í¸ì˜ í•¨ìˆ˜
```python
def test_load_config_function():
    """load_config() í¸ì˜ í•¨ìˆ˜ í™•ì¸"""
    config = load_config("baseline_kobart")

    assert config.experiment.name == "baseline_kobart"
    assert config.model.checkpoint is not None
```

**ê²°ê³¼:**
```
âœ… í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  Config ë¡œë“œ ì™„ë£Œ
  ëª¨ë“  ì„¹ì…˜ ì ‘ê·¼ ê°€ëŠ¥
```

### ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½

```
============================================================
Config Loader í…ŒìŠ¤íŠ¸ ì‹œì‘
============================================================

í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ Config ë¡œë“œ                    âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 2: ëª¨ë¸ íƒ€ì… Config ë¡œë“œ                âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 3: ëª¨ë¸ë³„ Config ë¡œë“œ                   âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 4: ì‹¤í—˜ Config ë¡œë“œ                     âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 5: ê³„ì¸µì  Config ë³‘í•©                   âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 6: í¸ì˜ í•¨ìˆ˜                           âœ… í†µê³¼

============================================================
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! (6/6)
============================================================
```

---

## ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ìƒˆë¡œìš´ ì‹¤í—˜ ì„¤ì • ì¶”ê°€

```yaml
# configs/experiments/finetuned_kobart.yaml
experiment:
  name: "finetuned_kobart_v2"
  description: "íŒŒì¸íŠœë‹ ì‹¤í—˜ ë²„ì „ 2"
  tags:
    - "finetuned"
    - "v2"

training:
  epochs: 30                    # ë” ê¸´ í•™ìŠµ
  batch_size: 32                # ì‘ì€ ë°°ì¹˜
  learning_rate: 5e-6           # ë‚®ì€ í•™ìŠµë¥ 
  warmup_steps: 1000            # ë” ê¸´ warmup

paths:
  output_dir: "outputs/finetuned_v2"
```

**ì‚¬ìš©:**
```python
config = load_config("finetuned_kobart_v2")
```

### ì˜ˆì‹œ 2: ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •

```yaml
# configs/experiments/debug.yaml
experiment:
  name: "debug"

debug:
  use_subset: true
  subset_size: 100              # 100ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©

training:
  epochs: 2                     # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
  batch_size: 4

wandb:
  enabled: false                # WandB ë¹„í™œì„±í™”
```

### ì˜ˆì‹œ 3: ëŸ°íƒ€ì„ì—ì„œ Config ìˆ˜ì •

```python
config = load_config("baseline_kobart")

# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
config.training.batch_size = 16

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì—í¬í¬ ì¤„ì´ê¸°
config.training.epochs = 5

# WandB ë¹„í™œì„±í™”
config.wandb.enabled = False

# ìˆ˜ì •ëœ Configë¡œ í•™ìŠµ ì§„í–‰
trainer = create_trainer(config, ...)
```

---

## ì£¼ì˜ì‚¬í•­

### 1. YAML ë¬¸ë²•
- ë“¤ì—¬ì“°ê¸°ëŠ” **ê³µë°± 2ì¹¸** ì‚¬ìš© (íƒ­ ì‚¬ìš© ê¸ˆì§€)
- ë¬¸ìì—´ì— íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì‹œ ë”°ì˜´í‘œ ì‚¬ìš©
- ë¦¬ìŠ¤íŠ¸ëŠ” `-` ì‚¬ìš©

### 2. íŒŒì¼ëª… ê·œì¹™
- `experiments/` í´ë”ì˜ YAML íŒŒì¼ëª… = ì‹¤í—˜ ì´ë¦„
- ì˜ˆ: `baseline_kobart.yaml` â†’ `load_config("baseline_kobart")`

### 3. Config ê°’ ì ‘ê·¼
```python
# âœ… ì˜¬ë°”ë¥¸ ì ‘ê·¼
config.training.batch_size

# âŒ ì˜ëª»ëœ ì ‘ê·¼ (Key Error ë°œìƒ ê°€ëŠ¥)
config['training']['batch_size']

# âœ… ì•ˆì „í•œ ì ‘ê·¼ (ê¸°ë³¸ê°’ ì œê³µ)
config.get('training', {}).get('batch_size', 32)
```

### 4. ëˆ„ë½ëœ Config ì²˜ë¦¬
- ConfigLoaderëŠ” ëˆ„ë½ëœ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê±´ë„ˆëœ€
- í•„ìˆ˜ ì„¤ì •ì€ `base/default.yaml`ì— ì •ì˜ ê¶Œì¥

---

# Part 3: Logger ì‹œìŠ¤í…œ

## Logger ê°œìš”

ëª¨ë“  ëª¨ë“ˆì—ì„œ print ëŒ€ì‹  `src/logging/logger.py`ì˜ Logger í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**Loggerì˜ ì´ì :**
- ì½˜ì†” ì¶œë ¥ê³¼ íŒŒì¼ ë¡œê¹… ë™ì‹œ ìˆ˜í–‰
- íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ê°€
- stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ ì§€ì›
- ì—ëŸ¬ ë©”ì‹œì§€ ê°•ì¡° ì¶œë ¥

---

## Logger ì‚¬ìš©ë²•

### 1. Logger ì´ˆê¸°í™”

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„±
log_path = create_log_path("outputs/logs", "train")  # outputs/logs/train_20251011_143000.log

# Logger ì´ˆê¸°í™”
logger = Logger(log_path, print_also=True)

# stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ ì‹œì‘
logger.start_redirect()

# ì‘ì—… ìˆ˜í–‰...

# ë¦¬ë‹¤ì´ë ‰ì…˜ ì¢…ë£Œ
logger.stop_redirect()
logger.close()
```

### 2. print ëŒ€ì‹  logger.write ì‚¬ìš©

```python
# âŒ ê¸°ì¡´ ë°©ì‹
print("=" * 60)
print("ëª¨ë¸ ë¡œë”© ì‹œì‘")
print("=" * 60)

# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹
logger.write("=" * 60)
logger.write("ëª¨ë¸ ë¡œë”© ì‹œì‘")
logger.write("=" * 60)
```

### 3. ì—ëŸ¬ ë¡œê¹…

```python
# ì—ëŸ¬ ë©”ì‹œì§€ (ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ì¶œë ¥)
logger.write("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!", print_error=True)
```

### 4. Logger ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš© (ê¶Œì¥)

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

log_path = create_log_path("outputs/logs", "train")

# with ë¬¸ì„ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ start_redirect()ì™€ stop_redirect() í˜¸ì¶œ
with Logger(log_path, print_also=True) as logger:
    logger.write("=" * 60)
    logger.write("í•™ìŠµ ì‹œì‘")
    logger.write("=" * 60)

    # ì‘ì—… ìˆ˜í–‰...

# with ë¸”ë¡ì„ ë²—ì–´ë‚˜ë©´ ìë™ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜ ì¢…ë£Œ ë° íŒŒì¼ ë‹«ê¸°
```

---

## ëª¨ë“ˆë³„ Logger í†µí•©

### src/models/model_loader.py

```python
# ìˆ˜ì • ì „
print(f"í† í¬ë‚˜ì´ì € ë¡œë”©: {checkpoint}")
print(f"  â†’ íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")

# ìˆ˜ì • í›„
if logger:
    logger.write(f"í† í¬ë‚˜ì´ì € ë¡œë”©: {checkpoint}")
    logger.write(f"  â†’ íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")
```

**ì ìš© ì˜ˆì‹œ:**
```python
def load_model_and_tokenizer(config, logger=None):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    checkpoint = config.model.checkpoint

    if logger:
        logger.write(f"\n[ëª¨ë¸ ë¡œë”©]")
        logger.write(f"ì²´í¬í¬ì¸íŠ¸: {checkpoint}")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    # íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens = config.tokenizer.special_tokens
    num_added = tokenizer.add_tokens(special_tokens)

    if logger:
        logger.write(f"íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")

    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
    model.resize_token_embeddings(len(tokenizer))

    if logger:
        logger.write(f"ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(tokenizer)}")

    return model, tokenizer
```

### src/training/trainer.py

```python
# ìˆ˜ì • ì „
print("=" * 60)
print("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("=" * 60)

# ìˆ˜ì • í›„
if logger:
    logger.write("=" * 60)
    logger.write("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.write("=" * 60)
```

**ì ìš© ì˜ˆì‹œ:**
```python
def train(self, logger=None):
    """í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if logger:
        logger.write("=" * 60)
        logger.write("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.write("=" * 60)
        logger.write(f"ì—í¬í¬: {self.config.training.epochs}")
        logger.write(f"ë°°ì¹˜ í¬ê¸°: {self.config.training.batch_size}")
        logger.write(f"í•™ìŠµë¥ : {self.config.training.learning_rate}")

    # í•™ìŠµ ì‹¤í–‰
    results = self.trainer.train()

    if logger:
        logger.write("\ní•™ìŠµ ì™„ë£Œ!")
        logger.write(f"ìµœì¢… ì†ì‹¤: {results.training_loss:.4f}")

    return results
```

### src/inference/predictor.py

```python
# ìˆ˜ì • ì „
print("=" * 60)
print("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
print("=" * 60)

# ìˆ˜ì • í›„
if logger:
    logger.write("=" * 60)
    logger.write("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
    logger.write("=" * 60)
```

**ì ìš© ì˜ˆì‹œ:**
```python
def create_submission(self, test_df, output_path, batch_size=32, logger=None):
    """ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if logger:
        logger.write("=" * 60)
        logger.write("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
        logger.write("=" * 60)
        logger.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")
        logger.write(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")

    # ì¶”ë¡  ìˆ˜í–‰
    predictions = self.predict_batch(test_df['dialogue'].tolist(), batch_size)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission_df = pd.DataFrame({
        'id': test_df['id'],
        'summary': predictions
    })
    submission_df.to_csv(output_path, index=False)

    if logger:
        logger.write(f"\nì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")
        logger.write(f"ìƒì„±ëœ ìš”ì•½: {len(predictions)}ê°œ")

    return submission_df
```

---

## GPU ìœ í‹¸ë¦¬í‹° í†µí•©

### src/utils/gpu_optimization/team_gpu_check.py í™œìš©

```python
from src.utils.gpu_optimization.team_gpu_check import (
    check_gpu_tier,
    get_gpu_info,
    get_optimal_batch_size,
    get_memory_usage
)

# GPU ì •ë³´ ì¶œë ¥
gpu_info = get_gpu_info()
if logger:
    logger.write(f"GPU ì •ë³´: {gpu_info}")

# GPU tier í™•ì¸
gpu_tier = check_gpu_tier()
if logger:
    logger.write(f"GPU Tier: {gpu_tier}")

# ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
optimal_batch_size = get_optimal_batch_size("kobart", gpu_tier)
if logger:
    logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
```

**ì „ì²´ í†µí•© ì˜ˆì‹œ:**
```python
def main(args):
    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"train_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        logger.write("=" * 60)
        logger.write(f"í•™ìŠµ ì‹œì‘: {args.experiment}")
        logger.write("=" * 60)

        # GPU ì •ë³´ ì¶œë ¥
        logger.write("\n[GPU ì •ë³´]")
        gpu_info = get_gpu_info()
        for key, value in gpu_info.items():
            logger.write(f"  {key}: {value}")

        # GPU Tier í™•ì¸
        gpu_tier = check_gpu_tier()
        logger.write(f"\nGPU Tier: {gpu_tier}")

        # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
        optimal_batch = get_optimal_batch_size("kobart", gpu_tier)
        logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch}")

        # Config ë¡œë“œ
        logger.write("\n[1/6] Config ë¡œë”©...")
        config = load_config(args.experiment)
        logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")
        logger.write(f"  ëª¨ë¸: {config.model.checkpoint}")

        # ëª¨ë¸ ë¡œë“œ
        logger.write("\n[2/6] ëª¨ë¸ ë¡œë”©...")
        model, tokenizer = load_model_and_tokenizer(config, logger=logger)

        # ... ë‚˜ë¨¸ì§€ ì‘ì—…
```

---

## í†µí•© ì˜ˆì‹œ

### scripts/train.pyì— Logger ì™„ì „ í†µí•©

```python
import argparse
from pathlib import Path

from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.utils.gpu_optimization.team_gpu_check import (
    get_gpu_info, check_gpu_tier, get_optimal_batch_size
)
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", type=str, required=True,
                        help="ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: baseline_kobart)")
    args = parser.parse_args()

    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"train_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        try:
            logger.write("=" * 60)
            logger.write(f"í•™ìŠµ ì‹œì‘: {args.experiment}")
            logger.write("=" * 60)

            # GPU ì •ë³´ ì¶œë ¥
            logger.write("\n[GPU ì •ë³´]")
            gpu_info = get_gpu_info()
            for key, value in gpu_info.items():
                logger.write(f"  {key}: {value}")

            # GPU Tier í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
            gpu_tier = check_gpu_tier()
            logger.write(f"\nGPU Tier: {gpu_tier}")
            optimal_batch = get_optimal_batch_size("kobart", gpu_tier)
            logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch}")

            # Config ë¡œë“œ
            logger.write("\n[1/6] Config ë¡œë”©...")
            config = load_config(args.experiment)
            logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")
            logger.write(f"  ëª¨ë¸: {config.model.checkpoint}")
            logger.write(f"  ë°°ì¹˜ í¬ê¸°: {config.training.batch_size}")
            logger.write(f"  í•™ìŠµë¥ : {config.training.learning_rate}")
            logger.write(f"  ì—í¬í¬: {config.training.epochs}")

            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            logger.write("\n[2/6] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...")
            model, tokenizer = load_model_and_tokenizer(config, logger=logger)

            # ë°ì´í„° ë¡œë“œ
            logger.write("\n[3/6] ë°ì´í„° ë¡œë”©...")
            train_df = pd.read_csv(config.paths.train_data)
            logger.write(f"  í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")

            # ë°ì´í„°ì…‹ ìƒì„±
            logger.write("\n[4/6] ë°ì´í„°ì…‹ ìƒì„±...")
            train_dataset = DialogueSummarizationDataset(
                dialogues=train_df['dialogue'].tolist(),
                summaries=train_df['summary'].tolist(),
                tokenizer=tokenizer,
                logger=logger
            )

            # Trainer ìƒì„±
            logger.write("\n[5/6] Trainer ìƒì„±...")
            trainer = create_trainer(
                config=config,
                model=model,
                tokenizer=tokenizer,
                train_dataset=train_dataset,
                logger=logger
            )

            # í•™ìŠµ ì‹¤í–‰
            logger.write("\n[6/6] í•™ìŠµ ì‹¤í–‰...")
            results = trainer.train()

            logger.write("\n" + "=" * 60)
            logger.write("í•™ìŠµ ì™„ë£Œ!")
            logger.write("=" * 60)
            logger.write(f"ìµœì¢… ì†ì‹¤: {results.training_loss:.4f}")
            logger.write(f"ë¡œê·¸ íŒŒì¼: {log_path}")

        except Exception as e:
            logger.write(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}", print_error=True)
            raise

if __name__ == "__main__":
    main()
```

### scripts/inference.pyì— Logger ì™„ì „ í†µí•©

```python
import argparse
import pandas as pd
from pathlib import Path

from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.inference import create_predictor

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", type=str, required=True,
                        help="ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: baseline_kobart)")
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="submissions/submission.csv",
                        help="ì œì¶œ íŒŒì¼ ê²½ë¡œ")
    args = parser.parse_args()

    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"inference_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        try:
            logger.write("=" * 60)
            logger.write(f"ì¶”ë¡  ì‹œì‘: {args.experiment}")
            logger.write("=" * 60)

            # Config ë¡œë“œ
            logger.write("\n[1/4] Config ë¡œë”©...")
            config = load_config(args.experiment)
            logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")

            # ëª¨ë¸ ë¡œë“œ
            logger.write("\n[2/4] ëª¨ë¸ ë¡œë”©...")
            logger.write(f"  ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")
            model, tokenizer = load_model_and_tokenizer(
                config,
                checkpoint_path=args.checkpoint,
                logger=logger
            )

            # Predictor ìƒì„±
            logger.write("\n[3/4] Predictor ìƒì„±...")
            predictor = create_predictor(
                model=model,
                tokenizer=tokenizer,
                config=config,
                logger=logger
            )

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì¶”ë¡ 
            logger.write("\n[4/4] ì¶”ë¡  ì‹¤í–‰...")
            test_df = pd.read_csv(config.paths.test_data)
            logger.write(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")

            submission_df = predictor.create_submission(
                test_df=test_df,
                output_path=args.output,
                batch_size=config.inference.batch_size,
                logger=logger
            )

            logger.write("\n" + "=" * 60)
            logger.write("ì¶”ë¡  ì™„ë£Œ!")
            logger.write("=" * 60)
            logger.write(f"ì œì¶œ íŒŒì¼: {args.output}")
            logger.write(f"ìƒì„±ëœ ìš”ì•½: {len(submission_df)}ê°œ")
            logger.write(f"ë¡œê·¸ íŒŒì¼: {log_path}")

        except Exception as e:
            logger.write(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}", print_error=True)
            raise

if __name__ == "__main__":
    main()
```

---

## Logger í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•µì‹¬ ëª¨ë“ˆ
- [ ] `src/models/model_loader.py` - Logger ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ ë° print ëŒ€ì²´
- [ ] `src/training/trainer.py` - Logger í†µí•©
- [ ] `src/inference/predictor.py` - Logger í†µí•©
- [ ] `src/data/preprocessor.py` - Logger í†µí•© (ì„ íƒ)
- [ ] `src/data/dataset.py` - Logger í†µí•© (ì„ íƒ)

### ìŠ¤í¬ë¦½íŠ¸
- [ ] `scripts/train.py` - Logger ì™„ì „ í†µí•©
- [ ] `scripts/inference.py` - Logger ì™„ì „ í†µí•©
- [ ] `scripts/full_pipeline.py` - Logger í†µí•© (ìˆëŠ” ê²½ìš°)

### ìœ í‹¸ë¦¬í‹°
- [ ] GPU ì •ë³´ ì¶œë ¥ ì‹œ Logger ì‚¬ìš©
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ì— `print_error=True` ì ìš©
- [ ] ëª¨ë“  print ë¬¸ì„ logger.writeë¡œ ëŒ€ì²´

---

## ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/logging/logger.py` - Logger í´ë˜ìŠ¤
- `src/utils/core/common.py` - create_log_path ìœ í‹¸ë¦¬í‹°
- `src/utils/gpu_optimization/team_gpu_check.py` - GPU ìœ í‹¸ë¦¬í‹°

**ì„¤ì • íŒŒì¼:**
- `configs/base/default.yaml` - ê¸°ë³¸ ì„¤ì •
- `configs/base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì… ì„¤ì •
- `configs/models/kobart.yaml` - KoBART ì„¤ì •
- `configs/experiments/baseline_kobart.yaml` - ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜

**ë¬¸ì„œ:**
- `docs/ëª¨ë“ˆí™”/00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md` - ì‹œìŠ¤í…œ ì „ì²´ ê°œìš”
- `docs/ëª¨ë“ˆí™”/01_Config_ì‹œìŠ¤í…œ.md` - Config ìƒì„¸ ê°€ì´ë“œ
