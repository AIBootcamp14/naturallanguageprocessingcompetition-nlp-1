# 02. í•µì‹¬ ì‹œìŠ¤í…œ (ì „ì²´ ì•„í‚¤í…ì²˜, Config, Logger)

## ëª©ì°¨
- [Part 1: ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#part-1-ì „ì²´-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
  - [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
  - [ëª¨ë“ˆ êµ¬ì¡°](#ëª¨ë“ˆ-êµ¬ì¡°)
  - [ë°ì´í„° í”Œë¡œìš°](#ë°ì´í„°-í”Œë¡œìš°)
  - [ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •](#ì„¤ì¹˜-ë°-í™˜ê²½-ì„¤ì •)
  - [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
  - [Config ì‹œìŠ¤í…œ ê°œìš”](#config-ì‹œìŠ¤í…œ-ê°œìš”)
  - [í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½](#í…ŒìŠ¤íŠ¸-ê²°ê³¼-ìš”ì•½)
- [Part 2: Config ì‹œìŠ¤í…œ](#part-2-config-ì‹œìŠ¤í…œ)
  - [ê°œìš”](#ê°œìš”)
  - [Config êµ¬ì¡°](#config-êµ¬ì¡°)
  - [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
  - [Config íŒŒì¼ ì‘ì„±](#config-íŒŒì¼-ì‘ì„±)
  - [í…ŒìŠ¤íŠ¸ ê²°ê³¼](#í…ŒìŠ¤íŠ¸-ê²°ê³¼)
  - [ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ](#ì‹¤ì „-ì‚¬ìš©-ì˜ˆì‹œ)
  - [ì£¼ì˜ì‚¬í•­](#ì£¼ì˜ì‚¬í•­)
- [Part 3: Logger ì‹œìŠ¤í…œ](#part-3-logger-ì‹œìŠ¤í…œ)
  - [Logger ê°œìš”](#logger-ê°œìš”)
  - [Logger ì‚¬ìš©ë²•](#logger-ì‚¬ìš©ë²•)
  - [ëª¨ë“ˆë³„ Logger í†µí•©](#ëª¨ë“ˆë³„-logger-í†µí•©)
  - [GPU ìœ í‹¸ë¦¬í‹° í†µí•©](#gpu-ìœ í‹¸ë¦¬í‹°-í†µí•©)
  - [í†µí•© ì˜ˆì‹œ](#í†µí•©-ì˜ˆì‹œ)

---

# Part 1: ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "ì…ë ¥ ê³„ì¸µ"
        A[Config YAML íŒŒì¼ë“¤] --> B[ConfigLoader]
        C[Raw Data CSV] --> D[DialoguePreprocessor]
    end

    subgraph "ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"
        B --> E[í†µí•© Config]
        D --> F[ì „ì²˜ë¦¬ëœ ë°ì´í„°]
        F --> G[PyTorch Dataset]
    end

    subgraph "ëª¨ë¸ ê³„ì¸µ"
        E --> H[ModelLoader]
        H --> I[ì‚¬ì „í•™ìŠµ ëª¨ë¸]
        H --> J[Tokenizer]
    end

    subgraph "í•™ìŠµ ê³„ì¸µ"
        G --> K[Trainer]
        I --> K
        J --> K
        E --> K
        K --> L[í•™ìŠµëœ ëª¨ë¸]
        K --> M[WandB ë¡œê·¸]
    end

    subgraph "í‰ê°€ ê³„ì¸µ"
        K --> N[RougeCalculator]
        N --> O[ROUGE ì ìˆ˜]
    end

    subgraph "ì¶”ë¡  ê³„ì¸µ"
        L --> P[Predictor]
        J --> P
        P --> Q[ì˜ˆì¸¡ ê²°ê³¼]
        Q --> R[ì œì¶œ íŒŒì¼ CSV]
    end

    style A fill:#e1f5ff,color:#000
    style B fill:#e1f5ff,color:#000
    style C fill:#e1f5ff,color:#000
    style D fill:#e1f5ff,color:#000
    style E fill:#fff3e0,color:#000
    style F fill:#fff3e0,color:#000
    style G fill:#fff3e0,color:#000
    style H fill:#c8e6c9,color:#000
    style I fill:#c8e6c9,color:#000
    style J fill:#c8e6c9,color:#000
    style K fill:#f3e5f5,color:#000
    style L fill:#f3e5f5,color:#000
    style M fill:#fff9c4,color:#000
    style N fill:#ffccbc,color:#000
    style O fill:#ffccbc,color:#000
    style P fill:#b39ddb,color:#000
    style Q fill:#b39ddb,color:#000
    style R fill:#b39ddb,color:#000
```

---

## ëª¨ë“ˆ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
src/
â”œâ”€â”€ config/              # Config ê´€ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loader.py       # ê³„ì¸µì  Config ë³‘í•©
â”‚
â”œâ”€â”€ data/               # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessor.py # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
â”‚   â””â”€â”€ dataset.py      # PyTorch Dataset í´ë˜ìŠ¤
â”‚
â”œâ”€â”€ models/             # ëª¨ë¸ ë¡œë”©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_loader.py # HuggingFace ëª¨ë¸ ë¡œë”
â”‚
â”œâ”€â”€ evaluation/         # í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ metrics.py      # ROUGE ê³„ì‚°
â”‚
â”œâ”€â”€ training/           # í•™ìŠµ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py      # Seq2SeqTrainer ë˜í¼
â”‚
â””â”€â”€ inference/          # ì¶”ë¡  ì‹œìŠ¤í…œ
    â”œâ”€â”€ __init__.py
    â””â”€â”€ predictor.py    # ë°°ì¹˜ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
```

### ëª¨ë“ˆë³„ ì—­í• 

| ëª¨ë“ˆ | ì£¼ìš” í´ë˜ìŠ¤ | í•µì‹¬ ê¸°ëŠ¥ |
|------|------------|----------|
| **config** | `ConfigLoader` | YAML ê¸°ë°˜ ê³„ì¸µì  ì„¤ì • ê´€ë¦¬ |
| **data** | `DialoguePreprocessor`<br>`DialogueSummarizationDataset`<br>`InferenceDataset` | ë°ì´í„° ì „ì²˜ë¦¬<br>í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹<br>ì¶”ë¡  ë°ì´í„°ì…‹ |
| **models** | `ModelLoader` | ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”©<br>íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ |
| **evaluation** | `RougeCalculator` | ROUGE-1/2/L ê³„ì‚°<br>Multi-reference ì§€ì› |
| **training** | `ModelTrainer` | í•™ìŠµ íŒŒì´í”„ë¼ì¸<br>WandB ë¡œê¹… í†µí•© |
| **inference** | `Predictor` | ë°°ì¹˜ ì¶”ë¡ <br>ì œì¶œ íŒŒì¼ ìƒì„± |

---

## ë°ì´í„° í”Œë¡œìš°

### í•™ìŠµ íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Config
    participant Data
    participant Model
    participant Trainer
    participant WandB

    User->>Config: load_config("baseline_kobart")
    Config-->>User: í†µí•© Config ê°ì²´

    User->>Model: load_model_and_tokenizer(config)
    Model-->>User: model, tokenizer

    User->>Data: DialogueSummarizationDataset(...)
    Data-->>User: train_dataset, eval_dataset

    User->>Trainer: ModelTrainer(config, model, ...)
    Trainer->>WandB: init_run()

    User->>Trainer: trainer.train()

    loop ê° ì—í¬í¬
        Trainer->>Model: í•™ìŠµ ìŠ¤í…
        Trainer->>Data: ê²€ì¦ ë°ì´í„° ë¡œë“œ
        Trainer->>Model: í‰ê°€ ìŠ¤í…
        Trainer->>Trainer: ROUGE ê³„ì‚°
        Trainer->>WandB: log_metrics()
        Trainer->>Trainer: ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    end

    Trainer-->>User: í•™ìŠµ ì™„ë£Œ (ìµœì¢… ëª¨ë¸)
```

### ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Model
    participant Data
    participant Predictor
    participant File

    User->>Model: í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    Model-->>User: model, tokenizer

    User->>Predictor: Predictor(model, tokenizer, config)

    User->>Data: test.csv ì½ê¸°
    Data-->>User: test_df

    User->>Predictor: create_submission(test_df, output_path)

    loop ê° ë°°ì¹˜
        Predictor->>Data: ë°°ì¹˜ ë°ì´í„° ë¡œë“œ
        Predictor->>Model: generate()
        Model-->>Predictor: ìƒì„±ëœ ìš”ì•½
    end

    Predictor->>File: submission.csv ì €ì¥
    Predictor-->>User: submission_df
```

---

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ í™œì„±í™”

```bash
# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# ë˜ëŠ” pyenv ëª…ë ¹ì–´ ì‚¬ìš©
pyenv activate nlp_py3_11_9
```

### 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

**ì£¼ìš” íŒ¨í‚¤ì§€:**
- `torch==2.8.0` - PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `transformers==4.57.0` - HuggingFace Transformers
- `pandas==2.3.3` - ë°ì´í„° ì²˜ë¦¬
- `omegaconf==2.3.0` - Config ê´€ë¦¬
- `rouge-score==0.1.2` - ROUGE í‰ê°€
- `wandb==0.22.2` - ì‹¤í—˜ ë¡œê¹…

### 3. í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
tree -L 2 src/
tree -L 2 configs/
tree -L 2 tests/
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. Config ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰

```python
# ==================== Config ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰ ì˜ˆì œ ==================== #

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
import pandas as pd

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer

# ---------------------- Config ë¡œë“œ ---------------------- #
# ì‹¤í—˜ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í†µí•© Config ê°ì²´ ìƒì„±
config = load_config("baseline_kobart")

# ---------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ Config ì„¤ì •ì— ë”°ë¼ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- ë°ì´í„° ë¡œë“œ ---------------------- #
# í•™ìŠµ ë°ì´í„° CSV íŒŒì¼ ì½ê¸°
train_df = pd.read_csv("data/raw/train.csv")

# PyTorch Dataset ê°ì²´ ìƒì„±
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),  # ëŒ€í™” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    summaries=train_df['summary'].tolist(),   # ìš”ì•½ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    tokenizer=tokenizer                       # í† í¬ë‚˜ì´ì € ê°ì²´
)

# ---------------------- Trainer ìƒì„± ë° í•™ìŠµ ---------------------- #
# HuggingFace Trainer ë˜í¼ ìƒì„±
trainer = create_trainer(
    config=config,              # í†µí•© Config ê°ì²´
    model=model,                # ëª¨ë¸ ê°ì²´
    tokenizer=tokenizer,        # í† í¬ë‚˜ì´ì € ê°ì²´
    train_dataset=train_dataset # í•™ìŠµ ë°ì´í„°ì…‹
)

# í•™ìŠµ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
results = trainer.train()
```

### 2. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±

```python
# ==================== ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ì˜ˆì œ ==================== #

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
import pandas as pd

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.models import load_model_and_tokenizer
from src.inference import create_predictor

# ---------------------- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ---------------------- #
# Config ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# ë˜ëŠ” ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì§ì ‘ ë¡œë“œ (ëŒ€ì•ˆ)
# model = AutoModelForSeq2SeqLM.from_pretrained("outputs/best_model")

# ---------------------- Predictor ìƒì„± ---------------------- #
# ë°°ì¹˜ ì¶”ë¡ ì„ ìœ„í•œ Predictor ê°ì²´ ìƒì„±
predictor = create_predictor(
    model=model,        # í•™ìŠµëœ ëª¨ë¸ ê°ì²´
    tokenizer=tokenizer,# í† í¬ë‚˜ì´ì € ê°ì²´
    config=config       # ì¶”ë¡  ì„¤ì •ì´ í¬í•¨ëœ Config ê°ì²´
)

# ---------------------- í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ---------------------- #
# í…ŒìŠ¤íŠ¸ ë°ì´í„° CSV íŒŒì¼ ì½ê¸°
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- ì œì¶œ íŒŒì¼ ìƒì„± ---------------------- #
# ë°°ì¹˜ ì¶”ë¡  ìˆ˜í–‰ ë° ì œì¶œ íŒŒì¼ ì €ì¥
submission_df = predictor.create_submission(
    test_df=test_df,                          # í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
    output_path="submissions/submission.csv", # ì œì¶œ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    batch_size=32                             # ë°°ì¹˜ í¬ê¸°
)
```

### 3. ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ ê° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

python tests/test_config_loader.py
python tests/test_preprocessor.py
python tests/test_model_loader.py
python tests/test_metrics.py
python tests/test_trainer.py
python tests/test_predictor.py
```

---

## Config ì‹œìŠ¤í…œ ê°œìš”

### Config ê³„ì¸µ êµ¬ì¡°

```mermaid
graph LR
    A[base/default.yaml] --> F[ìµœì¢… Config]
    B[base/encoder_decoder.yaml] --> F
    C[models/kobart.yaml] --> F
    D[experiments/baseline_kobart.yaml] --> F

    F --> G[í•™ìŠµ ì‹¤í–‰]

    style A fill:#e3f2fd,color:#000
    style B fill:#e3f2fd,color:#000
    style C fill:#fff3e0,color:#000
    style D fill:#f3e5f5,color:#000
    style F fill:#c8e6c9,color:#000
```

### Config ë³‘í•© ìš°ì„ ìˆœìœ„

1. `base/default.yaml` - ê¸°ë³¸ ì„¤ì •
2. `base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì…ë³„ ì„¤ì •
3. `models/{model_name}.yaml` - íŠ¹ì • ëª¨ë¸ ì„¤ì •
4. `experiments/{experiment_name}.yaml` - ì‹¤í—˜ë³„ ì„¤ì • (ìµœìš°ì„ )

**ì˜ˆì‹œ:**
```yaml
# base/default.yaml
training:
  batch_size: 8
  learning_rate: 5e-5

# experiments/baseline_kobart.yaml
training:
  batch_size: 50        # ì˜¤ë²„ë¼ì´ë“œë¨
  learning_rate: 1e-5   # ì˜¤ë²„ë¼ì´ë“œë¨
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½

### ì „ì²´ í…ŒìŠ¤íŠ¸ í˜„í™©

| ëª¨ë“ˆ | í…ŒìŠ¤íŠ¸ íŒŒì¼ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|------|------------|-----------|------|
| Config | `test_config_loader.py` | 6ê°œ | âœ… í†µê³¼ |
| Data | `test_preprocessor.py` | 5ê°œ | âœ… í†µê³¼ |
| Models | `test_model_loader.py` | 5ê°œ | âœ… í†µê³¼ |
| Evaluation | `test_metrics.py` | 6ê°œ | âœ… í†µê³¼ |
| Training | `test_trainer.py` | 5ê°œ | âœ… í†µê³¼ |
| Inference | `test_predictor.py` | 6ê°œ | âœ… í†µê³¼ |
| **ì „ì²´** | **6ê°œ íŒŒì¼** | **33ê°œ** | **âœ… ëª¨ë‘ í†µê³¼** |

### ì£¼ìš” ê²€ì¦ í•­ëª©

âœ… **Config Loader**
- ê³„ì¸µì  YAML ë³‘í•© ì •ìƒ ë™ì‘
- ì‹¤í—˜ë³„ Config ì˜¤ë²„ë¼ì´ë“œ ì •ìƒ ì‘ë™

âœ… **Data Preprocessing**
- 12,457ê°œ ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ
- ë…¸ì´ì¦ˆ ì œê±°, í™”ì ì¶”ì¶œ, í„´ ê³„ì‚° ì •ìƒ ë™ì‘

âœ… **Model Loader**
- KoBART ëª¨ë¸ (123M íŒŒë¼ë¯¸í„°) ë¡œë”© ì„±ê³µ
- GPU ìë™ ê°ì§€ ë° ë°°ì¹˜ ì •ìƒ ë™ì‘

âœ… **Metrics**
- ROUGE-1/2/L ê³„ì‚° ì •ìƒ ë™ì‘
- Multi-reference í‰ê°€ ì§€ì› í™•ì¸

âœ… **Trainer**
- Seq2SeqTrainer ë˜í•‘ ì •ìƒ ë™ì‘
- WandB ë¡œê¹… í†µí•© í™•ì¸

âœ… **Inference**
- ë°°ì¹˜ ì¶”ë¡  ì •ìƒ ë™ì‘
- ì œì¶œ íŒŒì¼ ìƒì„± ì •ìƒ ë™ì‘

---

# Part 2: Config ì‹œìŠ¤í…œ

## ê°œìš”

### ëª©ì 
- ì‹¤í—˜ ì„¤ì •ì˜ ì²´ê³„ì  ê´€ë¦¬
- ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í™˜ê²½
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²„ì „ ê´€ë¦¬
- ë‹¤ì–‘í•œ ì‹¤í—˜ ì„¤ì •ì˜ ë¹ ë¥¸ ì „í™˜

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ê³„ì¸µì  YAML ë³‘í•©
- âœ… ì‹¤í—˜ë³„ Config ì˜¤ë²„ë¼ì´ë“œ
- âœ… OmegaConf ê¸°ë°˜ íƒ€ì… ì•ˆì „ì„±
- âœ… ëˆ„ë½ëœ íŒŒì¼ ìë™ ì²˜ë¦¬

---

## Config êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
configs/
â”œâ”€â”€ base/                           # ê¸°ë³¸ ì„¤ì •
â”‚   â”œâ”€â”€ default.yaml               # ì „ì²´ ê¸°ë³¸ê°’
â”‚   â””â”€â”€ encoder_decoder.yaml       # ëª¨ë¸ íƒ€ì…ë³„ ê¸°ë³¸ê°’
â”‚
â”œâ”€â”€ models/                         # ëª¨ë¸ë³„ ì„¤ì •
â”‚   â”œâ”€â”€ kobart.yaml                # KoBART ì„¤ì •
â”‚   â”œâ”€â”€ t5.yaml                    # T5 ì„¤ì • (ì˜ˆì‹œ)
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ experiments/                    # ì‹¤í—˜ë³„ ì„¤ì •
    â”œâ”€â”€ baseline_kobart.yaml       # ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜
    â”œâ”€â”€ finetuned_kobart.yaml      # íŒŒì¸íŠœë‹ ì‹¤í—˜ (ì˜ˆì‹œ)
    â””â”€â”€ ...
```

### Config ë³‘í•© í”Œë¡œìš°

```mermaid
graph TD
    A[ì‹¤í—˜ ì‹œì‘] --> B{ì‹¤í—˜ ì´ë¦„ ì§€ì •}
    B --> C[1. base/default.yaml ë¡œë“œ]
    C --> D[2. base/encoder_decoder.yaml ë¡œë“œ]
    D --> E[3. models/kobart.yaml ë¡œë“œ]
    E --> F[4. experiments/baseline_kobart.yaml ë¡œë“œ]

    F --> G[OmegaConfë¡œ ë³‘í•©]
    G --> H[ìµœì¢… í†µí•© Config]

    H --> I[í•™ìŠµ/ì¶”ë¡  ì‹œìŠ¤í…œìœ¼ë¡œ ì „ë‹¬]

    style C fill:#e3f2fd,color:#000
    style D fill:#e3f2fd,color:#000
    style E fill:#fff3e0,color:#000
    style F fill:#f3e5f5,color:#000
    style H fill:#c8e6c9,color:#000
```

### ë³‘í•© ìš°ì„ ìˆœìœ„

**ë‚®ìŒ â†’ ë†’ìŒ ìˆœì„œ:**
1. `base/default.yaml` - ì „ì²´ ê¸°ë³¸ ì„¤ì •
2. `base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì… ì„¤ì •
3. `models/{model_name}.yaml` - íŠ¹ì • ëª¨ë¸ ì„¤ì •
4. `experiments/{experiment_name}.yaml` - **ì‹¤í—˜ ì„¤ì • (ìµœìš°ì„ )**

**ì˜ˆì‹œ:**
```yaml
# base/default.yaml
training:
  batch_size: 8
  learning_rate: 5e-5
  epochs: 10

# experiments/baseline_kobart.yaml
training:
  batch_size: 50        # ì˜¤ë²„ë¼ì´ë“œ
  learning_rate: 1e-5   # ì˜¤ë²„ë¼ì´ë“œ
  # epochs: 10ì€ default ê°’ ì‚¬ìš©
```

**ë³‘í•© ê²°ê³¼:**
```yaml
training:
  batch_size: 50        # experimentsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
  learning_rate: 1e-5   # experimentsì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
  epochs: 10            # default ê°’ ìœ ì§€
```

---

## ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
# ==================== Config ê¸°ë³¸ ì‚¬ìš©ë²• ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import load_config

# ---------------------- Config ë¡œë“œ ---------------------- #
# ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ Config ë¡œë“œ (ê³„ì¸µì  ë³‘í•© ìë™ ìˆ˜í–‰)
config = load_config("baseline_kobart")

# ---------------------- Config ê°’ ì ‘ê·¼ ---------------------- #
# í•™ìŠµ ë°°ì¹˜ í¬ê¸° ì¶œë ¥
print(config.training.batch_size)     # 50

# ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¶œë ¥
print(config.model.checkpoint)        # "digit82/kobart-summarization"

# ì‹¤í—˜ëª… ì¶œë ¥
print(config.experiment.name)         # "baseline_kobart"
```

### 2. Config ê°’ í™•ì¸

```python
# ==================== Config ê°’ í™•ì¸ ==================== #

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
from omegaconf import OmegaConf

# ---------------------- Config ì „ì²´ ì¶œë ¥ ---------------------- #
# ëª¨ë“  Config ì„¤ì •ì„ YAML í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
print(OmegaConf.to_yaml(config))

# ---------------------- íŠ¹ì • ì„¹ì…˜ë§Œ ì¶œë ¥ ---------------------- #
# í•™ìŠµ ê´€ë ¨ ì„¤ì •ë§Œ ì¶œë ¥
print(OmegaConf.to_yaml(config.training))

# ëª¨ë¸ ê´€ë ¨ ì„¤ì •ë§Œ ì¶œë ¥
print(OmegaConf.to_yaml(config.model))
```

### 3. Config ê°’ ìˆ˜ì • (ëŸ°íƒ€ì„)

```python
# ==================== Config ê°’ ëŸ°íƒ€ì„ ìˆ˜ì • ==================== #

# ---------------------- ê¸°ì¡´ ê°’ ë³€ê²½ ---------------------- #
# ë°°ì¹˜ í¬ê¸° ë³€ê²½
config.training.batch_size = 32

# í•™ìŠµë¥  ë³€ê²½
config.training.learning_rate = 2e-5

# ---------------------- ìƒˆë¡œìš´ í‚¤ ì¶”ê°€ ---------------------- #
# ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„° ë™ì  ì¶”ê°€
config.custom_param = "value"
```

### 4. ConfigLoader ì§ì ‘ ì‚¬ìš©

```python
# ==================== ConfigLoader ì§ì ‘ ì‚¬ìš© ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import ConfigLoader

# ---------------------- ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---------------------- #
# Config ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ Loader ìƒì„±
loader = ConfigLoader(config_dir="configs")

# ---------------------- Config ìˆ˜ë™ ë³‘í•© ---------------------- #
# ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ ê³„ì¸µì  Config ë³‘í•© ìˆ˜í–‰
config = loader.merge_configs("baseline_kobart")

# ---------------------- íŠ¹ì • íŒŒì¼ë§Œ ë¡œë“œ ---------------------- #
# ê¸°ë³¸ Config íŒŒì¼ë§Œ ë¡œë“œ
base_config = loader.load_base_config()

# ëª¨ë¸ë³„ Config íŒŒì¼ë§Œ ë¡œë“œ
model_config = loader.load_model_config("kobart")
```

---

## Config íŒŒì¼ ì‘ì„±

### base/default.yaml êµ¬ì¡°

```yaml
# ==================== ê¸°ë³¸ ì„¤ì • ==================== #

# ì‹¤í—˜ ì •ë³´
experiment:
  name: "default"
  seed: 42
  deterministic: true

# ê²½ë¡œ ì„¤ì •
paths:
  train_data: "data/raw/train.csv"
  dev_data: "data/raw/dev.csv"
  test_data: "data/raw/test.csv"
  output_dir: "outputs"

# í•™ìŠµ ì„¤ì •
training:
  output_dir: "outputs"
  epochs: 10
  batch_size: 8
  learning_rate: 5e-5
  device: "cuda"

# í‰ê°€ ì„¤ì •
evaluation:
  metric: "rouge"
  rouge_types:
    - "rouge1"
    - "rouge2"
    - "rougeL"
```

### base/encoder_decoder.yaml êµ¬ì¡°

```yaml
# ==================== Encoder-Decoder ê³µí†µ ì„¤ì • ==================== #

model:
  type: "encoder_decoder"
  architecture: "bart"

# í† í¬ë‚˜ì´ì € ì„¤ì •
tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    # ... ì¶”ê°€ íŠ¹ìˆ˜ í† í°

# ì¶”ë¡  ì„¤ì •
inference:
  batch_size: 32
  num_beams: 4
  early_stopping: true
  generate_max_length: 100
  no_repeat_ngram_size: 2
```

### models/kobart.yaml êµ¬ì¡°

```yaml
# ==================== KoBART ëª¨ë¸ ì„¤ì • ==================== #

model:
  name: "kobart"
  checkpoint: "digit82/kobart-summarization"

# KoBART íŠ¹í™” ì„¤ì •
# (í•„ìš”ì‹œ ì¶”ê°€)
```

### experiments/baseline_kobart.yaml êµ¬ì¡°

```yaml
# ==================== ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜ ì„¤ì • ==================== #

experiment:
  name: "baseline_kobart"
  description: "ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸ ì¬í˜„ ì‹¤í—˜"
  tags:
    - "baseline"
    - "kobart"

# WandB ì„¤ì •
wandb:
  enabled: true
  project: "nlp-competition"
  entity: "ieyeppo"

# ëª¨ë¸ ì„¤ì •
model:
  name: "kobart"

# í•™ìŠµ ì„¤ì • (ë² ì´ìŠ¤ë¼ì¸ ì˜¤ë²„ë¼ì´ë“œ)
training:
  epochs: 20
  batch_size: 50
  learning_rate: 1e-5

# ê²½ë¡œ ì„¤ì •
paths:
  output_dir: "outputs/baseline_kobart"
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼

### í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python tests/test_config_loader.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 6ê°œ)

#### 1. âœ… ê¸°ë³¸ Config ë¡œë“œ
```python
# ==================== ê¸°ë³¸ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ==================== #

def test_load_base_config():
    """base/default.yaml ë¡œë“œ í™•ì¸"""
    # ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    loader = ConfigLoader()

    # ê¸°ë³¸ Config íŒŒì¼ ë¡œë“œ
    config = loader.load_base_config()

    # ê¸°ë³¸ ì„¤ì • ê²€ì¦
    assert config.experiment.seed == 42
    assert config.paths.train_data == "data/raw/train.csv"
```

**ê²°ê³¼:**
```
âœ… ê¸°ë³¸ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  experiment.seed: 42
  paths.train_data: data/raw/train.csv
```

#### 2. âœ… ëª¨ë¸ íƒ€ì… Config ë¡œë“œ
```python
# ==================== ëª¨ë¸ íƒ€ì… Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ==================== #

def test_load_model_type_config():
    """base/encoder_decoder.yaml ë¡œë“œ í™•ì¸"""
    # ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    loader = ConfigLoader()

    # ëª¨ë¸ íƒ€ì…ë³„ Config íŒŒì¼ ë¡œë“œ
    config = loader.load_model_type_config("encoder_decoder")

    # ëª¨ë¸ íƒ€ì… ì„¤ì • ê²€ì¦
    assert config.model.type == "encoder_decoder"
    assert config.tokenizer.encoder_max_len == 512
```

**ê²°ê³¼:**
```
âœ… ëª¨ë¸ íƒ€ì… Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  model.type: encoder_decoder
  tokenizer.encoder_max_len: 512
```

#### 3. âœ… ëª¨ë¸ë³„ Config ë¡œë“œ
```python
# ==================== ëª¨ë¸ë³„ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ==================== #

def test_load_model_config():
    """models/kobart.yaml ë¡œë“œ í™•ì¸"""
    # ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    loader = ConfigLoader()

    # íŠ¹ì • ëª¨ë¸ì˜ Config íŒŒì¼ ë¡œë“œ
    config = loader.load_model_config("kobart")

    # ëª¨ë¸ ì„¤ì • ê²€ì¦
    assert config.model.name == "kobart"
    assert "digit82" in config.model.checkpoint
```

**ê²°ê³¼:**
```
âœ… ëª¨ë¸ë³„ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  model.name: kobart
  model.checkpoint: digit82/kobart-summarization
```

#### 4. âœ… ì‹¤í—˜ Config ë¡œë“œ
```python
# ==================== ì‹¤í—˜ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ==================== #

def test_load_experiment_config():
    """experiments/baseline_kobart.yaml ë¡œë“œ í™•ì¸"""
    # ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    loader = ConfigLoader()

    # ì‹¤í—˜ë³„ Config íŒŒì¼ ë¡œë“œ
    config = loader.load_experiment_config("baseline_kobart")

    # ì‹¤í—˜ ì„¤ì • ê²€ì¦
    assert config.experiment.name == "baseline_kobart"
    assert config.wandb.enabled == True
```

**ê²°ê³¼:**
```
âœ… ì‹¤í—˜ Config ë¡œë“œ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  experiment.name: baseline_kobart
  wandb.enabled: True
```

#### 5. âœ… ê³„ì¸µì  Config ë³‘í•©
```python
# ==================== ê³„ì¸µì  Config ë³‘í•© í…ŒìŠ¤íŠ¸ ==================== #

def test_merge_configs():
    """ì „ì²´ Config ë³‘í•© í™•ì¸"""
    # ConfigLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    loader = ConfigLoader()

    # ëª¨ë“  ê³„ì¸µì˜ Config íŒŒì¼ì„ ë³‘í•©
    config = loader.merge_configs("baseline_kobart")

    # ë³‘í•© ìš°ì„ ìˆœìœ„ í™•ì¸
    assert config.training.batch_size == 50      # experimentì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    assert config.training.epochs == 20          # experimentì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    assert config.tokenizer.encoder_max_len == 512  # base ê°’ ìœ ì§€
```

**ê²°ê³¼:**
```
âœ… ê³„ì¸µì  Config ë³‘í•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  training.batch_size: 50 (ì˜¤ë²„ë¼ì´ë“œë¨)
  training.epochs: 20 (ì˜¤ë²„ë¼ì´ë“œë¨)
  tokenizer.encoder_max_len: 512 (base ê°’ ìœ ì§€)
```

#### 6. âœ… í¸ì˜ í•¨ìˆ˜
```python
# ==================== load_config í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ==================== #

def test_load_config_function():
    """load_config() í¸ì˜ í•¨ìˆ˜ í™•ì¸"""
    # í¸ì˜ í•¨ìˆ˜ë¡œ Config ë¡œë“œ (ê³„ì¸µì  ë³‘í•© ìë™ ìˆ˜í–‰)
    config = load_config("baseline_kobart")

    # Config ë¡œë“œ ê²€ì¦
    assert config.experiment.name == "baseline_kobart"
    assert config.model.checkpoint is not None
```

**ê²°ê³¼:**
```
âœ… í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!
  Config ë¡œë“œ ì™„ë£Œ
  ëª¨ë“  ì„¹ì…˜ ì ‘ê·¼ ê°€ëŠ¥
```

### ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½

```
============================================================
Config Loader í…ŒìŠ¤íŠ¸ ì‹œì‘
============================================================

í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ Config ë¡œë“œ                    âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 2: ëª¨ë¸ íƒ€ì… Config ë¡œë“œ                âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 3: ëª¨ë¸ë³„ Config ë¡œë“œ                   âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 4: ì‹¤í—˜ Config ë¡œë“œ                     âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 5: ê³„ì¸µì  Config ë³‘í•©                   âœ… í†µê³¼
í…ŒìŠ¤íŠ¸ 6: í¸ì˜ í•¨ìˆ˜                           âœ… í†µê³¼

============================================================
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! (6/6)
============================================================
```

---

## ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ìƒˆë¡œìš´ ì‹¤í—˜ ì„¤ì • ì¶”ê°€

```yaml
# configs/experiments/finetuned_kobart.yaml
experiment:
  name: "finetuned_kobart_v2"
  description: "íŒŒì¸íŠœë‹ ì‹¤í—˜ ë²„ì „ 2"
  tags:
    - "finetuned"
    - "v2"

training:
  epochs: 30                    # ë” ê¸´ í•™ìŠµ
  batch_size: 32                # ì‘ì€ ë°°ì¹˜
  learning_rate: 5e-6           # ë‚®ì€ í•™ìŠµë¥ 
  warmup_steps: 1000            # ë” ê¸´ warmup

paths:
  output_dir: "outputs/finetuned_v2"
```

**ì‚¬ìš©:**
```python
# ==================== ìƒˆë¡œìš´ ì‹¤í—˜ ì„¤ì • ì‚¬ìš© ==================== #

# ìƒˆë¡œ ìƒì„±í•œ ì‹¤í—˜ ì„¤ì • ë¡œë“œ
config = load_config("finetuned_kobart_v2")
```

### ì˜ˆì‹œ 2: ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •

```yaml
# configs/experiments/debug.yaml
experiment:
  name: "debug"

debug:
  use_subset: true
  subset_size: 100              # 100ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©

training:
  epochs: 2                     # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
  batch_size: 4

wandb:
  enabled: false                # WandB ë¹„í™œì„±í™”
```

### ì˜ˆì‹œ 3: ëŸ°íƒ€ì„ì—ì„œ Config ìˆ˜ì •

```python
# ==================== ëŸ°íƒ€ì„ Config ìˆ˜ì • ì˜ˆì œ ==================== #

# ---------------------- Config ë¡œë“œ ---------------------- #
# ê¸°ë³¸ ì‹¤í—˜ ì„¤ì • ë¡œë“œ
config = load_config("baseline_kobart")

# ---------------------- Config ë™ì  ìˆ˜ì • ---------------------- #
# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
config.training.batch_size = 16

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì—í¬í¬ ì¤„ì´ê¸°
config.training.epochs = 5

# WandB ë¹„í™œì„±í™”
config.wandb.enabled = False

# ---------------------- ìˆ˜ì •ëœ Configë¡œ í•™ìŠµ ì§„í–‰ ---------------------- #
# ë™ì ìœ¼ë¡œ ìˆ˜ì •ëœ Configë¡œ Trainer ìƒì„±
trainer = create_trainer(config, ...)
```

---

## ì£¼ì˜ì‚¬í•­

### 1. YAML ë¬¸ë²•
- ë“¤ì—¬ì“°ê¸°ëŠ” **ê³µë°± 2ì¹¸** ì‚¬ìš© (íƒ­ ì‚¬ìš© ê¸ˆì§€)
- ë¬¸ìì—´ì— íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ì‹œ ë”°ì˜´í‘œ ì‚¬ìš©
- ë¦¬ìŠ¤íŠ¸ëŠ” `-` ì‚¬ìš©

### 2. íŒŒì¼ëª… ê·œì¹™
- `experiments/` í´ë”ì˜ YAML íŒŒì¼ëª… = ì‹¤í—˜ ì´ë¦„
- ì˜ˆ: `baseline_kobart.yaml` â†’ `load_config("baseline_kobart")`

### 3. Config ê°’ ì ‘ê·¼
```python
# ==================== Config ê°’ ì ‘ê·¼ ë°©ë²• ==================== #

# ---------------------- ì˜¬ë°”ë¥¸ ì ‘ê·¼ ---------------------- #
# OmegaConf ê°ì²´ëŠ” ì†ì„± ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©
config.training.batch_size

# ---------------------- ì˜ëª»ëœ ì ‘ê·¼ ---------------------- #
# ë”•ì…”ë„ˆë¦¬ ë°©ì‹ ì ‘ê·¼ì€ Key Error ë°œìƒ ê°€ëŠ¥
config['training']['batch_size']

# ---------------------- ì•ˆì „í•œ ì ‘ê·¼ ---------------------- #
# ê¸°ë³¸ê°’ì„ ì œê³µí•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
config.get('training', {}).get('batch_size', 32)
```

### 4. ëˆ„ë½ëœ Config ì²˜ë¦¬
- ConfigLoaderëŠ” ëˆ„ë½ëœ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê±´ë„ˆëœ€
- í•„ìˆ˜ ì„¤ì •ì€ `base/default.yaml`ì— ì •ì˜ ê¶Œì¥

---

# Part 3: Logger ì‹œìŠ¤í…œ

## Logger ê°œìš”

ëª¨ë“  ëª¨ë“ˆì—ì„œ print ëŒ€ì‹  `src/logging/logger.py`ì˜ Logger í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

**Loggerì˜ ì´ì :**
- ì½˜ì†” ì¶œë ¥ê³¼ íŒŒì¼ ë¡œê¹… ë™ì‹œ ìˆ˜í–‰
- íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ê°€
- stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ ì§€ì›
- ì—ëŸ¬ ë©”ì‹œì§€ ê°•ì¡° ì¶œë ¥

---

## Logger ì‚¬ìš©ë²•

### 1. Logger ì´ˆê¸°í™”

```python
# ==================== Logger ì´ˆê¸°í™” ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„± ---------------------- #
# íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìë™ ìƒì„±
log_path = create_log_path("outputs/logs", "train")  # outputs/logs/train_20251011_143000.log

# ---------------------- Logger ì´ˆê¸°í™” ---------------------- #
# íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œ ì¶œë ¥í•˜ëŠ” Logger ìƒì„±
logger = Logger(log_path, print_also=True)

# ---------------------- stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜ ì‹œì‘ ---------------------- #
# ëª¨ë“  print ì¶œë ¥ì„ ë¡œê·¸ íŒŒì¼ì—ë„ ì €ì¥
logger.start_redirect()

# ---------------------- ì‘ì—… ìˆ˜í–‰ ---------------------- #
# ... í•™ìŠµ ë˜ëŠ” ì¶”ë¡  ì‘ì—… ...

# ---------------------- ë¦¬ë‹¤ì´ë ‰ì…˜ ì¢…ë£Œ ë° ì •ë¦¬ ---------------------- #
# ë¦¬ë‹¤ì´ë ‰ì…˜ ì¢…ë£Œ
logger.stop_redirect()

# ë¡œê·¸ íŒŒì¼ ë‹«ê¸°
logger.close()
```

### 2. print ëŒ€ì‹  logger.write ì‚¬ìš©

```python
# ==================== Logger ì‚¬ìš© ë°©ë²• ==================== #

# ---------------------- ê¸°ì¡´ ë°©ì‹ (ì§€ì–‘) ---------------------- #
# printëŠ” íŒŒì¼ ë¡œê¹…ì´ ë˜ì§€ ì•ŠìŒ
print("=" * 60)
print("ëª¨ë¸ ë¡œë”© ì‹œì‘")
print("=" * 60)

# ---------------------- ì˜¬ë°”ë¥¸ ë°©ì‹ (ê¶Œì¥) ---------------------- #
# logger.writeëŠ” íŒŒì¼ê³¼ ì½˜ì†”ì— ë™ì‹œ ì¶œë ¥
logger.write("=" * 60)
logger.write("ëª¨ë¸ ë¡œë”© ì‹œì‘")
logger.write("=" * 60)
```

### 3. ì—ëŸ¬ ë¡œê¹…

```python
# ==================== ì—ëŸ¬ ë¡œê¹… ==================== #

# ---------------------- ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥ ---------------------- #
# print_error=Trueë¡œ ë¹¨ê°„ìƒ‰ ê°•ì¡° ì¶œë ¥
logger.write("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!", print_error=True)
```

### 4. Logger ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš© (ê¶Œì¥)

```python
# ==================== Logger ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš© ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ìƒì„± ---------------------- #
log_path = create_log_path("outputs/logs", "train")

# ---------------------- ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ Logger ì‚¬ìš© ---------------------- #
# with ë¬¸ì„ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ start_redirect()ì™€ stop_redirect() í˜¸ì¶œ
with Logger(log_path, print_also=True) as logger:
    logger.write("=" * 60)
    logger.write("í•™ìŠµ ì‹œì‘")
    logger.write("=" * 60)

    # ì‘ì—… ìˆ˜í–‰...

# with ë¸”ë¡ì„ ë²—ì–´ë‚˜ë©´ ìë™ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜ ì¢…ë£Œ ë° íŒŒì¼ ë‹«ê¸°
```

---

## ëª¨ë“ˆë³„ Logger í†µí•©

### src/models/model_loader.py

```python
# ==================== model_loader.py Logger í†µí•© ==================== #

# ---------------------- ìˆ˜ì • ì „ (print ì‚¬ìš©) ---------------------- #
# íŒŒì¼ ë¡œê¹…ì´ ë˜ì§€ ì•ŠìŒ
print(f"í† í¬ë‚˜ì´ì € ë¡œë”©: {checkpoint}")
print(f"  â†’ íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")

# ---------------------- ìˆ˜ì • í›„ (logger ì‚¬ìš©) ---------------------- #
# loggerê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ ë¡œê¹…
if logger:
    logger.write(f"í† í¬ë‚˜ì´ì € ë¡œë”©: {checkpoint}")
    logger.write(f"  â†’ íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")
```

**ì ìš© ì˜ˆì‹œ:**
```python
# ==================== load_model_and_tokenizer í•¨ìˆ˜ ì˜ˆì œ ==================== #

def load_model_and_tokenizer(config, logger=None):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # ---------------------- ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° ---------------------- #
    checkpoint = config.model.checkpoint

    # ---------------------- ëª¨ë¸ ë¡œë”© ì‹œì‘ ë¡œê·¸ ---------------------- #
    if logger:
        logger.write(f"\n[ëª¨ë¸ ë¡œë”©]")
        logger.write(f"ì²´í¬í¬ì¸íŠ¸: {checkpoint}")

    # ---------------------- í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
    # HuggingFace Hubì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    # ---------------------- íŠ¹ìˆ˜ í† í° ì¶”ê°€ ---------------------- #
    # Configì— ì •ì˜ëœ íŠ¹ìˆ˜ í† í° ì¶”ê°€
    special_tokens = config.tokenizer.special_tokens
    num_added = tokenizer.add_tokens(special_tokens)

    # ì¶”ê°€ëœ í† í° ìˆ˜ ë¡œê¹…
    if logger:
        logger.write(f"íŠ¹ìˆ˜ í† í° {num_added}ê°œ ì¶”ê°€ë¨")

    # ---------------------- ëª¨ë¸ ë¡œë“œ ---------------------- #
    # HuggingFace Hubì—ì„œ Seq2Seq ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

    # ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • (íŠ¹ìˆ˜ í† í° ì¶”ê°€ì— ë§ì¶°)
    model.resize_token_embeddings(len(tokenizer))

    # ì„ë² ë”© í¬ê¸° ì¡°ì • ë¡œê¹…
    if logger:
        logger.write(f"ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì •: {len(tokenizer)}")

    # ---------------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë°˜í™˜ ---------------------- #
    return model, tokenizer
```

### src/training/trainer.py

```python
# ==================== trainer.py Logger í†µí•© ==================== #

# ---------------------- ìˆ˜ì • ì „ (print ì‚¬ìš©) ---------------------- #
# íŒŒì¼ ë¡œê¹…ì´ ë˜ì§€ ì•ŠìŒ
print("=" * 60)
print("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
print("=" * 60)

# ---------------------- ìˆ˜ì • í›„ (logger ì‚¬ìš©) ---------------------- #
# loggerê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ ë¡œê¹…
if logger:
    logger.write("=" * 60)
    logger.write("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.write("=" * 60)
```

**ì ìš© ì˜ˆì‹œ:**
```python
# ==================== train ë©”ì„œë“œ ì˜ˆì œ ==================== #

def train(self, logger=None):
    """í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # ---------------------- í•™ìŠµ ì‹œì‘ ë¡œê·¸ ---------------------- #
    if logger:
        logger.write("=" * 60)
        logger.write("ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.write("=" * 60)
        logger.write(f"ì—í¬í¬: {self.config.training.epochs}")
        logger.write(f"ë°°ì¹˜ í¬ê¸°: {self.config.training.batch_size}")
        logger.write(f"í•™ìŠµë¥ : {self.config.training.learning_rate}")

    # ---------------------- í•™ìŠµ ì‹¤í–‰ ---------------------- #
    # HuggingFace Trainerë¡œ í•™ìŠµ ìˆ˜í–‰
    results = self.trainer.train()

    # ---------------------- í•™ìŠµ ì™„ë£Œ ë¡œê·¸ ---------------------- #
    if logger:
        logger.write("\ní•™ìŠµ ì™„ë£Œ!")
        logger.write(f"ìµœì¢… ì†ì‹¤: {results.training_loss:.4f}")

    # ---------------------- í•™ìŠµ ê²°ê³¼ ë°˜í™˜ ---------------------- #
    return results
```

### src/inference/predictor.py

```python
# ==================== predictor.py Logger í†µí•© ==================== #

# ---------------------- ìˆ˜ì • ì „ (print ì‚¬ìš©) ---------------------- #
# íŒŒì¼ ë¡œê¹…ì´ ë˜ì§€ ì•ŠìŒ
print("=" * 60)
print("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
print("=" * 60)

# ---------------------- ìˆ˜ì • í›„ (logger ì‚¬ìš©) ---------------------- #
# loggerê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ ë¡œê¹…
if logger:
    logger.write("=" * 60)
    logger.write("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
    logger.write("=" * 60)
```

**ì ìš© ì˜ˆì‹œ:**
```python
def create_submission(self, test_df, output_path, batch_size=32, logger=None):
    """ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if logger:
        logger.write("=" * 60)
        logger.write("ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘")
        logger.write("=" * 60)
        logger.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")
        logger.write(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")

    # ì¶”ë¡  ìˆ˜í–‰
    predictions = self.predict_batch(test_df['dialogue'].tolist(), batch_size)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission_df = pd.DataFrame({
        'id': test_df['id'],
        'summary': predictions
    })
    submission_df.to_csv(output_path, index=False)

    if logger:
        logger.write(f"\nì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")
        logger.write(f"ìƒì„±ëœ ìš”ì•½: {len(predictions)}ê°œ")

    return submission_df
```

---

## GPU ìœ í‹¸ë¦¬í‹° í†µí•©

### src/utils/gpu_optimization/team_gpu_check.py í™œìš©

```python
from src.utils.gpu_optimization.team_gpu_check import (
    check_gpu_tier,
    get_gpu_info,
    get_optimal_batch_size,
    get_memory_usage
)

# GPU ì •ë³´ ì¶œë ¥
gpu_info = get_gpu_info()
if logger:
    logger.write(f"GPU ì •ë³´: {gpu_info}")

# GPU tier í™•ì¸
gpu_tier = check_gpu_tier()
if logger:
    logger.write(f"GPU Tier: {gpu_tier}")

# ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
optimal_batch_size = get_optimal_batch_size("kobart", gpu_tier)
if logger:
    logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
```

**ì „ì²´ í†µí•© ì˜ˆì‹œ:**
```python
def main(args):
    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"train_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        logger.write("=" * 60)
        logger.write(f"í•™ìŠµ ì‹œì‘: {args.experiment}")
        logger.write("=" * 60)

        # GPU ì •ë³´ ì¶œë ¥
        logger.write("\n[GPU ì •ë³´]")
        gpu_info = get_gpu_info()
        for key, value in gpu_info.items():
            logger.write(f"  {key}: {value}")

        # GPU Tier í™•ì¸
        gpu_tier = check_gpu_tier()
        logger.write(f"\nGPU Tier: {gpu_tier}")

        # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
        optimal_batch = get_optimal_batch_size("kobart", gpu_tier)
        logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch}")

        # Config ë¡œë“œ
        logger.write("\n[1/6] Config ë¡œë”©...")
        config = load_config(args.experiment)
        logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")
        logger.write(f"  ëª¨ë¸: {config.model.checkpoint}")

        # ëª¨ë¸ ë¡œë“œ
        logger.write("\n[2/6] ëª¨ë¸ ë¡œë”©...")
        model, tokenizer = load_model_and_tokenizer(config, logger=logger)

        # ... ë‚˜ë¨¸ì§€ ì‘ì—…
```

---

## í†µí•© ì˜ˆì‹œ

### scripts/train.pyì— Logger ì™„ì „ í†µí•©

```python
import argparse
from pathlib import Path

from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.utils.gpu_optimization.team_gpu_check import (
    get_gpu_info, check_gpu_tier, get_optimal_batch_size
)
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", type=str, required=True,
                        help="ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: baseline_kobart)")
    args = parser.parse_args()

    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"train_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        try:
            logger.write("=" * 60)
            logger.write(f"í•™ìŠµ ì‹œì‘: {args.experiment}")
            logger.write("=" * 60)

            # GPU ì •ë³´ ì¶œë ¥
            logger.write("\n[GPU ì •ë³´]")
            gpu_info = get_gpu_info()
            for key, value in gpu_info.items():
                logger.write(f"  {key}: {value}")

            # GPU Tier í™•ì¸ ë° ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
            gpu_tier = check_gpu_tier()
            logger.write(f"\nGPU Tier: {gpu_tier}")
            optimal_batch = get_optimal_batch_size("kobart", gpu_tier)
            logger.write(f"ì¶”ì²œ ë°°ì¹˜ í¬ê¸°: {optimal_batch}")

            # Config ë¡œë“œ
            logger.write("\n[1/6] Config ë¡œë”©...")
            config = load_config(args.experiment)
            logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")
            logger.write(f"  ëª¨ë¸: {config.model.checkpoint}")
            logger.write(f"  ë°°ì¹˜ í¬ê¸°: {config.training.batch_size}")
            logger.write(f"  í•™ìŠµë¥ : {config.training.learning_rate}")
            logger.write(f"  ì—í¬í¬: {config.training.epochs}")

            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            logger.write("\n[2/6] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©...")
            model, tokenizer = load_model_and_tokenizer(config, logger=logger)

            # ë°ì´í„° ë¡œë“œ
            logger.write("\n[3/6] ë°ì´í„° ë¡œë”©...")
            train_df = pd.read_csv(config.paths.train_data)
            logger.write(f"  í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")

            # ë°ì´í„°ì…‹ ìƒì„±
            logger.write("\n[4/6] ë°ì´í„°ì…‹ ìƒì„±...")
            train_dataset = DialogueSummarizationDataset(
                dialogues=train_df['dialogue'].tolist(),
                summaries=train_df['summary'].tolist(),
                tokenizer=tokenizer,
                logger=logger
            )

            # Trainer ìƒì„±
            logger.write("\n[5/6] Trainer ìƒì„±...")
            trainer = create_trainer(
                config=config,
                model=model,
                tokenizer=tokenizer,
                train_dataset=train_dataset,
                logger=logger
            )

            # í•™ìŠµ ì‹¤í–‰
            logger.write("\n[6/6] í•™ìŠµ ì‹¤í–‰...")
            results = trainer.train()

            logger.write("\n" + "=" * 60)
            logger.write("í•™ìŠµ ì™„ë£Œ!")
            logger.write("=" * 60)
            logger.write(f"ìµœì¢… ì†ì‹¤: {results.training_loss:.4f}")
            logger.write(f"ë¡œê·¸ íŒŒì¼: {log_path}")

        except Exception as e:
            logger.write(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}", print_error=True)
            raise

if __name__ == "__main__":
    main()
```

### scripts/inference.pyì— Logger ì™„ì „ í†µí•©

```python
import argparse
import pandas as pd
from pathlib import Path

from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.inference import create_predictor

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", type=str, required=True,
                        help="ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: baseline_kobart)")
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="submissions/submission.csv",
                        help="ì œì¶œ íŒŒì¼ ê²½ë¡œ")
    args = parser.parse_args()

    # Logger ì´ˆê¸°í™”
    log_path = create_log_path("outputs/logs", f"inference_{args.experiment}")

    with Logger(log_path, print_also=True) as logger:
        try:
            logger.write("=" * 60)
            logger.write(f"ì¶”ë¡  ì‹œì‘: {args.experiment}")
            logger.write("=" * 60)

            # Config ë¡œë“œ
            logger.write("\n[1/4] Config ë¡œë”©...")
            config = load_config(args.experiment)
            logger.write(f"  ì‹¤í—˜ëª…: {config.experiment.name}")

            # ëª¨ë¸ ë¡œë“œ
            logger.write("\n[2/4] ëª¨ë¸ ë¡œë”©...")
            logger.write(f"  ì²´í¬í¬ì¸íŠ¸: {args.checkpoint}")
            model, tokenizer = load_model_and_tokenizer(
                config,
                checkpoint_path=args.checkpoint,
                logger=logger
            )

            # Predictor ìƒì„±
            logger.write("\n[3/4] Predictor ìƒì„±...")
            predictor = create_predictor(
                model=model,
                tokenizer=tokenizer,
                config=config,
                logger=logger
            )

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì¶”ë¡ 
            logger.write("\n[4/4] ì¶”ë¡  ì‹¤í–‰...")
            test_df = pd.read_csv(config.paths.test_data)
            logger.write(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")

            submission_df = predictor.create_submission(
                test_df=test_df,
                output_path=args.output,
                batch_size=config.inference.batch_size,
                logger=logger
            )

            logger.write("\n" + "=" * 60)
            logger.write("ì¶”ë¡  ì™„ë£Œ!")
            logger.write("=" * 60)
            logger.write(f"ì œì¶œ íŒŒì¼: {args.output}")
            logger.write(f"ìƒì„±ëœ ìš”ì•½: {len(submission_df)}ê°œ")
            logger.write(f"ë¡œê·¸ íŒŒì¼: {log_path}")

        except Exception as e:
            logger.write(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {str(e)}", print_error=True)
            raise

if __name__ == "__main__":
    main()
```

---

## Logger í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•µì‹¬ ëª¨ë“ˆ
- [ ] `src/models/model_loader.py` - Logger ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ ë° print ëŒ€ì²´
- [ ] `src/training/trainer.py` - Logger í†µí•©
- [ ] `src/inference/predictor.py` - Logger í†µí•©
- [ ] `src/data/preprocessor.py` - Logger í†µí•© (ì„ íƒ)
- [ ] `src/data/dataset.py` - Logger í†µí•© (ì„ íƒ)

### ìŠ¤í¬ë¦½íŠ¸
- [ ] `scripts/train.py` - Logger ì™„ì „ í†µí•©
- [ ] `scripts/inference.py` - Logger ì™„ì „ í†µí•©
- [ ] `scripts/full_pipeline.py` - Logger í†µí•© (ìˆëŠ” ê²½ìš°)

### ìœ í‹¸ë¦¬í‹°
- [ ] GPU ì •ë³´ ì¶œë ¥ ì‹œ Logger ì‚¬ìš©
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ì— `print_error=True` ì ìš©
- [ ] ëª¨ë“  print ë¬¸ì„ logger.writeë¡œ ëŒ€ì²´

---

# Part 4: WandB ê³ ê¸‰ ì‹œê°í™”

## ê°œìš”

`src/logging/wandb_logger.py`ì—ëŠ” ê¸°ë³¸ ë¡œê¹… ê¸°ëŠ¥ ì™¸ì— **5ê°€ì§€ ê³ ê¸‰ ì‹œê°í™” ë©”ì„œë“œ**ê°€ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ëŠ¥ë“¤ì€ í•™ìŠµ ê³¼ì •ì„ ë” ì„¸ë°€í•˜ê²Œ ëª¨ë‹ˆí„°ë§í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

### 5ê°€ì§€ ê³ ê¸‰ ì‹œê°í™” ê¸°ëŠ¥

| ê¸°ëŠ¥ | ë©”ì„œë“œ | ì£¼ìš” ìš©ë„ |
|------|--------|----------|
| ğŸ¯ **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„** | `log_learning_rate_schedule()` | í•™ìŠµë¥  ë³€í™” ì¶”ì  |
| ğŸ“Š **ê·¸ë˜ë””ì–¸íŠ¸ Norm** | `log_gradient_norms()` | ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ/ì†Œì‹¤ ê°ì§€ |
| ğŸ“ˆ **Loss Curve** | `log_loss_curve()` | ê³¼ì í•© ëª¨ë‹ˆí„°ë§ |
| ğŸ’¾ **GPU ë©”ëª¨ë¦¬** | `log_gpu_memory()` | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  |
| âš¡ **í•™ìŠµ ì†ë„** | `log_training_speed()` | í•™ìŠµ íš¨ìœ¨ì„± ì¸¡ì • |

---

## 1. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë¡œê¹…

### ê°œìš”

í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(Warmup, Cosine, Linear ë“±)ì˜ ë™ì‘ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜

```python
def log_learning_rate_schedule(
    self,
    step: int,                # í˜„ì¬ ìŠ¤í…
    learning_rate: float      # í˜„ì¬ í•™ìŠµë¥ 
)
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
from src.logging.wandb_logger import WandbLogger

# WandB Logger ì´ˆê¸°í™”
wandb_logger = WandbLogger(
    project_name="nlp-competition",
    experiment_name="baseline_kobart"
)
wandb_logger.init_run()

# í•™ìŠµ ë£¨í”„ ë‚´ì—ì„œ
for step in range(total_steps):
    # í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°
    current_lr = optimizer.param_groups[0]['lr']

    # í•™ìŠµë¥  ë¡œê¹…
    wandb_logger.log_learning_rate_schedule(
        step=step,
        learning_rate=current_lr
    )

    # í•™ìŠµ ìŠ¤í…...
```

### Trainer í†µí•© ì˜ˆì‹œ

```python
# src/training/trainer.pyì—ì„œ ì‚¬ìš©
class ModelTrainer:
    def train(self):
        for step, batch in enumerate(train_dataloader):
            # Forward/backward pass
            loss = self.model(**batch).loss
            loss.backward()

            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()

            # í•™ìŠµë¥  ë¡œê¹…
            if self.wandb_logger:
                current_lr = self.optimizer.param_groups[0]['lr']
                self.wandb_logger.log_learning_rate_schedule(
                    step=step,
                    learning_rate=current_lr
                )
```

### WandB ì¶œë ¥

WandB ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ ì°¨íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `learning_rate` vs `step` ê·¸ë˜í”„
- Warmup, Decay ë‹¨ê³„ ì‹œê°í™”

---

## 2. ê·¸ë˜ë””ì–¸íŠ¸ Norm ë¡œê¹…

### ê°œìš”

ê·¸ë˜ë””ì–¸íŠ¸ normì„ ì¶”ì í•˜ì—¬ **ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ(exploding)** ë˜ëŠ” **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤(vanishing)** ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•©ë‹ˆë‹¤.

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜

```python
def log_gradient_norms(
    self,
    model,          # PyTorch ëª¨ë¸
    step: int       # í˜„ì¬ ìŠ¤í…
)
```

### í•µì‹¬ ê¸°ëŠ¥

- **ì „ì²´ ê·¸ë˜ë””ì–¸íŠ¸ norm** ê³„ì‚° (L2 norm)
- **ë ˆì´ì–´ë³„ norm** ì¶”ì  (ìƒìœ„ 10ê°œë§Œ)
- Gradient clipping í•„ìš” ì—¬ë¶€ íŒë‹¨

### ì‚¬ìš© ì˜ˆì‹œ

```python
from src.logging.wandb_logger import WandbLogger

wandb_logger = WandbLogger(
    project_name="nlp-competition",
    experiment_name="baseline_kobart"
)
wandb_logger.init_run()

# í•™ìŠµ ë£¨í”„ ë‚´ì—ì„œ
for step, batch in enumerate(train_dataloader):
    # Forward pass
    outputs = model(**batch)
    loss = outputs.loss

    # Backward pass
    loss.backward()

    # ê·¸ë˜ë””ì–¸íŠ¸ norm ë¡œê¹… (optimizer.step() ì „ì— í˜¸ì¶œ)
    wandb_logger.log_gradient_norms(
        model=model,
        step=step
    )

    # Gradient clipping (í•„ìš”ì‹œ)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # Optimizer step
    optimizer.step()
    optimizer.zero_grad()
```

### WandB ì¶œë ¥

WandB ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `gradient/total_norm`: ì „ì²´ ê·¸ë˜ë””ì–¸íŠ¸ norm
- `grad_norm/{layer_name}`: ìƒìœ„ 10ê°œ ë ˆì´ì–´ì˜ norm

### ê·¸ë˜ë””ì–¸íŠ¸ Norm í•´ì„

| Total Norm ë²”ìœ„ | ìƒíƒœ | ì¡°ì¹˜ |
|-----------------|------|------|
| 0.001 ~ 1.0 | âœ… ì •ìƒ | ì¡°ì¹˜ ë¶ˆí•„ìš” |
| 1.0 ~ 10.0 | âš ï¸ ì£¼ì˜ | ëª¨ë‹ˆí„°ë§ í•„ìš” |
| > 10.0 | âŒ í­ë°œ | Gradient clipping ì ìš© |
| < 0.001 | âŒ ì†Œì‹¤ | í•™ìŠµë¥  ì¦ê°€ ë˜ëŠ” Batch Norm |

---

## 3. Loss Curve ë¡œê¹…

### ê°œìš”

Train Lossì™€ Validation Lossë¥¼ ë™ì‹œì— ë¡œê¹…í•˜ì—¬ **ê³¼ì í•©(overfitting)**ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜

```python
def log_loss_curve(
    self,
    train_loss: float,          # í•™ìŠµ ì†ì‹¤
    val_loss: float = None,     # ê²€ì¦ ì†ì‹¤ (ì„ íƒ)
    step: int = None            # ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
)
```

### í•µì‹¬ ê¸°ëŠ¥

- Train/Validation Loss ë™ì‹œ ì¶”ì 
- **Train-Val Loss ì°¨ì´** ìë™ ê³„ì‚° (overfitting ì§€í‘œ)

### ì‚¬ìš© ì˜ˆì‹œ

```python
from src.logging.wandb_logger import WandbLogger

wandb_logger = WandbLogger(
    project_name="nlp-competition",
    experiment_name="baseline_kobart"
)
wandb_logger.init_run()

# ì—í¬í¬ë§ˆë‹¤ ë¡œê¹…
for epoch in range(num_epochs):
    # í•™ìŠµ
    train_loss = train_one_epoch(model, train_loader)

    # ê²€ì¦
    val_loss = validate(model, val_loader)

    # Loss curve ë¡œê¹…
    wandb_logger.log_loss_curve(
        train_loss=train_loss,
        val_loss=val_loss,
        step=epoch
    )

    # ê³¼ì í•© ê°ì§€
    loss_diff = train_loss - val_loss
    if loss_diff < -0.5:  # Val lossê°€ train lossë³´ë‹¤ 0.5 ì´ìƒ ë†’ìœ¼ë©´
        print("âš ï¸ ê³¼ì í•© ê°ì§€! Early stopping ê³ ë ¤")
```

### WandB ì¶œë ¥

WandB ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `loss/train`: í•™ìŠµ ì†ì‹¤
- `loss/val`: ê²€ì¦ ì†ì‹¤
- `loss/train_val_diff`: í•™ìŠµ-ê²€ì¦ ì†ì‹¤ ì°¨ì´ (overfitting ì§€í‘œ)

### Overfitting íŒë‹¨ ê¸°ì¤€

| Train-Val Diff | ìƒíƒœ | ì¡°ì¹˜ |
|----------------|------|------|
| -0.2 ~ 0.0 | âœ… ì •ìƒ | í•™ìŠµ ê³„ì† |
| -0.5 ~ -0.2 | âš ï¸ ê²½ë¯¸í•œ ê³¼ì í•© | Dropout/Weight decay ì¦ê°€ |
| < -0.5 | âŒ ì‹¬ê°í•œ ê³¼ì í•© | Early stopping ê³ ë ¤ |

---

## 4. GPU ë©”ëª¨ë¦¬ ë¡œê¹…

### ê°œìš”

GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ì—¬ **OOM(Out of Memory)** ì˜¤ë¥˜ë¥¼ ì˜ˆë°©í•©ë‹ˆë‹¤.

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜

```python
def log_gpu_memory(
    self,
    step: int = None        # ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
)
```

### í•µì‹¬ ê¸°ëŠ¥

- **Allocated Memory**: ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬
- **Reserved Memory**: PyTorchê°€ ì˜ˆì•½í•œ ë©”ëª¨ë¦¬
- **Max Allocated**: ìµœëŒ€ ì‚¬ìš© ë©”ëª¨ë¦¬
- **Memory Utilization**: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)

### ì‚¬ìš© ì˜ˆì‹œ

```python
from src.logging.wandb_logger import WandbLogger

wandb_logger = WandbLogger(
    project_name="nlp-competition",
    experiment_name="baseline_kobart"
)
wandb_logger.init_run()

# ì •ê¸°ì ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ë¡œê¹… (ì˜ˆ: 100 stepë§ˆë‹¤)
for step, batch in enumerate(train_dataloader):
    # Forward/backward pass
    loss = model(**batch).loss
    loss.backward()
    optimizer.step()

    # 100 stepë§ˆë‹¤ GPU ë©”ëª¨ë¦¬ ë¡œê¹…
    if step % 100 == 0:
        wandb_logger.log_gpu_memory(step=step)
```

### Multi-GPU ì§€ì›

ë©”ì„œë“œëŠ” **ìë™ìœ¼ë¡œ ëª¨ë“  GPUë¥¼ ê°ì§€**í•˜ê³  ê° GPUì˜ ë©”ëª¨ë¦¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤.

```python
# GPU 0, 1, 2ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
wandb_logger.log_gpu_memory(step=step)

# WandBì— ë‹¤ìŒì´ ë¡œê¹…ë¨:
# - gpu_0/memory_allocated_gb
# - gpu_0/memory_reserved_gb
# - gpu_0/memory_utilization
# - gpu_1/memory_allocated_gb
# - gpu_1/memory_reserved_gb
# - gpu_1/memory_utilization
# - gpu_2/memory_allocated_gb
# ...
```

### WandB ì¶œë ¥

WandB ëŒ€ì‹œë³´ë“œì—ì„œ GPUë³„ë¡œ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `gpu_{i}/memory_allocated_gb`: ì‹¤ì œ ì‚¬ìš© ë©”ëª¨ë¦¬ (GB)
- `gpu_{i}/memory_reserved_gb`: ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ (GB)
- `gpu_{i}/memory_max_allocated_gb`: ìµœëŒ€ ì‚¬ìš© ë©”ëª¨ë¦¬ (GB)
- `gpu_{i}/memory_utilization`: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (%)

### ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í•´ì„

| Utilization | ìƒíƒœ | ì¡°ì¹˜ |
|-------------|------|------|
| 0 ~ 70% | âœ… ì•ˆì „ | ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥ |
| 70 ~ 90% | âš ï¸ ì£¼ì˜ | í˜„ì¬ ì„¤ì • ìœ ì§€ |
| > 90% | âŒ ìœ„í—˜ | ë°°ì¹˜ í¬ê¸° ê°ì†Œ ë˜ëŠ” Gradient Accumulation |

---

## 5. í•™ìŠµ ì†ë„ ë¡œê¹…

### ê°œìš”

í•™ìŠµ íš¨ìœ¨ì„±ì„ ì¸¡ì •í•˜ì—¬ **ë³‘ëª© êµ¬ê°„**ì„ íŒŒì•…í•˜ê³  ìµœì í™” ë°©í–¥ì„ ê²°ì •í•©ë‹ˆë‹¤.

### ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜

```python
def log_training_speed(
    self,
    samples_per_second: float = None,    # ì´ˆë‹¹ ìƒ˜í”Œ ìˆ˜
    steps_per_second: float = None,      # ì´ˆë‹¹ ìŠ¤í… ìˆ˜
    epoch_time: float = None,            # ì—í¬í¬ ì†Œìš” ì‹œê°„ (ì´ˆ)
    step: int = None                     # ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
)
```

### í•µì‹¬ ê¸°ëŠ¥

- **ì´ˆë‹¹ ìƒ˜í”Œ ìˆ˜**: ë°ì´í„° ì²˜ë¦¬ ì†ë„
- **ì´ˆë‹¹ ìŠ¤í… ìˆ˜**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì†ë„
- **ì—í¬í¬ ì†Œìš” ì‹œê°„**: ì „ì²´ ì—í¬í¬ ì™„ë£Œ ì‹œê°„ (ì´ˆ/ë¶„)

### ì‚¬ìš© ì˜ˆì‹œ

```python
import time
from src.logging.wandb_logger import WandbLogger

wandb_logger = WandbLogger(
    project_name="nlp-competition",
    experiment_name="baseline_kobart"
)
wandb_logger.init_run()

# ì—í¬í¬ ë ˆë²¨ ì†ë„ ì¸¡ì •
for epoch in range(num_epochs):
    epoch_start_time = time.time()
    total_samples = 0
    total_steps = 0

    for step, batch in enumerate(train_dataloader):
        step_start_time = time.time()

        # Forward/backward pass
        loss = model(**batch).loss
        loss.backward()
        optimizer.step()

        # ìŠ¤í… ì†ë„ ê³„ì‚°
        step_time = time.time() - step_start_time
        batch_size = batch['input_ids'].size(0)

        total_samples += batch_size
        total_steps += 1

        # 100 stepë§ˆë‹¤ ë¡œê¹…
        if step % 100 == 0:
            samples_per_sec = batch_size / step_time
            steps_per_sec = 1 / step_time

            wandb_logger.log_training_speed(
                samples_per_second=samples_per_sec,
                steps_per_second=steps_per_sec,
                step=step
            )

    # ì—í¬í¬ ì¢…ë£Œ ì‹œ ì „ì²´ ì†ë„ ë¡œê¹…
    epoch_time = time.time() - epoch_start_time
    avg_samples_per_sec = total_samples / epoch_time
    avg_steps_per_sec = total_steps / epoch_time

    wandb_logger.log_training_speed(
        samples_per_second=avg_samples_per_sec,
        steps_per_second=avg_steps_per_sec,
        epoch_time=epoch_time,
        step=epoch
    )
```

### WandB ì¶œë ¥

WandB ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `speed/samples_per_second`: ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜
- `speed/steps_per_second`: ì´ˆë‹¹ ìŠ¤í… ìˆ˜
- `speed/epoch_time_seconds`: ì—í¬í¬ ì†Œìš” ì‹œê°„ (ì´ˆ)
- `speed/epoch_time_minutes`: ì—í¬í¬ ì†Œìš” ì‹œê°„ (ë¶„)

### í•™ìŠµ ì†ë„ ë²¤ì¹˜ë§ˆí¬ (KoBART ê¸°ì¤€)

| GPU | Batch Size | Samples/sec | ë¹„ê³  |
|-----|-----------|-------------|------|
| T4 | 32 | ~8 | ê¸°ë³¸ ì„¤ì • |
| V100 | 64 | ~25 | 2-3ë°° ë¹ ë¦„ |
| A100 | 128 | ~60 | 6-8ë°° ë¹ ë¦„ |

---

## ì „ì²´ í†µí•© ì˜ˆì‹œ

### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— 5ê°€ì§€ ì‹œê°í™” ëª¨ë‘ ì ìš©

```python
import time
import torch
from src.logging.wandb_logger import WandbLogger
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import create_dataloader

def main():
    # Config ë¡œë“œ
    config = load_config("baseline_kobart")

    # WandB Logger ì´ˆê¸°í™”
    wandb_logger = WandbLogger(
        project_name=config.wandb.project,
        experiment_name=config.experiment.name,
        config=config
    )
    wandb_logger.init_run()

    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)
    train_loader = create_dataloader(config, tokenizer, split='train')
    val_loader = create_dataloader(config, tokenizer, split='val')

    # Optimizer & Scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.learning_rate)
    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)

    # í•™ìŠµ ë£¨í”„
    global_step = 0

    for epoch in range(config.training.epochs):
        epoch_start_time = time.time()
        model.train()

        train_losses = []

        for step, batch in enumerate(train_loader):
            step_start_time = time.time()

            # Forward pass
            outputs = model(**batch)
            loss = outputs.loss
            train_losses.append(loss.item())

            # Backward pass
            loss.backward()

            # 1ï¸âƒ£ ê·¸ë˜ë””ì–¸íŠ¸ norm ë¡œê¹… (optimizer.step ì „)
            if global_step % 100 == 0:
                wandb_logger.log_gradient_norms(
                    model=model,
                    step=global_step
                )

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # Optimizer step
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            # 2ï¸âƒ£ í•™ìŠµë¥  ë¡œê¹…
            current_lr = optimizer.param_groups[0]['lr']
            wandb_logger.log_learning_rate_schedule(
                step=global_step,
                learning_rate=current_lr
            )

            # 3ï¸âƒ£ GPU ë©”ëª¨ë¦¬ ë¡œê¹… (100 stepë§ˆë‹¤)
            if global_step % 100 == 0:
                wandb_logger.log_gpu_memory(step=global_step)

            # 4ï¸âƒ£ í•™ìŠµ ì†ë„ ë¡œê¹… (100 stepë§ˆë‹¤)
            if global_step % 100 == 0:
                step_time = time.time() - step_start_time
                batch_size = batch['input_ids'].size(0)

                wandb_logger.log_training_speed(
                    samples_per_second=batch_size / step_time,
                    steps_per_second=1 / step_time,
                    step=global_step
                )

            global_step += 1

        # ê²€ì¦
        model.eval()
        val_losses = []

        with torch.no_grad():
            for batch in val_loader:
                outputs = model(**batch)
                val_losses.append(outputs.loss.item())

        # 5ï¸âƒ£ Loss curve ë¡œê¹…
        avg_train_loss = sum(train_losses) / len(train_losses)
        avg_val_loss = sum(val_losses) / len(val_losses)

        wandb_logger.log_loss_curve(
            train_loss=avg_train_loss,
            val_loss=avg_val_loss,
            step=epoch
        )

        # ì—í¬í¬ ì†Œìš” ì‹œê°„ ë¡œê¹…
        epoch_time = time.time() - epoch_start_time
        wandb_logger.log_training_speed(
            epoch_time=epoch_time,
            step=epoch
        )

        print(f"Epoch {epoch+1}/{config.training.epochs}")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        print(f"  Val Loss: {avg_val_loss:.4f}")
        print(f"  Epoch Time: {epoch_time/60:.2f}ë¶„")

    # WandB ì¢…ë£Œ
    wandb_logger.finish()

if __name__ == "__main__":
    main()
```

---

## ì‹œê°í™” í™œìš© ì „ëµ

### 1. ë””ë²„ê¹… ë‹¨ê³„ (í•™ìŠµ ì´ˆê¸°)

**ëª¨ë“  ì‹œê°í™”ë¥¼ ìì£¼ ë¡œê¹…**í•˜ì—¬ ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•©ë‹ˆë‹¤.

```python
# ë§¤ stepë§ˆë‹¤ ë¡œê¹… (ë””ë²„ê·¸ ëª¨ë“œ)
for step, batch in enumerate(train_loader):
    # ... í•™ìŠµ ë¡œì§ ...

    # ëª¨ë“  ë©”íŠ¸ë¦­ ë¡œê¹…
    wandb_logger.log_gradient_norms(model, step)
    wandb_logger.log_learning_rate_schedule(step, current_lr)
    wandb_logger.log_gpu_memory(step)
```

### 2. ì •ìƒ í•™ìŠµ ë‹¨ê³„

**ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ì„ íƒì ìœ¼ë¡œ ë¡œê¹…**í•˜ì—¬ WandB ë¶€í•˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.

```python
# 100 stepë§ˆë‹¤ ë¡œê¹… (ì •ìƒ í•™ìŠµ)
if step % 100 == 0:
    wandb_logger.log_gradient_norms(model, step)
    wandb_logger.log_gpu_memory(step)
    wandb_logger.log_training_speed(samples_per_sec, steps_per_sec, step)

# ë§¤ stepë§ˆë‹¤ í•™ìŠµë¥ ë§Œ ë¡œê¹…
wandb_logger.log_learning_rate_schedule(step, current_lr)
```

### 3. í”„ë¡œë•ì…˜ ë‹¨ê³„

**ì—í¬í¬ ë ˆë²¨ ë©”íŠ¸ë¦­ë§Œ ë¡œê¹…**í•©ë‹ˆë‹¤.

```python
# ì—í¬í¬ë§ˆë‹¤ë§Œ ë¡œê¹… (í”„ë¡œë•ì…˜)
for epoch in range(num_epochs):
    # ... í•™ìŠµ ...

    wandb_logger.log_loss_curve(train_loss, val_loss, epoch)
    wandb_logger.log_training_speed(epoch_time=epoch_time, step=epoch)
```

---

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 1. ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ê°ì§€ ì‹œ

**ì¦ìƒ**: `gradient/total_norm`ì´ 10 ì´ìƒ

**í•´ê²°ì±…**:
```python
# Gradient clipping ì ìš©
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# ë˜ëŠ” í•™ìŠµë¥  ê°ì†Œ
config.training.learning_rate = 5e-6  # ê¸°ì¡´: 1e-5
```

### 2. ê³¼ì í•© ê°ì§€ ì‹œ

**ì¦ìƒ**: `loss/train_val_diff` < -0.5

**í•´ê²°ì±…**:
```python
# Dropout ì¦ê°€
config.model.dropout = 0.3  # ê¸°ì¡´: 0.1

# Weight decay ì¦ê°€
config.training.weight_decay = 0.01  # ê¸°ì¡´: 0.0

# Early stopping
if val_loss > best_val_loss:
    patience_counter += 1
    if patience_counter >= 3:
        print("Early stopping!")
        break
```

### 3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

**ì¦ìƒ**: `gpu_0/memory_utilization` > 90%

**í•´ê²°ì±…**:
```python
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
config.training.batch_size = 32  # ê¸°ì¡´: 64

# Gradient accumulation ì‚¬ìš©
config.training.gradient_accumulation_steps = 2

# Mixed precision í•™ìŠµ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    outputs = model(**batch)
    loss = outputs.loss

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 4. í•™ìŠµ ì†ë„ ëŠë¦¼

**ì¦ìƒ**: `speed/samples_per_second` < 5 (T4 ê¸°ì¤€)

**í•´ê²°ì±…**:
```python
# DataLoader ì›Œì»¤ ìˆ˜ ì¦ê°€
train_loader = DataLoader(
    train_dataset,
    batch_size=config.training.batch_size,
    num_workers=4,  # ê¸°ì¡´: 0
    pin_memory=True
)

# ì»´íŒŒì¼ ëª¨ë“œ í™œì„±í™” (PyTorch 2.0+)
model = torch.compile(model)

# Mixed precision í•™ìŠµ
from torch.cuda.amp import autocast
with autocast():
    outputs = model(**batch)
```

---

## ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/logging/logger.py` - Logger í´ë˜ìŠ¤
- `src/logging/wandb_logger.py` - WandbLogger í´ë˜ìŠ¤ (5ê°€ì§€ ì‹œê°í™” í¬í•¨)
- `src/utils/core/common.py` - create_log_path ìœ í‹¸ë¦¬í‹°
- `src/utils/gpu_optimization/team_gpu_check.py` - GPU ìœ í‹¸ë¦¬í‹°

**ì„¤ì • íŒŒì¼:**
- `configs/base/default.yaml` - ê¸°ë³¸ ì„¤ì •
- `configs/base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì… ì„¤ì •
- `configs/models/kobart.yaml` - KoBART ì„¤ì •
- `configs/experiments/baseline_kobart.yaml` - ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜

**ê´€ë ¨ ë¬¸ì„œ:**
- [01_ì‹œì‘_ê°€ì´ë“œ.md](./01_ì‹œì‘_ê°€ì´ë“œ.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- [06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) - ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°•
- [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) - ëª¨ë¸ ì‹œìŠ¤í…œ
- [08_í‰ê°€_ìµœì í™”.md](./08_í‰ê°€_ìµœì í™”.md) - í‰ê°€ ë° ìµœì í™”
- [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ì „ì²´ ëª…ë ¹ì–´ ê°€ì´ë“œ
