# í‰ê°€ ì‹œìŠ¤í…œ ìƒì„¸ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [RougeCalculator í´ë˜ìŠ¤](#rougecalculator-í´ë˜ìŠ¤)
3. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
4. [Multi-reference ì§€ì›](#multi-reference-ì§€ì›)
5. [ë°°ì¹˜ ê³„ì‚°](#ë°°ì¹˜-ê³„ì‚°)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- ROUGE ì ìˆ˜ ìë™ ê³„ì‚° (ê²½ì§„ëŒ€íšŒ í‰ê°€ ê¸°ì¤€)
- Multi-reference í‰ê°€ ì§€ì›
- ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ ì •ë³´ ì œê³µ
- í•™ìŠµ/í‰ê°€ ì‹œ ìë™ í†µí•©

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ROUGE-1/2/L F1 ì ìˆ˜ ê³„ì‚°
- âœ… ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€) ìë™ ê³„ì‚°
- âœ… Multi-reference ì§€ì› (ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
- âœ… ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€)
- âœ… HuggingFace Trainerì™€ ìë™ í†µí•©

---

## ğŸ—ï¸ RougeCalculator í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/evaluation/metrics.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
class RougeCalculator:
    def __init__(self, rouge_types=['rouge1', 'rouge2', 'rougeL'], use_stemmer=False):
        """ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”"""

    def calculate_single(self, prediction: str, reference: Union[str, List[str]]) -> Dict:
        """ë‹¨ì¼ ìƒ˜í”Œ ROUGE ê³„ì‚°"""

    def calculate_batch(self, predictions: List[str], references: List[str]) -> Dict:
        """ë°°ì¹˜ ìƒ˜í”Œ ROUGE í‰ê·  ê³„ì‚°"""

    def _empty_scores(self) -> Dict:
        """ë¹ˆ ì…ë ¥ì— ëŒ€í•œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜"""
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ì¼ ìƒ˜í”Œ)

```python
from src.evaluation import RougeCalculator

# ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”
calculator = RougeCalculator()

# ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
reference = "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤"

scores = calculator.calculate_single(prediction, reference)

print(scores)
# ì¶œë ¥:
# {
#     'rouge1': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706},
#     'rouge2': {'precision': 0.5, 'recall': 0.4, 'fmeasure': 0.444},
#     'rougeL': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706}
# }
```

### 2. í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©

```python
from src.evaluation import calculate_rouge_scores

# ë‹¨ì¼ ìƒ˜í”Œ
scores = calculate_rouge_scores(
    predictions="ì˜ˆì¸¡ ìš”ì•½",
    references="ì •ë‹µ ìš”ì•½"
)

# ë°°ì¹˜ ìƒ˜í”Œ
predictions = ["ì˜ˆì¸¡1", "ì˜ˆì¸¡2", "ì˜ˆì¸¡3"]
references = ["ì •ë‹µ1", "ì •ë‹µ2", "ì •ë‹µ3"]

scores = calculate_rouge_scores(predictions, references)
```

### 3. ì ìˆ˜ í¬ë§·íŒ…

```python
from src.evaluation import calculate_rouge_scores, format_rouge_scores

scores = calculate_rouge_scores(predictions, references)
print(format_rouge_scores(scores))

# ì¶œë ¥:
# ROUGE1:
#   fmeasure: 0.7060
#   std: 0.1200
#   min: 0.5500
#   max: 0.8500
#
# ROUGE2:
#   fmeasure: 0.4440
#   ...
```

---

## ğŸ”„ Multi-reference ì§€ì›

### ê°œìš”

í•˜ë‚˜ì˜ ëŒ€í™”ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ì •ë‹µ ìš”ì•½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-reference í‰ê°€ëŠ” ê° ì •ë‹µì— ëŒ€í•´ ROUGEë¥¼ ê³„ì‚°í•œ í›„ **ìµœëŒ€ F1 ì ìˆ˜**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

### ì‚¬ìš© ë°©ë²•

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

# ë‹¨ì¼ ì˜ˆì¸¡, ë‹¤ì¤‘ ì •ë‹µ
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",
    "ì €ë…ì— ë§Œë‚˜ê¸°ë¡œ í–ˆë‹¤",
    "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"
]

scores = calculator.calculate_single(prediction, references)
```

### ì²˜ë¦¬ ê³¼ì •

1. **ê° ì •ë‹µì— ëŒ€í•´ ROUGE ê³„ì‚°**
   ```python
   for ref in references:
       score = scorer.score(prediction, ref)
       all_scores.append(score)
   ```

2. **ìµœëŒ€ F1 ì ìˆ˜ ì„ íƒ**
   ```python
   max_score = max(all_scores, key=lambda x: x['rouge1'].fmeasure)
   ```

3. **ê²°ê³¼ ë°˜í™˜**
   ```python
   {
       'rouge1': {'precision': 1.0, 'recall': 1.0, 'fmeasure': 1.0},  # "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"ì™€ ì™„ì „ ì¼ì¹˜
       'rouge2': {...},
       'rougeL': {...}
   }
   ```

---

## ğŸ“Š ë°°ì¹˜ ê³„ì‚°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

predictions = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤",
    "íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤"
]

references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",
    "íšŒì˜ë¥¼ ì˜¤í›„ 3ì‹œì— í•˜ê¸°ë¡œ í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œë¥¼ ë¨¹ê¸°ë¡œ í–ˆë‹¤"
]

scores = calculator.calculate_batch(predictions, references)
```

### ì¶œë ¥ í˜•ì‹

```python
{
    'rouge1': {
        'fmeasure': 0.7060,      # í‰ê·  F1 ì ìˆ˜
        'std': 0.1200,           # í‘œì¤€í¸ì°¨
        'min': 0.5500,           # ìµœì†Œê°’
        'max': 0.8500            # ìµœëŒ€ê°’
    },
    'rouge2': {
        'fmeasure': 0.4440,
        'std': 0.0800,
        'min': 0.3000,
        'max': 0.6000
    },
    'rougeL': {
        'fmeasure': 0.7060,
        'std': 0.1200,
        'min': 0.5500,
        'max': 0.8500
    },
    'rouge_sum': {               # ROUGE-1 + ROUGE-2 + ROUGE-L
        'fmeasure': 1.8560,
        'std': 0.0,
        'min': 0.0,
        'max': 0.0
    }
}
```

### ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€)

ê²½ì§„ëŒ€íšŒì—ì„œëŠ” ROUGE-1, ROUGE-2, ROUGE-Lì˜ F1 ì ìˆ˜ í•©ê³„ë¥¼ ìµœì¢… í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
rouge_sum = rouge1_f1 + rouge2_f1 + rougeL_f1
# ì˜ˆ: 0.706 + 0.444 + 0.706 = 1.856
```

---

## ğŸ”— HuggingFace Trainer í†µí•©

### ModelTrainerì—ì„œ ìë™ ì‚¬ìš©

`src/training/trainer.py`ì˜ `ModelTrainer` í´ë˜ìŠ¤ëŠ” ìë™ìœ¼ë¡œ ROUGEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

```python
from src.training import create_trainer

trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ROUGE ê³„ì‚°
results = trainer.train()

# í‰ê°€ ê²°ê³¼
print(results['eval_metrics'])
# {
#     'eval_rouge1': 0.706,
#     'eval_rouge2': 0.444,
#     'eval_rougeL': 0.706,
#     'eval_rouge_sum': 1.856
# }
```

### compute_metrics í•¨ìˆ˜

Trainerì—ì„œ ì‚¬ìš©í•˜ëŠ” í‰ê°€ í•¨ìˆ˜:

```python
def compute_metrics(self, eval_preds) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE)"""
    predictions, labels = eval_preds

    # -100ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ë³€ê²½
    labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)

    # ë””ì½”ë”©
    decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ROUGE ê³„ì‚°
    scores = self.rouge_calculator.calculate_batch(
        decoded_preds,
        decoded_labels
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    result = {
        'rouge1': scores['rouge1']['fmeasure'],
        'rouge2': scores['rouge2']['fmeasure'],
        'rougeL': scores['rougeL']['fmeasure'],
        'rouge_sum': scores['rouge_sum']['fmeasure']
    }

    return result
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ íŒŒì¼ ìœ„ì¹˜
```
src/tests/test_metrics.py
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python src/tests/test_metrics.py
```

### í…ŒìŠ¤íŠ¸ í•­ëª© (ì´ 6ê°œ)

1. âœ… ê¸°ë³¸ ROUGE ê³„ì‚° (ë‹¨ì¼ ìƒ˜í”Œ)
2. âœ… Multi-reference ROUGE ê³„ì‚°
3. âœ… ë°°ì¹˜ ROUGE ê³„ì‚°
4. âœ… ë¹ˆ ì…ë ¥ ì²˜ë¦¬
5. âœ… í¸ì˜ í•¨ìˆ˜ (calculate_rouge_scores)
6. âœ… ì ìˆ˜ í¬ë§·íŒ… (format_rouge_scores)

---

## ğŸ¯ ì‹¤ì „ í™œìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ëª¨ë¸ ì„±ëŠ¥ ì§ì ‘ í‰ê°€

```python
from src.evaluation import calculate_rouge_scores
import pandas as pd

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±
predictions = model_predict(test_df)  # ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜

# ROUGE ê³„ì‚°
scores = calculate_rouge_scores(
    predictions=predictions,
    references=test_df['summary'].tolist()
)

print(f"ROUGE-1 F1: {scores['rouge1']['fmeasure']:.4f}")
print(f"ROUGE-2 F1: {scores['rouge2']['fmeasure']:.4f}")
print(f"ROUGE-L F1: {scores['rougeL']['fmeasure']:.4f}")
print(f"ROUGE Sum: {scores['rouge_sum']['fmeasure']:.4f}")
```

### ì˜ˆì‹œ 2: ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

models = {
    "KoBART": model1,
    "T5-small": model2,
    "mBART": model3
}

results = {}

for model_name, model in models.items():
    predictions = model_predict(model, test_df)
    scores = calculator.calculate_batch(predictions, references)
    results[model_name] = scores['rouge_sum']['fmeasure']

# ê²°ê³¼ ì¶œë ¥
for model_name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):
    print(f"{model_name}: {score:.4f}")

# ì¶œë ¥:
# KoBART: 1.8560
# mBART: 1.7320
# T5-small: 1.6100
```

### ì˜ˆì‹œ 3: ìƒ˜í”Œë³„ ìƒì„¸ ë¶„ì„

```python
from src.evaluation import RougeCalculator

calculator = RougeCalculator()

# ê° ìƒ˜í”Œë³„ ROUGE ê³„ì‚°
sample_scores = []

for pred, ref in zip(predictions, references):
    score = calculator.calculate_single(pred, ref)
    sample_scores.append({
        'prediction': pred,
        'reference': ref,
        'rouge1': score['rouge1']['fmeasure'],
        'rouge2': score['rouge2']['fmeasure'],
        'rougeL': score['rougeL']['fmeasure']
    })

# DataFrameìœ¼ë¡œ ë³€í™˜
import pandas as pd
df_scores = pd.DataFrame(sample_scores)

# ì„±ëŠ¥ì´ ë‚®ì€ ìƒ˜í”Œ ì°¾ê¸°
low_scores = df_scores[df_scores['rouge1'] < 0.3].sort_values('rouge1')
print("ì„±ëŠ¥ì´ ë‚®ì€ ìƒ˜í”Œ:")
print(low_scores[['prediction', 'reference', 'rouge1']])
```

---

## ğŸ“Œ ì£¼ì˜ì‚¬í•­

### 1. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°

ê¸°ë³¸ì ìœ¼ë¡œ `use_stemmer=False`ë¡œ ì„¤ì •ë©ë‹ˆë‹¤. í•œêµ­ì–´ì—ëŠ” ì˜ì–´ìš© stemmerê°€ ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
# âœ… í•œêµ­ì–´ì— ì í•©
calculator = RougeCalculator(use_stemmer=False)

# âŒ í•œêµ­ì–´ì— ë¶€ì í•©
calculator = RougeCalculator(use_stemmer=True)
```

### 2. ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬

ë¹ˆ ì˜ˆì¸¡ì´ë‚˜ ì •ë‹µì€ ìë™ìœ¼ë¡œ 0ì  ì²˜ë¦¬ë©ë‹ˆë‹¤:

```python
scores = calculator.calculate_single("", "ì •ë‹µ ìš”ì•½")
# {'rouge1': {'precision': 0.0, 'recall': 0.0, 'fmeasure': 0.0}, ...}
```

### 3. Multi-reference ì„±ëŠ¥

Multi-reference í‰ê°€ëŠ” ì •ë‹µ ê°œìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ê³„ì‚° ì‹œê°„ì´ ì¦ê°€í•©ë‹ˆë‹¤:

```python
# ë‹¨ì¼ ì •ë‹µ: O(1)
scores = calculator.calculate_single(pred, ref)

# 3ê°œ ì •ë‹µ: O(3)
scores = calculator.calculate_single(pred, [ref1, ref2, ref3])
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/evaluation/metrics.py` - RougeCalculator í´ë˜ìŠ¤
- `src/evaluation/__init__.py` - ì™¸ë¶€ API

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_metrics.py` - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**í†µí•©:**
- `src/training/trainer.py` - Trainerì—ì„œ ìë™ ì‚¬ìš©
