# ì‹¤í–‰ ëª…ë ¹ì–´ ì´ì •ë¦¬

## ğŸ“‹ ëª©ì°¨
1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [í•™ìŠµ ì‹¤í–‰](#í•™ìŠµ-ì‹¤í–‰)
3. [ì¶”ë¡  ì‹¤í–‰](#ì¶”ë¡ -ì‹¤í–‰)
4. [Full Pipeline ì‹¤í–‰](#full-pipeline-ì‹¤í–‰)
5. [í…ŒìŠ¤íŠ¸ ì‹¤í–‰](#í…ŒìŠ¤íŠ¸-ì‹¤í–‰)
6. [ê²°ê³¼ íŒŒì¼ ê²½ë¡œ](#ê²°ê³¼-íŒŒì¼-ê²½ë¡œ)

---

## ğŸ”§ í™˜ê²½ ì„¤ì •

### ê°€ìƒí™˜ê²½ í™œì„±í™”

```bash
# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# ë˜ëŠ”
pyenv activate nlp_py3_11_9

# íŒ¨í‚¤ì§€ í™•ì¸
pip list | grep -E "(torch|transformers|wandb)"
```

### í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™

```bash
cd /home/ieyeppo/AI_Lab/natural-language-processing-competition
```

---

## ğŸš€ í•™ìŠµ ì‹¤í–‰

### 1. ê¸°ë³¸ í•™ìŠµ

```bash
python scripts/train.py --experiment baseline_kobart
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/baseline_kobart/final_model/`
- ì²´í¬í¬ì¸íŠ¸: `outputs/baseline_kobart/checkpoint-{N}/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_baseline_kobart_YYYYMMDD_HHMMSS.log`

### 2. ë””ë²„ê·¸ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)

```bash
python scripts/train.py --experiment baseline_kobart --debug
```

**ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •:**
- ë°ì´í„°: í•™ìŠµ 100ê°œ, ê²€ì¦ 20ê°œ
- ì—í¬í¬: 2íšŒ
- ë°°ì¹˜ í¬ê¸°: 4
- WandB: ë¹„í™œì„±í™”

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/baseline_kobart/final_model/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_baseline_kobart_YYYYMMDD_HHMMSS.log`

### 3. ë‹¤ë¥¸ ì‹¤í—˜ Config ì‚¬ìš©

```bash
# ë‹¤ë¥¸ Config íŒŒì¼ì´ ìˆë‹¤ë©´
python scripts/train.py --experiment my_custom_experiment
```

**í•„ìš” íŒŒì¼:**
- Config: `configs/experiments/my_custom_experiment.yaml`

### 4. LLM íŒŒì¸íŠœë‹ í•™ìŠµ (Llama/Qwen + QLoRA)

```bash
# Llama-3.2-3B í•™ìŠµ
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/llama_3.2_3b_qlora/final_model/`
- ë¡œê·¸: `logs/YYYYMMDD/train/train_llm_llama_3.2_3b_YYYYMMDD_HHMMSS.log`

### 5. LLM Instruction Tuning (ë°ì´í„° 5ë°° ì¦ê°•)

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**íš¨ê³¼:**
- í•™ìŠµ ë°ì´í„°: 12,457ê°œ â†’ 62,285ê°œ (5ë°°)
- 5ê°€ì§€ instruction í…œí”Œë¦¿ ì ìš©

### 6. LLM ë””ë²„ê·¸ ëª¨ë“œ

```bash
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •:**
- ë°ì´í„°: í•™ìŠµ 50ê°œ, ê²€ì¦ 10ê°œ
- ì—í¬í¬: 1íšŒ
- ë°°ì¹˜ í¬ê¸°: 2

---

## ğŸ”® ì¶”ë¡  ì‹¤í–‰

### 1. ê¸°ë³¸ ì¶”ë¡ 

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

**ê²°ê³¼ íŒŒì¼:**
- ì œì¶œ íŒŒì¼: `submissions/submission.csv`
- ë¡œê·¸: `logs/YYYYMMDD/inference/inference_final_model_YYYYMMDD_HHMMSS.log`

### 2. íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ 

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/checkpoint-1000 \
    --output submissions/checkpoint_1000.csv
```

**ê²°ê³¼ íŒŒì¼:**
- ì œì¶œ íŒŒì¼: `submissions/checkpoint_1000.csv`
- ë¡œê·¸: `logs/YYYYMMDD/inference/inference_checkpoint-1000_YYYYMMDD_HHMMSS.log`

### 3. ì¶”ë¡  íŒŒë¼ë¯¸í„° ì¡°ì •

```bash
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv \
    --batch_size 16 \
    --num_beams 8
```

**ì˜µì…˜:**
- `--test_data`: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸: `data/raw/test.csv`)
- `--batch_size`: ì¶”ë¡  ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32)
- `--num_beams`: Beam search ë¹” ê°œìˆ˜ (ê¸°ë³¸: 4)
- `--experiment`: Config ì´ë¦„ (ê¸°ë³¸: `baseline_kobart`)

**ê²°ê³¼ íŒŒì¼:**
- ì œì¶œ íŒŒì¼: `submissions/submission.csv`
- ë¡œê·¸: `logs/YYYYMMDD/inference/inference_final_model_YYYYMMDD_HHMMSS.log`

---

## ğŸ”„ Full Pipeline ì‹¤í–‰

### 1. í•™ìŠµ + ì¶”ë¡  í•œ ë²ˆì—

```bash
python scripts/run_pipeline.py --experiment baseline_kobart
```

**ì‹¤í–‰ íë¦„:**
1. í•™ìŠµ ì‹¤í–‰ â†’ `outputs/baseline_kobart/final_model/` ìƒì„±
2. ì¶”ë¡  ì‹¤í–‰ â†’ `submissions/submission.csv` ìƒì„±

**ê²°ê³¼ íŒŒì¼:**
- ëª¨ë¸: `outputs/baseline_kobart/final_model/`
- ì œì¶œ íŒŒì¼: `submissions/submission.csv`
- í•™ìŠµ ë¡œê·¸: `logs/YYYYMMDD/train/train_baseline_kobart_YYYYMMDD_HHMMSS.log`
- ì¶”ë¡  ë¡œê·¸: `logs/YYYYMMDD/inference/inference_final_model_YYYYMMDD_HHMMSS.log`

### 2. í•™ìŠµ ê±´ë„ˆë›°ê³  ì¶”ë¡ ë§Œ

```bash
python scripts/run_pipeline.py \
    --experiment baseline_kobart \
    --skip_training \
    --model_path outputs/baseline_kobart/final_model \
    --output submissions/submission_v2.csv
```

**ê²°ê³¼ íŒŒì¼:**
- ì œì¶œ íŒŒì¼: `submissions/submission_v2.csv`
- ì¶”ë¡  ë¡œê·¸: `logs/YYYYMMDD/inference/inference_final_model_YYYYMMDD_HHMMSS.log`

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ì „ì²´ í…ŒìŠ¤íŠ¸ í•œ ë²ˆì—

```bash
python src/tests/test_config_loader.py && \
python src/tests/test_preprocessor.py && \
python src/tests/test_model_loader.py && \
python src/tests/test_metrics.py && \
python src/tests/test_trainer.py && \
python src/tests/test_predictor.py && \
python src/tests/test_lora_loader.py && \
python src/tests/test_augmentation.py && \
python src/tests/test_kfold.py
```

### ê°œë³„ í…ŒìŠ¤íŠ¸

```bash
# Config Loader í…ŒìŠ¤íŠ¸
python src/tests/test_config_loader.py

# ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
python src/tests/test_preprocessor.py

# ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸
python src/tests/test_model_loader.py

# í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python src/tests/test_metrics.py

# í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python src/tests/test_trainer.py

# ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python src/tests/test_predictor.py

# LLM LoRA Loader í…ŒìŠ¤íŠ¸
python src/tests/test_lora_loader.py

# ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸
python src/tests/test_augmentation.py

# K-Fold êµì°¨ ê²€ì¦ í…ŒìŠ¤íŠ¸
python src/tests/test_kfold.py
```

**í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- ì½˜ì†”ì— ì¶œë ¥ë¨
- ì´ 50ê°œ í…ŒìŠ¤íŠ¸ (ê° ëª¨ë“ˆë‹¹ 4-7ê°œ)

---

## ğŸ“‚ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
natural-language-processing-competition/
â”œâ”€â”€ logs/                                    # ë¡œê·¸ íŒŒì¼
â”‚   â””â”€â”€ YYYYMMDD/                            # ë‚ ì§œë³„
â”‚       â”œâ”€â”€ train/                           # í•™ìŠµ ë¡œê·¸
â”‚       â”‚   â””â”€â”€ train_baseline_kobart_YYYYMMDD_HHMMSS.log
â”‚       â””â”€â”€ inference/                       # ì¶”ë¡  ë¡œê·¸
â”‚           â””â”€â”€ inference_final_model_YYYYMMDD_HHMMSS.log
â”‚
â”œâ”€â”€ outputs/                                 # ëª¨ë¸ ì¶œë ¥
â”‚   â””â”€â”€ baseline_kobart/                     # ì‹¤í—˜ë³„
â”‚       â”œâ”€â”€ checkpoint-500/                  # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚       â”œâ”€â”€ checkpoint-1000/
â”‚       â”œâ”€â”€ checkpoint-1500/
â”‚       â”œâ”€â”€ final_model/                     # ìµœì¢… ëª¨ë¸
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚       â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚       â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚       â”‚   â””â”€â”€ vocab.txt
â”‚       â””â”€â”€ logs/                            # Trainer ë¡œê·¸
â”‚
â””â”€â”€ submissions/                             # ì œì¶œ íŒŒì¼
    â”œâ”€â”€ submission.csv
    â”œâ”€â”€ checkpoint_1000.csv
    â””â”€â”€ submission_v2.csv
```

### ë¡œê·¸ íŒŒì¼ (`logs/`)

**ê²½ë¡œ ê·œì¹™:** `logs/YYYYMMDD/{log_type}/{filename}.log`

**ì˜ˆì‹œ:**
- í•™ìŠµ: `logs/20251011/train/train_baseline_kobart_20251011_150300.log`
- ì¶”ë¡ : `logs/20251011/inference/inference_final_model_20251011_150500.log`

**ë¡œê·¸ ë‚´ìš©:**
- ì‹¤í–‰ ì‹œì‘/ì¢…ë£Œ ì‹œê°
- GPU ì •ë³´ (ì´ë¦„, ë©”ëª¨ë¦¬, tier)
- Config ì„¤ì •ê°’
- ë°ì´í„° ë¡œë“œ ì •ë³´
- í•™ìŠµ/ì¶”ë¡  ì§„í–‰ ìƒí™©
- ìµœì¢… í‰ê°€ ê²°ê³¼
- ì—ëŸ¬ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)

### ëª¨ë¸ íŒŒì¼ (`outputs/`)

**ê²½ë¡œ ê·œì¹™:** `outputs/{experiment_name}/`

**êµ¬ì¡°:**
```
outputs/baseline_kobart/
â”œâ”€â”€ checkpoint-500/              # ì—í¬í¬ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ trainer_state.json
â”‚   â””â”€â”€ training_args.bin
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ final_model/                 # ìµœì¢… ì €ì¥ ëª¨ë¸
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ vocab.txt
â””â”€â”€ logs/                        # HuggingFace Trainer ë¡œê·¸
    â””â”€â”€ events.out.tfevents.*
```

**ìš©ëŸ‰:**
- KoBART ëª¨ë¸: ì•½ 500MB/ì²´í¬í¬ì¸íŠ¸
- ìµœëŒ€ 3ê°œ ì²´í¬í¬ì¸íŠ¸ ìœ ì§€ (save_total_limit: 3)

### ì œì¶œ íŒŒì¼ (`submissions/`)

**ê²½ë¡œ:** `submissions/{filename}.csv`

**í˜•ì‹:**
```csv
fname,summary
test_001,ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤
test_002,íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤
test_003,ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤
...
```

**ìƒ˜í”Œ ìˆ˜:** 2,500ê°œ (test.csv ê¸°ì¤€)

---

## ğŸ¯ ì‹¤ì „ ì‚¬ìš© íŒ¨í„´

### íŒ¨í„´ 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…

```bash
# 1. ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python scripts/train.py --experiment baseline_kobart --debug

# 2. í•™ìŠµëœ ëª¨ë¸ í™•ì¸
ls outputs/baseline_kobart/final_model/

# 3. ì‘ì€ ë°ì´í„°ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/debug_test.csv \
    --batch_size 4
```

### íŒ¨í„´ 2: ì „ì²´ í•™ìŠµ ë° ì œì¶œ

```bash
# 1. Full pipeline ì‹¤í–‰
python scripts/run_pipeline.py --experiment baseline_kobart

# 2. ê²°ê³¼ í™•ì¸
cat submissions/submission.csv | head -5

# 3. ë¡œê·¸ í™•ì¸
tail -100 logs/$(date +%Y%m%d)/train/train_baseline_kobart_*.log
```

### íŒ¨í„´ 3: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ ë¹„êµ

```bash
# ê° ì²´í¬í¬ì¸íŠ¸ë¡œ ì¶”ë¡ 
for checkpoint in checkpoint-500 checkpoint-1000 checkpoint-1500 final_model
do
    python scripts/inference.py \
        --model outputs/baseline_kobart/$checkpoint \
        --output submissions/${checkpoint}.csv
done

# ê²°ê³¼ ë¹„êµ
ls -lh submissions/
```

### íŒ¨í„´ 4: ì‹¤í—˜ ì¶”ì 

```bash
# ì—¬ëŸ¬ ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰
experiments=("baseline_kobart" "experiment_v2" "experiment_v3")

for exp in "${experiments[@]}"
do
    echo "ì‹¤í—˜ ì‹œì‘: $exp"
    python scripts/run_pipeline.py --experiment $exp
    echo "ì‹¤í—˜ ì™„ë£Œ: $exp"
    echo "---"
done

# ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ í™•ì¸
ls -R outputs/
ls submissions/
```

---

## ğŸ“Š ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (A6000 ê¸°ì¤€)

### í•™ìŠµ (train.py)

| ëª¨ë“œ | ë°ì´í„° í¬ê¸° | ì—í¬í¬ | ë°°ì¹˜ í¬ê¸° | ì˜ˆìƒ ì‹œê°„ |
|-----|-----------|--------|---------|----------|
| ë””ë²„ê·¸ | 100ê°œ | 2 | 4 | ~2ë¶„ |
| ì „ì²´ | 12,457ê°œ | 20 | 50 | ~4-6ì‹œê°„ |

### ì¶”ë¡  (inference.py)

| ë°ì´í„° í¬ê¸° | ë°°ì¹˜ í¬ê¸° | ì˜ˆìƒ ì‹œê°„ |
|-----------|---------|----------|
| 2,500ê°œ | 32 | ~3-5ë¶„ |
| 2,500ê°œ | 16 | ~5-7ë¶„ |

### Full Pipeline (run_pipeline.py)

| ëª¨ë“œ | ì˜ˆìƒ ì‹œê°„ |
|-----|----------|
| í•™ìŠµ + ì¶”ë¡  | ~4-6ì‹œê°„ |
| ì¶”ë¡ ë§Œ (--skip_training) | ~3-5ë¶„ |

---

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° (ë””ë²„ê·¸ ëª¨ë“œ ì‚¬ìš©)
python scripts/train.py --experiment baseline_kobart --debug
```

ë˜ëŠ” Config ìˆ˜ì •:
```yaml
# configs/experiments/baseline_kobart.yaml
training:
  batch_size: 16  # 50 â†’ 16
```

### WandB ë¡œê·¸ì¸ í•„ìš”

```bash
# WandB ë¡œê·¸ì¸
wandb login

# ë˜ëŠ” ë””ë²„ê·¸ ëª¨ë“œ (WandB ë¹„í™œì„±í™”)
python scripts/train.py --experiment baseline_kobart --debug
```

### ëª¨ë¸ ê²½ë¡œ ì°¾ì„ ìˆ˜ ì—†ìŒ

```bash
# ê²½ë¡œ í™•ì¸
ls -la outputs/baseline_kobart/

# ì •í™•í•œ ê²½ë¡œ ì‚¬ìš©
python scripts/inference.py \
    --model outputs/baseline_kobart/final_model \
    --output submissions/submission.csv
```

### ë¡œê·¸ íŒŒì¼ ì°¾ê¸°

```bash
# ì˜¤ëŠ˜ ë‚ ì§œ ë¡œê·¸ í™•ì¸
ls logs/$(date +%Y%m%d)/

# ìµœì‹  í•™ìŠµ ë¡œê·¸
ls -lt logs/$(date +%Y%m%d)/train/ | head

# ë¡œê·¸ ë‚´ìš© ë³´ê¸°
tail -100 logs/$(date +%Y%m%d)/train/train_baseline_kobart_*.log
```

---

## ğŸ“ Config íŒŒì¼ ìœ„ì¹˜

### ì‹¤í—˜ Config

```
configs/experiments/
â””â”€â”€ baseline_kobart.yaml         # ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜ ì„¤ì •
```

### Base Config

```
configs/base/
â”œâ”€â”€ default.yaml                 # ì „ì²´ ê¸°ë³¸ ì„¤ì •
â””â”€â”€ encoder_decoder.yaml         # Encoder-Decoder ê³µí†µ ì„¤ì •
```

### ëª¨ë¸ Config

```
configs/models/
â””â”€â”€ kobart.yaml                  # KoBART ì „ìš© ì„¤ì •
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md](./00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
- [ìŠ¤í¬ë¦½íŠ¸_ì‚¬ìš©ë²•.md](./ìŠ¤í¬ë¦½íŠ¸_ì‚¬ìš©ë²•.md) - ìƒì„¸ ì‚¬ìš©ë²•
- [ë¹ ë¥¸_ì‹œì‘_ê°€ì´ë“œ.md](./ë¹ ë¥¸_ì‹œì‘_ê°€ì´ë“œ.md) - 5ë¶„ ì‹œì‘ ê°€ì´ë“œ
