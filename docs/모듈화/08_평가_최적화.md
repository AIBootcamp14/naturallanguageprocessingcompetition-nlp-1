# í‰ê°€ ë° ìµœì í™” ì‹œìŠ¤í…œ ê°€ì´ë“œ

> **í†µí•© ë¬¸ì„œ:** í‰ê°€ ì‹œìŠ¤í…œ + K-Fold êµì°¨ ê²€ì¦ + Optuna ìµœì í™”

## ğŸ“‹ ëª©ì°¨

### Part 1: í‰ê°€ ì‹œìŠ¤í…œ
- [ê°œìš”](#part-1-í‰ê°€-ì‹œìŠ¤í…œ)
- [RougeCalculator í´ë˜ìŠ¤](#rougecalculator-í´ë˜ìŠ¤)
- [ì‚¬ìš© ë°©ë²•](#í‰ê°€-ì‹œìŠ¤í…œ-ì‚¬ìš©-ë°©ë²•)
- [Multi-reference ì§€ì›](#multi-reference-ì§€ì›)
- [ë°°ì¹˜ ê³„ì‚°](#ë°°ì¹˜-ê³„ì‚°)
- [HuggingFace Trainer í†µí•©](#huggingface-trainer-í†µí•©)

### Part 2: K-Fold êµì°¨ ê²€ì¦
- [ê°œìš”](#part-2-k-fold-êµì°¨-ê²€ì¦)
- [KFoldSplitter](#kfoldsplitter)
- [ì‚¬ìš© ë°©ë²•](#êµì°¨-ê²€ì¦-ì‚¬ìš©-ë°©ë²•)
- [ì‹¤í–‰ ëª…ë ¹ì–´](#êµì°¨-ê²€ì¦-ì‹¤í–‰-ëª…ë ¹ì–´)

### Part 3: Optuna ìµœì í™”
- [ê°œìš”](#part-3-optuna-ìµœì í™”)
- [OptunaOptimizer í´ë˜ìŠ¤](#optunaoptimizer-í´ë˜ìŠ¤)
- [íƒìƒ‰ ê³µê°„ ì •ì˜](#íƒìƒ‰-ê³µê°„-ì •ì˜)
- [ìµœì í™” ì „ëµ](#ìµœì í™”-ì „ëµ)
- [ì‚¬ìš© ë°©ë²•](#optuna-ì‚¬ìš©-ë°©ë²•)
- [ì‹¤í–‰ ëª…ë ¹ì–´](#optuna-ì‹¤í–‰-ëª…ë ¹ì–´)

---

# ğŸ“Œ Part 1: í‰ê°€ ì‹œìŠ¤í…œ

## ğŸ“ ê°œìš”

### ëª©ì 
- ROUGE ì ìˆ˜ ìë™ ê³„ì‚° (ê²½ì§„ëŒ€íšŒ í‰ê°€ ê¸°ì¤€)
- Multi-reference í‰ê°€ ì§€ì›
- ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ ì •ë³´ ì œê³µ
- í•™ìŠµ/í‰ê°€ ì‹œ ìë™ í†µí•©

### í•µì‹¬ ê¸°ëŠ¥
- âœ… ROUGE-1/2/L F1 ì ìˆ˜ ê³„ì‚°
- âœ… ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€) ìë™ ê³„ì‚°
- âœ… Multi-reference ì§€ì› (ì •ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
- âœ… ë°°ì¹˜ ê³„ì‚° ë° í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€)
- âœ… HuggingFace Trainerì™€ ìë™ í†µí•©

---

## ğŸ—ï¸ RougeCalculator í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/evaluation/metrics.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== RougeCalculator í´ë˜ìŠ¤ ì •ì˜ ==================== #
class RougeCalculator:
    # ---------------------- ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™” ---------------------- #
    def __init__(self, rouge_types=['rouge1', 'rouge2', 'rougeL'], use_stemmer=False):
        """ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™”"""

    # ---------------------- ë‹¨ì¼ ìƒ˜í”Œ ROUGE ê³„ì‚° ---------------------- #
    def calculate_single(self, prediction: str, reference: Union[str, List[str]]) -> Dict:
        """ë‹¨ì¼ ìƒ˜í”Œ ROUGE ê³„ì‚°"""

    # ---------------------- ë°°ì¹˜ ìƒ˜í”Œ ROUGE í‰ê·  ê³„ì‚° ---------------------- #
    def calculate_batch(self, predictions: List[str], references: List[str]) -> Dict:
        """ë°°ì¹˜ ìƒ˜í”Œ ROUGE í‰ê·  ê³„ì‚°"""

    # ---------------------- ë¹ˆ ì…ë ¥ì— ëŒ€í•œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜ ---------------------- #
    def _empty_scores(self) -> Dict:
        """ë¹ˆ ì…ë ¥ì— ëŒ€í•œ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜"""
```

---

## ğŸ’» í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ì¼ ìƒ˜í”Œ)

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.evaluation import RougeCalculator

# ---------------------- ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™” ---------------------- #
calculator = RougeCalculator()

# ---------------------- ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€ ---------------------- #
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"        # ì˜ˆì¸¡ ìš”ì•½ë¬¸
reference = "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤"    # ì •ë‹µ ìš”ì•½ë¬¸

# ROUGE ì ìˆ˜ ê³„ì‚°
scores = calculator.calculate_single(prediction, reference)

# ê²°ê³¼ ì¶œë ¥
print(scores)
# ì¶œë ¥:
# {
#     'rouge1': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706},
#     'rouge2': {'precision': 0.5, 'recall': 0.4, 'fmeasure': 0.444},
#     'rougeL': {'precision': 0.75, 'recall': 0.667, 'fmeasure': 0.706}
# }
```

### 2. í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.evaluation import calculate_rouge_scores

# ---------------------- ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€ ---------------------- #
scores = calculate_rouge_scores(
    predictions="ì˜ˆì¸¡ ìš”ì•½",              # ì˜ˆì¸¡ ìš”ì•½ë¬¸
    references="ì •ë‹µ ìš”ì•½"                # ì •ë‹µ ìš”ì•½ë¬¸
)

# ---------------------- ë°°ì¹˜ ìƒ˜í”Œ í‰ê°€ ---------------------- #
predictions = ["ì˜ˆì¸¡1", "ì˜ˆì¸¡2", "ì˜ˆì¸¡3"]     # ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸
references = ["ì •ë‹µ1", "ì •ë‹µ2", "ì •ë‹µ3"]      # ì •ë‹µ ë¦¬ìŠ¤íŠ¸

# ë°°ì¹˜ ROUGE ì ìˆ˜ ê³„ì‚°
scores = calculate_rouge_scores(predictions, references)
```

### 3. ì ìˆ˜ í¬ë§·íŒ…

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.evaluation import calculate_rouge_scores, format_rouge_scores

# ---------------------- ROUGE ì ìˆ˜ ê³„ì‚° ë° í¬ë§·íŒ… ---------------------- #
# ROUGE ì ìˆ˜ ê³„ì‚°
scores = calculate_rouge_scores(predictions, references)

# í¬ë§·íŒ…ëœ ì¶œë ¥
print(format_rouge_scores(scores))

# ì¶œë ¥:
# ROUGE1:
#   fmeasure: 0.7060
#   std: 0.1200
#   min: 0.5500
#   max: 0.8500
#
# ROUGE2:
#   fmeasure: 0.4440
#   ...
```

---

## ğŸ”„ Multi-reference ì§€ì›

### ê°œìš”

í•˜ë‚˜ì˜ ëŒ€í™”ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ ì •ë‹µ ìš”ì•½ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-reference í‰ê°€ëŠ” ê° ì •ë‹µì— ëŒ€í•´ ROUGEë¥¼ ê³„ì‚°í•œ í›„ **ìµœëŒ€ F1 ì ìˆ˜**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

### ì‚¬ìš© ë°©ë²•

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.evaluation import RougeCalculator

# ---------------------- ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™” ---------------------- #
calculator = RougeCalculator()

# ---------------------- Multi-reference í‰ê°€ ---------------------- #
# ë‹¨ì¼ ì˜ˆì¸¡ë¬¸
prediction = "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"

# ë‹¤ì¤‘ ì •ë‹µë¬¸ (ì—¬ëŸ¬ ê°œì˜ ê°€ëŠ¥í•œ ì •ë‹µ)
references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",    # ì •ë‹µ 1
    "ì €ë…ì— ë§Œë‚˜ê¸°ë¡œ í–ˆë‹¤",               # ì •ë‹µ 2
    "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"                  # ì •ë‹µ 3
]

# ìµœëŒ€ F1 ì ìˆ˜ë¥¼ ì„ íƒí•˜ì—¬ ê³„ì‚°
scores = calculator.calculate_single(prediction, references)
```

### ì²˜ë¦¬ ê³¼ì •

1. **ê° ì •ë‹µì— ëŒ€í•´ ROUGE ê³„ì‚°**
   ```python
   # ëª¨ë“  ì •ë‹µì— ëŒ€í•´ ROUGE ì ìˆ˜ ê³„ì‚°
   for ref in references:
       score = scorer.score(prediction, ref)        # ê°œë³„ ROUGE ê³„ì‚°
       all_scores.append(score)                     # ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
   ```

2. **ìµœëŒ€ F1 ì ìˆ˜ ì„ íƒ**
   ```python
   # ROUGE-1 F1 ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ì •ë‹µ ì„ íƒ
   max_score = max(all_scores, key=lambda x: x['rouge1'].fmeasure)
   ```

3. **ê²°ê³¼ ë°˜í™˜**
   ```python
   {
       'rouge1': {'precision': 1.0, 'recall': 1.0, 'fmeasure': 1.0},  # "ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤"ì™€ ì™„ì „ ì¼ì¹˜
       'rouge2': {...},
       'rougeL': {...}
   }
   ```

---

## ğŸ“Š ë°°ì¹˜ ê³„ì‚°

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.evaluation import RougeCalculator

# ---------------------- ROUGE ê³„ì‚°ê¸° ì´ˆê¸°í™” ---------------------- #
calculator = RougeCalculator()

# ---------------------- ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„ ---------------------- #
# ì˜ˆì¸¡ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
predictions = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì¡ì•˜ë‹¤",
    "íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ ë©”ë‰´ëŠ” ê¹€ì¹˜ì°Œê°œë‹¤"
]

# ì •ë‹µ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
references = [
    "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤",
    "íšŒì˜ë¥¼ ì˜¤í›„ 3ì‹œì— í•˜ê¸°ë¡œ í–ˆë‹¤",
    "ë‚´ì¼ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œë¥¼ ë¨¹ê¸°ë¡œ í–ˆë‹¤"
]

# ---------------------- ë°°ì¹˜ ROUGE ê³„ì‚° ---------------------- #
# í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€ ì ìˆ˜ ê³„ì‚°
scores = calculator.calculate_batch(predictions, references)
```

### ì¶œë ¥ í˜•ì‹

```python
{
    'rouge1': {
        'fmeasure': 0.7060,      # í‰ê·  F1 ì ìˆ˜
        'std': 0.1200,           # í‘œì¤€í¸ì°¨
        'min': 0.5500,           # ìµœì†Œê°’
        'max': 0.8500            # ìµœëŒ€ê°’
    },
    'rouge2': {
        'fmeasure': 0.4440,
        'std': 0.0800,
        'min': 0.3000,
        'max': 0.6000
    },
    'rougeL': {
        'fmeasure': 0.7060,
        'std': 0.1200,
        'min': 0.5500,
        'max': 0.8500
    },
    'rouge_sum': {               # ROUGE-1 + ROUGE-2 + ROUGE-L
        'fmeasure': 1.8560,
        'std': 0.0,
        'min': 0.0,
        'max': 0.0
    }
}
```

### ROUGE Sum (ê²½ì§„ëŒ€íšŒ ê¸°ì¤€)

ê²½ì§„ëŒ€íšŒì—ì„œëŠ” ROUGE-1, ROUGE-2, ROUGE-Lì˜ F1 ì ìˆ˜ í•©ê³„ë¥¼ ìµœì¢… í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
rouge_sum = rouge1_f1 + rouge2_f1 + rougeL_f1
# ì˜ˆ: 0.706 + 0.444 + 0.706 = 1.856
```

---

## ğŸ”— HuggingFace Trainer í†µí•©

### ModelTrainerì—ì„œ ìë™ ì‚¬ìš©

`src/training/trainer.py`ì˜ `ModelTrainer` í´ë˜ìŠ¤ëŠ” ìë™ìœ¼ë¡œ ROUGEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.training import create_trainer

# ---------------------- Trainer ìƒì„± ---------------------- #
trainer = create_trainer(
    config=config,                          # ì„¤ì • íŒŒì¼
    model=model,                            # í•™ìŠµí•  ëª¨ë¸
    tokenizer=tokenizer,                    # í† í¬ë‚˜ì´ì €
    train_dataset=train_dataset,            # í•™ìŠµ ë°ì´í„°ì…‹
    eval_dataset=eval_dataset               # í‰ê°€ ë°ì´í„°ì…‹
)

# ---------------------- í•™ìŠµ ì‹¤í–‰ ---------------------- #
# í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ROUGE ê³„ì‚°
results = trainer.train()

# ---------------------- í‰ê°€ ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(results['eval_metrics'])
# {
#     'eval_rouge1': 0.706,
#     'eval_rouge2': 0.444,
#     'eval_rougeL': 0.706,
#     'eval_rouge_sum': 1.856
# }
```

### compute_metrics í•¨ìˆ˜

Trainerì—ì„œ ì‚¬ìš©í•˜ëŠ” í‰ê°€ í•¨ìˆ˜:

```python
# ---------------------- í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ---------------------- #
def compute_metrics(self, eval_preds) -> Dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ROUGE)"""
    # -------------- ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ë ˆì´ë¸” ì¶”ì¶œ -------------- #
    predictions, labels = eval_preds

    # -------------- ë ˆì´ë¸” ì „ì²˜ë¦¬ -------------- #
    # -100ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ë³€ê²½ (ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œë˜ëŠ” ê°’)
    labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)

    # -------------- í† í° ë””ì½”ë”© -------------- #
    # ì˜ˆì¸¡ í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    decoded_preds = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # ì •ë‹µ í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # -------------- ROUGE ì ìˆ˜ ê³„ì‚° -------------- #
    scores = self.rouge_calculator.calculate_batch(
        decoded_preds,              # ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        decoded_labels              # ì •ë‹µ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    )

    # -------------- ê²°ê³¼ í¬ë§·íŒ… -------------- #
    result = {
        'rouge1': scores['rouge1']['fmeasure'],             # ROUGE-1 F1
        'rouge2': scores['rouge2']['fmeasure'],             # ROUGE-2 F1
        'rougeL': scores['rougeL']['fmeasure'],             # ROUGE-L F1
        'rouge_sum': scores['rouge_sum']['fmeasure']        # ROUGE Sum
    }

    return result
```

---

# ğŸ“Œ Part 2: K-Fold êµì°¨ ê²€ì¦

## ğŸ“ ê°œìš”

### ëª©ì 
- K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
- Stratified K-Fold ì§€ì› (ì¸µí™” ì¶”ì¶œ)
- Fold ê²°ê³¼ ì§‘ê³„ ë° í†µê³„ ë¶„ì„

### í•µì‹¬ ê¸°ëŠ¥
- âœ… K-Fold ë¶„í• 
- âœ… Stratified K-Fold ë¶„í•  (ëŒ€í™” ê¸¸ì´/í† í”½ ê¸°ë°˜)
- âœ… Fold ê²°ê³¼ ì§‘ê³„
- âœ… ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

---

## ğŸ—ï¸ KFoldSplitter

### íŒŒì¼ ìœ„ì¹˜
```
src/validation/kfold.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== KFoldSplitter í´ë˜ìŠ¤ ì •ì˜ ==================== #
class KFoldSplitter:
    # ---------------------- ì´ˆê¸°í™” í•¨ìˆ˜ ---------------------- #
    def __init__(n_splits=5, shuffle=True, random_state=42, stratified=False)

    # ---------------------- ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ---------------------- #
    def split(data, stratify_column=None)
```

### ì£¼ìš” ê¸°ëŠ¥

#### 1. ê¸°ë³¸ K-Fold

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation.kfold import KFoldSplitter

# ---------------------- K-Fold Splitter ì´ˆê¸°í™” ---------------------- #
splitter = KFoldSplitter(
    n_splits=5,                     # 5ê°œ foldë¡œ ë¶„í• 
    shuffle=True,                   # ë°ì´í„° ì…”í”Œ
    random_state=42                 # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
)

# ---------------------- ë°ì´í„° ë¶„í•  ì‹¤í–‰ ---------------------- #
folds = splitter.split(data)
# ê²°ê³¼: [(train_df1, val_df1), (train_df2, val_df2), ...]
```

**íŠ¹ì§•:**
- ë°ì´í„°ë¥¼ Kê°œ foldë¡œ ê· ë“± ë¶„í• 
- ê° foldì—ì„œ K-1ê°œëŠ” í•™ìŠµ, 1ê°œëŠ” ê²€ì¦
- ëª¨ë“  ë°ì´í„°ê°€ ì •í™•íˆ í•œ ë²ˆì”© ê²€ì¦ì— ì‚¬ìš©ë¨

**ì˜ˆì‹œ (100ê°œ ë°ì´í„°, 5-Fold):**
```
Fold 1: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 0-19)
Fold 2: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 20-39)
Fold 3: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 40-59)
Fold 4: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 60-79)
Fold 5: í•™ìŠµ 80ê°œ, ê²€ì¦ 20ê°œ (index 80-99)
```

---

#### 2. Stratified K-Fold (ì¸µí™” ì¶”ì¶œ)

```python
# ---------------------- Stratified K-Fold Splitter ì´ˆê¸°í™” ---------------------- #
splitter = KFoldSplitter(
    n_splits=5,                     # 5ê°œ foldë¡œ ë¶„í• 
    shuffle=True,                   # ë°ì´í„° ì…”í”Œ
    random_state=42,                # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
    stratified=True                 # ì¸µí™” ì¶”ì¶œ í™œì„±í™”
)

# ---------------------- ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™” ë¶„í•  ---------------------- #
# ëŒ€í™” ê¸¸ì´ë¥¼ 4ë¶„ìœ„ë¡œ ë‚˜ëˆ„ì–´ ê° foldì— ê· ë“± ë¶„í¬
folds = splitter.split(data, stratify_column='length')

# ---------------------- í† í”½ ê¸°ë°˜ ì¸µí™” ë¶„í•  ---------------------- #
# ê° í† í”½ì´ ëª¨ë“  foldì— ê· ë“± ë¶„í¬
folds = splitter.split(data, stratify_column='topic')
```

**ì¸µí™” ê¸°ì¤€:**

1. **ëŒ€í™” ê¸¸ì´ (`stratify_column='length'`)**
   - ëŒ€í™” ê¸¸ì´ë¥¼ 4ë¶„ìœ„ë¡œ ë‚˜ëˆ”
   - ê° foldì— ëª¨ë“  ê¸¸ì´ ë²”ìœ„ì˜ ë°ì´í„° ê³ ë¥´ê²Œ ë¶„í¬

2. **í† í”½ (`stratify_column='topic'`)**
   - ë°ì´í„°ì— 'topic' ì»¬ëŸ¼ ì¡´ì¬ ì‹œ
   - ê° foldì— ëª¨ë“  í† í”½ì´ ê· ë“±í•˜ê²Œ ë¶„í¬

**íš¨ê³¼:**
- ê° foldê°€ ì „ì²´ ë°ì´í„° ë¶„í¬ë¥¼ ëŒ€í‘œ
- ê²€ì¦ ê²°ê³¼ì˜ ì•ˆì •ì„± í–¥ìƒ

---

## ğŸ’» êµì°¨ ê²€ì¦ ì‚¬ìš© ë°©ë²•

### 1. í¸ì˜ í•¨ìˆ˜ ì‚¬ìš© (ì¶”ì²œ)

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation.kfold import create_kfold_splits
import pandas as pd

# ---------------------- ë°ì´í„° ë¡œë“œ ---------------------- #
train_df = pd.read_csv("data/raw/train.csv")

# ---------------------- 5-Fold ë¶„í•  ìƒì„± ---------------------- #
folds = create_kfold_splits(
    data=train_df,                  # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
    n_splits=5,                     # 5ê°œ foldë¡œ ë¶„í• 
    stratified=False                # ê¸°ë³¸ K-Fold (ì¸µí™” ì•ˆí•¨)
)

# ---------------------- ê° Foldë¡œ í•™ìŠµ ë° í‰ê°€ ---------------------- #
for fold_idx, (train_fold, val_fold) in enumerate(folds):
    # í˜„ì¬ fold ì •ë³´ ì¶œë ¥
    print(f"\n=== Fold {fold_idx + 1}/{len(folds)} ===")
    print(f"í•™ìŠµ ë°ì´í„°: {len(train_fold)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_fold)}ê°œ")

    # ëª¨ë¸ í•™ìŠµ
    # model.train(train_fold)

    # ëª¨ë¸ í‰ê°€
    # metrics = model.evaluate(val_fold)
```

---

### 2. ì¸µí™” ì¶”ì¶œ ì‚¬ìš©

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation.kfold import create_kfold_splits

# ---------------------- ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™” K-Fold ---------------------- #
folds = create_kfold_splits(
    data=train_df,                  # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
    n_splits=5,                     # 5ê°œ foldë¡œ ë¶„í• 
    stratified=True,                # ì¸µí™” ì¶”ì¶œ í™œì„±í™”
    stratify_column='length'        # ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™” (ìë™ìœ¼ë¡œ 4ë¶„ìœ„ ê³„ì‚°)
)
```

---

### 3. Fold ê²°ê³¼ ì§‘ê³„

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation.kfold import aggregate_fold_results

# ---------------------- ê° Fold í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ---------------------- #
fold_results = []                       # ëª¨ë“  fold ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸

for fold_idx, (train_fold, val_fold) in enumerate(folds):
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    metrics = {
        'rouge1': 0.85,                 # ROUGE-1 F1 ì ìˆ˜
        'rouge2': 0.75,                 # ROUGE-2 F1 ì ìˆ˜
        'rougeL': 0.80                  # ROUGE-L F1 ì ìˆ˜
    }
    fold_results.append(metrics)        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

# ---------------------- Fold ê²°ê³¼ ì§‘ê³„ ---------------------- #
# í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€ ê³„ì‚°
aggregated = aggregate_fold_results(fold_results)

# ---------------------- ì§‘ê³„ ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(f"ROUGE-1: {aggregated['rouge1_mean']:.4f} (Â±{aggregated['rouge1_std']:.4f})")
print(f"  - Min: {aggregated['rouge1_min']:.4f}")
print(f"  - Max: {aggregated['rouge1_max']:.4f}")
```

**ì§‘ê³„ ê²°ê³¼:**
```
{
    'rouge1_mean': 0.8600,
    'rouge1_std': 0.0141,
    'rouge1_min': 0.8400,
    'rouge1_max': 0.8800,
    'rouge2_mean': 0.7600,
    'rouge2_std': 0.0141,
    ...
}
```

---

### 4. ì „ì²´ êµì°¨ ê²€ì¦ íŒŒì´í”„ë¼ì¸

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.config import load_config
from src.validation.kfold import create_kfold_splits, aggregate_fold_results
from src.models.model_loader import load_model_and_tokenizer
from src.data.preprocessor import create_dataset
from src.training.trainer import create_trainer
from src.evaluation.metrics import compute_metrics
import pandas as pd

# ==================== 1. Config ë¡œë“œ ==================== #
config = load_config("baseline_kobart")

# ==================== 2. ë°ì´í„° ë¡œë“œ ==================== #
train_df = pd.read_csv(config.data.train_path)

# ==================== 3. K-Fold ë¶„í•  ==================== #
folds = create_kfold_splits(
    data=train_df,                  # í•™ìŠµ ë°ì´í„°í”„ë ˆì„
    n_splits=5,                     # 5ê°œ foldë¡œ ë¶„í• 
    stratified=True,                # ì¸µí™” ì¶”ì¶œ í™œì„±í™”
    stratify_column='length'        # ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™”
)

# ==================== 4. ê° Foldë¡œ í•™ìŠµ ë° í‰ê°€ ==================== #
fold_results = []                   # ëª¨ë“  fold ê²°ê³¼ ì €ì¥

for fold_idx, (train_fold, val_fold) in enumerate(folds):
    # -------------- í˜„ì¬ Fold ì •ë³´ ì¶œë ¥ -------------- #
    print(f"\n{'='*60}")
    print(f"Fold {fold_idx + 1}/{len(folds)}")
    print(f"{'='*60}")

    # -------------- ëª¨ë¸ ì´ˆê¸°í™” -------------- #
    model, tokenizer = load_model_and_tokenizer(config)

    # -------------- Dataset ìƒì„± -------------- #
    # í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = create_dataset(
        train_fold['dialogue'].tolist(),        # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        train_fold['summary'].tolist(),         # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        tokenizer,                              # í† í¬ë‚˜ì´ì €
        config                                  # ì„¤ì •
    )

    # ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
    val_dataset = create_dataset(
        val_fold['dialogue'].tolist(),          # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        val_fold['summary'].tolist(),           # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        tokenizer,                              # í† í¬ë‚˜ì´ì €
        config                                  # ì„¤ì •
    )

    # -------------- Trainer ìƒì„± -------------- #
    trainer = create_trainer(
        config,                     # ì„¤ì •
        model,                      # ëª¨ë¸
        tokenizer,                  # í† í¬ë‚˜ì´ì €
        train_dataset,              # í•™ìŠµ ë°ì´í„°ì…‹
        val_dataset                 # ê²€ì¦ ë°ì´í„°ì…‹
    )

    # -------------- í•™ìŠµ ì‹¤í–‰ -------------- #
    trainer.train()

    # -------------- í‰ê°€ ì‹¤í–‰ -------------- #
    eval_results = trainer.evaluate()
    fold_results.append(eval_results)           # ê²°ê³¼ ì €ì¥

# ==================== 5. ê²°ê³¼ ì§‘ê³„ ==================== #
aggregated = aggregate_fold_results(fold_results)

# -------------- ìµœì¢… ê²°ê³¼ ì¶œë ¥ -------------- #
print(f"\n{'='*60}")
print("êµì°¨ ê²€ì¦ ìµœì¢… ê²°ê³¼")
print(f"{'='*60}")
print(f"ROUGE-1: {aggregated['rouge1_mean']:.4f} (Â±{aggregated['rouge1_std']:.4f})")
print(f"ROUGE-2: {aggregated['rouge2_mean']:.4f} (Â±{aggregated['rouge2_std']:.4f})")
print(f"ROUGE-L: {aggregated['rougeL_mean']:.4f} (Â±{aggregated['rougeL_std']:.4f})")
```

---

## ğŸ”§ êµì°¨ ê²€ì¦ ì‹¤í–‰ ëª…ë ¹ì–´

### Config ì„¤ì •

**íŒŒì¼:** `configs/experiments/baseline_kobart.yaml`

```yaml
# ------------------------- êµì°¨ ê²€ì¦ ì„¤ì • ------------------------- #
validation:
  use_kfold: true                               # K-Fold êµì°¨ ê²€ì¦ í™œì„±í™”
  n_splits: 5                                   # 5ê°œ foldë¡œ ë¶„í• 
  stratified: true                              # ì¸µí™” ì¶”ì¶œ í™œì„±í™”
  stratify_column: 'length'                     # ëŒ€í™” ê¸¸ì´ ê¸°ë°˜ ì¸µí™”
```

---

### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— í†µí•©

êµì°¨ ê²€ì¦ì„ ì§€ì›í•˜ëŠ” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ):

```bash
# K-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰
# --experiment: ì‹¤í—˜ config ì´ë¦„
# --mode: kfold ëª¨ë“œ
# --n_splits: fold ê°œìˆ˜
python scripts/train.py --experiment baseline_kobart --mode kfold --n_splits 5
```

---

# ğŸ“Œ Part 3: Optuna ìµœì í™”

## ğŸ“ ê°œìš”

### ëª©ì 
- Bayesian Optimizationì„ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
- NLP íŠ¹í™” íƒìƒ‰ ê³µê°„ ì •ì˜ (LoRA, Generation íŒŒë¼ë¯¸í„° ë“±)
- ì¡°ê¸° ì¢…ë£Œë¥¼ í†µí•œ íš¨ìœ¨ì  íƒìƒ‰
- ROUGE ì ìˆ˜ ê¸°ë°˜ ìµœì í™”

### í•µì‹¬ ê¸°ëŠ¥
- âœ… TPE (Tree-structured Parzen Estimator) Sampler
- âœ… Median Prunerë¥¼ í†µí•œ ì¡°ê¸° ì¢…ë£Œ
- âœ… 15ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ íƒìƒ‰
- âœ… ìµœì  íŒŒë¼ë¯¸í„° ìë™ ì €ì¥
- âœ… ì‹œê°í™” ì§€ì› (Plotly)

---

## ğŸ”§ OptunaOptimizer í´ë˜ìŠ¤

### íŒŒì¼ ìœ„ì¹˜
```
src/optimization/optuna_optimizer.py
```

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== OptunaOptimizer í´ë˜ìŠ¤ ì •ì˜ ==================== #
class OptunaOptimizer:
    # ---------------------- ì´ˆê¸°í™” í•¨ìˆ˜ ---------------------- #
    def __init__(config, train_dataset, val_dataset, n_trials, ...)

    # ---------------------- íƒìƒ‰ ê³µê°„ ìƒì„± ---------------------- #
    def create_search_space(trial)

    # ---------------------- ëª©ì  í•¨ìˆ˜ (ìµœì í™” ëŒ€ìƒ) ---------------------- #
    def objective(trial)

    # ---------------------- ìµœì í™” ì‹¤í–‰ ---------------------- #
    def optimize()

    # ---------------------- ìµœì  íŒŒë¼ë¯¸í„° ì¡°íšŒ ---------------------- #
    def get_best_params()

    # ---------------------- ìµœì  ì ìˆ˜ ì¡°íšŒ ---------------------- #
    def get_best_value()

    # ---------------------- ê²°ê³¼ ì €ì¥ ---------------------- #
    def save_results(output_path)

    # ---------------------- ìµœì í™” íˆìŠ¤í† ë¦¬ ì‹œê°í™” ---------------------- #
    def plot_optimization_history(output_path)
```

### ì´ˆê¸°í™”

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.optimization import OptunaOptimizer
from src.data import load_and_preprocess_data

# ==================== ë°ì´í„° ë¡œë“œ ==================== #
train_df, val_df = load_and_preprocess_data(
    train_path,                     # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    split_ratio=0.9                 # í•™ìŠµ:ê²€ì¦ = 9:1
)

# ==================== Config ë¡œë“œ ==================== #
from src.config import ConfigLoader
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ==================== ë°ì´í„°ì…‹ ìƒì„± ==================== #
from src.data import DialogueSummarizationDataset

# í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),      # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    train_df['summary'].tolist(),       # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer,                          # í† í¬ë‚˜ì´ì €
    config                              # ì„¤ì •
)

# ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),        # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    val_df['summary'].tolist(),         # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer,                          # í† í¬ë‚˜ì´ì €
    config                              # ì„¤ì •
)

# ==================== Optimizer ì´ˆê¸°í™” ==================== #
optimizer = OptunaOptimizer(
    config=config,                      # ì„¤ì • íŒŒì¼
    train_dataset=train_dataset,        # í•™ìŠµ ë°ì´í„°ì…‹
    val_dataset=val_dataset,            # ê²€ì¦ ë°ì´í„°ì…‹
    n_trials=50,                        # 50íšŒ ì‹œë„
    timeout=None,                       # ë¬´ì œí•œ ì‹œê°„
    study_name="kobart_optuna",         # Study ì´ë¦„
    direction="maximize"                # ROUGE ìµœëŒ€í™” ëª©í‘œ
)
```

---

## ğŸ” íƒìƒ‰ ê³µê°„ ì •ì˜ (15ê°œ íŒŒë¼ë¯¸í„°)

OptunaOptimizerëŠ” **15ê°œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°**ë¥¼ ë™ì‹œì— ìµœì í™”í•©ë‹ˆë‹¤.

### íŒŒë¼ë¯¸í„° ìš”ì•½ í…Œì´ë¸”

| ì¹´í…Œê³ ë¦¬ | íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | íƒ€ì… | ìš°ì„ ìˆœìœ„ |
|---------|---------|----------|------|---------|
| **LoRA** | lora_r | [8, 16, 32, 64] | Categorical | ë†’ìŒ |
| **LoRA** | lora_alpha | [16, 32, 64, 128] | Categorical | ë†’ìŒ |
| **LoRA** | lora_dropout | 0.0 ~ 0.2 | Float | ì¤‘ê°„ |
| **í•™ìŠµ** | learning_rate | 1e-6 ~ 1e-4 (log) | Float | ë§¤ìš° ë†’ìŒ |
| **í•™ìŠµ** | batch_size | [8, 16, 32, 64] | Categorical | ë†’ìŒ |
| **í•™ìŠµ** | num_epochs | 3 ~ 10 | Integer | ë†’ìŒ |
| **í•™ìŠµ** | warmup_ratio | 0.0 ~ 0.2 | Float | ì¤‘ê°„ |
| **í•™ìŠµ** | weight_decay | 0.0 ~ 0.1 | Float | ì¤‘ê°„ |
| **Scheduler** | scheduler_type | [linear, cosine, cosine_with_restarts, polynomial] | Categorical | ì¤‘ê°„ |
| **Generation** | temperature | 0.1 ~ 1.0 | Float | ë†’ìŒ |
| **Generation** | top_p | 0.5 ~ 1.0 | Float | ì¤‘ê°„ |
| **Generation** | num_beams | [2, 4, 6, 8] | Categorical | ë§¤ìš° ë†’ìŒ |
| **Generation** | length_penalty | 0.5 ~ 2.0 | Float | ë†’ìŒ |
| **Dropout** | hidden_dropout | 0.0 ~ 0.3 | Float | ë‚®ìŒ |
| **Dropout** | attention_dropout | 0.0 ~ 0.3 | Float | ë‚®ìŒ |

---

### 1. LoRA íŒŒë¼ë¯¸í„° (3ê°œ)

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|----------|------|-------|
| lora_r | [8, 16, 32, 64] | LoRA rank (ì €ì°¨ì› í–‰ë ¬ í¬ê¸°) | 16 or 32 |
| lora_alpha | [16, 32, 64, 128] | LoRA scaling factor (Î±/rì´ í•™ìŠµ ê°•ë„ ê²°ì •) | 32 or 64 |
| lora_dropout | 0.0 ~ 0.2 | LoRA ë ˆì´ì–´ì˜ dropout ë¹„ìœ¨ | 0.05 ~ 0.1 |

**ì˜í–¥:**
- `lora_r`: í´ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€, ë©”ëª¨ë¦¬/ì‹œê°„ ì¦ê°€
- `lora_alpha`: í´ìˆ˜ë¡ LoRA ê°€ì¤‘ì¹˜ ì˜í–¥ ì¦ê°€
- `lora_dropout`: ê³¼ì í•© ë°©ì§€

**ì½”ë“œ:**
```python
# íŒŒì¼: src/optimization/optuna_optimizer.py (96-98ë²ˆ ì¤„)

# ---------------------- LoRA íŒŒë¼ë¯¸í„° íƒìƒ‰ ---------------------- #
if config.training.use_lora:
    params['lora_r'] = trial.suggest_categorical('lora_r', [8, 16, 32, 64])                      # LoRA rank
    params['lora_alpha'] = trial.suggest_categorical('lora_alpha', [16, 32, 64, 128])            # LoRA scaling
    params['lora_dropout'] = trial.suggest_float('lora_dropout', 0.0, 0.2)                       # LoRA dropout
```

---

### 2. í•™ìŠµ íŒŒë¼ë¯¸í„° (5ê°œ)

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|----------|------|-------|
| learning_rate | 1e-6 ~ 1e-4 (log scale) | Optimizer í•™ìŠµë¥  | 2e-5 ~ 5e-5 |
| batch_size | [8, 16, 32, 64] | ë°°ì¹˜ í¬ê¸° | 16 or 32 |
| num_epochs | 3 ~ 10 | ì „ì²´ ì—í¬í¬ ìˆ˜ | 5 ~ 7 |
| warmup_ratio | 0.0 ~ 0.2 | Learning rate warmup ë¹„ìœ¨ | 0.05 ~ 0.1 |
| weight_decay | 0.0 ~ 0.1 | L2 ì •ê·œí™” ê°•ë„ | 0.01 ~ 0.05 |

**ì˜í–¥:**
- `learning_rate`: ê°€ì¥ ì¤‘ìš”! ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¼
- `batch_size`: í´ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
- `num_epochs`: ë§ì„ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥í•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜
- `warmup_ratio`: ì´ˆë°˜ í•™ìŠµ ì•ˆì •ì„±
- `weight_decay`: ê³¼ì í•© ë°©ì§€

**ì½”ë“œ:**
```python
# íŒŒì¼: src/optimization/optuna_optimizer.py (101-105ë²ˆ ì¤„)

# ---------------------- í•™ìŠµ íŒŒë¼ë¯¸í„° íƒìƒ‰ ---------------------- #
params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)            # í•™ìŠµë¥  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
params['batch_size'] = trial.suggest_categorical('batch_size', [8, 16, 32, 64])                 # ë°°ì¹˜ í¬ê¸°
params['num_epochs'] = trial.suggest_int('num_epochs', 3, 10)                                   # ì—í¬í¬ ìˆ˜
params['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)                          # Warmup ë¹„ìœ¨
params['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)                          # ê°€ì¤‘ì¹˜ ê°ì‡ 
```

**log=Trueì˜ ì˜ë¯¸:**
```python
# ë¡œê·¸ ìŠ¤ì¼€ì¼ íƒìƒ‰ (ê· ë“±í•˜ê²Œ ì‘ì€ ê°’ íƒìƒ‰)
# 1e-6, 3e-6, 1e-5, 3e-5, 1e-4 ë“±
# ì¼ë°˜ ìŠ¤ì¼€ì¼ì€ 1e-6, 2.5e-5, 5e-5, 7.5e-5, 1e-4 ë“±ìœ¼ë¡œ í° ê°’ì— í¸í–¥
```

---

### 3. Scheduler íŒŒë¼ë¯¸í„° (1ê°œ)

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|----------|------|-------|
| scheduler_type | [linear, cosine, cosine_with_restarts, polynomial] | Learning rate scheduler ì¢…ë¥˜ | cosine |

**Scheduler ì¢…ë¥˜ ë¹„êµ:**

| Scheduler | íŠ¹ì§• | ìˆ˜ì‹ | ê¶Œì¥ ì‚¬ìš© |
|-----------|------|------|----------|
| `linear` | ì„ í˜• ê°ì†Œ | `lr = lr_init Ã— (1 - step/total)` | ì§§ì€ í•™ìŠµ |
| `cosine` | Cosine ê³¡ì„  ê°ì†Œ | `lr = lr_min + 0.5 Ã— (lr_init - lr_min) Ã— (1 + cos(Ï€ Ã— step/total))` | **ì¼ë°˜ì  ê¶Œì¥** |
| `cosine_with_restarts` | ì£¼ê¸°ì  ì¬ì‹œì‘ | Cosine + ì£¼ê¸°ì ìœ¼ë¡œ lr ì¦ê°€ | ê¸´ í•™ìŠµ, ë‹¤ì–‘í•œ ìµœì†Œê°’ íƒìƒ‰ |
| `polynomial` | ë‹¤í•­ì‹ ê°ì†Œ | `lr = lr_init Ã— (1 - step/total)^power` | ë¹ ë¥¸ ì´ˆë°˜ ê°ì†Œ |

**ì½”ë“œ:**
```python
# íŒŒì¼: src/optimization/optuna_optimizer.py (108-111ë²ˆ ì¤„)

# ---------------------- Scheduler íŒŒë¼ë¯¸í„° íƒìƒ‰ ---------------------- #
params['scheduler_type'] = trial.suggest_categorical(
    'scheduler_type',                                       # Learning rate scheduler ì¢…ë¥˜
    ['linear', 'cosine', 'cosine_with_restarts', 'polynomial']
)
```

**ì‹œê°í™”:**
```
Linear:          â•²
                  â•²
                   â•²

Cosine:          â•²
                  â•²___
                     â•²__

Cosine w/ Restarts: â•²  â•±â•²  â•±â•²
                     â•²â•±  â•²â•±  â•²

Polynomial:      â•²___
                    â•²__
                      â•²_
```

---

### 4. Generation íŒŒë¼ë¯¸í„° (4ê°œ)

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|----------|------|-------|
| temperature | 0.1 ~ 1.0 | ìƒì„± í™•ë¥  ë¶„í¬ í‰í™œí™” ì •ë„ | 0.7 ~ 0.9 |
| top_p | 0.5 ~ 1.0 | Nucleus sampling (ìƒìœ„ p% ëˆ„ì  í™•ë¥  í† í°ë§Œ ì‚¬ìš©) | 0.9 ~ 0.95 |
| num_beams | [2, 4, 6, 8] | Beam search ë¹” ê°œìˆ˜ | 4 or 6 |
| length_penalty | 0.5 ~ 2.0 | ìƒì„± ê¸¸ì´ íŒ¨ë„í‹° (>1: ê¸¸ê²Œ, <1: ì§§ê²Œ) | 1.0 ~ 1.5 |

**ì˜í–¥:**
- `temperature`:
  - ë‚®ìŒ (0.1): ê²°ì •ì  ìƒì„± (í•­ìƒ ìµœê³  í™•ë¥  í† í°)
  - ë†’ìŒ (1.0): ë‹¤ì–‘í•œ ìƒì„± (í™•ë¥  ë¶„í¬ ê·¸ëŒ€ë¡œ)
- `top_p`:
  - ë‚®ìŒ (0.5): ì•ˆì „í•˜ì§€ë§Œ ë‹¨ì¡°ë¡œì›€
  - ë†’ìŒ (0.95): ë‹¤ì–‘í•˜ì§€ë§Œ ë¶ˆì•ˆì •
- `num_beams`:
  - ë§ì„ìˆ˜ë¡ í’ˆì§ˆ í–¥ìƒ, ì†ë„ ëŠë¦¼
  - 2: ë¹ ë¦„, 8: ëŠë¦¼
- `length_penalty`:
  - <1.0: ì§§ì€ ìš”ì•½ ì„ í˜¸
  - >1.0: ê¸´ ìš”ì•½ ì„ í˜¸

**ì½”ë“œ:**
```python
# íŒŒì¼: src/optimization/optuna_optimizer.py (114-117ë²ˆ ì¤„)

# ---------------------- Generation íŒŒë¼ë¯¸í„° íƒìƒ‰ ---------------------- #
params['temperature'] = trial.suggest_float('temperature', 0.1, 1.0)                             # ìƒì„± ì˜¨ë„
params['top_p'] = trial.suggest_float('top_p', 0.5, 1.0)                                        # Nucleus sampling
params['num_beams'] = trial.suggest_categorical('num_beams', [2, 4, 6, 8])                      # Beam ê°œìˆ˜
params['length_penalty'] = trial.suggest_float('length_penalty', 0.5, 2.0)                      # ê¸¸ì´ íŒ¨ë„í‹°
```

**ì˜ˆì‹œ:**
```python
# Temperature íš¨ê³¼
# temp=0.3: "ë‘ ì‚¬ëŒì´ ì €ë… ì‹ì‚¬ ì•½ì†ì„ ì •í–ˆë‹¤" (í•­ìƒ ë™ì¼)
# temp=1.0: "ë‘ ì‚¬ëŒì€ ì €ë…ì— ë§Œë‚˜ê¸°ë¡œ í–ˆë‹¤" (ë‹¤ì–‘í•œ í‘œí˜„)

# Length penalty íš¨ê³¼
# penalty=0.5: "ì €ë… ì•½ì†" (ë§¤ìš° ì§§ìŒ)
# penalty=1.0: "ë‘ ì‚¬ëŒì´ ì €ë… ì•½ì†ì„ ì •í–ˆë‹¤" (ì ì ˆ)
# penalty=2.0: "ë‘ ì‚¬ëŒì´ ì˜¤ëŠ˜ ì €ë… ì‹ì‚¬ë¥¼ í•¨ê»˜ í•˜ê¸°ë¡œ ì•½ì†ì„ ì •í–ˆë‹¤" (ê¸¸ìŒ)
```

---

### 5. Dropout íŒŒë¼ë¯¸í„° (2ê°œ)

| íŒŒë¼ë¯¸í„° | íƒìƒ‰ ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|----------|------|-------|
| hidden_dropout | 0.0 ~ 0.3 | Hidden layer dropout ë¹„ìœ¨ | 0.1 ~ 0.15 |
| attention_dropout | 0.0 ~ 0.3 | Attention layer dropout ë¹„ìœ¨ | 0.1 ~ 0.15 |

**ì˜í–¥:**
- ê³¼ì í•© ë°©ì§€
- ë„ˆë¬´ ë†’ìœ¼ë©´ í•™ìŠµ ë¶ˆì•ˆì •
- ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì 

**ì½”ë“œ:**
```python
# íŒŒì¼: src/optimization/optuna_optimizer.py (121-122ë²ˆ ì¤„)

# ---------------------- Dropout íŒŒë¼ë¯¸í„° íƒìƒ‰ ---------------------- #
# ëª¨ë¸ì´ dropoutì„ ì§€ì›í•˜ëŠ” ê²½ìš°ì—ë§Œ íƒìƒ‰
if config.model.get('hidden_dropout_prob') is not None:
    params['hidden_dropout'] = trial.suggest_float('hidden_dropout', 0.0, 0.3)                   # Hidden layer dropout
    params['attention_dropout'] = trial.suggest_float('attention_dropout', 0.0, 0.3)             # Attention dropout
```

**Dropout ì‘ë™ ì›ë¦¬:**
```
ì…ë ¥: [1.0, 2.0, 3.0, 4.0]
dropout=0.2 ì ìš©:
â†’ [1.0, 0.0, 3.0, 4.0]  (20% í™•ë¥ ë¡œ 0ìœ¼ë¡œ ë§Œë“¦)
â†’ [1.25, 0.0, 3.75, 5.0]  (ë‚˜ë¨¸ì§€ë¥¼ 1/(1-0.2)ë°° ì¦í­)
```

---

### íŒŒë¼ë¯¸í„° ìš°ì„ ìˆœìœ„ ê°€ì´ë“œ

ìµœì í™” ì‹œê°„ì´ ì œí•œì ì¼ ë•Œ íƒìƒ‰ ê³µê°„ ì¶•ì†Œ ê¶Œì¥:

#### ğŸ”´ í•„ìˆ˜ (5ê°œ) - í•­ìƒ ìµœì í™”
1. `learning_rate` - ê°€ì¥ ì¤‘ìš”
2. `num_beams` - ì„±ëŠ¥ì— í° ì˜í–¥
3. `lora_r` - LoRA ì‚¬ìš© ì‹œ
4. `lora_alpha` - LoRA ì‚¬ìš© ì‹œ
5. `batch_size` - ë©”ëª¨ë¦¬ì— ë”°ë¼

#### ğŸŸ¡ ê¶Œì¥ (5ê°œ) - ì‹œê°„ ìˆìœ¼ë©´ ìµœì í™”
6. `temperature` - ìƒì„± í’ˆì§ˆ
7. `num_epochs` - í•™ìŠµ ì •ë„
8. `length_penalty` - ìš”ì•½ ê¸¸ì´
9. `warmup_ratio` - í•™ìŠµ ì•ˆì •ì„±
10. `scheduler_type` - í•™ìŠµ ê³¡ì„ 

#### ğŸŸ¢ ì„ íƒ (5ê°œ) - ì—¬ìœ  ìˆìœ¼ë©´ ìµœì í™”
11. `lora_dropout` - ë¯¸ì„¸ ì¡°ì •
12. `weight_decay` - ì •ê·œí™”
13. `top_p` - ìƒì„± ë‹¤ì–‘ì„±
14. `hidden_dropout` - ê³¼ì í•© ë°©ì§€
15. `attention_dropout` - ê³¼ì í•© ë°©ì§€

**ë¹ ë¥¸ ìµœì í™” (5ê°œ íŒŒë¼ë¯¸í„°):**
```python
def create_search_space_fast(trial):
    return {
        'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True),
        'num_beams': trial.suggest_categorical('num_beams', [2, 4, 6, 8]),
        'lora_r': trial.suggest_categorical('lora_r', [8, 16, 32, 64]),
        'lora_alpha': trial.suggest_categorical('lora_alpha', [16, 32, 64, 128]),
        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64])
    }
```

**ì „ì²´ ìµœì í™” (15ê°œ íŒŒë¼ë¯¸í„°):**
```python
# src/optimization/optuna_optimizer.pyì˜ create_search_space() ë©”ì„œë“œ ì‚¬ìš©
```

---

## âš¡ ìµœì í™” ì „ëµ

### 1. Bayesian Optimization (TPE)

**íŠ¹ì§•:**
- Tree-structured Parzen Estimator
- ì´ì „ trial ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ íƒìƒ‰ ìœ„ì¹˜ ê²°ì •
- Random searchë³´ë‹¤ íš¨ìœ¨ì 

**ì„¤ì •:**
```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from optuna.samplers import TPESampler

# ---------------------- TPE Sampler ìƒì„± ---------------------- #
sampler = TPESampler(seed=42)           # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
```

---

### 2. Median Pruner (ì¡°ê¸° ì¢…ë£Œ)

**íŠ¹ì§•:**
- ì¤‘ê°„ ê²°ê³¼ê°€ medianë³´ë‹¤ ë‚®ìœ¼ë©´ trial ì¢…ë£Œ
- ë¦¬ì†ŒìŠ¤ ì ˆì•½ (ë¶ˆí•„ìš”í•œ trial ì¡°ê¸° ì¤‘ë‹¨)

**ì„¤ì •:**
```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from optuna.pruners import MedianPruner

# ---------------------- Median Pruner ìƒì„± ---------------------- #
pruner = MedianPruner(
    n_startup_trials=5,                 # ì²˜ìŒ 5ê°œ trialì€ pruning ì•ˆí•¨
    n_warmup_steps=3,                   # 3 ì—í¬í¬ í›„ë¶€í„° ì²´í¬ ì‹œì‘
    interval_steps=1                    # ë§¤ ì—í¬í¬ë§ˆë‹¤ ì²´í¬
)
```

**ë™ì‘ ë°©ì‹:**
```
Trial 0: [ì—í¬í¬1: 0.30] [ì—í¬í¬2: 0.32] [ì—í¬í¬3: 0.35] â†’ ê³„ì†
Trial 1: [ì—í¬í¬1: 0.28] [ì—í¬í¬2: 0.29] [ì—í¬í¬3: 0.30] â†’ ê³„ì†
Trial 2: [ì—í¬í¬1: 0.25] [ì—í¬í¬2: 0.26] [ì—í¬í¬3: 0.27] â†’ Pruned! (median=0.32ë³´ë‹¤ ë‚®ìŒ)
```

---

### 3. ëª©ì  í•¨ìˆ˜ (Objective Function)

**ëª©í‘œ:** ROUGE-L F1 ì ìˆ˜ ìµœëŒ€í™”

**íë¦„:**
1. Trialì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
2. Config ì—…ë°ì´íŠ¸
3. ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ
4. ê²€ì¦ ë°ì´í„° í‰ê°€
5. ROUGE-L F1 ë°˜í™˜

**ì½”ë“œ:**
```python
# ---------------------- ëª©ì  í•¨ìˆ˜ ì •ì˜ ---------------------- #
def objective(self, trial: optuna.Trial) -> float:
    # -------------- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§ -------------- #
    params = self.create_search_space(trial)

    # -------------- 2. Config ì—…ë°ì´íŠ¸ -------------- #
    config.training.learning_rate = params['learning_rate']         # í•™ìŠµë¥  ì ìš©
    config.training.batch_size = params['batch_size']               # ë°°ì¹˜ í¬ê¸° ì ìš©
    # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

    # -------------- 3. ëª¨ë¸ í•™ìŠµ -------------- #
    # ëª¨ë¸ ë¡œë“œ
    model_loader = ModelLoader(config)
    model, tokenizer = model_loader.load()

    # Trainer ìƒì„± ë° í•™ìŠµ ì‹¤í–‰
    trainer = ModelTrainer(...)
    trainer.train()

    # -------------- 4. í‰ê°€ -------------- #
    metrics = trainer.evaluate()                                    # ê²€ì¦ ë°ì´í„° í‰ê°€
    rouge_l_f1 = metrics['rouge_l_f1']                              # ROUGE-L F1 ì¶”ì¶œ

    # -------------- 5. Pruning ì²´í¬ -------------- #
    trial.report(rouge_l_f1, step=config.training.num_epochs)       # ì¤‘ê°„ ê²°ê³¼ ë³´ê³ 
    if trial.should_prune():                                        # Pruning ì¡°ê±´ í™•ì¸
        raise optuna.TrialPruned()                                  # Trial ì¡°ê¸° ì¢…ë£Œ

    return rouge_l_f1                                               # ìµœì¢… ì ìˆ˜ ë°˜í™˜
```

---

## ğŸ’» Optuna ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ìµœì í™”

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.optimization import OptunaOptimizer
from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset

# ==================== Config ë¡œë“œ ==================== #
config_loader = ConfigLoader()
config = config_loader.load("baseline_kobart")

# ==================== ë°ì´í„° ë¡œë“œ ==================== #
train_df, val_df = load_and_preprocess_data(
    "data/raw/train.csv",               # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    split_ratio=0.9                     # í•™ìŠµ:ê²€ì¦ = 9:1
)

# ==================== í† í¬ë‚˜ì´ì € ë¡œë“œ ==================== #
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(config.model.name)

# ==================== ë°ì´í„°ì…‹ ìƒì„± ==================== #
# í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
train_dataset = DialogueSummarizationDataset(
    train_df['dialogue'].tolist(),      # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    train_df['summary'].tolist(),       # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer,                          # í† í¬ë‚˜ì´ì €
    config                              # ì„¤ì •
)

# ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
val_dataset = DialogueSummarizationDataset(
    val_df['dialogue'].tolist(),        # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    val_df['summary'].tolist(),         # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
    tokenizer,                          # í† í¬ë‚˜ì´ì €
    config                              # ì„¤ì •
)

# ==================== Optimizer ì´ˆê¸°í™” ==================== #
optimizer = OptunaOptimizer(
    config=config,                      # ì„¤ì • íŒŒì¼
    train_dataset=train_dataset,        # í•™ìŠµ ë°ì´í„°ì…‹
    val_dataset=val_dataset,            # ê²€ì¦ ë°ì´í„°ì…‹
    n_trials=50                         # 50íšŒ ì‹œë„
)

# ==================== ìµœì í™” ì‹¤í–‰ ==================== #
study = optimizer.optimize()

# ==================== ìµœì  íŒŒë¼ë¯¸í„° í™•ì¸ ==================== #
best_params = optimizer.get_best_params()       # ìµœì  íŒŒë¼ë¯¸í„° ì¡°íšŒ
best_value = optimizer.get_best_value()         # ìµœì  ì ìˆ˜ ì¡°íšŒ

# ê²°ê³¼ ì¶œë ¥
print(f"ìµœì  ROUGE-L F1: {best_value:.4f}")
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
```

---

### 2. ê²°ê³¼ ì €ì¥

```python
# ---------------------- ê²°ê³¼ ì €ì¥ ---------------------- #
optimizer.save_results("outputs/optuna_results")

# ì €ì¥ë˜ëŠ” íŒŒì¼:
# - outputs/optuna_results/best_params.json    : ìµœì  íŒŒë¼ë¯¸í„°
# - outputs/optuna_results/all_trials.csv      : ëª¨ë“  trial ê²°ê³¼
# - outputs/optuna_results/study_stats.json    : Study í†µê³„
```

**best_params.json ì˜ˆì‹œ:**
```json
{
  "best_params": {
    "learning_rate": 3.5e-05,           // ìµœì  í•™ìŠµë¥ 
    "batch_size": 32,                   // ìµœì  ë°°ì¹˜ í¬ê¸°
    "num_epochs": 5,                    // ìµœì  ì—í¬í¬ ìˆ˜
    "lora_r": 16,                       // ìµœì  LoRA rank
    "lora_alpha": 32,                   // ìµœì  LoRA alpha
    "temperature": 0.8,                 // ìµœì  ìƒì„± ì˜¨ë„
    "num_beams": 6                      // ìµœì  beam ê°œìˆ˜
  },
  "best_value": 0.4521,                 // ìµœì  ROUGE-L F1 ì ìˆ˜
  "n_trials": 50                        // ì´ trial ìˆ˜
}
```

---

## ğŸ”§ Optuna ì‹¤í–‰ ëª…ë ¹ì–´

### Optuna ìµœì í™” ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ)

**ì°¸ê³ :** Optuna ìµœì í™”ëŠ” `scripts/train.py --mode optuna`ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

```python
# ---------------------- ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
import argparse
from pathlib import Path

from src.config import ConfigLoader
from src.data import load_and_preprocess_data, DialogueSummarizationDataset
from src.optimization import OptunaOptimizer
from transformers import AutoTokenizer


# ==================== ë©”ì¸ í•¨ìˆ˜ ==================== #
def main():
    # -------------- ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± -------------- #
    parser = argparse.ArgumentParser()
    parser.add_argument("--experiment", default="baseline_kobart", help="ì‹¤í—˜ config ì´ë¦„")
    parser.add_argument("--n_trials", type=int, default=50, help="Trial íšŸìˆ˜")
    parser.add_argument("--timeout", type=int, default=None, help="ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--output_dir", default="outputs/optuna_results", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    args = parser.parse_args()

    # -------------- Config ë¡œë“œ -------------- #
    config_loader = ConfigLoader()
    config = config_loader.load(args.experiment)

    # -------------- ë°ì´í„° ë¡œë“œ -------------- #
    train_df, val_df = load_and_preprocess_data(
        "data/raw/train.csv",               # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
        split_ratio=0.9                     # í•™ìŠµ:ê²€ì¦ = 9:1
    )

    # -------------- í† í¬ë‚˜ì´ì € ë¡œë“œ -------------- #
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # -------------- ë°ì´í„°ì…‹ ìƒì„± -------------- #
    # í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = DialogueSummarizationDataset(
        train_df['dialogue'].tolist(),      # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        train_df['summary'].tolist(),       # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        tokenizer,                          # í† í¬ë‚˜ì´ì €
        config                              # ì„¤ì •
    )

    # ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
    val_dataset = DialogueSummarizationDataset(
        val_df['dialogue'].tolist(),        # ëŒ€í™” ë¦¬ìŠ¤íŠ¸
        val_df['summary'].tolist(),         # ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        tokenizer,                          # í† í¬ë‚˜ì´ì €
        config                              # ì„¤ì •
    )

    # -------------- Optimizer ì´ˆê¸°í™” -------------- #
    optimizer = OptunaOptimizer(
        config=config,                              # ì„¤ì • íŒŒì¼
        train_dataset=train_dataset,                # í•™ìŠµ ë°ì´í„°ì…‹
        val_dataset=val_dataset,                    # ê²€ì¦ ë°ì´í„°ì…‹
        n_trials=args.n_trials,                     # Trial íšŸìˆ˜
        timeout=args.timeout,                       # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„
        study_name=f"optuna_{args.experiment}"      # Study ì´ë¦„
    )

    # -------------- ìµœì í™” ì‹¤í–‰ -------------- #
    study = optimizer.optimize()

    # -------------- ê²°ê³¼ ì €ì¥ -------------- #
    optimizer.save_results(args.output_dir)

    # -------------- ì‹œê°í™” -------------- #
    try:
        optimizer.plot_optimization_history(args.output_dir)
    except ImportError:
        print("plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")

    # -------------- ìµœì¢… ê²°ê³¼ ì¶œë ¥ -------------- #
    print(f"\n{'='*60}")
    print(f"ìµœì í™” ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ìµœì  ROUGE-L F1: {optimizer.get_best_value():.4f}")
    print(f"ê²°ê³¼ ì €ì¥: {args.output_dir}")


# ==================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
# ---------------------- ê¸°ë³¸ ì‹¤í–‰ (50 trials) ---------------------- #
python scripts/train.py --experiment baseline_kobart --mode optuna --n_trials 50

# ---------------------- Trial íšŸìˆ˜ ì¡°ì • ---------------------- #
python scripts/train.py --experiment baseline_kobart --mode optuna --n_trials 100

# ---------------------- ì‹œê°„ ì œí•œ (12ì‹œê°„ = 43200ì´ˆ) ---------------------- #
python scripts/train.py --experiment baseline_kobart --mode optuna --n_trials 50 --timeout 43200

# ---------------------- ê²°ê³¼ ë””ë ‰í† ë¦¬ ì§€ì • ---------------------- #
python scripts/train.py --experiment baseline_kobart --mode optuna --n_trials 50 --output_dir outputs/kobart_optuna
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/evaluation/metrics.py` - RougeCalculator í´ë˜ìŠ¤
- `src/evaluation/__init__.py` - ì™¸ë¶€ API
- `src/validation/kfold.py` - K-Fold ì‹œìŠ¤í…œ
- `src/validation/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
- `src/optimization/optuna_optimizer.py` - Optuna optimizer
- `src/optimization/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_metrics.py` - ROUGE í…ŒìŠ¤íŠ¸
- `src/tests/test_kfold.py` - K-Fold í…ŒìŠ¤íŠ¸
- `src/tests/test_optuna.py` - Optuna í…ŒìŠ¤íŠ¸

**í†µí•©:**
- `src/training/trainer.py` - Trainerì—ì„œ ìë™ ì‚¬ìš©

**ê´€ë ¨ ë¬¸ì„œ:**
- [01_ì‹œì‘_ê°€ì´ë“œ.md](./01_ì‹œì‘_ê°€ì´ë“œ.md) - ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
- [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) - í•µì‹¬ ì‹œìŠ¤í…œ ë° Config
- [06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md](./06_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md) - ë°ì´í„° ì²˜ë¦¬ ë° ì¦ê°•
- [07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md](./07_ëª¨ë¸_í•™ìŠµ_ì¶”ë¡ .md) - ëª¨ë¸ ì‹œìŠ¤í…œ
- [04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ì „ì²´ ëª…ë ¹ì–´ ê°€ì´ë“œ

**Config:**
- `configs/base/default.yaml` - ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `configs/experiments/*.yaml` - ì‹¤í—˜ë³„ Config
