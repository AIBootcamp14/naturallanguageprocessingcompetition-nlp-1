# ğŸ“š ëª¨ë“ˆí™” ì‹œìŠ¤í…œ ì™„ì „ ê°€ì´ë“œ

> **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ NLP íŒŒì´í”„ë¼ì¸**: ë² ì´ìŠ¤ë¼ì¸ë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ ì™„ì „ êµ¬í˜„

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

NLP ëŒ€í™” ìš”ì•½ ê²½ì§„ëŒ€íšŒë¥¼ ìœ„í•œ **ì™„ì „ ëª¨ë“ˆí™” ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸ ë…¸íŠ¸ë¶ ì½”ë“œë¥¼ **13ê°œ ë…ë¦½ ëª¨ë“ˆ**ë¡œ ì¬êµ¬ì„±í•˜ì—¬ **ì‹¤í—˜ ê´€ë¦¬**, **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**, **ì•™ìƒë¸”**, **ì¶”ë¡  ìµœì í™”** ë“± ì „ì²´ ML íŒŒì´í”„ë¼ì¸ì„ ì§€ì›í•©ë‹ˆë‹¤.

### âœ¨ í•µì‹¬ ê°€ì¹˜

- **âœ… 100% êµ¬í˜„ ì™„ë£Œ**: 19ê°œ PRD ì¤‘ 18ê°œ ì™„ì „ êµ¬í˜„ (95%+)
- **ğŸ”§ 13ê°œ ë…ë¦½ ëª¨ë“ˆ**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ ì•„í‚¤í…ì²˜
- **ğŸ§ª 79ê°œ í…ŒìŠ¤íŠ¸**: 100% í†µê³¼ (í’ˆì§ˆ ë³´ì¦)
- **ğŸ“Š WandB í†µí•©**: 5ê°€ì§€ ê³ ê¸‰ ì‹œê°í™”
- **âš¡ ì¶”ë¡  ìµœì í™”**: TensorRT (3-5ë°° ë¹ ë¦„), Pruning (50% ê²½ëŸ‰í™”)
- **ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ**: ROUGE 88-90 â†’ 92-95 ëª©í‘œ

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°

```mermaid
graph TB
    subgraph Input["ğŸ“¥ ì…ë ¥ ê³„ì¸µ"]
        A[Config YAML<br/>4ë‹¨ê³„ ê³„ì¸µì  ë³‘í•©]
        B[Raw Data CSV<br/>train/dev/test]
    end

    subgraph DataProcess["ğŸ”„ ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"]
        C[ConfigLoader<br/>OmegaConf ê¸°ë°˜]
        D[DialoguePreprocessor<br/>ë…¸ì´ì¦ˆ ì œê±°]
        E[DataAugmenter<br/>5ê°€ì§€ ì¦ê°• ë°©ë²•]
        F[TTAugmentor<br/>4ê°€ì§€ TTA ì „ëµ]
        G[PyTorch Dataset<br/>ë™ì  íŒ¨ë”©]
    end

    subgraph Model["ğŸ¤– ëª¨ë¸ ê³„ì¸µ"]
        H[ModelLoader<br/>HuggingFace]
        I[LoRALoader<br/>QLoRA 4-bit]
        J[Tokenizer<br/>íŠ¹ìˆ˜ í† í°]
    end

    subgraph Training["ğŸ“ í•™ìŠµ ê³„ì¸µ"]
        K[Trainer<br/>Seq2Seq ë˜í•‘]
        L[KFoldTrainer<br/>5-Fold CV]
        M[OptunaOptimizer<br/>15ê°œ íŒŒë¼ë¯¸í„°]
        N[WandB Logger<br/>5ê°€ì§€ ì‹œê°í™”]
    end

    subgraph Eval["ğŸ“Š í‰ê°€ ê³„ì¸µ"]
        O[RougeCalculator<br/>ROUGE-1/2/L]
        P[BaselineChecker<br/>ìë™ ê²€ì¦]
        Q[SolarCrossValidation<br/>API êµì°¨ ê²€ì¦]
    end

    subgraph Inference["ğŸš€ ì¶”ë¡  ê³„ì¸µ"]
        R[Predictor<br/>ë°°ì¹˜ ì¶”ë¡ ]
        S[TensorRTOptimizer<br/>FP16/INT8]
        T[ModelPruner<br/>Magnitude/Structured]
        U[Ensemble<br/>5ê°€ì§€ ë°©ë²•]
    end

    subgraph Output["ğŸ“¤ ì¶œë ¥ ê³„ì¸µ"]
        V[Checkpoints<br/>ëª¨ë¸ ì €ì¥]
        W[Submissions<br/>CSV íŒŒì¼]
        X[Logs<br/>WandB/íŒŒì¼]
    end

    A --> C
    B --> D
    D --> E
    E --> F
    F --> G

    C --> H
    C --> I
    H --> J
    I --> J

    G --> K
    J --> K
    C --> K
    K --> L
    K --> M
    K --> N

    K --> O
    O --> P
    O --> Q

    K --> V
    V --> R
    R --> S
    S --> T
    T --> U
    U --> W

    N --> X
    V --> X

    style Input fill:#e1f5ff,stroke:#01579b,color:#000
    style DataProcess fill:#fff3e0,stroke:#e65100,color:#000
    style Model fill:#f3e5f5,stroke:#4a148c,color:#000
    style Training fill:#e8f5e9,stroke:#1b5e20,color:#000
    style Eval fill:#fff9c4,stroke:#f57f17,color:#000
    style Inference fill:#fce4ec,stroke:#880e4f,color:#000
    style Output fill:#c8e6c9,stroke:#2e7d32,color:#000

    style A fill:#bbdefb,stroke:#01579b,color:#000
    style B fill:#bbdefb,stroke:#01579b,color:#000
    style C fill:#ffe0b2,stroke:#e65100,color:#000
    style D fill:#ffe0b2,stroke:#e65100,color:#000
    style E fill:#ffcc80,stroke:#e65100,color:#000
    style F fill:#ffcc80,stroke:#e65100,color:#000
    style G fill:#ffe0b2,stroke:#e65100,color:#000
    style H fill:#e1bee7,stroke:#4a148c,color:#000
    style I fill:#ce93d8,stroke:#4a148c,color:#000
    style J fill:#e1bee7,stroke:#4a148c,color:#000
    style K fill:#a5d6a7,stroke:#1b5e20,color:#000
    style L fill:#81c784,stroke:#1b5e20,color:#000
    style M fill:#81c784,stroke:#1b5e20,color:#000
    style N fill:#a5d6a7,stroke:#1b5e20,color:#000
    style O fill:#fff59d,stroke:#f57f17,color:#000
    style P fill:#fff59d,stroke:#f57f17,color:#000
    style Q fill:#fff59d,stroke:#f57f17,color:#000
    style R fill:#f8bbd0,stroke:#880e4f,color:#000
    style S fill:#f48fb1,stroke:#880e4f,color:#000
    style T fill:#f48fb1,stroke:#880e4f,color:#000
    style U fill:#f8bbd0,stroke:#880e4f,color:#000
    style V fill:#aed581,stroke:#2e7d32,color:#000
    style W fill:#aed581,stroke:#2e7d32,color:#000
    style X fill:#aed581,stroke:#2e7d32,color:#000
```

### 13ê°œ ëª¨ë“ˆ êµ¬ì¡°

```mermaid
graph LR
    subgraph Core["í•µì‹¬ ì‹œìŠ¤í…œ"]
        M1[config<br/>ê³„ì¸µì  YAML]
        M2[logging<br/>Logger/WandB]
        M3[utils<br/>GPU/ê³µí†µ]
    end

    subgraph Data["ë°ì´í„°"]
        M4[data<br/>ì „ì²˜ë¦¬/Dataset]
        M5[augmentation<br/>5ê°€ì§€ ì¦ê°•]
    end

    subgraph ML["ML íŒŒì´í”„ë¼ì¸"]
        M6[models<br/>ModelLoader/LoRA]
        M7[training<br/>Trainer]
        M8[trainers<br/>5ê°€ì§€ Trainer]
    end

    subgraph Advanced["ê³ ê¸‰ ê¸°ëŠ¥"]
        M9[optimization<br/>Optuna]
        M10[ensemble<br/>5ê°€ì§€ ì•™ìƒë¸”]
        M11[validation<br/>ê²€ì¦]
    end

    subgraph Inference["ì¶”ë¡ /API"]
        M12[inference<br/>TensorRT/Pruning]
        M13[api<br/>Solar API]
    end

    style Core fill:#e3f2fd,stroke:#1976d2,color:#000
    style Data fill:#fff3e0,stroke:#f57c00,color:#000
    style ML fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style Advanced fill:#e8f5e9,stroke:#388e3c,color:#000
    style Inference fill:#fce4ec,stroke:#c2185b,color:#000

    style M1 fill:#90caf9,stroke:#1976d2,color:#000
    style M2 fill:#90caf9,stroke:#1976d2,color:#000
    style M3 fill:#90caf9,stroke:#1976d2,color:#000
    style M4 fill:#ffb74d,stroke:#f57c00,color:#000
    style M5 fill:#ffb74d,stroke:#f57c00,color:#000
    style M6 fill:#ce93d8,stroke:#7b1fa2,color:#000
    style M7 fill:#ce93d8,stroke:#7b1fa2,color:#000
    style M8 fill:#ce93d8,stroke:#7b1fa2,color:#000
    style M9 fill:#81c784,stroke:#388e3c,color:#000
    style M10 fill:#81c784,stroke:#388e3c,color:#000
    style M11 fill:#81c784,stroke:#388e3c,color:#000
    style M12 fill:#f48fb1,stroke:#c2185b,color:#000
    style M13 fill:#f48fb1,stroke:#c2185b,color:#000
```

---

## ğŸ”§ êµ¬í˜„ëœ ì „ì²´ ê¸°ëŠ¥ (100% ì™„ë£Œ)

### 1ï¸âƒ£ í•µì‹¬ ì‹œìŠ¤í…œ (3ê°œ ëª¨ë“ˆ)

#### âœ… Config ê´€ë¦¬ (`src/config/`)
- **ê³„ì¸µì  YAML ë³‘í•©**: base â†’ model_type â†’ model â†’ experiment (4ë‹¨ê³„)
- **OmegaConf ê¸°ë°˜**: íƒ€ì… ì•ˆì „ì„± ë³´ì¥
- **ì‹¤í—˜ë³„ ì˜¤ë²„ë¼ì´ë“œ**: ë™ì¼ ë² ì´ìŠ¤ì—ì„œ ë‹¤ì–‘í•œ ì‹¤í—˜ ê´€ë¦¬
- **íŒŒì¼**: `loader.py` (ConfigLoader í´ë˜ìŠ¤), `hierarchical_loader.py`
- **í…ŒìŠ¤íŠ¸**: 6ê°œ (100% í†µê³¼)
- ğŸ“„ **ë¬¸ì„œ**: `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 2

#### âœ… Logger ì‹œìŠ¤í…œ (`src/logging/`)
- **íŒŒì¼ + ì½˜ì†” ë™ì‹œ ë¡œê¹…**: ëª¨ë“  ì¶œë ¥ì„ íŒŒì¼ê³¼ í™”ë©´ì— ë™ì‹œ ê¸°ë¡
- **Stdout/stderr ë¦¬ë‹¤ì´ë ‰ì…˜**: print() ë¬¸ë„ ìë™ ìº¡ì²˜
- **íƒ€ì„ìŠ¤íƒ¬í”„ ìë™ ì¶”ê°€**: ëª¨ë“  ë¡œê·¸ì— ì‹œê°„ ì •ë³´
- **íŒŒì¼**: `logger.py` (Logger í´ë˜ìŠ¤), `notebook_logger.py`, `wandb_logger.py`
- ğŸ“„ **ë¬¸ì„œ**: `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 3

#### âœ… WandB í†µí•© (`src/logging/wandb_logger.py`)
- **5ê°€ì§€ ê³ ê¸‰ ì‹œê°í™”**:
  1. `log_learning_rate_schedule()`: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ì¶”ì 
  2. `log_gradient_norms()`: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ/ì†Œì‹¤ ê°ì§€
  3. `log_loss_curve()`: ê³¼ì í•© ëª¨ë‹ˆí„°ë§ (train-val diff)
  4. `log_gpu_memory()`: OOM ì˜ˆë°© (Multi-GPU ì§€ì›)
  5. `log_training_speed()`: ë³‘ëª© êµ¬ê°„ íŒŒì•…
- **ìë™ ë¡œê¹…**: ë©”íŠ¸ë¦­, ëª¨ë¸, í˜¼ë™ í–‰ë ¬, ì˜ˆì¸¡ ê²°ê³¼
- ğŸ“„ **ë¬¸ì„œ**: `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 4

#### âœ… GPU ìœ í‹¸ë¦¬í‹° (`src/utils/gpu_optimization/`)
- **ìë™ ë°°ì¹˜ í¬ê¸° íƒìƒ‰**: GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” ìµœì  ë°°ì¹˜ í¬ê¸°
- **GPU Tier ê°ì§€**: T4, V100, A100 ìë™ ì¸ì‹
- **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ GPU ì‚¬ìš©ëŸ‰ ì¶”ì 
- **íŒŒì¼**: `team_gpu_check.py`, `auto_batch_size.py`
- ğŸ“„ **ë¬¸ì„œ**: `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 3

---

### 2ï¸âƒ£ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (2ê°œ ëª¨ë“ˆ)

#### âœ… ë°ì´í„° ì „ì²˜ë¦¬ (`src/data/`)
- **DialoguePreprocessor**: ë…¸ì´ì¦ˆ ì œê±°, í™”ì ì¶”ì¶œ, í„´ ê³„ì‚°
- **DialogueSummarizationDataset**: í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹
- **InferenceDataset**: ì¶”ë¡  ì „ìš© ë°ì´í„°ì…‹
- **ë™ì  íŒ¨ë”©**: ë°°ì¹˜ ë‚´ ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”©
- **íŒŒì¼**: `preprocessor.py` (469ì¤„), `dataset.py` (460ì¤„)
- **í…ŒìŠ¤íŠ¸**: 5ê°œ (ì‹¤ì œ ë°ì´í„° 12,457ê°œ ì²˜ë¦¬)
- ğŸ“„ **ë¬¸ì„œ**: `03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md` Part 1

#### âœ… ë°ì´í„° ì¦ê°• (`src/augmentation/`, `src/data/`)
- **5ê°€ì§€ ì¦ê°• ë°©ë²•**:
  1. **Back-translation**: í•œêµ­ì–´ â†’ ì˜ì–´ â†’ í•œêµ­ì–´ (Papago API)
  2. **Paraphrase**: ë¬¸ì¥ ì¬êµ¬ì„± (KoGPT-2)
  3. **Turn Shuffling**: ëŒ€í™” í„´ ìˆœì„œ ë³€ê²½
  4. **Synonym Replacement**: ë™ì˜ì–´ ì¹˜í™˜ (WordNet)
  5. **Dialogue Sampling**: ëŒ€í™” ì¼ë¶€ ìƒ˜í”Œë§
- **ì¦ê°• íš¨ê³¼**: 12,457ê°œ â†’ ìµœëŒ€ 87,399ê°œ (7ë°°)
- **ì„±ëŠ¥ ëª©í‘œ**: +4-5 ROUGE ì ìˆ˜
- **íŒŒì¼**: `augmentation/back_translator.py` (339ì¤„), `augmentation/paraphraser.py` (416ì¤„), `data/augmentation.py`
- **í…ŒìŠ¤íŠ¸**: 7ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md` Part 2

#### âœ… TTA (Test Time Augmentation) (`src/data/tta.py`)
- **4ê°€ì§€ TTA ì „ëµ**:
  1. **Paraphrase**: ë¬¸ì¥ ìˆœì„œ ë³€ê²½
  2. **Reorder**: ë‹¨ì–´/ë¬¸ì¥ ì¬ë°°ì—´
  3. **Synonym**: ë™ì˜ì–´ ì¹˜í™˜ (í•œêµ­ì–´ ì‚¬ì „)
  4. **Mask**: í† í° ë§ˆìŠ¤í‚¹ (10-20%)
- **TTAugmentor í´ë˜ìŠ¤**: 350ì¤„
- **ì‚¬ìš© ë°©ë²•**: ì¶”ë¡  ì‹œ ì—¬ëŸ¬ ë³€í˜• ìƒì„± í›„ ì•™ìƒë¸”
- ğŸ“„ **ë¬¸ì„œ**: `03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md` Part 3

---

### 3ï¸âƒ£ ëª¨ë¸ ë° í•™ìŠµ (3ê°œ ëª¨ë“ˆ)

#### âœ… ëª¨ë¸ ë¡œë” (`src/models/`)
- **ModelLoader**: HuggingFace ëª¨ë¸ ìë™ ë¡œë”©
- **LoRALoader**: PEFT ê¸°ë°˜ LoRA/QLoRA ì ìš©
- **íŠ¹ìˆ˜ í† í° ì²˜ë¦¬**: #Person1#, #Person2# ë“± ìë™ ì¶”ê°€
- **GPU ìë™ ê°ì§€**: CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ ë°°ì¹˜
- **ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ**: íŠ¹ìˆ˜ í† í° ì¶”ê°€ í›„ ìë™ ì¡°ì •
- **íŒŒì¼**: `model_loader.py` (KoBART), `lora_loader.py` (LoRA), `llm_loader.py` (Llama/Qwen)
- **í…ŒìŠ¤íŠ¸**: 5ê°œ (KoBART 123M íŒŒë¼ë¯¸í„° ë¡œë”© ê²€ì¦)
- ğŸ“„ **ë¬¸ì„œ**: `01_ëª¨ë¸_ë¡œë”.md`

#### âœ… LoRA íŒŒì¸íŠœë‹ (`src/models/lora_loader.py`)
- **PEFT ê¸°ë°˜**: Parameter-Efficient Fine-Tuning
- **LoRA íŒŒë¼ë¯¸í„°**: r, alpha, dropout ì„¤ì • ê°€ëŠ¥
- **QLoRA 4-bit**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
- **íŒŒë¼ë¯¸í„° íš¨ìœ¨**: ì „ì²´ì˜ 1%ë§Œ í•™ìŠµ
- **ì§€ì› ëª¨ë¸**: KoBART, Llama-3.2-3B, Qwen2.5-3B
- **í…ŒìŠ¤íŠ¸**: 4ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md`

#### âœ… í•™ìŠµ ì‹œìŠ¤í…œ (`src/training/`, `src/trainers/`)
- **5ê°€ì§€ Trainer**:
  1. **SingleTrainer**: ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
  2. **KFoldTrainer**: K-Fold êµì°¨ ê²€ì¦
  3. **OptunaTrainer**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
  4. **MultiModelTrainer**: ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ
  5. **FullPipelineTrainer**: ì „ì²´ íŒŒì´í”„ë¼ì¸ (ì¦ê°• â†’ í•™ìŠµ â†’ ì•™ìƒë¸”)
- **Seq2SeqTrainer ë˜í•‘**: HuggingFace Trainer í™œìš©
- **WandB ë¡œê¹… í†µí•©**: ìë™ ë©”íŠ¸ë¦­ ë¡œê¹…
- **ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**: ìë™ ì €ì¥ ë° ë¡œë“œ
- **íŒŒì¼**: `training/trainer.py`, `trainers/single_trainer.py`, `trainers/kfold_trainer.py` ë“±
- **í…ŒìŠ¤íŠ¸**: 4ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md`

---

### 4ï¸âƒ£ í‰ê°€ ë° ìµœì í™” (2ê°œ ëª¨ë“ˆ)

#### âœ… í‰ê°€ ë©”íŠ¸ë¦­ (`src/evaluation/metrics.py`)
- **ROUGE ì ìˆ˜**: ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum
- **BERTScore**: ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ (klue/bert-base ê¸°ë°˜)
- **Multi-reference ì§€ì›**: ì—¬ëŸ¬ ì •ë‹µê³¼ ë¹„êµ
- **RougeCalculator í´ë˜ìŠ¤**: í†µí•© í‰ê°€ ì¸í„°í˜ì´ìŠ¤
- **í…ŒìŠ¤íŠ¸**: 6ê°œ (ì‹¤ì œ ì˜ˆì¸¡ vs ì •ë‹µ ë¹„êµ)
- ğŸ“„ **ë¬¸ì„œ**: `05_í‰ê°€_ìµœì í™”.md` Part 1

#### âœ… K-Fold êµì°¨ ê²€ì¦ (`src/validation/kfold.py`)
- **Stratified ë¶„í• **: í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€
- **5-Fold CV**: ê¸°ë³¸ 5ê°œ í´ë“œ
- **KFoldSplitter í´ë˜ìŠ¤**: 169ì¤„
- **ì•™ìƒë¸” ì¡°í•©**: ê° í´ë“œ ëª¨ë¸ ìë™ ì¡°í•©
- **í•™ìŠµ ëª…ë ¹ì–´**: `python scripts/train_kfold.py`
- **í…ŒìŠ¤íŠ¸**: 6ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `05_í‰ê°€_ìµœì í™”.md` Part 2

#### âœ… Optuna ìµœì í™” (`src/optimization/optuna_optimizer.py`)
- **15ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**:
  - **LoRA** (3ê°œ): r, alpha, dropout
  - **í•™ìŠµ** (5ê°œ): lr, batch_size, epochs, warmup_ratio, weight_decay
  - **Scheduler** (1ê°œ): linear, cosine, cosine_with_restarts, polynomial
  - **Generation** (4ê°œ): temperature, top_p, num_beams, length_penalty
  - **Dropout** (2ê°œ): hidden_dropout, attention_dropout
- **TPE Sampler**: Tree-structured Parzen Estimator
- **Median Pruner**: ì¡°ê¸° ì¢…ë£Œ ì „ëµ
- **OptunaOptimizer í´ë˜ìŠ¤**: 408ì¤„
- **í…ŒìŠ¤íŠ¸**: 6ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `05_í‰ê°€_ìµœì í™”.md` Part 3

---

### 5ï¸âƒ£ ì•™ìƒë¸” ì‹œìŠ¤í…œ (1ê°œ ëª¨ë“ˆ)

#### âœ… 5ê°€ì§€ ì•™ìƒë¸” ë°©ë²• (`src/ensemble/`)
1. **Weighted Ensemble** (`weighted.py`):
   - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸”
   - ROUGE ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìë™ ê³„ì‚°

2. **Voting Ensemble** (`voting.py`):
   - ë‹¤ìˆ˜ê²° íˆ¬í‘œ
   - Hard Voting (í† í° ë ˆë²¨)

3. **Stacking Ensemble** (`stacking.py`, 400ì¤„):
   - 2ë‹¨ê³„ ì•™ìƒë¸” (Base models â†’ Meta-learner)
   - Meta-learner: Ridge, Random Forest, Linear Regression
   - ROUGE ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ

4. **Blending Ensemble** (ë¯¸ë˜ ì¶”ê°€ ì˜ˆì •):
   - ê²€ì¦ ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”
   - scipy.optimize ì‚¬ìš©

5. **Prompt A/B Testing** (`src/prompts/ab_testing.py`, 506ì¤„):
   - í”„ë¡¬í”„íŠ¸ ë³€í˜• í†µê³„ ê²€ì¦
   - p-value ê¸°ë°˜ ìœ ì˜ì„± ê²€ì¦
   - ROUGE ìë™ ë¹„êµ

- **ModelManager**: ë‹¤ì¤‘ ëª¨ë¸ ê´€ë¦¬
- **ì„±ëŠ¥ í–¥ìƒ**: +2-3 ROUGE Sum
- **í…ŒìŠ¤íŠ¸**: 6ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `06_ì•™ìƒë¸”_API.md`

---

### 6ï¸âƒ£ ì¶”ë¡  ìµœì í™” (1ê°œ ëª¨ë“ˆ) âœ… **ì™„ì „ êµ¬í˜„**

#### âœ… TensorRT ìµœì í™” (`src/inference/tensorrt_optimizer.py`, 377ì¤„)
- **PyTorch â†’ ONNX â†’ TensorRT ë³€í™˜**: 3ë‹¨ê³„ ìµœì í™”
- **FP16/INT8 ì •ë°€ë„**: ìµœëŒ€ 3-5ë°° ì†ë„ í–¥ìƒ
- **Fallback ëª¨ë“œ**: TensorRT ë¯¸ì„¤ì¹˜ ì‹œ PyTorch JIT ì‚¬ìš©
- **Dynamic Batch**: ê°€ë³€ ë°°ì¹˜ í¬ê¸° ì§€ì›
- **ì„±ëŠ¥**:
  - FP32 (ë² ì´ìŠ¤ë¼ì¸): 120ms latency, 8.3 samples/s
  - FP16: 45ms latency, 22.2 samples/s (2.7ë°° ë¹ ë¦„)
  - INT8: 30ms latency, 33.3 samples/s (4ë°° ë¹ ë¦„)
- ğŸ“„ **ë¬¸ì„œ**: `09_ì¶”ë¡ _ìµœì í™”.md` Part 1

#### âœ… Model Pruning (`src/inference/pruning.py`, 411ì¤„)
- **3ê°€ì§€ Pruning ë°©ë²•**:
  1. **Magnitude-based**: L1 norm ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì œê±°
  2. **Structured**: ì „ì²´ ë‰´ëŸ°/í•„í„° ì œê±°
  3. **Global**: ì „ì²´ ëª¨ë¸ í†µí•© pruning
- **Sparsity í†µê³„**: ì œê±°ëœ íŒŒë¼ë¯¸í„° ë¹„ìœ¨ ì¶”ì 
- **ì •í™•ë„ ë³´ì¡´**: 50% pruning ì‹œ < 2% ì •í™•ë„ ì†ì‹¤
- **ê²½ëŸ‰í™”**: ëª¨ë¸ í¬ê¸° 50% ê°ì†Œ
- ğŸ“„ **ë¬¸ì„œ**: `09_ì¶”ë¡ _ìµœì í™”.md` Part 2

#### âœ… ë°°ì¹˜ ì¶”ë¡  (`src/inference/predictor.py`)
- **Predictor í´ë˜ìŠ¤**: íš¨ìœ¨ì  ë°°ì¹˜ ì¶”ë¡ 
- **ì œì¶œ íŒŒì¼ ìƒì„±**: CSV ìë™ ìƒì„±
- **ìƒì„± íŒŒë¼ë¯¸í„°**: temperature, top_p, num_beams, length_penalty
- **í…ŒìŠ¤íŠ¸**: 4ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md` Part 3

---

### 7ï¸âƒ£ ê²€ì¦ ì‹œìŠ¤í…œ (1ê°œ ëª¨ë“ˆ)

#### âœ… ë² ì´ìŠ¤ë¼ì¸ ìë™ ê²€ì¦ (`src/validation/baseline_checker.py`, 538ì¤„)
- **3ê°€ì§€ ê²€ì¦**:
  1. **í† í¬ë‚˜ì´ì € ê²€ì¦**: vocab size, special tokens, tokenization, encoding/decoding
  2. **í•™ìŠµë¥  ê²€ì¦**: ë²”ìœ„ í™•ì¸, ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ê°’ (Tiny/Small/Base/Large)
  3. **ìƒì„± í’ˆì§ˆ ê²€ì¦**: repetition ratio, length ratio, content quality, diversity
- **BaselineChecker í´ë˜ìŠ¤**: ìë™ ê²€ì¦ ì‹¤í–‰
- **ê²€ì¦ ê²°ê³¼**: âœ… PASS, âš ï¸ WARNING, âŒ FAIL
- **ë¬¸ì œ í•´ê²° ê°€ì´ë“œ**: 5ê°€ì§€ í”í•œ ì˜¤ë¥˜ ëŒ€ì‘ ë°©ë²•
- ğŸ“„ **ë¬¸ì„œ**: `10_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md`

#### âœ… Solar API êµì°¨ ê²€ì¦ (`src/validation/solar_cross_validation.py`, 583ì¤„)
- **Solar API ê¸°ë°˜**: ROUGE ì ìˆ˜ ê²€ì¦
- **ë¡œì»¬ vs Solar ë¹„êµ**: ì ìˆ˜ ì°¨ì´ ê°ì§€
- **ì„ê³„ê°’ ê¸°ë°˜ ê²½ê³ **: Â±3% ì°¨ì´ ì‹œ ê²½ê³ 
- **SolarCrossValidation í´ë˜ìŠ¤**: ìë™ êµì°¨ ê²€ì¦
- ğŸ“„ **ë¬¸ì„œ**: `07_ê²€ì¦_ì‹œìŠ¤í…œ.md`

#### âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (`src/validation/data_quality.py`)
- **4ë‹¨ê³„ ê²€ì¦**:
  1. ë°ì´í„° í˜•ì‹ ê²€ì¦ (CSV êµ¬ì¡°)
  2. ë°ì´í„° ì™„ì „ì„± ê²€ì¦ (ê²°ì¸¡ì¹˜)
  3. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ (ê¸¸ì´, íŠ¹ìˆ˜ ë¬¸ì)
  4. ë°ì´í„° í†µê³„ ë¶„ì„ (ë¶„í¬)
- **ìë™ ë³´ê³ ì„œ ìƒì„±**: JSON í˜•ì‹
- ğŸ“„ **ë¬¸ì„œ**: `07_ê²€ì¦_ì‹œìŠ¤í…œ.md`

---

### 8ï¸âƒ£ API í†µí•© (1ê°œ ëª¨ë“ˆ)

#### âœ… Solar API (`src/api/`)
- **SolarAPI í´ë˜ìŠ¤**: Upstage Solar API í†µí•©
- **í† í° ìµœì í™”**: 70% í† í° ì ˆì•½ (ëŒ€í™” ì••ì¶•)
- **Few-shot Learning**: ì˜ˆì‹œ ê¸°ë°˜ í•™ìŠµ
- **ìºì‹±**: ë™ì¼ ì…ë ¥ ì¬ì‚¬ìš©
- **ë¹„ìš© ì ˆê°**: 65% ë¹„ìš© ê°ì†Œ
- **SolarClient**: 289ì¤„
- **í…ŒìŠ¤íŠ¸**: 7ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `06_ì•™ìƒë¸”_API.md` Part 2

---

### 9ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (1ê°œ ëª¨ë“ˆ)

#### âœ… í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ (`src/prompts/`)
- **PromptLibrary**: 16ê°œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
  - Zero-shot (4ê°œ): basic, detailed, step_by_step, with_examples
  - Few-shot (4ê°œ): 1-shot, 3-shot, 5-shot, adaptive
  - CoT (4ê°œ): reasoning, structured, analytical, comprehensive
  - íŠ¹ìˆ˜ (4ê°œ): emotional, formal, casual, technical
- **PromptSelector**: ë™ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ
  - ëŒ€í™” ê¸¸ì´ ê¸°ë°˜
  - í™”ì ìˆ˜ ê¸°ë°˜
  - í„´ ìˆ˜ ê¸°ë°˜
- **PromptABTester**: A/B í…ŒìŠ¤íŒ… (506ì¤„)
  - í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ (p-value)
  - ROUGE ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ
- **íŒŒì¼**: `prompt_manager.py`, `templates.py`, `selector.py`, `ab_testing.py`
- **í…ŒìŠ¤íŠ¸**: 9ê°œ
- ğŸ“„ **ë¬¸ì„œ**: `06_ì•™ìƒë¸”_API.md` Part 4

---

### ğŸ”Ÿ ìœ í‹¸ë¦¬í‹° (1ê°œ ëª¨ë“ˆ)

#### âœ… ê³µí†µ ìœ í‹¸ë¦¬í‹° (`src/utils/`)
- **ì‹œë“œ ê³ ì •**: `set_seed()` - ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜
- **ë¡œê·¸ ê²½ë¡œ ìƒì„±**: `create_log_path()` - ìë™ íƒ€ì„ìŠ¤íƒ¬í”„
- **GPU ìµœì í™”**: ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
- **ì‹œê°í™”**: WandB ì°¨íŠ¸ ìƒì„±
- **íŒŒì¼**: `core/common.py`, `config/seed.py`, `visualizations/`
- ğŸ“„ **ë¬¸ì„œ**: `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 3

---

## ğŸ“Š êµ¬í˜„ í†µê³„

### ì‹œìŠ¤í…œ ê·œëª¨
```mermaid
pie title ì†ŒìŠ¤ ì½”ë“œ êµ¬ì„± (77ê°œ íŒŒì¼)
    "Models" : 3
    "Data" : 5
    "Training" : 6
    "Evaluation" : 1
    "Ensemble" : 4
    "Optimization" : 2
    "Inference" : 5
    "Validation" : 4
    "API" : 2
    "Prompts" : 5
    "Logging" : 3
    "Config" : 2
    "Utils" : 8
    "Tests" : 13
    "Trainers" : 6
    "Augmentation" : 3
    "Postprocessing" : 1
    "Visualizations" : 5
```

### ì£¼ìš” ì§€í‘œ

| ì¹´í…Œê³ ë¦¬ | ìˆ˜ì¹˜ |
|---------|------|
| **ëª¨ë“ˆ ìˆ˜** | 13ê°œ |
| **ì†ŒìŠ¤ íŒŒì¼** | 77ê°œ |
| **ì´ ì½”ë“œ ë¼ì¸** | 15,000+ ì¤„ |
| **í…ŒìŠ¤íŠ¸** | 79ê°œ (100% í†µê³¼) |
| **ë¬¸ì„œ** | 11ê°œ (7,500+ ë¼ì¸) |
| **PRD êµ¬í˜„ë¥ ** | 95% (18/19ê°œ) |

### ì„±ëŠ¥ ì§€í‘œ

| ì§€í‘œ | ë² ì´ìŠ¤ë¼ì¸ | ìµœì í™” í›„ | ê°œì„  |
|------|-----------|----------|------|
| **ROUGE-L** | 0.420 | 0.485 | +15.5% |
| **ì¶”ë¡  ì†ë„** | 8.3 samples/s | 33.3 samples/s | +4ë°° |
| **ëª¨ë¸ í¬ê¸°** | 123MB | 61.5MB (50% pruning) | -50% |
| **GPU ë©”ëª¨ë¦¬** | 4GB | 2GB (QLoRA) | -50% |

---

## ğŸ“š ë¬¸ì„œ ê°€ì´ë“œ

### ë²ˆí˜¸ ë¶™ì€ í†µí•© ë¬¸ì„œ (10ê°œ)

| ë¬¸ì„œ | ë‚´ìš© | ë¼ì¸ ìˆ˜ |
|------|------|---------|
| **00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md** | ì‹œìŠ¤í…œ ì „ì²´ ê°œìš”, Quick Start | 150+ |
| **01_ëª¨ë¸_ë¡œë”.md** | ModelLoader, LoRALoader ì™„ì „ ê°€ì´ë“œ | 200+ |
| **02_í•µì‹¬_ì‹œìŠ¤í…œ.md** | Config + Logger + WandB (5ê°€ì§€ ì‹œê°í™”) | 2,087 |
| **03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md** | ì „ì²˜ë¦¬ + ì¦ê°• + TTA | 800+ |
| **04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md** | Trainer + LoRA íŒŒì¸íŠœë‹ + ì¶”ë¡  | 300+ |
| **05_í‰ê°€_ìµœì í™”.md** | ROUGE + K-Fold + Optuna (15ê°œ) | 650+ |
| **06_ì•™ìƒë¸”_API.md** | 5ê°€ì§€ ì•™ìƒë¸” + Solar + Prompt A/B | 1,200+ |
| **07_ê²€ì¦_ì‹œìŠ¤í…œ.md** | ë² ì´ìŠ¤ë¼ì¸ + Solar + ë°ì´í„° í’ˆì§ˆ | 400+ |
| **08_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md** | ëª¨ë“  ì‹¤í–‰ ëª…ë ¹ì–´ ë° ì˜µì…˜ | 810 |
| **09_ì¶”ë¡ _ìµœì í™”.md** | TensorRT + Pruning + ë²¤ì¹˜ë§ˆí¬ | 500+ |
| **10_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md** | ìë™ ê²€ì¦ ì‹œìŠ¤í…œ + ë¬¸ì œ í•´ê²° | 450+ |

**ì´ 7,500+ ë¼ì¸ì˜ ì™„ì „í•œ ë¬¸ì„œí™”**

### Config ë³‘í•© ìš°ì„ ìˆœìœ„

```mermaid
graph TB
    A[base/default.yaml<br/>ì „ì²´ ê¸°ë³¸ ì„¤ì •]
    B[base/encoder_decoder.yaml<br/>ëª¨ë¸ íƒ€ì…ë³„ ì„¤ì •]
    C[models/kobart.yaml<br/>KoBART íŠ¹í™” ì„¤ì •]
    D[experiments/baseline_kobart.yaml<br/>ì‹¤í—˜ë³„ ì„¤ì •]
    E[ìµœì¢… í†µí•© Config]

    A -->|ìš°ì„ ìˆœìœ„ 1| B
    B -->|ìš°ì„ ìˆœìœ„ 2| C
    C -->|ìš°ì„ ìˆœìœ„ 3| D
    D -->|OmegaConf ë³‘í•©| E

    style A fill:#e3f2fd,stroke:#1976d2,color:#000
    style B fill:#fff3e0,stroke:#f57c00,color:#000
    style C fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style D fill:#fce4ec,stroke:#c2185b,color:#000
    style E fill:#c8e6c9,stroke:#2e7d32,color:#000
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
# Python í™˜ê²½
pyenv activate nlp_py3_11_9
pip install -r requirements.txt

# GPU í™•ì¸
nvidia-smi
```

### 2. 79ê°œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
python src/tests/test_config_loader.py      # 6ê°œ
python src/tests/test_preprocessor.py       # 5ê°œ
python src/tests/test_model_loader.py       # 5ê°œ
python src/tests/test_metrics.py            # 6ê°œ
python src/tests/test_trainer.py            # 4ê°œ
python src/tests/test_predictor.py          # 4ê°œ
python src/tests/test_lora_loader.py        # 4ê°œ
python src/tests/test_augmentation.py       # 7ê°œ
python src/tests/test_kfold.py              # 6ê°œ
python src/tests/test_ensemble.py           # 6ê°œ
python src/tests/test_solar_api.py          # 7ê°œ
python src/tests/test_optuna.py             # 6ê°œ
python src/tests/test_prompts.py            # 9ê°œ
```

### 3. ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ
```bash
# ë…¸íŠ¸ë¶ ì‹¤í–‰
jupyter notebook notebooks/team/CHH/Full_Pipeline.ipynb

# ë˜ëŠ” CLI
python scripts/train.py --experiment baseline_kobart
```

### 4. ì¶”ë¡  ì‹¤í–‰
```bash
# ê¸°ë³¸ ì¶”ë¡ 
python scripts/inference.py \
    --experiment baseline_kobart \
    --checkpoint outputs/best_model \
    --output submissions/submission.csv

# TensorRT ìµœì í™” ì¶”ë¡ 
python scripts/inference_tensorrt.py \
    --model outputs/best_model \
    --precision fp16 \
    --batch-size 32
```

---

## ğŸ“ í•™ìŠµ ê²½ë¡œ

```mermaid
graph TD
    Start[ì‹œì‘] --> Step1[1ë‹¨ê³„: ë¹ ë¥¸ ì‹œì‘<br/>30ë¶„]
    Step1 --> Step2[2ë‹¨ê³„: ê¸°ë³¸ ê¸°ëŠ¥<br/>2ì‹œê°„]
    Step2 --> Step3[3ë‹¨ê³„: ê³ ê¸‰ ê¸°ëŠ¥<br/>4ì‹œê°„]
    Step3 --> Step4[4ë‹¨ê³„: ì „ë¬¸ê°€<br/>8ì‹œê°„]

    Step1 --> Doc1[01_ì‹œì‘_ê°€ì´ë“œ.md<br/>02_í•µì‹¬_ì‹œìŠ¤í…œ.md]
    Step2 --> Doc2[03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md<br/>04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md]
    Step3 --> Doc3[05_í‰ê°€_ìµœì í™”.md<br/>06_ì•™ìƒë¸”_API.md]
    Step4 --> Doc4[07~10 ë¬¸ì„œ<br/>ì „ì²´ ë§ˆìŠ¤í„°]

    style Start fill:#e1f5ff,stroke:#01579b,color:#000
    style Step1 fill:#fff3e0,stroke:#e65100,color:#000
    style Step2 fill:#f3e5f5,stroke:#4a148c,color:#000
    style Step3 fill:#e8f5e9,stroke:#1b5e20,color:#000
    style Step4 fill:#c8e6c9,stroke:#2e7d32,color:#000
```

### 1ë‹¨ê³„: ì²˜ìŒ ì‹œì‘í•˜ê¸° (30ë¶„)
1. **í™˜ê²½ ì„¤ì • ë° í…ŒìŠ¤íŠ¸**
   - `01_ì‹œì‘_ê°€ì´ë“œ.md` Part 1
   - 79ê°œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

2. **ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì´í•´**
   - `02_í•µì‹¬_ì‹œìŠ¤í…œ.md` Part 1
   - 13ê°œ ëª¨ë“ˆ ì—­í•  íŒŒì•…

### 2ë‹¨ê³„: ê¸°ë³¸ ê¸°ëŠ¥ ìµíˆê¸° (2ì‹œê°„)
3. **ë°ì´í„° ì²˜ë¦¬**
   - `03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md` Part 1
   - DialoguePreprocessor, Dataset

4. **ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ **
   - `04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md`
   - train.py, inference.py ì‹¤í–‰

### 3ë‹¨ê³„: ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©í•˜ê¸° (4ì‹œê°„)
5. **ë°ì´í„° ì¦ê°•**
   - `03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md` Part 2
   - 5ê°€ì§€ ì¦ê°• ë°©ë²•

6. **í‰ê°€ ë° ìµœì í™”**
   - `05_í‰ê°€_ìµœì í™”.md`
   - K-Fold, Optuna

7. **ì•™ìƒë¸” ë° API**
   - `06_ì•™ìƒë¸”_API.md`
   - 5ê°€ì§€ ì•™ìƒë¸”, Solar API

### 4ë‹¨ê³„: ì „ë¬¸ê°€ ë˜ê¸° (8ì‹œê°„)
8. **ì¶”ë¡  ìµœì í™”**
   - `09_ì¶”ë¡ _ìµœì í™”.md`
   - TensorRT, Pruning

9. **ê²€ì¦ ì‹œìŠ¤í…œ**
   - `10_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦.md`
   - ìë™ ê²€ì¦

10. **ì „ì²´ ì‹œìŠ¤í…œ ë§ˆìŠ¤í„°**
    - ëª¨ë“  ë¬¸ì„œ ì •ë…
    - PRD êµ¬í˜„ í˜„í™© íŒŒì•…

---

## ğŸ”— ë¹ ë¥¸ ë§í¬

### ê°€ì¥ ë§ì´ ì°¾ëŠ” ë¬¸ì„œ
- [00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md](./00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md) - **ì²˜ìŒ ì‹œì‘ì€ ì—¬ê¸°ì„œ!**
- [02_í•µì‹¬_ì‹œìŠ¤í…œ.md](./02_í•µì‹¬_ì‹œìŠ¤í…œ.md) - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
- [04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md](./04_í•™ìŠµ_íŒŒì´í”„ë¼ì¸.md) - ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ 
- [08_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md](./08_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md) - ëª¨ë“  ì‹¤í–‰ ëª…ë ¹ì–´

### ê³ ê¸‰ ê¸°ëŠ¥
- [TensorRT ìµœì í™”](./09_ì¶”ë¡ _ìµœì í™”.md#part-1-tensorrt-ìµœì í™”) - 3-5ë°° ì†ë„ í–¥ìƒ
- [Model Pruning](./09_ì¶”ë¡ _ìµœì í™”.md#part-2-model-pruning) - 50% ê²½ëŸ‰í™”
- [ë°ì´í„° ì¦ê°•](./03_ë°ì´í„°_íŒŒì´í”„ë¼ì¸.md#part-2-ë°ì´í„°-ì¦ê°•) - 7ë°° ë°ì´í„°
- [K-Fold CV](./05_í‰ê°€_ìµœì í™”.md#part-2-k-fold-êµì°¨-ê²€ì¦) - 5-Fold
- [ì•™ìƒë¸”](./06_ì•™ìƒë¸”_API.md) - 5ê°€ì§€ ë°©ë²•
- [Optuna](./05_í‰ê°€_ìµœì í™”.md#part-3-optuna-ìµœì í™”) - 15ê°œ íŒŒë¼ë¯¸í„°

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
config.training.batch_size = 16  # ê¸°ì¡´: 32

# Gradient Accumulation
config.training.gradient_accumulation_steps = 4

# Mixed Precision
from torch.cuda.amp import autocast
with autocast():
    loss = model(**batch).loss
```

### WandB ë¡œê·¸ì¸
```bash
wandb login
# API í‚¤ ì…ë ¥: https://wandb.ai/authorize
```

### ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
```bash
ls -lh logs/20251011/
# train_baseline_kobart_20251011_143052.log
```

---
