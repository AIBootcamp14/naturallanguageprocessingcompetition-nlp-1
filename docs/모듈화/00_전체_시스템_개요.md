# ëª¨ë“ˆí™” ì‹œìŠ¤í…œ ì „ì²´ ê°œìš”

## ğŸ“‹ ëª©ì°¨
1. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
2. [ëª¨ë“ˆ êµ¬ì¡°](#ëª¨ë“ˆ-êµ¬ì¡°)
3. [ë°ì´í„° í”Œë¡œìš°](#ë°ì´í„°-í”Œë¡œìš°)
4. [ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •](#ì„¤ì¹˜-ë°-í™˜ê²½-ì„¤ì •)
5. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TB
    subgraph "ì…ë ¥ ê³„ì¸µ"
        A[Config YAML íŒŒì¼ë“¤] --> B[ConfigLoader]
        C[Raw Data CSV] --> D[DialoguePreprocessor]
    end

    subgraph "ë°ì´í„° ì²˜ë¦¬ ê³„ì¸µ"
        B --> E[í†µí•© Config]
        D --> F[ì „ì²˜ë¦¬ëœ ë°ì´í„°]
        F --> G[PyTorch Dataset]
    end

    subgraph "ëª¨ë¸ ê³„ì¸µ"
        E --> H[ModelLoader]
        H --> I[ì‚¬ì „í•™ìŠµ ëª¨ë¸]
        H --> J[Tokenizer]
    end

    subgraph "í•™ìŠµ ê³„ì¸µ"
        G --> K[Trainer]
        I --> K
        J --> K
        E --> K
        K --> L[í•™ìŠµëœ ëª¨ë¸]
        K --> M[WandB ë¡œê·¸]
    end

    subgraph "í‰ê°€ ê³„ì¸µ"
        K --> N[RougeCalculator]
        N --> O[ROUGE ì ìˆ˜]
    end

    subgraph "ì¶”ë¡  ê³„ì¸µ"
        L --> P[Predictor]
        J --> P
        P --> Q[ì˜ˆì¸¡ ê²°ê³¼]
        Q --> R[ì œì¶œ íŒŒì¼ CSV]
    end

    style A fill:#e1f5ff
    style C fill:#e1f5ff
    style L fill:#c8e6c9
    style R fill:#c8e6c9
    style M fill:#fff9c4
```

---

## ğŸ“¦ ëª¨ë“ˆ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
src/
â”œâ”€â”€ config/              # Config ê´€ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loader.py       # ê³„ì¸µì  Config ë³‘í•©
â”‚
â”œâ”€â”€ data/               # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessor.py # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
â”‚   â””â”€â”€ dataset.py      # PyTorch Dataset í´ë˜ìŠ¤
â”‚
â”œâ”€â”€ models/             # ëª¨ë¸ ë¡œë”©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_loader.py # HuggingFace ëª¨ë¸ ë¡œë”
â”‚
â”œâ”€â”€ evaluation/         # í‰ê°€ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ metrics.py      # ROUGE ê³„ì‚°
â”‚
â”œâ”€â”€ training/           # í•™ìŠµ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py      # Seq2SeqTrainer ë˜í¼
â”‚
â””â”€â”€ inference/          # ì¶”ë¡  ì‹œìŠ¤í…œ
    â”œâ”€â”€ __init__.py
    â””â”€â”€ predictor.py    # ë°°ì¹˜ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
```

### ëª¨ë“ˆë³„ ì—­í• 

| ëª¨ë“ˆ | ì£¼ìš” í´ë˜ìŠ¤ | í•µì‹¬ ê¸°ëŠ¥ |
|------|------------|----------|
| **config** | `ConfigLoader` | YAML ê¸°ë°˜ ê³„ì¸µì  ì„¤ì • ê´€ë¦¬ |
| **data** | `DialoguePreprocessor`<br>`DialogueSummarizationDataset`<br>`InferenceDataset` | ë°ì´í„° ì „ì²˜ë¦¬<br>í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹<br>ì¶”ë¡  ë°ì´í„°ì…‹ |
| **models** | `ModelLoader` | ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”©<br>íŠ¹ìˆ˜ í† í° ì²˜ë¦¬ |
| **evaluation** | `RougeCalculator` | ROUGE-1/2/L ê³„ì‚°<br>Multi-reference ì§€ì› |
| **training** | `ModelTrainer` | í•™ìŠµ íŒŒì´í”„ë¼ì¸<br>WandB ë¡œê¹… í†µí•© |
| **inference** | `Predictor` | ë°°ì¹˜ ì¶”ë¡ <br>ì œì¶œ íŒŒì¼ ìƒì„± |

---

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### í•™ìŠµ íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Config
    participant Data
    participant Model
    participant Trainer
    participant WandB

    User->>Config: load_config("baseline_kobart")
    Config-->>User: í†µí•© Config ê°ì²´

    User->>Model: load_model_and_tokenizer(config)
    Model-->>User: model, tokenizer

    User->>Data: DialogueSummarizationDataset(...)
    Data-->>User: train_dataset, eval_dataset

    User->>Trainer: ModelTrainer(config, model, ...)
    Trainer->>WandB: init_run()

    User->>Trainer: trainer.train()

    loop ê° ì—í¬í¬
        Trainer->>Model: í•™ìŠµ ìŠ¤í…
        Trainer->>Data: ê²€ì¦ ë°ì´í„° ë¡œë“œ
        Trainer->>Model: í‰ê°€ ìŠ¤í…
        Trainer->>Trainer: ROUGE ê³„ì‚°
        Trainer->>WandB: log_metrics()
        Trainer->>Trainer: ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    end

    Trainer-->>User: í•™ìŠµ ì™„ë£Œ (ìµœì¢… ëª¨ë¸)
```

### ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```mermaid
sequenceDiagram
    participant User
    participant Model
    participant Data
    participant Predictor
    participant File

    User->>Model: í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    Model-->>User: model, tokenizer

    User->>Predictor: Predictor(model, tokenizer, config)

    User->>Data: test.csv ì½ê¸°
    Data-->>User: test_df

    User->>Predictor: create_submission(test_df, output_path)

    loop ê° ë°°ì¹˜
        Predictor->>Data: ë°°ì¹˜ ë°ì´í„° ë¡œë“œ
        Predictor->>Model: generate()
        Model-->>Predictor: ìƒì„±ëœ ìš”ì•½
    end

    Predictor->>File: submission.csv ì €ì¥
    Predictor-->>User: submission_df
```

---

## ğŸ› ï¸ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ í™œì„±í™”

```bash
# pyenv ê°€ìƒí™˜ê²½ í™œì„±í™”
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# ë˜ëŠ” pyenv ëª…ë ¹ì–´ ì‚¬ìš©
pyenv activate nlp_py3_11_9
```

### 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

**ì£¼ìš” íŒ¨í‚¤ì§€:**
- `torch==2.8.0` - PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `transformers==4.57.0` - HuggingFace Transformers
- `pandas==2.3.3` - ë°ì´í„° ì²˜ë¦¬
- `omegaconf==2.3.0` - Config ê´€ë¦¬
- `rouge-score==0.1.2` - ROUGE í‰ê°€
- `wandb==0.22.2` - ì‹¤í—˜ ë¡œê¹…

### 3. í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
tree -L 2 src/
tree -L 2 configs/
tree -L 2 tests/
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. Config ê¸°ë°˜ í•™ìŠµ ì‹¤í–‰

```python
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# 1. Config ë¡œë“œ
config = load_config("baseline_kobart")

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)

# 3. ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("data/raw/train.csv")
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),
    summaries=train_df['summary'].tolist(),
    tokenizer=tokenizer
)

# 4. Trainer ìƒì„± ë° í•™ìŠµ
trainer = create_trainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset
)

# 5. í•™ìŠµ ì‹¤í–‰
results = trainer.train()
```

### 2. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±

```python
from src.models import load_model_and_tokenizer
from src.inference import create_predictor
import pandas as pd

# 1. í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model, tokenizer = load_model_and_tokenizer(config)
# ë˜ëŠ” ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
# model = AutoModelForSeq2SeqLM.from_pretrained("outputs/best_model")

# 2. Predictor ìƒì„±
predictor = create_predictor(
    model=model,
    tokenizer=tokenizer,
    config=config
)

# 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
test_df = pd.read_csv("data/raw/test.csv")

# 4. ì œì¶œ íŒŒì¼ ìƒì„±
submission_df = predictor.create_submission(
    test_df=test_df,
    output_path="submissions/submission.csv",
    batch_size=32
)
```

### 3. ë‹¨ì¼ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ ê° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

python tests/test_config_loader.py
python tests/test_preprocessor.py
python tests/test_model_loader.py
python tests/test_metrics.py
python tests/test_trainer.py
python tests/test_predictor.py
```

---

## ğŸ“Š Config ì‹œìŠ¤í…œ ìƒì„¸

### Config ê³„ì¸µ êµ¬ì¡°

```mermaid
graph LR
    A[base/default.yaml] --> F[ìµœì¢… Config]
    B[base/encoder_decoder.yaml] --> F
    C[models/kobart.yaml] --> F
    D[experiments/baseline_kobart.yaml] --> F

    F --> G[í•™ìŠµ ì‹¤í–‰]

    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style F fill:#c8e6c9
```

### Config ë³‘í•© ìš°ì„ ìˆœìœ„

1. `base/default.yaml` - ê¸°ë³¸ ì„¤ì •
2. `base/encoder_decoder.yaml` - ëª¨ë¸ íƒ€ì…ë³„ ì„¤ì •
3. `models/{model_name}.yaml` - íŠ¹ì • ëª¨ë¸ ì„¤ì •
4. `experiments/{experiment_name}.yaml` - ì‹¤í—˜ë³„ ì„¤ì • (ìµœìš°ì„ )

**ì˜ˆì‹œ:**
```yaml
# base/default.yaml
training:
  batch_size: 8
  learning_rate: 5e-5

# experiments/baseline_kobart.yaml
training:
  batch_size: 50        # ì˜¤ë²„ë¼ì´ë“œë¨
  learning_rate: 1e-5   # ì˜¤ë²„ë¼ì´ë“œë¨
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½

### ì „ì²´ í…ŒìŠ¤íŠ¸ í˜„í™©

| ëª¨ë“ˆ | í…ŒìŠ¤íŠ¸ íŒŒì¼ | í…ŒìŠ¤íŠ¸ ê°œìˆ˜ | ìƒíƒœ |
|------|------------|-----------|------|
| Config | `test_config_loader.py` | 6ê°œ | âœ… í†µê³¼ |
| Data | `test_preprocessor.py` | 5ê°œ | âœ… í†µê³¼ |
| Models | `test_model_loader.py` | 5ê°œ | âœ… í†µê³¼ |
| Evaluation | `test_metrics.py` | 6ê°œ | âœ… í†µê³¼ |
| Training | `test_trainer.py` | 5ê°œ | âœ… í†µê³¼ |
| Inference | `test_predictor.py` | 6ê°œ | âœ… í†µê³¼ |
| **ì „ì²´** | **6ê°œ íŒŒì¼** | **33ê°œ** | **âœ… ëª¨ë‘ í†µê³¼** |

### ì£¼ìš” ê²€ì¦ í•­ëª©

âœ… **Config Loader**
- ê³„ì¸µì  YAML ë³‘í•© ì •ìƒ ë™ì‘
- ì‹¤í—˜ë³„ Config ì˜¤ë²„ë¼ì´ë“œ ì •ìƒ ì‘ë™

âœ… **Data Preprocessing**
- 12,457ê°œ ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ ì„±ê³µ
- ë…¸ì´ì¦ˆ ì œê±°, í™”ì ì¶”ì¶œ, í„´ ê³„ì‚° ì •ìƒ ë™ì‘

âœ… **Model Loader**
- KoBART ëª¨ë¸ (123M íŒŒë¼ë¯¸í„°) ë¡œë”© ì„±ê³µ
- GPU ìë™ ê°ì§€ ë° ë°°ì¹˜ ì •ìƒ ë™ì‘

âœ… **Metrics**
- ROUGE-1/2/L ê³„ì‚° ì •ìƒ ë™ì‘
- Multi-reference í‰ê°€ ì§€ì› í™•ì¸

âœ… **Trainer**
- Seq2SeqTrainer ë˜í•‘ ì •ìƒ ë™ì‘
- WandB ë¡œê¹… í†µí•© í™•ì¸

âœ… **Inference**
- ë°°ì¹˜ ì¶”ë¡  ì •ìƒ ë™ì‘
- ì œì¶œ íŒŒì¼ ìƒì„± ì •ìƒ ë™ì‘

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ìŠ¤í¬ë¦½íŠ¸** - í•™ìŠµë¶€í„° ì¶”ë¡ ê¹Œì§€ ì›ìŠ¤í… ì‹¤í–‰
2. **Cross-validation ì‹œìŠ¤í…œ** - K-Fold êµì°¨ ê²€ì¦
3. **Ensemble ì‹œìŠ¤í…œ** - ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
4. **Optuna íŠœë‹** - í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [01_Config_ì‹œìŠ¤í…œ.md](./01_Config_ì‹œìŠ¤í…œ.md) - Config ìƒì„¸ ê°€ì´ë“œ
- [02_ë°ì´í„°_ì²˜ë¦¬.md](./02_ë°ì´í„°_ì²˜ë¦¬.md) - ë°ì´í„° ì „ì²˜ë¦¬ ë° Dataset
- [03_ëª¨ë¸_ë¡œë”©.md](./03_ëª¨ë¸_ë¡œë”©.md) - ëª¨ë¸ ë¡œë” ì‚¬ìš©ë²•
- [04_í‰ê°€_ì‹œìŠ¤í…œ.md](./04_í‰ê°€_ì‹œìŠ¤í…œ.md) - ROUGE í‰ê°€ ê°€ì´ë“œ
- [05_í•™ìŠµ_ì‹œìŠ¤í…œ.md](./05_í•™ìŠµ_ì‹œìŠ¤í…œ.md) - Trainer ì‚¬ìš©ë²•
- [06_ì¶”ë¡ _ì‹œìŠ¤í…œ.md](./06_ì¶”ë¡ _ì‹œìŠ¤í…œ.md) - Predictor ì‚¬ìš©ë²•

---

**ì‘ì„±ì¼:** 2025-10-11
**ë²„ì „:** 1.0.0
**ì‘ì„±ì:** AI Assistant
