# 모델 로더 시스템 상세 가이드

## 📋 목차
1. [개요](#개요)
2. [ModelLoader 클래스](#modelloader-클래스)
3. [사용 방법](#사용-방법)
4. [디바이스 관리](#디바이스-관리)
5. [특수 토큰 처리](#특수-토큰-처리)

---

## 📝 개요

### 목적
- HuggingFace 사전학습 모델 자동 로드
- 토크나이저 초기화 및 특수 토큰 추가
- 디바이스 자동 감지 및 배치
- 임베딩 크기 자동 조정

### 핵심 기능
- ✅ Config 기반 모델 로딩
- ✅ 특수 토큰 자동 추가 및 임베딩 리사이즈
- ✅ GPU/CPU 자동 감지
- ✅ 모델 파라미터 정보 출력
- ✅ Logger 통합 지원

---

## 🏗️ ModelLoader 클래스

### 파일 위치
```
src/models/model_loader.py
```

### 클래스 구조

```python
class ModelLoader:
    def __init__(self, config: DictConfig, logger=None):
        """모델 로더 초기화"""

    def _get_device(self) -> torch.device:
        """사용할 디바이스 결정 (GPU/CPU)"""

    def load_tokenizer(self) -> PreTrainedTokenizer:
        """토크나이저 로드 및 특수 토큰 추가"""

    def load_model(self, tokenizer=None) -> PreTrainedModel:
        """사전학습 모델 로드"""

    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """모델과 토크나이저를 함께 로드"""
```

---

## 💻 사용 방법

### 1. 기본 사용법

```python
from src.config import load_config
from src.models import load_model_and_tokenizer

# Config 로드
config = load_config("baseline_kobart")

# 모델 및 토크나이저 로드
model, tokenizer = load_model_and_tokenizer(config)

print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. Logger와 함께 사용

```python
from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.models import load_model_and_tokenizer

# Logger 초기화
log_path = create_log_path("train", "model_load.log")
logger = Logger(log_path, print_also=True)

# Logger를 전달하여 모델 로드
model, tokenizer = load_model_and_tokenizer(config, logger=logger)
```

**출력 예시:**
```
============================================================
모델 및 토크나이저 로딩 시작
============================================================
토크나이저 로딩: digit82/kobart-summarization
  → 특수 토큰 7개 추가됨
  → pad_token 설정: </s>

모델 로딩: digit82/kobart-summarization
  → 임베딩 크기 조정: 51200 → 51207
  → 디바이스: cuda
  → 전체 파라미터: 123,859,968
  → 학습 가능 파라미터: 123,859,968
============================================================
✅ 모델 및 토크나이저 로딩 완료
============================================================
```

### 3. ModelLoader 클래스 직접 사용

```python
from src.models.model_loader import ModelLoader

# ModelLoader 인스턴스 생성
loader = ModelLoader(config, logger=logger)

# 단계별 로드
tokenizer = loader.load_tokenizer()      # 1. 토크나이저만 로드
model = loader.load_model(tokenizer)     # 2. 모델 로드

# 또는 한 번에 로드
model, tokenizer = loader.load_model_and_tokenizer()
```

---

## 🖥️ 디바이스 관리

### 디바이스 자동 감지

ModelLoader는 다음 우선순위로 디바이스를 결정합니다:

1. **Config 설정 확인**
   ```yaml
   # configs/base/default.yaml
   training:
     device: "cuda"  # 또는 "cpu", "cuda:0", "cuda:1" 등
   ```

2. **자동 감지 (Config 없는 경우)**
   ```python
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   ```

### 특정 GPU 지정

```yaml
# configs/experiments/my_experiment.yaml
training:
  device: "cuda:1"  # 두 번째 GPU 사용
```

### 디바이스 관련 동작

```python
# 1. 디바이스 결정
device = loader._get_device()

# 2. 모델을 디바이스로 이동
model = model.to(device)

# 3. GPU 사용 불가 시 경고
# CUDA가 설정되었으나 사용 불가능합니다. CPU를 사용합니다.
```

---

## 🔤 특수 토큰 처리

### 특수 토큰 자동 추가

Config에서 특수 토큰 리스트를 정의하면 자동으로 추가됩니다:

```yaml
# configs/base/encoder_decoder.yaml
tokenizer:
  special_tokens:
    - '#Person1#'
    - '#Person2#'
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateAndTime#'
```

### 처리 과정

1. **토크나이저 로드**
   ```python
   tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)
   ```

2. **특수 토큰 추가**
   ```python
   special_tokens = list(config.model.special_tokens)
   num_added = tokenizer.add_special_tokens({
       'additional_special_tokens': special_tokens
   })
   # 출력: "특수 토큰 7개 추가됨"
   ```

3. **패딩 토큰 설정** (BART 계열 모델)
   ```python
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

4. **모델 임베딩 리사이즈**
   ```python
   vocab_size = len(tokenizer)
   model_vocab_size = model.config.vocab_size

   if vocab_size != model_vocab_size:
       model.resize_token_embeddings(vocab_size)
       # 출력: "임베딩 크기 조정: 51200 → 51207"
   ```

### 임베딩 크기 조정 이유

특수 토큰을 추가하면 토크나이저 어휘 크기가 증가하므로, 모델의 임베딩 레이어 크기도 함께 늘려야 합니다.

**변경 전:**
- 토크나이저 어휘: 51,200개
- 모델 임베딩: 51,200개

**변경 후:**
- 토크나이저 어휘: 51,207개 (+7)
- 모델 임베딩: 51,207개 (+7)

---

## 🧪 테스트

### 테스트 파일 위치
```
src/tests/test_model_loader.py
```

### 테스트 실행

```bash
# 가상환경 활성화
source ~/.pyenv/versions/nlp_py3_11_9/bin/activate

# 테스트 실행
python src/tests/test_model_loader.py
```

### 테스트 항목 (총 5개)

1. ✅ 기본 Config 로드
2. ✅ 토크나이저 로드 및 특수 토큰 추가
3. ✅ 모델 로드
4. ✅ 모델 및 토크나이저 함께 로드
5. ✅ 편의 함수 (load_model_and_tokenizer)

---

## 🎯 실전 활용 예시

### 예시 1: 학습 스크립트에서 사용

```python
# scripts/train.py
from src.models import load_model_and_tokenizer
from src.logging.logger import Logger

def main():
    # Logger 초기화
    logger = Logger(log_path, print_also=True)
    logger.start_redirect()

    try:
        # Config 로드
        config = load_config(args.experiment)

        # 모델 로드 (Logger와 함께)
        logger.write("\n[3/6] 모델 로딩...")
        model, tokenizer = load_model_and_tokenizer(config, logger=logger)
        logger.write("  ✅ 모델 로드 완료")

        # ... 학습 진행

    finally:
        logger.stop_redirect()
        logger.close()
```

### 예시 2: 커스텀 모델 로드

```python
from src.models.model_loader import ModelLoader
from omegaconf import DictConfig

# 커스텀 Config 생성
custom_config = DictConfig({
    "model": {
        "checkpoint": "facebook/bart-base",
        "special_tokens": ["<CUSTOM>", "<TOKEN>"]
    },
    "training": {
        "device": "cuda:0"
    }
})

# 모델 로드
loader = ModelLoader(custom_config)
model, tokenizer = loader.load_model_and_tokenizer()
```

### 예시 3: CPU 전용 로드

```python
# GPU가 없는 환경에서 강제로 CPU 사용
config.training.device = "cpu"
model, tokenizer = load_model_and_tokenizer(config)
```

---

## 📌 주의사항

### 1. 메모리 관리

대형 모델 로드 시 GPU 메모리 부족 주의:
```python
# GPU 메모리 확인
import torch
print(f"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

### 2. 토크나이저 저장

학습 후 토크나이저도 함께 저장해야 추론 시 사용 가능:
```python
# 학습 후
model.save_pretrained("outputs/my_model")
tokenizer.save_pretrained("outputs/my_model")  # 필수!

# 추론 시
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/my_model")
tokenizer = AutoTokenizer.from_pretrained("outputs/my_model")
```

### 3. 특수 토큰 일관성

학습과 추론 시 동일한 특수 토큰을 사용해야 합니다. Config를 통해 관리하면 자동으로 보장됩니다.

---

## 🔗 관련 파일

**소스 코드:**
- `src/models/model_loader.py` - ModelLoader 클래스
- `src/models/__init__.py` - 외부 API

**테스트:**
- `src/tests/test_model_loader.py` - 단위 테스트

**Config:**
- `configs/base/default.yaml` - 디바이스 설정
- `configs/base/encoder_decoder.yaml` - 특수 토큰 정의
- `configs/models/kobart.yaml` - KoBART 체크포인트
