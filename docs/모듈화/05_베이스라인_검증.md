# 10. ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ê°€ì´ë“œ

> **í†µí•© ë¬¸ì„œ:** í† í¬ë‚˜ì´ì € ê²€ì¦ + í•™ìŠµë¥  ê²€ì¦ + ìƒì„± í’ˆì§ˆ ê²€ì¦

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [BaselineChecker í´ë˜ìŠ¤](#baselinechecker-í´ë˜ìŠ¤)
- [ê²€ì¦ ë°©ë²•](#ê²€ì¦-ë°©ë²•)
  - [1. í† í¬ë‚˜ì´ì € ê²€ì¦](#1-í† í¬ë‚˜ì´ì €-ê²€ì¦)
  - [2. í•™ìŠµë¥  ê²€ì¦](#2-í•™ìŠµë¥ -ê²€ì¦)
  - [3. ìƒì„± í’ˆì§ˆ ê²€ì¦](#3-ìƒì„±-í’ˆì§ˆ-ê²€ì¦)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [ê²€ì¦ ê²°ê³¼ í•´ì„](#ê²€ì¦-ê²°ê³¼-í•´ì„)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ğŸ“ ê°œìš”

### ëª©ì 
- í•™ìŠµ ì‹œì‘ ì „ ê¸°ë³¸ ì„¤ì • ìë™ ê²€ì¦
- í”í•œ ì„¤ì • ì˜¤ë¥˜ ì‚¬ì „ ê°ì§€
- ìƒì„± í’ˆì§ˆ ì¡°ê¸° ê²€ì¦
- í•™ìŠµ ì‹¤íŒ¨ ìœ„í—˜ ê°ì†Œ

### í•µì‹¬ ê¸°ëŠ¥
- âœ… í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦ (vocab, special tokens, tokenization)
- âœ… í•™ìŠµë¥  ì ì •ì„± ê²€ì¦ (ë²”ìœ„, ëª¨ë¸ í¬ê¸° ê¸°ë°˜)
- âœ… ìƒì„± í’ˆì§ˆ ê²€ì¦ (repetition, length, ì •ìƒ ì‘ë™)
- âœ… ìë™ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
- âœ… JSON ê²°ê³¼ ì €ì¥

### íŒŒì¼ ìœ„ì¹˜
```
src/validation/baseline_checker.py
```

---

## ğŸ”§ BaselineChecker í´ë˜ìŠ¤

### í´ë˜ìŠ¤ êµ¬ì¡°

```python
# ==================== BaselineChecker í´ë˜ìŠ¤ êµ¬ì¡° ==================== #

class BaselineChecker:
    # ---------------------- ì´ˆê¸°í™” ë©”ì„œë“œ ---------------------- #
    def __init__(config=None, logger=None)
        """ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ê¸° ì´ˆê¸°í™”"""

    # ---------------------- ê°œë³„ ê²€ì¦ ë©”ì„œë“œ ---------------------- #
    def check_tokenization(tokenizer, sample_texts) -> Dict
        """í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦"""

    def check_learning_rate(learning_rate, model_size, batch_size) -> Dict
        """í•™ìŠµë¥  ì ì •ì„± ê²€ì¦"""

    def check_generation_quality(model, tokenizer, sample_inputs, reference_outputs) -> Dict
        """ìƒì„± í’ˆì§ˆ ê²€ì¦"""

    # ---------------------- ì „ì²´ ê²€ì¦ ë©”ì„œë“œ ---------------------- #
    def run_all_checks(tokenizer, learning_rate, model, sample_texts) -> Dict
        """ëª¨ë“  ê²€ì¦ í•­ëª© ì‹¤í–‰"""

    # ---------------------- ê²°ê³¼ ê´€ë¦¬ ë©”ì„œë“œ ---------------------- #
    def save_results(output_path)
        """ê²€ì¦ ê²°ê³¼ JSON íŒŒì¼ë¡œ ì €ì¥"""

    # ---------------------- ë‚´ë¶€ ë©”ì„œë“œ ---------------------- #
    def _analyze_generation_quality(text) -> Dict
        """ìƒì„± í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„"""

    def _create_summary() -> Dict
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±"""

    def _get_default_config() -> Dict
        """ê¸°ë³¸ ê²€ì¦ ì„¤ì • ë¡œë“œ"""
```

---

## âœ… ê²€ì¦ ë°©ë²•

### 1. í† í¬ë‚˜ì´ì € ê²€ì¦

#### ê²€ì¦ í•­ëª©

| í•­ëª© | ê²€ì¦ ë‚´ìš© | í†µê³¼ ê¸°ì¤€ |
|------|---------|---------|
| **Vocab Size** | ì–´íœ˜ í¬ê¸° | â‰¥ 1,000 |
| **Max Length** | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ | 128 ~ 2,048 |
| **Special Tokens** | íŠ¹ìˆ˜ í† í° ì¡´ì¬ | PAD, UNK, BOS, EOS |
| **Tokenization** | ìƒ˜í”Œ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì œì´ì…˜ | ì˜¤ë¥˜ ì—†ì´ ì‹¤í–‰ |

#### ì‚¬ìš© ì˜ˆì‹œ

```python
# ==================== í† í¬ë‚˜ì´ì € ê²€ì¦ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation import create_baseline_checker

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
from transformers import AutoTokenizer

# ---------------------- ê²€ì¦ê¸° ìƒì„± ---------------------- #
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ê¸° ìƒì„±
checker = create_baseline_checker()

# ---------------------- í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# KoBART ìš”ì•½ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# ---------------------- ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ---------------------- #
# ê²€ì¦ìš© ìƒ˜í”Œ ëŒ€í™” í…ìŠ¤íŠ¸
sample_texts = [
    "#Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "ì˜¤ëŠ˜ íšŒì˜ ì‹œê°„ì„ ì •í•˜ë ¤ê³  í•©ë‹ˆë‹¤.",
    "ì ì‹¬ ë©”ë‰´ë¥¼ ê¹€ì¹˜ì°Œê°œë¡œ í•˜ì"
]

# ---------------------- í† í¬ë‚˜ì´ì € ê²€ì¦ ì‹¤í–‰ ---------------------- #
# í† í¬ë‚˜ì´ì € ì„¤ì • ë° ë™ì‘ ê²€ì¦
result = checker.check_tokenization(
    tokenizer=tokenizer,      # ê²€ì¦í•  í† í¬ë‚˜ì´ì €
    sample_texts=sample_texts # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
)

# ---------------------- ê²€ì¦ ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(f"ê²€ì¦ ê²°ê³¼: {'PASS' if result['passed'] else 'FAIL'}")
print(f"Errors: {len(result['errors'])}")
print(f"Warnings: {len(result['warnings'])}")
```

#### ì¶œë ¥ ì˜ˆì‹œ

```
=== í† í¬ë‚˜ì´ì € ê²€ì¦ ===
  - Vocab size: 30,000
  - Max length: 1024

  [Special Tokens ê²€ì¦]
    - pad_token: <pad>
    - unk_token: <unk>
    - bos_token: <s>
    - eos_token: </s>

  [ìƒ˜í”Œ í† í¬ë‚˜ì´ì œì´ì…˜]
    - Sample 1: 15 tokens
    - Sample 2: 12 tokens
    - Sample 3: 8 tokens

  í† í¬ë‚˜ì´ì € ê²€ì¦: âœ“ PASS
```

#### ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

```python
{
    'passed': True,
    'errors': [],
    'warnings': [],
    'info': {
        'vocab_size': 30000,
        'model_max_length': 1024,
        'special_tokens': {
            'pad_token': '<pad>',
            'unk_token': '<unk>',
            'bos_token': '<s>',
            'eos_token': '</s>'
        },
        'tokenization_samples': [
            {'text': 'Sample 1...', 'token_count': 15, 'success': True},
            {'text': 'Sample 2...', 'token_count': 12, 'success': True},
            {'text': 'Sample 3...', 'token_count': 8, 'success': True}
        ]
    }
}
```

---

### 2. í•™ìŠµë¥  ê²€ì¦

#### ê²€ì¦ ê¸°ì¤€

| í•™ìŠµë¥  ë²”ìœ„ | ìƒíƒœ | ì„¤ëª… |
|-----------|------|------|
| < 1e-6 | âŒ Error | ë„ˆë¬´ ì‘ìŒ (ìˆ˜ë ´ ë¶ˆê°€) |
| 1e-6 ~ 1e-5 | âš ï¸ Warning | ê¶Œì¥ ë²”ìœ„ ì´í•˜ |
| 1e-5 ~ 5e-4 | âœ… Good | **ê¶Œì¥ ë²”ìœ„** |
| 5e-4 ~ 1e-3 | âš ï¸ Warning | ê¶Œì¥ ë²”ìœ„ ì´ˆê³¼ |
| > 1e-3 | âŒ Error | ë„ˆë¬´ í¼ (ë°œì‚° ìœ„í—˜) |

#### ëª¨ë¸ í¬ê¸°ë³„ ê¶Œì¥ í•™ìŠµë¥ 

| ëª¨ë¸ í¬ê¸° | ê¶Œì¥ í•™ìŠµë¥  | ì˜ˆì‹œ ëª¨ë¸ |
|----------|-----------|---------|
| < 100M | 2e-5 ~ 5e-5 | DistilBERT |
| 100M ~ 1B | 1e-5 ~ 3e-5 | BERT, KoBART |
| > 1B | 5e-6 ~ 1e-5 | GPT-2, GPT-3 |

#### ì‚¬ìš© ì˜ˆì‹œ

```python
# ==================== í•™ìŠµë¥  ê²€ì¦ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
from transformers import AutoModelForSeq2SeqLM

# ---------------------- ëª¨ë¸ ë¡œë“œ ---------------------- #
# KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("digit82/kobart-summarization")

# ---------------------- ëª¨ë¸ í¬ê¸° ê³„ì‚° ---------------------- #
# ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
model_size = sum(p.numel() for p in model.parameters())
print(f"Model size: {model_size:,} parameters")

# ---------------------- í•™ìŠµë¥  ê²€ì¦ ì‹¤í–‰ ---------------------- #
# í•™ìŠµë¥ ì˜ ì ì •ì„± ê²€ì¦
result = checker.check_learning_rate(
    learning_rate=2e-5,  # ê²€ì¦í•  í•™ìŠµë¥ 
    model_size=model_size,  # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
    batch_size=16  # ë°°ì¹˜ í¬ê¸°
)

# ---------------------- ê²€ì¦ ê²°ê³¼ ì¶œë ¥ ---------------------- #
print(f"ê²€ì¦ ê²°ê³¼: {'PASS' if result['passed'] else 'FAIL'}")
```

#### ì¶œë ¥ ì˜ˆì‹œ

```
=== í•™ìŠµë¥  ê²€ì¦ ===
  - Learning rate: 2e-05
  - Model size: 139,420,672 params
  - Batch size: 16

  í•™ìŠµë¥  ê²€ì¦: âœ“ PASS
```

#### ê²½ê³  ë©”ì‹œì§€ ì˜ˆì‹œ

```python
# ë„ˆë¬´ í° í•™ìŠµë¥ 
learning_rate = 1e-3
# Warning: "Learning rate above recommended: 0.001 > 0.0005"

# í° ëª¨ë¸ì— í° í•™ìŠµë¥ 
model_size = 500e6  # 500M params
learning_rate = 1e-4
# Warning: "For medium model (>100M), consider smaller LR: 2e-05"

# ì‘ì€ ë°°ì¹˜ì— í° í•™ìŠµë¥ 
batch_size = 4
learning_rate = 5e-4
# Warning: "Small batch size may need smaller learning rate"
```

---

### 3. ìƒì„± í’ˆì§ˆ ê²€ì¦

#### ê²€ì¦ í•­ëª©

| í•­ëª© | ê²€ì¦ ë‚´ìš© | í†µê³¼ ê¸°ì¤€ |
|------|---------|---------|
| **ì •ìƒ ìƒì„±** | ì˜¤ë¥˜ ì—†ì´ ìƒì„± | ì˜ˆì™¸ ì—†ìŒ |
| **ì¶œë ¥ ê¸¸ì´** | ìƒì„± í† í° ìˆ˜ | â‰¥ 10 tokens |
| **ë°˜ë³µ ë¹„ìœ¨** | ì¤‘ë³µ ë‹¨ì–´ ë¹„ìœ¨ | â‰¤ 30% |
| **ë¹ˆ ì¶œë ¥** | ë‚´ìš© ì¡´ì¬ | ê¸¸ì´ > 0 |

#### ì‚¬ìš© ì˜ˆì‹œ

```python
# ìƒ˜í”Œ ì…ë ¥
sample_inputs = [
    "#Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ì•ˆë…•í•˜ì„¸ìš” #Person1#: ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”? #Person2#: ê¹€ì¹˜ì°Œê°œ ì–´ë•Œìš”?",
    "#Person1#: íšŒì˜ ì‹œê°„ ì •í•´ì•¼ í•´ìš” #Person2#: 3ì‹œì— í•˜ì£  #Person1#: ì¢‹ì•„ìš”",
    "#Person1#: ë‚´ì¼ ë‚ ì”¨ ì–´ë•Œìš”? #Person2#: ë¹„ ì˜¨ëŒ€ìš” #Person1#: ìš°ì‚° ì±™ê²¨ì•¼ê² ë„¤ìš”"
]

# ìƒì„± í’ˆì§ˆ ê²€ì¦
result = checker.check_generation_quality(
    model=model,
    tokenizer=tokenizer,
    sample_inputs=sample_inputs
)

print(f"ê²€ì¦ ê²°ê³¼: {'PASS' if result['passed'] else 'FAIL'}")
```

#### ì¶œë ¥ ì˜ˆì‹œ

```
=== ìƒì„± í’ˆì§ˆ ê²€ì¦ ===

  [Sample 1]
  Input: #Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ì•ˆë…•í•˜ì„¸ìš” #Person1#: ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”? #Person2#: ê¹€ì¹˜ì°Œê°œ ì–´ë•Œìš”?
  Output: ë‘ ì‚¬ëŒì´ ì ì‹¬ ë©”ë‰´ë¥¼ ê¹€ì¹˜ì°Œê°œë¡œ ì •í–ˆë‹¤.

  [Sample 2]
  Input: #Person1#: íšŒì˜ ì‹œê°„ ì •í•´ì•¼ í•´ìš” #Person2#: 3ì‹œì— í•˜ì£  #Person1#: ì¢‹ì•„ìš”
  Output: íšŒì˜ ì‹œê°„ì„ 3ì‹œë¡œ ì •í–ˆë‹¤.

  [Sample 3]
  Input: #Person1#: ë‚´ì¼ ë‚ ì”¨ ì–´ë•Œìš”? #Person2#: ë¹„ ì˜¨ëŒ€ìš” #Person1#: ìš°ì‚° ì±™ê²¨ì•¼ê² ë„¤ìš”
  Output: ë‚´ì¼ ë¹„ê°€ ì˜¨ë‹¤ëŠ” ì˜ˆë³´ì— ìš°ì‚°ì„ ì±™ê¸°ê¸°ë¡œ í–ˆë‹¤.

  ìƒì„± í’ˆì§ˆ ê²€ì¦: âœ“ PASS
```

#### í’ˆì§ˆ ë¶„ì„ ê²°ê³¼

```python
{
    'length': 12,              # ë‹¨ì–´ ìˆ˜
    'unique_words': 10,        # ê³ ìœ  ë‹¨ì–´ ìˆ˜
    'repetition_ratio': 0.17,  # ë°˜ë³µ ë¹„ìœ¨ (17%)
    'has_content': True        # ë‚´ìš© ì¡´ì¬ ì—¬ë¶€
}
```

#### ê²½ê³  ë©”ì‹œì§€ ì˜ˆì‹œ

```python
# ë†’ì€ ë°˜ë³µ ë¹„ìœ¨
# Output: "ì ì‹¬ ì ì‹¬ ì ì‹¬ì„ ì ì‹¬ìœ¼ë¡œ ì •í–ˆë‹¤ ì •í–ˆë‹¤."
# Warning: "Sample 1: High repetition ratio 45.00%"

# ë„ˆë¬´ ì§§ì€ ì¶œë ¥
# Output: "íšŒì˜"
# Warning: "Sample 2: Output too short (1 tokens)"
```

---

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (ì „ì²´ ê²€ì¦)

```python
# ==================== ë² ì´ìŠ¤ë¼ì¸ ì „ì²´ ê²€ì¦ ì‚¬ìš© ì˜ˆì‹œ ==================== #

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---------------------- #
from src.validation import create_baseline_checker

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---------------------- #
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ---------------------- 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---------------------- #
# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/kobart_model")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# ---------------------- 2. ê²€ì¦ê¸° ìƒì„± ---------------------- #
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ê¸° ìƒì„±
checker = create_baseline_checker()

# ---------------------- 3. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ---------------------- #
# ê²€ì¦ìš© ìƒ˜í”Œ ëŒ€í™” í…ìŠ¤íŠ¸
sample_texts = [
    "#Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤",
    "#Person1#: íšŒì˜ ì‹œê°„ì€? #Person2#: 3ì‹œìš”",
    "#Person1#: ì ì‹¬ì€? #Person2#: ê¹€ì¹˜ì°Œê°œ"
]

# ---------------------- 4. ì „ì²´ ê²€ì¦ ì‹¤í–‰ ---------------------- #
# í† í¬ë‚˜ì´ì €, í•™ìŠµë¥ , ìƒì„± í’ˆì§ˆ ëª¨ë‘ ê²€ì¦
results = checker.run_all_checks(
    tokenizer=tokenizer,        # í† í¬ë‚˜ì´ì € ê°ì²´
    learning_rate=2e-5,          # í•™ìŠµë¥ 
    model=model,                 # ëª¨ë¸ ê°ì²´
    sample_texts=sample_texts    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
)

# ---------------------- 5. ê²°ê³¼ í™•ì¸ ---------------------- #
# ì „ì²´ ê²€ì¦ ê²°ê³¼ ì¶œë ¥
print(f"\nì „ì²´ ê²°ê³¼: {'PASS' if results['all_passed'] else 'FAIL'}")
print(f"  - Passed: {results['passed_count']}/{results['total_checks']}")
print(f"  - Errors: {results['total_errors']}")
print(f"  - Warnings: {results['total_warnings']}")
```

#### ì¶œë ¥ ì˜ˆì‹œ

```
============================================================
ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ì‹œì‘
============================================================

=== í† í¬ë‚˜ì´ì € ê²€ì¦ ===
  - Vocab size: 30,000
  - Max length: 1024
  ...
  í† í¬ë‚˜ì´ì € ê²€ì¦: âœ“ PASS

=== í•™ìŠµë¥  ê²€ì¦ ===
  - Learning rate: 2e-05
  - Model size: 139,420,672 params
  ...
  í•™ìŠµë¥  ê²€ì¦: âœ“ PASS

=== ìƒì„± í’ˆì§ˆ ê²€ì¦ ===
  [Sample 1]
  Input: #Person1#: ì•ˆë…•í•˜ì„¸ìš” #Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤
  Output: ë‘ ì‚¬ëŒì´ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤.
  ...
  ìƒì„± í’ˆì§ˆ ê²€ì¦: âœ“ PASS

============================================================
ê²€ì¦ ì™„ë£Œ
============================================================
ì „ì²´ ê²°ê³¼: âœ“ PASS
  - Passed: 3/3
  - Errors: 0
  - Warnings: 0
```

---

### 2. ê°œë³„ ê²€ì¦

#### í† í¬ë‚˜ì´ì €ë§Œ ê²€ì¦

```python
checker = create_baseline_checker()

result = checker.check_tokenization(
    tokenizer=tokenizer,
    sample_texts=sample_texts
)

if not result['passed']:
    print("í† í¬ë‚˜ì´ì € ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤:")
    for error in result['errors']:
        print(f"  - {error}")
```

#### í•™ìŠµë¥ ë§Œ ê²€ì¦

```python
result = checker.check_learning_rate(
    learning_rate=3e-5,
    model_size=139_420_672,
    batch_size=16
)

for warning in result['warnings']:
    print(f"Warning: {warning}")
```

---

### 3. ì»¤ìŠ¤í…€ ì„¤ì •

```python
# ì»¤ìŠ¤í…€ ê²€ì¦ ê¸°ì¤€
custom_config = {
    'tokenizer': {
        'max_length_min': 256,      # ìµœì†Œ ê¸¸ì´ ì¦ê°€
        'vocab_size_min': 5000,     # ìµœì†Œ vocab í¬ê¸° ì¦ê°€
    },
    'learning_rate': {
        'min': 5e-6,
        'max': 1e-4,
        'recommended_min': 1e-5,
        'recommended_max': 3e-5     # ê¶Œì¥ ë²”ìœ„ ì¶•ì†Œ
    },
    'generation': {
        'min_length': 15,            # ìµœì†Œ ê¸¸ì´ ì¦ê°€
        'max_repetition_ratio': 0.2  # ë°˜ë³µ í—ˆìš© ë²”ìœ„ ì¶•ì†Œ
    }
}

# ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ê²€ì¦ê¸° ìƒì„±
checker = create_baseline_checker(config=custom_config)

results = checker.run_all_checks(
    tokenizer=tokenizer,
    learning_rate=2e-5,
    model=model,
    sample_texts=sample_texts
)
```

---

### 4. ê²°ê³¼ ì €ì¥

```python
# JSONìœ¼ë¡œ ì €ì¥
checker.save_results("results/baseline_validation.json")
```

**ì €ì¥ í˜•ì‹:**

```json
{
  "all_passed": true,
  "total_checks": 3,
  "passed_count": 3,
  "total_errors": 0,
  "total_warnings": 1,
  "details": {
    "tokenization": {
      "passed": true,
      "errors": [],
      "warnings": [],
      "info": {...}
    },
    "learning_rate": {
      "passed": true,
      "errors": [],
      "warnings": ["Learning rate below recommended: 1e-05 < 2e-05"],
      "info": {...}
    },
    "generation_quality": {
      "passed": true,
      "errors": [],
      "warnings": [],
      "info": {...}
    }
  }
}
```

---

## ğŸ” ê²€ì¦ ê²°ê³¼ í•´ì„

### ê²°ê³¼ ìƒíƒœ

| ìƒíƒœ | ì˜ë¯¸ | ì¡°ì¹˜ |
|------|------|------|
| **âœ“ PASS (0 errors)** | ëª¨ë“  ê²€ì¦ í†µê³¼ | í•™ìŠµ ì§„í–‰ ê°€ëŠ¥ |
| **âš ï¸ PASS (warnings)** | í†µê³¼í•˜ì§€ë§Œ ê²½ê³  ì¡´ì¬ | ê²€í†  í›„ ì§„í–‰ |
| **âœ— FAIL** | ê²€ì¦ ì‹¤íŒ¨ | **ì„¤ì • ìˆ˜ì • í•„ìˆ˜** |

---

### í”í•œ ì˜¤ë¥˜ ë° í•´ê²°

#### 1. Vocab Size ì˜¤ë¥˜

**ì˜¤ë¥˜:**
```
Error: Vocab size too small: 500 < 1000
```

**ì›ì¸:** í† í¬ë‚˜ì´ì €ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```python
# ì˜¬ë°”ë¥¸ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# Vocab size í™•ì¸
print(f"Vocab size: {len(tokenizer)}")  # 30,000 ì´ìƒì´ì–´ì•¼ í•¨
```

---

#### 2. Special Token ëˆ„ë½

**ê²½ê³ :**
```
Warning: pad_token is not set
Warning: eos_token is not set
```

**ì›ì¸:** í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ë¯¸ì„¤ì •

**í•´ê²°:**
```python
# Special tokens ìˆ˜ë™ ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ë˜ëŠ” ëª¨ë¸ configì—ì„œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    "digit82/kobart-summarization",
    use_fast=True
)
```

---

#### 3. í•™ìŠµë¥  ë²”ìœ„ ì´ˆê³¼

**ì˜¤ë¥˜:**
```
Error: Learning rate too large: 0.01 > 0.001
```

**ì›ì¸:** í•™ìŠµë¥ ì´ ë„ˆë¬´ í¼ (ë°œì‚° ìœ„í—˜)

**í•´ê²°:**
```python
# í•™ìŠµë¥  ì¡°ì •
learning_rate = 2e-5  # 0.00002

# Config íŒŒì¼ ìˆ˜ì •
# configs/experiments/baseline_kobart.yaml
training:
  learning_rate: 2e-5  # 1e-3ì—ì„œ ìˆ˜ì •
```

---

#### 4. ë†’ì€ ë°˜ë³µ ë¹„ìœ¨

**ê²½ê³ :**
```
Warning: Sample 1: High repetition ratio 45.00%
```

**ì›ì¸:** ëª¨ë¸ì´ ê°™ì€ ë‹¨ì–´ë¥¼ ë°˜ë³µ ìƒì„±

**í•´ê²°:**
```python
# ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
outputs = model.generate(
    **inputs,
    max_length=150,
    num_beams=4,
    no_repeat_ngram_size=3,  # ì¶”ê°€: 3-gram ë°˜ë³µ ë°©ì§€
    repetition_penalty=1.2,  # ì¶”ê°€: ë°˜ë³µ íŒ¨ë„í‹°
    early_stopping=True
)
```

---

#### 5. ìƒì„± ì¶œë ¥ì´ ë„ˆë¬´ ì§§ìŒ

**ê²½ê³ :**
```
Warning: Sample 1: Output too short (3 tokens)
```

**ì›ì¸:** ëª¨ë¸ì´ ì¡°ê¸° ì¢…ë£Œ

**í•´ê²°:**
```python
# min_length ì„¤ì •
outputs = model.generate(
    **inputs,
    min_length=15,      # ì¶”ê°€: ìµœì†Œ ê¸¸ì´ ì„¤ì •
    max_length=150,
    num_beams=4,
    early_stopping=True
)
```

---

## ğŸ“ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•™ìŠµ ì‹œì‘ ì „ í•„ìˆ˜ ê²€ì¦

```
â–¡ í† í¬ë‚˜ì´ì € ì„¤ì • í™•ì¸
  â–¡ Vocab size â‰¥ 1,000
  â–¡ Special tokens ëª¨ë‘ ì„¤ì •
  â–¡ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì •ìƒ í† í¬ë‚˜ì´ì œì´ì…˜

â–¡ í•™ìŠµë¥  ì ì •ì„± í™•ì¸
  â–¡ 1e-6 ~ 1e-3 ë²”ìœ„ ë‚´
  â–¡ ëª¨ë¸ í¬ê¸° ê³ ë ¤
  â–¡ ë°°ì¹˜ í¬ê¸° ê³ ë ¤

â–¡ ìƒì„± í’ˆì§ˆ í™•ì¸ (ëª¨ë¸ ìˆëŠ” ê²½ìš°)
  â–¡ ì˜¤ë¥˜ ì—†ì´ ìƒì„±
  â–¡ ì¶œë ¥ ê¸¸ì´ ì ì ˆ
  â–¡ ë°˜ë³µ ë¹„ìœ¨ < 30%
```

---

## ğŸš€ ì‹¤ì „ í™œìš© ì˜ˆì‹œ

### í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— í†µí•©

```python
# scripts/train.py

from src.validation import create_baseline_checker
from src.config import load_config
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

def main():
    # Config ë¡œë“œ
    config = load_config("baseline_kobart")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = AutoModelForSeq2SeqLM.from_pretrained(config.model.name)
    tokenizer = AutoTokenizer.from_pretrained(config.model.name)

    # === ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ===
    print("\n" + "="*60)
    print("í•™ìŠµ ì‹œì‘ ì „ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦")
    print("="*60)

    checker = create_baseline_checker()

    # ìƒ˜í”Œ ë°ì´í„°
    import pandas as pd
    train_df = pd.read_csv(config.data.train_path)
    sample_texts = train_df['dialogue'].tolist()[:5]

    # ì „ì²´ ê²€ì¦
    results = checker.run_all_checks(
        tokenizer=tokenizer,
        learning_rate=config.training.learning_rate,
        model=model,
        sample_texts=sample_texts
    )

    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ
    if not results['all_passed']:
        print("\nâŒ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ì‹¤íŒ¨!")
        print(f"Errors: {results['total_errors']}")
        print("\nì„¤ì •ì„ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    # ê²½ê³ ê°€ ìˆìœ¼ë©´ í™•ì¸ ìš”ì²­
    if results['total_warnings'] > 0:
        print(f"\nâš ï¸ {results['total_warnings']}ê°œì˜ ê²½ê³ ê°€ ìˆìŠµë‹ˆë‹¤.")
        response = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            return

    # ê²€ì¦ ê²°ê³¼ ì €ì¥
    checker.save_results("results/baseline_validation.json")

    print("\nâœ“ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ í†µê³¼!")
    print("="*60)

    # === í•™ìŠµ ì§„í–‰ ===
    # í•™ìŠµ ì½”ë“œ...
```

---

## ğŸ“Š íŒ©í† ë¦¬ í•¨ìˆ˜

### create_baseline_checker()

```python
from src.validation import create_baseline_checker

# ê¸°ë³¸ ì„¤ì •
checker = create_baseline_checker()

# ì»¤ìŠ¤í…€ ì„¤ì •
custom_config = {
    'tokenizer': {...},
    'learning_rate': {...},
    'generation': {...}
}
checker = create_baseline_checker(config=custom_config)

# Logger ì‚¬ìš©
from src.logging import WandBLogger

logger = WandBLogger(project="nlp-competition")
checker = create_baseline_checker(logger=logger)
```

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

**ì†ŒìŠ¤ ì½”ë“œ:**
- `src/validation/baseline_checker.py` - BaselineChecker í´ë˜ìŠ¤
- `src/validation/__init__.py` - íŒ¨í‚¤ì§€ ì´ˆê¸°í™”

**í…ŒìŠ¤íŠ¸:**
- `src/tests/test_baseline_checker.py` - ê²€ì¦ í…ŒìŠ¤íŠ¸

**ë¬¸ì„œ:**
- `docs/ëª¨ë“ˆí™”/00_ì „ì²´_ì‹œìŠ¤í…œ_ê°œìš”.md` - ì‹œìŠ¤í…œ ê°œìš”
- `docs/PRD/18_ë² ì´ìŠ¤ë¼ì¸_ê²€ì¦_ì „ëµ.md` - PRD ë¬¸ì„œ

**Config:**
- `configs/validation/baseline.yaml` - ê²€ì¦ ì„¤ì •
