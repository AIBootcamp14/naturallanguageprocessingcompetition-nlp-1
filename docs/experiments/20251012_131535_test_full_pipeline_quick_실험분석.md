# ì‹¤í—˜ ë¶„ì„ ë³´ê³ ì„œ: 20251012_131535_test_full_pipeline_quick

## 1. ì‹¤í—˜ ê°œìš”

### 1.1 ì‹¤í—˜ ì •ë³´
- **ì‹¤í—˜ ID**: `20251012_131535_test_full_pipeline_quick`
- **ì‹¤í—˜ ì¼ì‹œ**: 2025ë…„ 10ì›” 12ì¼ 13:15:35
- **ì‹¤í—˜ ëª¨ë“œ**: Full Pipeline (í•™ìŠµ + ì•™ìƒë¸” + Solar API + TTA + ì¶”ë¡ )
- **ì‹¤í—˜ ëª©ì **: 6ê°œ ëª¨ë¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° BFloat16 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸

### 1.2 ëª¨ë¸ êµ¬ì„±
| ëª¨ë¸ëª… | íƒ€ì… | í¬ê¸° | ìƒíƒœ |
|--------|------|------|------|
| kobart | Seq2Seq | 123M | âœ… ì„±ê³µ |
| llama-3.2-korean-3b | Causal LM | 3B | âŒ ì‹¤íŒ¨ |
| qwen3-4b | Causal LM | 4B | âŒ ì‹¤íŒ¨ |
| solar-10.7b | Causal LM | 10.7B | âŒ ì‹¤íŒ¨ |
| polyglot-ko-12.8b | Causal LM | 12.8B | âŒ ì‹¤íŒ¨ |
| kullm-v2 | Causal LM | 12.8B | âŒ ì‹¤íŒ¨ |

### 1.3 ì‹¤í—˜ íƒ€ì„ë¼ì¸
```mermaid
gantt
    title ì‹¤í—˜ ì§„í–‰ íƒ€ì„ë¼ì¸
    dateFormat  HH:mm
    axisFormat %H:%M

    section ëª¨ë¸ í•™ìŠµ
    kobart (ì„±ê³µ)           :done, kobart, 13:15, 13:19
    llama-3.2-korean-3b (ì‹¤íŒ¨) :crit, llama, 13:19, 13:20
    qwen3-4b (ì‹¤íŒ¨)         :crit, qwen, 13:20, 13:27
    solar-10.7b (ì‹¤íŒ¨)      :crit, solar, 13:27, 13:36
    polyglot-ko-12.8b (ì‹¤íŒ¨) :crit, polyglot, 13:36, 13:49
    kullm-v2 (ì‹¤íŒ¨)         :crit, kullm, 13:49, 13:50

    section íŒŒì´í”„ë¼ì¸
    Solar API í‰ê°€          :done, solar_api, 13:49, 13:50
    ìµœì í™” ë° ì¶”ë¡           :done, inference, 13:50, 13:50
```

## 2. ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### 2.1 ì „ì²´ ì„±ê³µë¥ 
- **ì„±ê³µ**: 1/6 ëª¨ë¸ (16.7%)
- **ì‹¤íŒ¨**: 5/6 ëª¨ë¸ (83.3%)
- **ì£¼ìš” ì‹¤íŒ¨ ì›ì¸**: BFloat16 AMP í˜¸í™˜ì„± ë¬¸ì œ (80%), Device Map ì„¤ì • ë¬¸ì œ (20%)

### 2.2 íŒŒì´í”„ë¼ì¸ íë¦„ë„
```mermaid
flowchart TD
    Start([ì‹¤í—˜ ì‹œì‘<br/>20251012_131535]) --> ModelLoad[ëª¨ë¸ ë¡œë”© ë‹¨ê³„]

    ModelLoad --> KB[kobart<br/>Seq2Seq 123M]
    ModelLoad --> LL[llama-3.2-korean-3b<br/>Causal LM 3B]
    ModelLoad --> QW[qwen3-4b<br/>Causal LM 4B]
    ModelLoad --> SL[solar-10.7b<br/>Causal LM 10.7B]
    ModelLoad --> PG[polyglot-ko-12.8b<br/>Causal LM 12.8B]
    ModelLoad --> KU[kullm-v2<br/>Causal LM 12.8B]

    KB --> KBTrain[í•™ìŠµ ì§„í–‰]
    KBTrain --> KBSuccess[âœ… í•™ìŠµ ì„±ê³µ<br/>ROUGE-1: 0.413]

    LL --> LLError[âŒ BFloat16 ì—ëŸ¬<br/>AMP GradScaler í˜¸í™˜ ë¶ˆê°€]
    QW --> QWError[âŒ BFloat16 ì—ëŸ¬<br/>AMP GradScaler í˜¸í™˜ ë¶ˆê°€]
    SL --> SLError[âŒ BFloat16 ì—ëŸ¬<br/>AMP GradScaler í˜¸í™˜ ë¶ˆê°€]
    PG --> PGError[âŒ BFloat16 ì—ëŸ¬<br/>AMP GradScaler í˜¸í™˜ ë¶ˆê°€]
    KU --> KUError[âŒ Device Map ì—ëŸ¬<br/>offload_folder ëˆ„ë½]

    KBSuccess --> Ensemble[ì•™ìƒë¸”<br/>ë‹¨ì¼ ëª¨ë¸ë¡œ ì§„í–‰]
    Ensemble --> Solar[Solar API í‰ê°€<br/>50 ìƒ˜í”Œ]
    Solar --> TTA[TTA ì ìš© ì‹œë„<br/>ì‹¤íŒ¨: ë‹¨ì¼ ëª¨ë¸]
    TTA --> Inference[ì¶”ë¡  ì‹¤í–‰<br/>ì‹¤íŒ¨: token_type_ids ì—ëŸ¬]

    Inference --> End([ì‹¤í—˜ ì¢…ë£Œ<br/>ë¶€ë¶„ ì„±ê³µ])

    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef warningNode fill:#fff9c4,stroke:#f57f17,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class KBSuccess,Ensemble,Solar successNode
    class LLError,QWError,SLError,PGError,KUError,TTA,Inference errorNode
    class KB,LL,QW,SL,PG,KU processNode
    class Start,End warningNode
```

## 3. ì—ëŸ¬ ìƒì„¸ ë¶„ì„

### 3.1 ì—ëŸ¬ #1: BFloat16 AMP í˜¸í™˜ì„± ë¬¸ì œ (4ê°œ ëª¨ë¸)

#### ì˜í–¥ ë°›ì€ ëª¨ë¸
- llama-3.2-korean-3b
- qwen3-4b
- solar-10.7b
- polyglot-ko-12.8b

#### ì—ëŸ¬ ë©”ì‹œì§€
```
NotImplementedError: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
```

#### ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜
```python
File "torch/amp/grad_scaler.py", line 283, in _unscale_grads_
    torch._amp_foreach_non_finite_check_and_unscale_(...)
```

#### ê·¼ë³¸ ì›ì¸ ë¶„ì„

**1. ë¬¸ì œì˜ í•µì‹¬**
- PyTorchì˜ AMP (Automatic Mixed Precision) GradScalerê°€ BFloat16 ë°ì´í„° íƒ€ì…ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
- `_amp_foreach_non_finite_check_and_unscale_cuda` ì»¤ë„ì´ Float16ë§Œ ì§€ì›í•˜ê³  BFloat16ì€ ë¯¸êµ¬í˜„ ìƒíƒœ

**2. ì—ëŸ¬ ë°œìƒ ê²½ë¡œ**
```mermaid
flowchart LR
    A[Training Loop] --> B[Gradient Clipping]
    B --> C[accelerator.clip_grad_norm_]
    C --> D[unscale_gradients]
    D --> E[scaler.unscale_]
    E --> F[_unscale_grads_]
    F --> G[_amp_foreach_non_finite_check_and_unscale_cuda]
    G --> H[âŒ NotImplementedError<br/>BFloat16 ë¯¸ì§€ì›]

    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class H errorNode
    class A,B,C,D,E,F,G processNode
```

**3. ì½”ë“œ ë ˆë²¨ ì›ì¸**

`src/models/lora_loader.py:108-113`:
```python
# dtype ê²°ì • (Llama: bf16, Qwen: fp16)
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen ëª¨ë¸: fp16 ì‚¬ìš©")
else:
    self._log("  - Llama ëª¨ë¸: bf16 ì‚¬ìš©")
```

**ë¬¸ì œì **:
- Qwen ëª¨ë¸ì€ ì¡°ê±´ë¬¸ìœ¼ë¡œ Float16ìœ¼ë¡œ ì„¤ì •ë˜ì§€ë§Œ, **ì‹¤ì œë¡œëŠ” ì—¬ì „íˆ BFloat16ìœ¼ë¡œ ì‹¤í–‰ë¨**
- ì´ìœ : `llm_loader.py:48`ì—ì„œ configì— quantization ì„¤ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 'float16'ì„ ì‚¬ìš©í•˜ì§€ë§Œ, `lora_loader.py`ì—ì„œëŠ” í•˜ë“œì½”ë”©ëœ `torch.bfloat16`ì„ ë¨¼ì € ì ìš©
- Solar, Polyglot ëª¨ë¸ë„ Qwenì´ ì•„ë‹ˆë¯€ë¡œ BFloat16ìœ¼ë¡œ ì„¤ì •ë¨
- Llama ëª¨ë¸ë„ BFloat16ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ë™ì¼í•œ ì—ëŸ¬ ë°œìƒ

**4. ì™œ kobartëŠ” ì„±ê³µí–ˆëŠ”ê°€?**
- kobartëŠ” Seq2Seq ëª¨ë¸ë¡œ `llm_loader.py`ë‚˜ `lora_loader.py`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- `bart_loader.py`ë¥¼ í†µí•´ ë¡œë”©ë˜ë©°, QLoRAë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- ë”°ë¼ì„œ BFloat16 ê´€ë ¨ ì„¤ì •ì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: src/models/lora_loader.py ìˆ˜ì • (ê¶Œì¥)**

`src/models/lora_loader.py:108` ë¼ì¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •:

```python
# ë³€ê²½ ì „
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen ëª¨ë¸: fp16 ì‚¬ìš©")
else:
    self._log("  - Llama ëª¨ë¸: bf16 ì‚¬ìš©")

# ë³€ê²½ í›„
# PyTorch AMP GradScalerëŠ” BFloat16ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëª¨ë“  ëª¨ë¸ì— Float16 ì‚¬ìš©
compute_dtype = torch.float16
self._log("  - QLoRA compute dtype: fp16 (AMP í˜¸í™˜)")
```

**ë°©ë²• 2: src/models/llm_loader.py ìˆ˜ì • (ëŒ€ì•ˆ)**

`src/models/llm_loader.py:46-49` ë¼ì¸ ìˆ˜ì •:

```python
# ë³€ê²½ ì „
bnb_4bit_compute_dtype=getattr(
    torch,
    config.model.quantization.get('bnb_4bit_compute_dtype', 'float16')
),

# ë³€ê²½ í›„ (ëª…ì‹œì ìœ¼ë¡œ float16 ê°•ì œ)
bnb_4bit_compute_dtype=torch.float16,  # AMP í˜¸í™˜ì„±ì„ ìœ„í•´ float16 ê³ ì •
```

**ë°©ë²• 3: Training ì˜µì…˜ ìˆ˜ì • (ì„ì‹œ ë°©í¸)**

`configs/base/causal_lm.yaml`ì— ë‹¤ìŒ ì¶”ê°€:
```yaml
training:
  bf16: false  # BFloat16 ë¹„í™œì„±í™”
  fp16: true   # Float16 ì‚¬ìš©
```

**ê¶Œì¥ ì‚¬í•­**: **ë°©ë²• 1**ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
- ê°€ì¥ ì§ì ‘ì ì´ê³  ê·¼ë³¸ì ì¸ í•´ê²°ì±…
- ëª¨ë“  Causal LM ëª¨ë¸ì— ì¼ê´€ë˜ê²Œ ì ìš©
- AMP GradScalerì™€ì˜ í˜¸í™˜ì„± ë³´ì¥

### 3.2 ì—ëŸ¬ #2: Device Map Offload ì„¤ì • ë¬¸ì œ (1ê°œ ëª¨ë¸)

#### ì˜í–¥ ë°›ì€ ëª¨ë¸
- kullm-v2 (12.8B)

#### ì—ëŸ¬ ë©”ì‹œì§€
```
ValueError: The current `device_map` had weights offloaded to the disk.
Please provide an `offload_folder` for them.
Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.
```

#### ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜
```python
File "transformers/modeling_utils.py", line 5387, in _load_pretrained_model
    raise ValueError(...)
```

#### ê·¼ë³¸ ì›ì¸ ë¶„ì„

**1. ë¬¸ì œì˜ í•µì‹¬**
- kullm-v2ëŠ” 12.8B íŒŒë¼ë¯¸í„°ì˜ ëŒ€í˜• ëª¨ë¸
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¼ë¶€ ê°€ì¤‘ì¹˜ê°€ ë””ìŠ¤í¬ë¡œ ì˜¤í”„ë¡œë“œë¨
- ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ ì‹œ `offload_folder` ê²½ë¡œê°€ í•„ìš”í•˜ì§€ë§Œ ì§€ì •ë˜ì§€ ì•ŠìŒ

**2. ì—ëŸ¬ ë°œìƒ ê²½ë¡œ**
```mermaid
flowchart LR
    A[Model Loading] --> B[AutoModelForCausalLM.from_pretrained]
    B --> C[device_map='auto' ì ìš©]
    C --> D[GPU ë©”ëª¨ë¦¬ ì²´í¬]
    D --> E{ë©”ëª¨ë¦¬ ì¶©ë¶„?}
    E -->|Yes| F[GPUì— ì „ì²´ ë¡œë”©]
    E -->|No| G[ì¼ë¶€ ê°€ì¤‘ì¹˜ ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ]
    G --> H{offload_folder ì„¤ì •?}
    H -->|Yes| I[ë””ìŠ¤í¬ì— ì„ì‹œ ì €ì¥]
    H -->|No| J[âŒ ValueError<br/>offload_folder ëˆ„ë½]

    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class J errorNode
    class F,I successNode
    class A,B,C,D,G processNode
```

**3. ì½”ë“œ ë ˆë²¨ ì›ì¸**

`src/models/llm_loader.py:58-64`:
```python
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",  # ìë™ ë””ë°”ì´ìŠ¤ í• ë‹¹
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)
# offload_folder íŒŒë¼ë¯¸í„° ëˆ„ë½!
```

**ë¬¸ì œì **:
- `device_map="auto"`ëŠ” GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ ìë™ìœ¼ë¡œ CPU/ë””ìŠ¤í¬ë¡œ ì˜¤í”„ë¡œë“œ
- ëŒ€í˜• ëª¨ë¸(10B ì´ìƒ)ì€ 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•´ë„ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥
- `offload_folder` ë¯¸ì§€ì • ì‹œ ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ ë¶ˆê°€

#### í•´ê²° ë°©ë²•

**ë°©ë²• 1: offload_folder ì¶”ê°€ (ê¶Œì¥)**

`src/models/llm_loader.py:58` ë¼ì¸ ìˆ˜ì •:

```python
# ë³€ê²½ ì „
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)

# ë³€ê²½ í›„
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    offload_folder=str(offload_dir),  # ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œ í´ë” ì§€ì •
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)
```

**ë°©ë²• 2: safetensors ì„¤ì¹˜ í™•ì¸**

```bash
pip install safetensors
```

ì¼ë¶€ ëª¨ë¸ì€ safetensors í˜•ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì œê³µí•˜ë©°, ì´ ê²½ìš° ë” íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë”© ê°€ëŠ¥.

**ë°©ë²• 3: max_memory ì„¤ì •ìœ¼ë¡œ ëª…ì‹œì  ì œì–´**

```python
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    max_memory={0: "20GiB", "cpu": "30GiB"},  # GPU 0: 20GB, CPU: 30GB
    offload_folder=str(offload_dir),
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

**ê¶Œì¥ ì‚¬í•­**: **ë°©ë²• 1 + ë°©ë²• 2**ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©
- safetensors ë¨¼ì € ì„¤ì¹˜
- offload_folderë¥¼ ì‹¤í—˜ í´ë” ë‚´ `offload/` ì„œë¸Œë””ë ‰í† ë¦¬ë¡œ ì§€ì •
- ëŒ€í˜• ëª¨ë¸ì˜ ì•ˆì •ì ì¸ ë¡œë”© ë³´ì¥

### 3.3 ì—ëŸ¬ #3: ì¶”ë¡  ë‹¨ê³„ ì—ëŸ¬

#### ì—ëŸ¬ ë©”ì‹œì§€
```
The following `model_kwargs` are not used by the model: ['token_type_ids']
```

#### ê·¼ë³¸ ì›ì¸
- ì¼ë¶€ ëª¨ë¸(íŠ¹íˆ Causal LM)ì€ `token_type_ids`ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- Tokenizerê°€ ìë™ìœ¼ë¡œ `token_type_ids`ë¥¼ ìƒì„±í•˜ì§€ë§Œ ëª¨ë¸ì´ ì´ë¥¼ ë¬´ì‹œ
- ê²½ê³ ì„± ë©”ì‹œì§€ì´ì§€ë§Œ, ì¶”ë¡  ì‹¤í–‰ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

#### í•´ê²° ë°©ë²•

`src/inference/predictor.py` ë˜ëŠ” í•´ë‹¹ ì¶”ë¡  ì½”ë“œì—ì„œ:

```python
# ë³€ê²½ ì „
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
outputs = model.generate(**inputs, ...)

# ë³€ê²½ í›„
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
# token_type_ids ì œê±° (Causal LMì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']
outputs = model.generate(**inputs, ...)
```

## 4. ì„±ê³µ ëª¨ë¸ ë¶„ì„: kobart

### 4.1 í•™ìŠµ ì„±ëŠ¥

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| **ROUGE-1** | 0.4134 |
| **ROUGE-2** | 0.2552 |
| **ROUGE-L** | 0.4065 |
| **ROUGE-Sum** | 1.0751 |
| **Loss** | 1.4561 |
| **í•™ìŠµ ì‹œê°„** | ~4ë¶„ |
| **í‰ê°€ ì†ë„** | 6.37 samples/sec |

### 4.2 ëª¨ë¸ ì •ë³´
- **ì²´í¬í¬ì¸íŠ¸**: `gogamza/kobart-base-v2`
- **ì•„í‚¤í…ì²˜**: BART (Seq2Seq)
- **íŒŒë¼ë¯¸í„°**: 123M
- **í•™ìŠµ ì„¤ì •**:
  - Epochs: 1
  - Batch Size: 8
  - Learning Rate: 5e-6
  - Gradient Accumulation: 4
  - Warmup Ratio: 0.1

### 4.3 ì„±ëŠ¥ í‰ê°€

**ì¥ì **:
- âœ… ì•ˆì •ì ì¸ í•™ìŠµ ì™„ë£Œ
- âœ… BFloat16 ë¬¸ì œ ì—†ìŒ (Seq2Seq ì•„í‚¤í…ì²˜)
- âœ… ì–‘í˜¸í•œ ROUGE ì ìˆ˜ (ROUGE-1: 0.413)
- âœ… ë¹ ë¥¸ ì¶”ë¡  ì†ë„

**í•œê³„ì **:
- âš ï¸ ë‹¨ì¼ ëª¨ë¸ë¡œëŠ” ì•™ìƒë¸” íš¨ê³¼ ì—†ìŒ
- âš ï¸ Causal LM ëŒ€ë¹„ íŒŒë¼ë¯¸í„° ìˆ˜ ì ìŒ (123M vs 3B~12.8B)
- âš ï¸ ìµœì‹  LLM ëŒ€ë¹„ ì„±ëŠ¥ ì œí•œì 

## 5. Solar API í‰ê°€ ê²°ê³¼

### 5.1 Solar API ì„±ëŠ¥
| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| **Solar ROUGE-1 F1** | 0.2272 |
| **Solar ROUGE-2 F1** | 0.0765 |
| **Solar ROUGE-L F1** | 0.2177 |
| **í‰ê°€ ìƒ˜í”Œ ìˆ˜** | 50 |

### 5.2 kobart vs Solar API ë¹„êµ

```mermaid
graph LR
    subgraph "ROUGE-1 ë¹„êµ"
    K1[kobart: 0.413]
    S1[Solar API: 0.227]
    end

    subgraph "ROUGE-2 ë¹„êµ"
    K2[kobart: 0.255]
    S2[Solar API: 0.077]
    end

    subgraph "ROUGE-L ë¹„êµ"
    K3[kobart: 0.407]
    S3[Solar API: 0.218]
    end

    K1 -.->|+81.9%| S1
    K2 -.->|+231.4%| S2
    K3 -.->|+86.9%| S3

    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef warningNode fill:#fff9c4,stroke:#f57f17,color:#000

    class K1,K2,K3 successNode
    class S1,S2,S3 warningNode
```

**ë¶„ì„**:
- kobartê°€ Solar API ëŒ€ë¹„ **ëª¨ë“  ë©”íŠ¸ë¦­ì—ì„œ ìš°ìˆ˜**
- ROUGE-2ì—ì„œ ê°€ì¥ í° ì°¨ì´ (231.4% ë†’ìŒ)
- Solar APIëŠ” Zero-shot ì„±ëŠ¥ì´ë¯€ë¡œ, Fine-tuningëœ kobartê°€ ë‹¹ì—°íˆ ìš°ìˆ˜

## 6. TTA ë° ìµœì í™” ê²°ê³¼

### 6.1 TTA (Test-Time Augmentation)
- **ì ìš© ì—¬ë¶€**: âŒ ì‹¤íŒ¨
- **ì´ìœ **: ë‹¨ì¼ ëª¨ë¸ë§Œ ì„±ê³µí•˜ì—¬ ì•™ìƒë¸” ë¶ˆê°€
- **ì„¤ì •ëœ ì „ëµ**: paraphrase, reorder, synonym, mask (num_aug=2)

### 6.2 ìµœì í™”
- **ìµœì í™” í´ë”**: `experiments/20251012/20251012_131535_test_full_pipeline_quick/optimized/`
- **ìƒíƒœ**: ìƒì„±ë¨ (í•˜ì§€ë§Œ ì¶”ë¡  ì‹¤íŒ¨ë¡œ í™œìš©ë˜ì§€ ì•ŠìŒ)

## 7. ì‹¤í—˜ í•™ìŠµ ì‚¬í•­ ë° ê¶Œì¥ ì¡°ì¹˜

### 7.1 ì¦‰ì‹œ ì ìš© í•„ìš”í•œ ìˆ˜ì • ì‚¬í•­

#### ìš°ì„ ìˆœìœ„ 1: BFloat16 ë¬¸ì œ í•´ê²° (Critical)
```python
# src/models/lora_loader.py:108-113
# ë³€ê²½ ì „
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16

# ë³€ê²½ í›„
compute_dtype = torch.float16  # AMP í˜¸í™˜ì„±ì„ ìœ„í•´ ëª¨ë“  ëª¨ë¸ì— Float16 ì‚¬ìš©
self._log("  - QLoRA compute dtype: fp16 (AMP í˜¸í™˜)")
```

**ì˜í–¥**: 4ê°œ ëª¨ë¸ (llama, qwen3, solar, polyglot) í•™ìŠµ ê°€ëŠ¥

#### ìš°ì„ ìˆœìœ„ 2: offload_folder ì„¤ì • (High)
```python
# src/models/llm_loader.py:58
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    ...,
    offload_folder=str(offload_dir),
    ...
)
```

**ì˜í–¥**: kullm-v2 ëª¨ë¸ ë¡œë”© ê°€ëŠ¥

#### ìš°ì„ ìˆœìœ„ 3: token_type_ids ì œê±° (Medium)
```python
# ì¶”ë¡  ì½”ë“œ
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']
```

**ì˜í–¥**: ì¶”ë¡  ë‹¨ê³„ ì •ìƒ ì‹¤í–‰

### 7.2 ë¬¸ì„œ ì—…ë°ì´íŠ¸ í•„ìš” ì‚¬í•­

#### PRD ë¬¸ì„œ
- **PRD 08 (LLM íŒŒì¸íŠœë‹ ì „ëµ)**: BFloat16 â†’ Float16 ë³€ê²½ ì‚¬í•­ ë°˜ì˜
- QLoRA ì„¤ì •ì—ì„œ `bnb_4bit_compute_dtype=torch.float16` ëª…ì‹œ

#### ëª¨ë“ˆí™” ë¬¸ì„œ
- **`docs/ëª¨ë“ˆí™”/02_í•µì‹¬_ì‹œìŠ¤í…œ.md`**:
  - LLM Loader ì„¹ì…˜ì— AMP í˜¸í™˜ì„± ì£¼ì˜ì‚¬í•­ ì¶”ê°€
  - offload_folder ì„¤ì • ê°€ì´ë“œ ì¶”ê°€

- **`docs/ëª¨ë“ˆí™”/05_íŠ¸ëŸ¬ë¸”ìŠˆíŒ…_ê°€ì´ë“œ.md`** (ì¡´ì¬ ì‹œ):
  - BFloat16 ì—ëŸ¬ í•´ê²° ë°©ë²• ì¶”ê°€
  - Device Map ì—ëŸ¬ í•´ê²° ë°©ë²• ì¶”ê°€

### 7.3 ì¬ì‹¤í—˜ ê¶Œì¥ ì‚¬í•­

ìˆ˜ì • ì‚¬í•­ ì ìš© í›„ ë‹¤ìŒ ì‹¤í—˜ ê¶Œì¥:

```bash
python scripts/train.py \
  --mode full \
  --models kobart llama-3.2-korean-3b qwen3-4b solar-10.7b polyglot-ko-12.8b kullm-v2 \
  --epochs 1 \
  --batch_size 8 \
  --learning_rate 5e-6 \
  --gradient_accumulation_steps 4 \
  --warmup_ratio 0.1 \
  --weight_decay 0.01 \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --k_folds 2 \
  --fold_seed 42 \
  --ensemble_strategy stacking \
  --num_beams 4 \
  --temperature 0.7 \
  --top_p 0.9 \
  --top_k 50 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3 \
  --experiment_name test_full_pipeline_fixed \
  --seed 42
```

**ì˜ˆìƒ ê²°ê³¼**:
- âœ… 6/6 ëª¨ë¸ í•™ìŠµ ì„±ê³µ
- âœ… ì•™ìƒë¸” ì •ìƒ ë™ì‘
- âœ… TTA ì ìš© ê°€ëŠ¥
- âœ… ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ì„±ê³µ

## 8. ê²°ë¡ 

### 8.1 ì‹¤í—˜ ìš”ì•½
ì´ë²ˆ ì‹¤í—˜ì€ **ë¶€ë¶„ì  ì„±ê³µ**ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤:
- âœ… kobart ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì„±ê³µ
- âœ… Solar API êµì°¨ ê²€ì¦ ì™„ë£Œ
- âŒ 5ê°œ LLM ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨
- âŒ ì•™ìƒë¸” ë° TTA ë¶ˆê°€
- âŒ ì¶”ë¡  ë‹¨ê³„ ì—ëŸ¬

### 8.2 í•µì‹¬ ë°œê²¬ ì‚¬í•­

1. **BFloat16 í˜¸í™˜ì„± ë¬¸ì œ**: PyTorch AMP GradScalerê°€ BFloat16ì„ ì§€ì›í•˜ì§€ ì•ŠìŒì„ í™•ì¸
2. **ëŒ€í˜• ëª¨ë¸ ë©”ëª¨ë¦¬ ê´€ë¦¬**: 12.8B ëª¨ë¸ì€ offload_folder ì„¤ì • í•„ìˆ˜
3. **í† í¬ë‚˜ì´ì € í˜¸í™˜ì„±**: Causal LMì€ token_type_ids ë¶ˆí•„ìš”

### 8.3 ë‹¤ìŒ ë‹¨ê³„
1. âœ… ë³¸ ë¶„ì„ ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ
2. ğŸ”„ `src/models/lora_loader.py` ìˆ˜ì • (BFloat16 â†’ Float16)
3. ğŸ”„ `src/models/llm_loader.py` ìˆ˜ì • (offload_folder ì¶”ê°€)
4. ğŸ”„ ì¶”ë¡  ì½”ë“œ ìˆ˜ì • (token_type_ids ì œê±°)
5. ğŸ”„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ (`docs/ëª¨ë“ˆí™”/02_í•µì‹¬_ì‹œìŠ¤í…œ.md`)
6. ğŸ”„ ì¬ì‹¤í—˜ ì‹¤í–‰ (`test_full_pipeline_fixed`)


