# μ‹¤ν— λ¶„μ„ λ³΄κ³ μ„: 20251012_131535_test_full_pipeline_quick

## 1. μ‹¤ν— κ°μ”

### 1.1 μ‹¤ν— μ •λ³΄
- **μ‹¤ν— ID**: `20251012_131535_test_full_pipeline_quick`
- **μ‹¤ν— μΌμ‹**: 2025λ…„ 10μ›” 12μΌ 13:15:35
- **μ‹¤ν— λ¨λ“**: Full Pipeline (ν•™μµ + μ•™μƒλΈ” + Solar API + TTA + μ¶”λ΅ )
- **μ‹¤ν— λ©μ **: 6κ° λ¨λΈ μ „μ²΄ νμ΄ν”„λΌμΈ κ²€μ¦ λ° BFloat16 νΈν™μ„± ν…μ¤νΈ

### 1.2 λ¨λΈ κµ¬μ„±
| λ¨λΈλ… | νƒ€μ… | ν¬κΈ° | μƒνƒ |
|--------|------|------|------|
| kobart | Seq2Seq | 123M | β… μ„±κ³µ |
| llama-3.2-korean-3b | Causal LM | 3B | β μ‹¤ν¨ |
| qwen3-4b | Causal LM | 4B | β μ‹¤ν¨ |
| solar-10.7b | Causal LM | 10.7B | β μ‹¤ν¨ |
| polyglot-ko-12.8b | Causal LM | 12.8B | β μ‹¤ν¨ |
| kullm-v2 | Causal LM | 12.8B | β μ‹¤ν¨ |

### 1.3 μ‹¤ν— νƒ€μ„λΌμΈ
```mermaid
gantt
    title μ‹¤ν— μ§„ν–‰ νƒ€μ„λΌμΈ
    dateFormat  HH:mm
    axisFormat %H:%M

    section λ¨λΈ ν•™μµ
    kobart (μ„±κ³µ)           :done, kobart, 13:15, 13:19
    llama-3.2-korean-3b (μ‹¤ν¨) :crit, llama, 13:19, 13:20
    qwen3-4b (μ‹¤ν¨)         :crit, qwen, 13:20, 13:27
    solar-10.7b (μ‹¤ν¨)      :crit, solar, 13:27, 13:36
    polyglot-ko-12.8b (μ‹¤ν¨) :crit, polyglot, 13:36, 13:49
    kullm-v2 (μ‹¤ν¨)         :crit, kullm, 13:49, 13:50

    section νμ΄ν”„λΌμΈ
    Solar API ν‰κ°€          :done, solar_api, 13:49, 13:50
    μµμ ν™” λ° μ¶”λ΅           :done, inference, 13:50, 13:50
```

## 2. μ‹¤ν— κ²°κ³Ό λ¶„μ„

### 2.1 μ „μ²΄ μ„±κ³µλ¥ 
- **μ„±κ³µ**: 1/6 λ¨λΈ (16.7%)
- **μ‹¤ν¨**: 5/6 λ¨λΈ (83.3%)
- **μ£Όμ” μ‹¤ν¨ μ›μΈ**: BFloat16 AMP νΈν™μ„± λ¬Έμ  (80%), Device Map μ„¤μ • λ¬Έμ  (20%)

### 2.2 νμ΄ν”„λΌμΈ νλ¦„λ„
```mermaid
flowchart TD
    Start([μ‹¤ν— μ‹μ‘<br/>20251012_131535]) --> ModelLoad[λ¨λΈ λ΅λ”© λ‹¨κ³„]

    ModelLoad --> KB[kobart<br/>Seq2Seq 123M]
    ModelLoad --> LL[llama-3.2-korean-3b<br/>Causal LM 3B]
    ModelLoad --> QW[qwen3-4b<br/>Causal LM 4B]
    ModelLoad --> SL[solar-10.7b<br/>Causal LM 10.7B]
    ModelLoad --> PG[polyglot-ko-12.8b<br/>Causal LM 12.8B]
    ModelLoad --> KU[kullm-v2<br/>Causal LM 12.8B]

    KB --> KBTrain[ν•™μµ μ§„ν–‰]
    KBTrain --> KBSuccess[β… ν•™μµ μ„±κ³µ<br/>ROUGE-1: 0.413]

    LL --> LLError[β BFloat16 μ—λ¬<br/>AMP GradScaler νΈν™ λ¶κ°€]
    QW --> QWError[β BFloat16 μ—λ¬<br/>AMP GradScaler νΈν™ λ¶κ°€]
    SL --> SLError[β BFloat16 μ—λ¬<br/>AMP GradScaler νΈν™ λ¶κ°€]
    PG --> PGError[β BFloat16 μ—λ¬<br/>AMP GradScaler νΈν™ λ¶κ°€]
    KU --> KUError[β Device Map μ—λ¬<br/>offload_folder λ„λ½]

    KBSuccess --> Ensemble[μ•™μƒλΈ”<br/>λ‹¨μΌ λ¨λΈλ΅ μ§„ν–‰]
    Ensemble --> Solar[Solar API ν‰κ°€<br/>50 μƒν”]
    Solar --> TTA[TTA μ μ© μ‹λ„<br/>μ‹¤ν¨: λ‹¨μΌ λ¨λΈ]
    TTA --> Inference[μ¶”λ΅  μ‹¤ν–‰<br/>μ‹¤ν¨: token_type_ids μ—λ¬]

    Inference --> End([μ‹¤ν— μΆ…λ£<br/>λ¶€λ¶„ μ„±κ³µ])

    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef warningNode fill:#fff9c4,stroke:#f57f17,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class KBSuccess,Ensemble,Solar successNode
    class LLError,QWError,SLError,PGError,KUError,TTA,Inference errorNode
    class KB,LL,QW,SL,PG,KU processNode
    class Start,End warningNode
```

## 3. μ—λ¬ μƒμ„Έ λ¶„μ„

### 3.1 μ—λ¬ #1: BFloat16 AMP νΈν™μ„± λ¬Έμ  (4κ° λ¨λΈ)

#### μν–¥ λ°›μ€ λ¨λΈ
- llama-3.2-korean-3b
- qwen3-4b
- solar-10.7b
- polyglot-ko-12.8b

#### μ—λ¬ λ©”μ‹μ§€
```
NotImplementedError: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
```

#### μ—λ¬ λ°μƒ μ„μΉ
```python
File "torch/amp/grad_scaler.py", line 283, in _unscale_grads_
    torch._amp_foreach_non_finite_check_and_unscale_(...)
```

#### κ·Όλ³Έ μ›μΈ λ¶„μ„

**1. λ¬Έμ μ ν•µμ‹¬**
- PyTorchμ AMP (Automatic Mixed Precision) GradScalerκ°€ BFloat16 λ°μ΄ν„° νƒ€μ…μ„ μ§€μ›ν•μ§€ μ•μ
- `_amp_foreach_non_finite_check_and_unscale_cuda` μ»¤λ„μ΄ Float16λ§ μ§€μ›ν•κ³  BFloat16μ€ λ―Έκµ¬ν„ μƒνƒ

**2. μ—λ¬ λ°μƒ κ²½λ΅**
```mermaid
flowchart LR
    A[Training Loop] --> B[Gradient Clipping]
    B --> C[accelerator.clip_grad_norm_]
    C --> D[unscale_gradients]
    D --> E[scaler.unscale_]
    E --> F[_unscale_grads_]
    F --> G[_amp_foreach_non_finite_check_and_unscale_cuda]
    G --> H[β NotImplementedError<br/>BFloat16 λ―Έμ§€μ›]

    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class H errorNode
    class A,B,C,D,E,F,G processNode
```

**3. μ½”λ“ λ λ²¨ μ›μΈ**

`src/models/lora_loader.py:108-113`:
```python
# dtype κ²°μ • (Llama: bf16, Qwen: fp16)
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen λ¨λΈ: fp16 μ‚¬μ©")
else:
    self._log("  - Llama λ¨λΈ: bf16 μ‚¬μ©")
```

**λ¬Έμ μ **:
- Qwen λ¨λΈμ€ μ΅°κ±΄λ¬ΈμΌλ΅ Float16μΌλ΅ μ„¤μ •λμ§€λ§, **μ‹¤μ λ΅λ” μ—¬μ „ν BFloat16μΌλ΅ μ‹¤ν–‰λ¨**
- μ΄μ : `llm_loader.py:48`μ—μ„ configμ— quantization μ„¤μ •μ΄ μ—†μΌλ©΄ κΈ°λ³Έκ°’ 'float16'μ„ μ‚¬μ©ν•μ§€λ§, `lora_loader.py`μ—μ„λ” ν•λ“μ½”λ”©λ `torch.bfloat16`μ„ λ¨Όμ € μ μ©
- Solar, Polyglot λ¨λΈλ„ Qwenμ΄ μ•„λ‹λ―€λ΅ BFloat16μΌλ΅ μ„¤μ •λ¨
- Llama λ¨λΈλ„ BFloat16μΌλ΅ μ„¤μ •λμ–΄ λ™μΌν• μ—λ¬ λ°μƒ

**4. μ™ kobartλ” μ„±κ³µν–λ”κ°€?**
- kobartλ” Seq2Seq λ¨λΈλ΅ `llm_loader.py`λ‚ `lora_loader.py`λ¥Ό μ‚¬μ©ν•μ§€ μ•μ
- `bart_loader.py`λ¥Ό ν†µν•΄ λ΅λ”©λλ©°, QLoRAλ¥Ό μ‚¬μ©ν•μ§€ μ•μ
- λ”°λΌμ„ BFloat16 κ΄€λ ¨ μ„¤μ •μ μν–¥μ„ λ°›μ§€ μ•μ

#### ν•΄κ²° λ°©λ²•

**λ°©λ²• 1: src/models/lora_loader.py μμ • (κ¶μ¥)**

`src/models/lora_loader.py:108` λΌμΈμ„ λ‹¤μκ³Ό κ°™μ΄ μμ •:

```python
# λ³€κ²½ μ „
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen λ¨λΈ: fp16 μ‚¬μ©")
else:
    self._log("  - Llama λ¨λΈ: bf16 μ‚¬μ©")

# λ³€κ²½ ν›„
# PyTorch AMP GradScalerλ” BFloat16μ„ μ§€μ›ν•μ§€ μ•μΌλ―€λ΅ λ¨λ“  λ¨λΈμ— Float16 μ‚¬μ©
compute_dtype = torch.float16
self._log("  - QLoRA compute dtype: fp16 (AMP νΈν™)")
```

**λ°©λ²• 2: src/models/llm_loader.py μμ • (λ€μ•)**

`src/models/llm_loader.py:46-49` λΌμΈ μμ •:

```python
# λ³€κ²½ μ „
bnb_4bit_compute_dtype=getattr(
    torch,
    config.model.quantization.get('bnb_4bit_compute_dtype', 'float16')
),

# λ³€κ²½ ν›„ (λ…μ‹μ μΌλ΅ float16 κ°•μ )
bnb_4bit_compute_dtype=torch.float16,  # AMP νΈν™μ„±μ„ μ„ν•΄ float16 κ³ μ •
```

**λ°©λ²• 3: Training μµμ… μμ • (μ„μ‹ λ°©νΈ)**

`configs/base/causal_lm.yaml`μ— λ‹¤μ μ¶”κ°€:
```yaml
training:
  bf16: false  # BFloat16 λΉ„ν™μ„±ν™”
  fp16: true   # Float16 μ‚¬μ©
```

**κ¶μ¥ μ‚¬ν•­**: **λ°©λ²• 1**μ„ κ¶μ¥ν•©λ‹λ‹¤.
- κ°€μ¥ μ§μ ‘μ μ΄κ³  κ·Όλ³Έμ μΈ ν•΄κ²°μ±…
- λ¨λ“  Causal LM λ¨λΈμ— μΌκ΄€λκ² μ μ©
- AMP GradScalerμ™€μ νΈν™μ„± λ³΄μ¥

### 3.2 μ—λ¬ #2: Device Map Offload μ„¤μ • λ¬Έμ  (1κ° λ¨λΈ)

#### μν–¥ λ°›μ€ λ¨λΈ
- kullm-v2 (12.8B)

#### μ—λ¬ λ©”μ‹μ§€
```
ValueError: The current `device_map` had weights offloaded to the disk.
Please provide an `offload_folder` for them.
Alternatively, make sure you have `safetensors` installed if the model you are using offers the weights in this format.
```

#### μ—λ¬ λ°μƒ μ„μΉ
```python
File "transformers/modeling_utils.py", line 5387, in _load_pretrained_model
    raise ValueError(...)
```

#### κ·Όλ³Έ μ›μΈ λ¶„μ„

**1. λ¬Έμ μ ν•µμ‹¬**
- kullm-v2λ” 12.8B νλΌλ―Έν„°μ λ€ν• λ¨λΈ
- GPU λ©”λ¨λ¦¬ λ¶€μ΅±μΌλ΅ μΌλ¶€ κ°€μ¤‘μΉκ°€ λ””μ¤ν¬λ΅ μ¤ν”„λ΅λ“λ¨
- λ””μ¤ν¬ μ¤ν”„λ΅λ“ μ‹ `offload_folder` κ²½λ΅κ°€ ν•„μ”ν•μ§€λ§ μ§€μ •λμ§€ μ•μ

**2. μ—λ¬ λ°μƒ κ²½λ΅**
```mermaid
flowchart LR
    A[Model Loading] --> B[AutoModelForCausalLM.from_pretrained]
    B --> C[device_map='auto' μ μ©]
    C --> D[GPU λ©”λ¨λ¦¬ μ²΄ν¬]
    D --> E{λ©”λ¨λ¦¬ μ¶©λ¶„?}
    E -->|Yes| F[GPUμ— μ „μ²΄ λ΅λ”©]
    E -->|No| G[μΌλ¶€ κ°€μ¤‘μΉ λ””μ¤ν¬ μ¤ν”„λ΅λ“]
    G --> H{offload_folder μ„¤μ •?}
    H -->|Yes| I[λ””μ¤ν¬μ— μ„μ‹ μ €μ¥]
    H -->|No| J[β ValueError<br/>offload_folder λ„λ½]

    classDef errorNode fill:#ffccbc,stroke:#bf360c,color:#000
    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef processNode fill:#e1f5ff,stroke:#01579b,color:#000

    class J errorNode
    class F,I successNode
    class A,B,C,D,G processNode
```

**3. μ½”λ“ λ λ²¨ μ›μΈ**

`src/models/llm_loader.py:58-64`:
```python
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",  # μλ™ λ””λ°”μ΄μ¤ ν• λ‹Ή
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)
# offload_folder νλΌλ―Έν„° λ„λ½!
```

**λ¬Έμ μ **:
- `device_map="auto"`λ” GPU λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•  λ• μλ™μΌλ΅ CPU/λ””μ¤ν¬λ΅ μ¤ν”„λ΅λ“
- λ€ν• λ¨λΈ(10B μ΄μƒ)μ€ 4-bit μ–‘μν™”λ¥Ό μ‚¬μ©ν•΄λ„ GPU λ©”λ¨λ¦¬ λ¶€μ΅± κ°€λ¥
- `offload_folder` λ―Έμ§€μ • μ‹ λ””μ¤ν¬ μ¤ν”„λ΅λ“ λ¶κ°€

#### ν•΄κ²° λ°©λ²•

**λ°©λ²• 1: offload_folder μ¶”κ°€ (κ¶μ¥)**

`src/models/llm_loader.py:58` λΌμΈ μμ •:

```python
# λ³€κ²½ μ „
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)

# λ³€κ²½ ν›„
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    offload_folder=str(offload_dir),  # λ””μ¤ν¬ μ¤ν”„λ΅λ“ ν΄λ” μ§€μ •
    torch_dtype=torch.bfloat16 if config.training.get('bf16', True) else torch.float16,
    trust_remote_code=True
)
```

**λ°©λ²• 2: safetensors μ„¤μΉ ν™•μΈ**

```bash
pip install safetensors
```

μΌλ¶€ λ¨λΈμ€ safetensors ν•μ‹μΌλ΅ κ°€μ¤‘μΉλ¥Ό μ κ³µν•λ©°, μ΄ κ²½μ° λ” ν¨μ¨μ μΌλ΅ λ΅λ”© κ°€λ¥.

**λ°©λ²• 3: max_memory μ„¤μ •μΌλ΅ λ…μ‹μ  μ μ–΄**

```python
model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    max_memory={0: "20GiB", "cpu": "30GiB"},  # GPU 0: 20GB, CPU: 30GB
    offload_folder=str(offload_dir),
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

**κ¶μ¥ μ‚¬ν•­**: **λ°©λ²• 1 + λ°©λ²• 2**λ¥Ό μ΅°ν•©ν•μ—¬ μ‚¬μ©
- safetensors λ¨Όμ € μ„¤μΉ
- offload_folderλ¥Ό μ‹¤ν— ν΄λ” λ‚΄ `offload/` μ„λΈλ””λ ‰ν† λ¦¬λ΅ μ§€μ •
- λ€ν• λ¨λΈμ μ•μ •μ μΈ λ΅λ”© λ³΄μ¥

### 3.3 μ—λ¬ #3: μ¶”λ΅  λ‹¨κ³„ μ—λ¬

#### μ—λ¬ λ©”μ‹μ§€
```
The following `model_kwargs` are not used by the model: ['token_type_ids']
```

#### κ·Όλ³Έ μ›μΈ
- μΌλ¶€ λ¨λΈ(νΉν Causal LM)μ€ `token_type_ids`λ¥Ό μ§€μ›ν•μ§€ μ•μ
- Tokenizerκ°€ μλ™μΌλ΅ `token_type_ids`λ¥Ό μƒμ„±ν•μ§€λ§ λ¨λΈμ΄ μ΄λ¥Ό λ¬΄μ‹
- κ²½κ³ μ„± λ©”μ‹μ§€μ΄μ§€λ§, μ¶”λ΅  μ‹¤ν–‰μ—λ” μν–¥μ„ λ―ΈμΉμ§€ μ•μ„ μ μμ

#### ν•΄κ²° λ°©λ²•

`src/inference/predictor.py` λλ” ν•΄λ‹Ή μ¶”λ΅  μ½”λ“μ—μ„:

```python
# λ³€κ²½ μ „
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
outputs = model.generate(**inputs, ...)

# λ³€κ²½ ν›„
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
# token_type_ids μ κ±° (Causal LMμ€ μ‚¬μ©ν•μ§€ μ•μ)
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']
outputs = model.generate(**inputs, ...)
```

## 4. μ„±κ³µ λ¨λΈ λ¶„μ„: kobart

### 4.1 ν•™μµ μ„±λ¥

| λ©”νΈλ¦­ | κ°’ |
|--------|-----|
| **ROUGE-1** | 0.4134 |
| **ROUGE-2** | 0.2552 |
| **ROUGE-L** | 0.4065 |
| **ROUGE-Sum** | 1.0751 |
| **Loss** | 1.4561 |
| **ν•™μµ μ‹κ°„** | ~4λ¶„ |
| **ν‰κ°€ μ†λ„** | 6.37 samples/sec |

### 4.2 λ¨λΈ μ •λ³΄
- **μ²΄ν¬ν¬μΈνΈ**: `gogamza/kobart-base-v2`
- **μ•„ν‚¤ν…μ²**: BART (Seq2Seq)
- **νλΌλ―Έν„°**: 123M
- **ν•™μµ μ„¤μ •**:
  - Epochs: 1
  - Batch Size: 8
  - Learning Rate: 5e-6
  - Gradient Accumulation: 4
  - Warmup Ratio: 0.1

### 4.3 μ„±λ¥ ν‰κ°€

**μ¥μ **:
- β… μ•μ •μ μΈ ν•™μµ μ™„λ£
- β… BFloat16 λ¬Έμ  μ—†μ (Seq2Seq μ•„ν‚¤ν…μ²)
- β… μ–‘νΈν• ROUGE μ μ (ROUGE-1: 0.413)
- β… λΉ λ¥Έ μ¶”λ΅  μ†λ„

**ν•κ³„μ **:
- β οΈ λ‹¨μΌ λ¨λΈλ΅λ” μ•™μƒλΈ” ν¨κ³Ό μ—†μ
- β οΈ Causal LM λ€λΉ„ νλΌλ―Έν„° μ μ μ (123M vs 3B~12.8B)
- β οΈ μµμ‹  LLM λ€λΉ„ μ„±λ¥ μ ν•μ 

## 5. Solar API ν‰κ°€ κ²°κ³Ό

### 5.1 Solar API μ„±λ¥
| λ©”νΈλ¦­ | κ°’ |
|--------|-----|
| **Solar ROUGE-1 F1** | 0.2272 |
| **Solar ROUGE-2 F1** | 0.0765 |
| **Solar ROUGE-L F1** | 0.2177 |
| **ν‰κ°€ μƒν” μ** | 50 |

### 5.2 kobart vs Solar API λΉ„κµ

```mermaid
graph LR
    subgraph "ROUGE-1 λΉ„κµ"
    K1[kobart: 0.413]
    S1[Solar API: 0.227]
    end

    subgraph "ROUGE-2 λΉ„κµ"
    K2[kobart: 0.255]
    S2[Solar API: 0.077]
    end

    subgraph "ROUGE-L λΉ„κµ"
    K3[kobart: 0.407]
    S3[Solar API: 0.218]
    end

    K1 -.->|+81.9%| S1
    K2 -.->|+231.4%| S2
    K3 -.->|+86.9%| S3

    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    classDef warningNode fill:#fff9c4,stroke:#f57f17,color:#000

    class K1,K2,K3 successNode
    class S1,S2,S3 warningNode
```

**λ¶„μ„**:
- kobartκ°€ Solar API λ€λΉ„ **λ¨λ“  λ©”νΈλ¦­μ—μ„ μ°μ**
- ROUGE-2μ—μ„ κ°€μ¥ ν° μ°¨μ΄ (231.4% λ†’μ)
- Solar APIλ” Zero-shot μ„±λ¥μ΄λ―€λ΅, Fine-tuningλ kobartκ°€ λ‹Ήμ—°ν μ°μ

## 6. TTA λ° μµμ ν™” κ²°κ³Ό

### 6.1 TTA (Test-Time Augmentation)
- **μ μ© μ—¬λ¶€**: β μ‹¤ν¨
- **μ΄μ **: λ‹¨μΌ λ¨λΈλ§ μ„±κ³µν•μ—¬ μ•™μƒλΈ” λ¶κ°€
- **μ„¤μ •λ μ „λµ**: paraphrase, reorder, synonym, mask (num_aug=2)

### 6.2 μµμ ν™”
- **μµμ ν™” ν΄λ”**: `experiments/20251012/20251012_131535_test_full_pipeline_quick/optimized/`
- **μƒνƒ**: μƒμ„±λ¨ (ν•μ§€λ§ μ¶”λ΅  μ‹¤ν¨λ΅ ν™μ©λμ§€ μ•μ)

## 7. μ‹¤ν— ν•™μµ μ‚¬ν•­ λ° κ¶μ¥ μ΅°μΉ

### 7.1 μ¦‰μ‹ μ μ© ν•„μ”ν• μμ • μ‚¬ν•­

#### μ°μ„ μμ„ 1: BFloat16 λ¬Έμ  ν•΄κ²° (Critical)
```python
# src/models/lora_loader.py:108-113
# λ³€κ²½ μ „
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16

# λ³€κ²½ ν›„
compute_dtype = torch.float16  # AMP νΈν™μ„±μ„ μ„ν•΄ λ¨λ“  λ¨λΈμ— Float16 μ‚¬μ©
self._log("  - QLoRA compute dtype: fp16 (AMP νΈν™)")
```

**μν–¥**: 4κ° λ¨λΈ (llama, qwen3, solar, polyglot) ν•™μµ κ°€λ¥

#### μ°μ„ μμ„ 2: offload_folder μ„¤μ • (High)
```python
# src/models/llm_loader.py:58
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    ...,
    offload_folder=str(offload_dir),
    ...
)
```

**μν–¥**: kullm-v2 λ¨λΈ λ΅λ”© κ°€λ¥

#### μ°μ„ μμ„ 3: token_type_ids μ κ±° (Medium)
```python
# μ¶”λ΅  μ½”λ“
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']
```

**μν–¥**: μ¶”λ΅  λ‹¨κ³„ μ •μƒ μ‹¤ν–‰

### 7.2 λ¬Έμ„ μ—…λ°μ΄νΈ ν•„μ” μ‚¬ν•­

#### PRD λ¬Έμ„
- **PRD 08 (LLM νμΈνλ‹ μ „λµ)**: BFloat16 β†’ Float16 λ³€κ²½ μ‚¬ν•­ λ°μ
- QLoRA μ„¤μ •μ—μ„ `bnb_4bit_compute_dtype=torch.float16` λ…μ‹

#### λ¨λ“ν™” λ¬Έμ„
- **`docs/λ¨λ“ν™”/02_ν•µμ‹¬_μ‹μ¤ν….md`**:
  - LLM Loader μ„Ήμ…μ— AMP νΈν™μ„± μ£Όμμ‚¬ν•­ μ¶”κ°€
  - offload_folder μ„¤μ • κ°€μ΄λ“ μ¶”κ°€

- **`docs/λ¨λ“ν™”/05_νΈλ¬λΈ”μν…_κ°€μ΄λ“.md`** (μ΅΄μ¬ μ‹):
  - BFloat16 μ—λ¬ ν•΄κ²° λ°©λ²• μ¶”κ°€
  - Device Map μ—λ¬ ν•΄κ²° λ°©λ²• μ¶”κ°€

### 7.3 μ¬μ‹¤ν— κ¶μ¥ μ‚¬ν•­

μμ • μ‚¬ν•­ μ μ© ν›„ λ‹¤μ μ‹¤ν— κ¶μ¥:

```bash
python scripts/train.py \
  --mode full \
  --models kobart llama-3.2-korean-3b qwen3-4b solar-10.7b polyglot-ko-12.8b kullm-v2 \
  --epochs 1 \
  --batch_size 8 \
  --learning_rate 5e-6 \
  --gradient_accumulation_steps 4 \
  --warmup_ratio 0.1 \
  --weight_decay 0.01 \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --k_folds 2 \
  --fold_seed 42 \
  --ensemble_strategy stacking \
  --num_beams 4 \
  --temperature 0.7 \
  --top_p 0.9 \
  --top_k 50 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3 \
  --experiment_name test_full_pipeline_fixed \
  --seed 42
```

**μμƒ κ²°κ³Ό**:
- β… 6/6 λ¨λΈ ν•™μµ μ„±κ³µ
- β… μ•™μƒλΈ” μ •μƒ λ™μ‘
- β… TTA μ μ© κ°€λ¥
- β… μ¶”λ΅  λ° μ μ¶ νμΌ μƒμ„± μ„±κ³µ

---

## 7.2 ν›„μ† μ‹¤ν— κ²°κ³Ό (20251013_161056_test_strategy3_triple)

### 7.2.1 μ‹¤ν— κ°μ”
- **μ‹¤ν— μΌμ‹**: 2025λ…„ 10μ›” 13μΌ 16:10:56
- **μ‹¤ν— λ©μ **: BFloat16 λ¬Έμ  ν•΄κ²° ν›„ 3κ° λ¨λΈ Full Pipeline κ²€μ¦
- **λ¨λΈ κµ¬μ„±**: kobart, llama-3.2-korean-3b, qwen3-4b

### 7.2.2 ν•™μµ κ²°κ³Ό

#### μ„±κ³µ λ¨λΈ (3/3)

| λ¨λΈ | ROUGE-1 | ROUGE-2 | ROUGE-L | ν•™μµ μ‹κ°„ | μƒνƒ |
|------|---------|---------|---------|----------|------|
| **kobart** | 0.4048 | 0.2480 | 0.3952 | ~2μ‹κ°„ | β… μ„±κ³µ |
| **llama-3.2-korean-3b** | 0.1096 | 0.0735 | 0.1096 | ~8μ‹κ°„ | β… μ„±κ³µ |
| **qwen3-4b** | 0.1458 | 0.0985 | 0.1458 | ~8μ‹κ°„ 23λ¶„ | β… μ„±κ³µ |

**μ£Όμ” μ„±κ³Ό:**
- β… BFloat16 β†’ Float16 λ³€κ²½μΌλ΅ λ¨λ“  λ¨λΈ ν•™μµ μ„±κ³µ
- β… KoBARTκ°€ μ••λ„μ  μ„±λ¥ μ μ§€ (ROUGE-1: 0.4048)
- β… Full Pipeline μ •μƒ μ‘λ™ (μ•™μƒλΈ”, Solar API, μ¶”λ΅ )

### 7.2.3 κ°μ„  μ‚¬ν•­ ν™•μΈ

**ν•΄κ²°λ λ¬Έμ :**
1. β… BFloat16 AMP νΈν™μ„± β†’ Float16 μ‚¬μ©μΌλ΅ ν•΄κ²°
2. β… λ¨λΈ ν•™μµ μ •μƒ μ™„λ£
3. β… μ¶”λ΅  λ° μ μ¶ νμΌ μƒμ„± μ„±κ³µ

**λ‚¨μ€ κ³Όμ :**
1. β οΈ LLM λ¨λΈ μ„±λ¥ κ°μ„  ν•„μ” (ROUGE-1: 0.1096~0.1450)
2. β οΈ λ°μ΄ν„° μ¦κ°• 50% μ μ© ν•„μ”
3. β οΈ ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ (Solar API) ν™μ©

### 7.2.4 λ‹¤μ λ‹¨κ³„
- λ©ν† λ§ ν”Όλ“λ°± λ°μ (`docs/modify/02_λ©ν† λ§_ν”Όλ“λ°±_κΈ°λ°_κ°μ„ μ‚¬ν•­.md` μ°Έμ΅°)
- KoBART λ‹¨μΌ λ¨λΈ μµμ ν™” μ§‘μ¤‘
- λ°μ΄ν„° μ¦κ°• 50% μ μ© λ° μ¬ν•™μµ

---

## 9. κ²°λ΅ 

### 9.1 μ‹¤ν— μ”μ•½
μ΄λ² μ‹¤ν—μ€ **λ¶€λ¶„μ  μ„±κ³µ**μΌλ΅ ν‰κ°€λ©λ‹λ‹¤:
- β… kobart λ¨λΈ ν•™μµ λ° ν‰κ°€ μ„±κ³µ
- β… Solar API κµμ°¨ κ²€μ¦ μ™„λ£
- β 5κ° LLM λ¨λΈ ν•™μµ μ‹¤ν¨
- β μ•™μƒλΈ” λ° TTA λ¶κ°€
- β μ¶”λ΅  λ‹¨κ³„ μ—λ¬

### 9.2 ν•µμ‹¬ λ°κ²¬ μ‚¬ν•­

1. **BFloat16 νΈν™μ„± λ¬Έμ **: PyTorch AMP GradScalerκ°€ BFloat16μ„ μ§€μ›ν•μ§€ μ•μμ„ ν™•μΈ
2. **λ€ν• λ¨λΈ λ©”λ¨λ¦¬ κ΄€λ¦¬**: 12.8B λ¨λΈμ€ offload_folder μ„¤μ • ν•„μ
3. **ν† ν¬λ‚μ΄μ € νΈν™μ„±**: Causal LMμ€ token_type_ids λ¶ν•„μ”

### 9.3 λ‹¤μ λ‹¨κ³„
1. β… λ³Έ λ¶„μ„ λ³΄κ³ μ„ μ‘μ„± μ™„λ£
2. β… `src/models/lora_loader.py` μμ • (BFloat16 β†’ Float16)
3. π”„ `src/models/llm_loader.py` μμ • (offload_folder μ¶”κ°€)
4. π”„ μ¶”λ΅  μ½”λ“ μμ • (token_type_ids μ κ±°)
5. π”„ λ¬Έμ„ μ—…λ°μ΄νΈ (`docs/λ¨λ“ν™”/02_ν•µμ‹¬_μ‹μ¤ν….md`)
6. β… Qwen3-4B λ¨λΈ ν•™μµ μ™„λ£

---

## 10. κ°λ³„ λ¨λΈ μƒμ„Έ λ¶„μ„

### 10.1 Model 1: KoBART
*(μ•μ„ 4. μ„±κ³µ λ¨λΈ λ¶„μ„: kobart μ„Ήμ… μ°Έμ΅°)*

### 10.2 Model 2: Llama-3.2-Korean-3B
- **μ²΄ν¬ν¬μΈνΈ**: `beomi/Llama-3.2-Korean-3B-Instruct`
- **μ•„ν‚¤ν…μ²**: Causal LM (Auto-regressive)
- **νλΌλ―Έν„°**: 3B
- **μµμΆ… ROUGE-1**: 0.1096
- **ν•™μµ μ‹κ°„**: ~8μ‹κ°„
- **μƒνƒ**: β… μ„±κ³µ

**ν‰κ°€**:
- Float16 λ³€κ²½ ν›„ ν•™μµ μ„±κ³µ
- KoBART λ€λΉ„ λ‚®μ€ μ„±λ¥ (0.1096 vs 0.4048)
- ν•κµ­μ–΄ νΉν™” λ¨λΈμ΄μ§€λ§ μ”μ•½ νƒμ¤ν¬μ—μ„λ” μ ν•μ  μ„±λ¥

### 10.3 Model 3: Qwen3-4B

#### 10.3.1 λ¨λΈ μ •λ³΄
- **μ²΄ν¬ν¬μΈνΈ**: `Qwen/Qwen3-4B-Instruct-2507`
- **μ•„ν‚¤ν…μ²**: Causal LM (Auto-regressive)
- **νλΌλ―Έν„°**: 4,055,498,240 (μ „μ²΄)
- **LoRA νλΌλ―Έν„°**: 33,030,144 (0.81%)
- **ν•™μµ λ°μ΄ν„°**: ν•κµ­μ–΄ λ‰΄μ¤ μ”μ•½ λ°μ΄ν„°μ…‹

#### 10.3.2 ν•™μµ νƒ€μ„λΌμΈ
- **ν•™μµ μ‹μ‘**: 2025-10-13 18:21:06
- **ν•™μµ μ™„λ£**: 2025-10-14 02:44:22
- **μ΄ ν•™μµ μ‹κ°„**: μ•½ 8μ‹κ°„ 23λ¶„ (30,185μ΄)
- **Epochs**: 2

#### 10.3.3 μµμΆ… ν‰κ°€ μ§€ν‘ (Epoch 2.0)

| λ©”νΈλ¦­ | κ°’ | μ„¤λ… |
|--------|-----|------|
| **eval_loss** | 0.8513 | κ²€μ¦ μ†μ‹¤ |
| **eval_rouge1** | 0.1458 | ROUGE-1 F1 (14.58%) |
| **eval_rouge2** | 0.0985 | ROUGE-2 F1 (9.85%) |
| **eval_rougeL** | 0.1458 | ROUGE-L F1 (14.58%) |
| **eval_rouge_sum** | 0.3901 | μ „μ²΄ ROUGE ν•©κ³„ |
| **train_loss** | 1.1681 | ν•™μµ μ†μ‹¤ |
| **train_samples_per_second** | 0.166 | ν•™μµ μ†λ„ |
| **eval_samples_per_second** | 0.185 | ν‰κ°€ μ†λ„ |

#### 10.3.4 Epochλ³„ μ„±λ¥ λ³€ν™”

**Epoch 1.0 μ¤‘κ°„ ν‰κ°€:**
- eval_rouge1: 0.1432
- eval_rouge2: 0.0966
- eval_rougeL: 0.1432
- eval_runtime: 4,852μ΄

**Epoch 2.0 μµμΆ… ν‰κ°€:**
- eval_rouge1: 0.1458 (+1.8%)
- eval_rouge2: 0.0985 (+2.0%)
- eval_rougeL: 0.1458 (+1.8%)

**μ„±λ¥ κ°μ„  μ¶”μ΄:**
```mermaid
graph LR
    E1[Epoch 1.0<br/>ROUGE-1: 0.1432] --> E2[Epoch 2.0<br/>ROUGE-1: 0.1458]
    E1 -.->|+1.8%| E2

    classDef successNode fill:#a5d6a7,stroke:#1b5e20,color:#000
    class E1,E2 successNode
```

#### 10.3.5 Qwen μµμΆ… ν†µκ³„

**ν•™μµ ν¨μ¨μ„±:**
- LoRA μ‚¬μ©μΌλ΅ ν•™μµ νλΌλ―Έν„° 0.81%λ§ μ‚¬μ©
- Epochλ‹Ή ν‰κ·  ν•™μµ μ‹κ°„: ~4μ‹κ°„ 12λ¶„
- ν‰κ°€ λ°νƒ€μ„: μ•½ 81λ¶„/epoch

**μ†μ‹¤ ν•¨μ μ¶”μ΄:**
- Train Loss: 1.1681
- Eval Loss: 0.8513
- **κ³Όμ ν•© μ—†μ**: Train Loss > Eval Loss (κ±΄κ°•ν• ν•™μµ)

**μ²λ¦¬ μ†λ„:**
- ν•™μµ: 0.166 samples/sec (μ•½ 6μ΄/μƒν”)
- ν‰κ°€: 0.185 samples/sec (μ•½ 5.4μ΄/μƒν”)

#### 10.3.6 λ¨λΈ λΉ„κµ λ¶„μ„

| λ¨λΈ | νλΌλ―Έν„° | ROUGE-1 | ROUGE-2 | ROUGE-L | ν•™μµ μ‹κ°„ |
|------|----------|---------|---------|---------|----------|
| KoBART | 123M | **0.4048** | **0.2480** | **0.3952** | ~2μ‹κ°„ |
| Llama-3.2-Korean-3B | 3B | 0.1096 | 0.0735 | 0.1096 | ~8μ‹κ°„ |
| **Qwen3-4B** | 4B | 0.1458 | 0.0985 | 0.1458 | ~8μ‹κ°„ 23λ¶„ |

**Qwen vs Llama λΉ„κµ:**
- β… ROUGE-1: +33.0% λ†’μ (0.1458 vs 0.1096)
- β… ROUGE-2: +34.0% λ†’μ (0.0985 vs 0.0735)
- β… νλΌλ―Έν„°: 4B vs 3B (λ” ν° λ¨λΈ)
- β οΈ ν•™μµ μ‹κ°„: μ•½ 23λ¶„ λ” μ†μ”

**Qwen vs KoBART λΉ„κµ:**
- β ROUGE-1: -64.0% λ‚®μ (0.1458 vs 0.4048)
- β ROUGE-2: -60.3% λ‚®μ (0.0985 vs 0.2480)
- β νλΌλ―Έν„°: 32.9λ°° λ§μ§€λ§ μ„±λ¥μ€ λ‚®μ
- β ν•™μµ μ‹κ°„: 4.2λ°° λ” μ†μ”

#### 10.3.7 μ„±λ¥ ν‰κ°€

**κ°•μ :**
- β… Llama λ€λΉ„ μ°μν• μ„±λ¥ (33% λ†’μ€ ROUGE-1)
- β… μ•μ •μ μΈ ν•™μµ μ™„λ£ (2 epochs)
- β… κ³Όμ ν•© μ—†λ” κ±΄κ°•ν• ν•™μµ κ³΅μ„ 
- β… Float16 νΈν™μ„± ν™•μΈ
- β… λ€ν• λ¨λΈμ„μ—λ„ LoRAλ΅ ν¨μ¨μ  ν•™μµ

**μ•½μ :**
- β οΈ KoBART λ€λΉ„ λ‚®μ€ μ λ€ μ„±λ¥ (ROUGE-1: 0.1458 vs 0.4048)
- β οΈ λλ¦° ν•™μµ μ†λ„ (0.166 samples/sec)
- β οΈ κΈ΄ ν•™μµ μ‹κ°„ (8μ‹κ°„ 23λ¶„)
- β οΈ Seq2Seq λ€λΉ„ Causal LMμ μ”μ•½ νƒμ¤ν¬ μ ν•©μ„± λ‚®μ

#### 10.3.8 λ‹¤μ λ‹¨κ³„ μ μ•

**λ‹¨κΈ° κ°μ„  λ°©μ•:**
1. **Epoch μ¦κ°€**: 2 β†’ 3~4 epochs (μ„±λ¥ ν–¥μƒ μ—¬μ§€ ν™•μΈ)
2. **Learning Rate νλ‹**: ν„μ¬ μ„¤μ • κ²€ν†  λ° μµμ ν™”
3. **Prompt Engineering**: μ”μ•½ νƒμ¤ν¬μ— νΉν™”λ ν”„λ΅¬ν”„νΈ μ μ©

**μ¤‘μ¥κΈ° κ°μ„  λ°©μ•:**
1. **ν•μ΄λΈλ¦¬λ“ μ•™μƒλΈ”**: KoBART + Qwen + Llama μ΅°ν•©
2. **λ°μ΄ν„° μ¦κ°•**: 50% λ°μ΄ν„° μ¦κ°• μ μ©
3. **Solar API ν†µν•©**: Zero-shot ν”„λ΅¬ν”„νΈ μ—”μ§€λ‹μ–΄λ§ ν™μ©

**μƒνƒ**: β… ν•™μµ μ„±κ³µ


