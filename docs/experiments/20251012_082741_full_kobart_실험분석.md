# 실험 분석 보고서: 20251012_082741_full_kobart

> **실험 ID**: 20251012_082741_full_kobart
> **실행 일시**: 2025-10-12 08:27:41
> **실행 모드**: FULL Pipeline
> **실험 상태**: ⚠️ 부분 성공 (KoBART 완료, Llama 오류)

---

## 📋 목차

1. [실험 개요](#1-실험-개요)
2. [실험 설정](#2-실험-설정)
3. [학습 지표 분석](#3-학습-지표-분석)
4. [결과 분석](#4-결과-분석)
5. [발생 오류 및 원인](#5-발생-오류-및-원인)
6. [수정 방향 및 개선안](#6-수정-방향-및-개선안)

---

## 1. 실험 개요

### 1.1 실험 목적
- 전체 파이프라인 (`--mode full`) 기능 검증
- 6개 모델 다중 학습 및 앙상블 전략 테스트
- `--models all` 옵션 자동 확장 기능 검증

### 1.2 실행 명령어

```bash
# ==================== 실행된 명령어 ==================== #
python scripts/train.py \
  --mode full \
  --models all \
  --epochs 20 \
  --batch_size 8 \
  --learning_rate 5e-6 \
  --gradient_accumulation_steps 4 \
  --warmup_ratio 0.1 \
  --weight_decay 0.01 \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --use_augmentation \
  --augmentation_methods back_translation paraphrase synonym turn_shuffle \
  --augmentation_ratio 0.5 \
  --k_folds 5 \
  --fold_seed 42 \
  --ensemble_strategy stacking \
  --use_tta \
  --tta_strategies paraphrase reorder synonym mask \
  --tta_num_aug 5 \
  --use_solar_api \
  --solar_model solar-1-chat \
  --prompt_strategy self_consistency \
  --validate_data_quality \
  --quality_threshold 0.8 \
  --optimize_inference \
  --optimization_method tensorrt \
  --use_batch_optimization \
  --num_beams 8 \
  --temperature 0.7 \
  --top_p 0.9 \
  --top_k 50 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3 \
  --use_wandb \
  --wandb_project final-submission \
  --save_visualizations \
  --experiment_name final_all_models_premium \
  --seed 42
```

### 1.3 실험 결과 요약

| 항목 | 결과 |
|------|------|
| **전체 모델 수** | 6개 (kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2) |
| **성공한 모델** | 1개 (kobart) |
| **실패한 모델** | 5개 (llama-3.2-korean-3b에서 오류 발생) |
| **학습 완료 여부** | ⚠️ 부분 완료 (KoBART만 완료) |
| **최종 Loss** | 0.8411 (KoBART, Epoch 9.95) |
| **총 학습 시간** | 약 32분 (KoBART 모델만) |

---

## 2. 실험 설정

### 2.1 모델 설정

```python
# ==================== 모델 자동 확장 결과 ==================== #
# --models all 옵션이 다음과 같이 6개 모델로 자동 확장됨
models = [
    'kobart',                   # Encoder-Decoder 모델
    'llama-3.2-korean-3b',      # Causal LM 모델
    'qwen3-4b',                 # Causal LM 모델
    'solar-10.7b',              # Causal LM 모델
    'polyglot-ko-12.8b',        # Causal LM 모델
    'kullm-v2'                  # Causal LM 모델
]
```

### 2.2 데이터 설정

```python
# ==================== 데이터 통계 ==================== #
train_samples = 12457           # 학습 데이터 샘플 수
val_samples = 499               # 검증 데이터 샘플 수
total_samples = 12956           # 전체 데이터 샘플 수
```

### 2.3 학습 하이퍼파라미터

```python
# ==================== 학습 파라미터 ==================== #
epochs = 20                             # 총 에포크 수
batch_size = 8                          # 배치 크기
learning_rate = 5e-6                    # 초기 학습률
gradient_accumulation_steps = 4         # 그래디언트 누적 스텝
warmup_ratio = 0.1                      # Warmup 비율
weight_decay = 0.01                     # 가중치 감쇠
max_grad_norm = 1.0                     # 그래디언트 클리핑
label_smoothing = 0.1                   # 레이블 스무딩
```

### 2.4 앙상블 및 고급 기능

```python
# ==================== 앙상블 설정 ==================== #
ensemble_strategy = 'stacking'          # 앙상블 전략: Stacking
use_tta = True                          # TTA 활성화
tta_strategies = [                      # TTA 전략 목록
    'paraphrase',
    'reorder',
    'synonym',
    'mask'
]
tta_num_aug = 5                         # TTA 증강 횟수

# ==================== 데이터 증강 설정 ==================== #
use_augmentation = True                 # 데이터 증강 활성화
augmentation_methods = [                # 증강 방법 목록
    'back_translation',
    'paraphrase',
    'synonym',
    'turn_shuffle'
]
augmentation_ratio = 0.5                # 증강 비율: 50%

# ==================== K-Fold 교차 검증 설정 ==================== #
k_folds = 5                             # Fold 수
fold_seed = 42                          # Fold 분할 시드
```

---

## 3. 학습 지표 분석

### 3.1 KoBART 모델 학습 진행 상황

#### 3.1.1 초기 학습 단계 (Epoch 0.06 - 1.28)

```python
# ==================== 초기 학습 지표 (첫 20개 체크포인트) ==================== #
training_metrics_early = [
    {'epoch': 0.06, 'loss': 2.5852, 'grad_norm': 7.4475, 'lr': 1.98e-06},        # 학습 시작
    {'epoch': 0.13, 'loss': 1.8879, 'grad_norm': 8.8309, 'lr': 3.98e-06},        # Loss 급격히 감소
    {'epoch': 0.19, 'loss': 1.7644, 'grad_norm': 6.6867, 'lr': 5.98e-06},        # 계속 하락
    {'epoch': 0.26, 'loss': 1.6904, 'grad_norm': 6.2601, 'lr': 7.98e-06},        # 안정적 하락
    {'epoch': 0.32, 'loss': 1.6008, 'grad_norm': 5.4701, 'lr': 9.98e-06},        # Warmup 완료 직전
    {'epoch': 0.39, 'loss': 1.5742, 'grad_norm': 6.5695, 'lr': 9.93e-06},        # 학습률 감소 시작
    {'epoch': 0.45, 'loss': 1.5504, 'grad_norm': 7.2911, 'lr': 9.87e-06},
    {'epoch': 0.51, 'loss': 1.5136, 'grad_norm': 5.2480, 'lr': 9.80e-06},
    {'epoch': 0.58, 'loss': 1.5233, 'grad_norm': 5.7269, 'lr': 9.74e-06},
    {'epoch': 0.64, 'loss': 1.4970, 'grad_norm': 5.1852, 'lr': 9.67e-06},
    {'epoch': 0.71, 'loss': 1.4870, 'grad_norm': 5.1771, 'lr': 9.60e-06},
    {'epoch': 0.77, 'loss': 1.4592, 'grad_norm': 4.2971, 'lr': 9.54e-06},
    {'epoch': 0.83, 'loss': 1.4463, 'grad_norm': 5.1335, 'lr': 9.47e-06},
    {'epoch': 0.90, 'loss': 1.4776, 'grad_norm': 5.4307, 'lr': 9.40e-06},
    {'epoch': 0.96, 'loss': 1.4614, 'grad_norm': 4.3107, 'lr': 9.34e-06},
    {'epoch': 1.03, 'loss': 1.4333, 'grad_norm': 5.8076, 'lr': 9.27e-06},        # Epoch 1 통과
    {'epoch': 1.09, 'loss': 1.3341, 'grad_norm': 5.7977, 'lr': 9.20e-06},        # Loss 1.3대 진입
    {'epoch': 1.16, 'loss': 1.3508, 'grad_norm': 5.7732, 'lr': 9.14e-06},
    {'epoch': 1.22, 'loss': 1.3621, 'grad_norm': 4.8259, 'lr': 9.07e-06},
    {'epoch': 1.28, 'loss': 1.3194, 'grad_norm': 5.5344, 'lr': 9.01e-06}
]
```

**초기 학습 분석:**
- ✅ **Loss 감소**: 2.5852 → 1.3194 (약 49% 감소)
- ✅ **학습률 스케줄링**: Warmup 후 선형 감소 정상 작동
- ✅ **Gradient Norm**: 4.3 ~ 8.8 범위로 안정적 (max_grad_norm=1.0 클리핑 미작동, 정상)
- ⚠️ **Epoch 0.96에서 Loss 증가**: 1.4463 → 1.4776 (약간의 변동, 정상 범위)

#### 3.1.2 최종 학습 단계 (Epoch 8.73 - 9.95)

```python
# ==================== 최종 학습 지표 (마지막 20개 체크포인트) ==================== #
training_metrics_final = [
    {'epoch': 8.73, 'loss': 0.8601, 'grad_norm': 4.7400, 'lr': 1.31e-06},        # 후반부 안정화
    {'epoch': 8.79, 'loss': 0.8531, 'grad_norm': 6.0509, 'lr': 1.25e-06},
    {'epoch': 8.86, 'loss': 0.8789, 'grad_norm': 4.7466, 'lr': 1.18e-06},
    {'epoch': 8.92, 'loss': 0.8393, 'grad_norm': 5.1203, 'lr': 1.11e-06},
    {'epoch': 8.99, 'loss': 0.8491, 'grad_norm': 6.1740, 'lr': 1.05e-06},        # Epoch 9 진입
    {'epoch': 9.05, 'loss': 0.8383, 'grad_norm': 5.0246, 'lr': 9.82e-07},        # 가장 낮은 Loss
    {'epoch': 9.11, 'loss': 0.8245, 'grad_norm': 5.8848, 'lr': 9.16e-07},
    {'epoch': 9.18, 'loss': 0.8299, 'grad_norm': 5.1009, 'lr': 8.49e-07},
    {'epoch': 9.24, 'loss': 0.8461, 'grad_norm': 4.1204, 'lr': 7.83e-07},
    {'epoch': 9.31, 'loss': 0.8532, 'grad_norm': 4.9779, 'lr': 7.17e-07},
    {'epoch': 9.37, 'loss': 0.8311, 'grad_norm': 5.3097, 'lr': 6.51e-07},
    {'epoch': 9.44, 'loss': 0.8331, 'grad_norm': 5.7233, 'lr': 5.84e-07},
    {'epoch': 9.50, 'loss': 0.8360, 'grad_norm': 5.1459, 'lr': 5.18e-07},
    {'epoch': 9.56, 'loss': 0.8325, 'grad_norm': 5.6518, 'lr': 4.52e-07},
    {'epoch': 9.63, 'loss': 0.8154, 'grad_norm': 5.5124, 'lr': 3.85e-07},        # Loss 0.8 이하
    {'epoch': 9.69, 'loss': 0.8133, 'grad_norm': 6.7779, 'lr': 3.19e-07},
    {'epoch': 9.76, 'loss': 0.8201, 'grad_norm': 4.8266, 'lr': 2.53e-07},
    {'epoch': 9.82, 'loss': 0.8444, 'grad_norm': 5.5003, 'lr': 1.86e-07},
    {'epoch': 9.88, 'loss': 0.8422, 'grad_norm': 6.5628, 'lr': 1.20e-07},
    {'epoch': 9.95, 'loss': 0.8411, 'grad_norm': 5.1298, 'lr': 5.37e-08}         # 학습 완료
]
```

**최종 학습 분석:**
- ✅ **Loss 수렴**: 0.8601 → 0.8411 (안정적 수렴)
- ✅ **최저 Loss**: 0.8133 (Epoch 9.69)
- ✅ **학습률 감소**: 1.31e-06 → 5.37e-08 (선형 감소 정상 작동)
- ✅ **Gradient Norm 안정화**: 4.1 ~ 6.8 범위로 매우 안정적

### 3.2 학습 곡선 분석

```python
# ==================== Loss 감소 추이 분석 ==================== #
loss_trend_analysis = {
    'initial_loss': 2.5852,                     # 초기 Loss
    'final_loss': 0.8411,                       # 최종 Loss
    'min_loss': 0.8133,                         # 최저 Loss (Epoch 9.69)
    'total_reduction': 1.7441,                  # 총 Loss 감소량 (67.5%)
    'reduction_rate': 0.675                     # Loss 감소율
}

# ---------------------- Epoch별 Loss 감소 속도 ---------------------- #
epoch_0_to_1 = {                                # Epoch 0-1 구간
    'start': 2.5852,
    'end': 1.3194,
    'reduction': 1.2658,                        # 초기 빠른 감소
    'reduction_rate': 0.489                     # 48.9% 감소
}

epoch_1_to_5 = {                                # Epoch 1-5 구간 (추정)
    'avg_reduction_per_epoch': 0.10             # 에포크당 약 0.10 감소
}

epoch_5_to_10 = {                               # Epoch 5-10 구간
    'start': 1.0,                               # 추정값
    'end': 0.8411,
    'reduction': 0.1589,                        # 후반부 느린 감소
    'reduction_rate': 0.159                     # 15.9% 감소
}
```

**학습 곡선 특징:**
1. **초기 급격한 감소** (Epoch 0-1): Loss가 2.59 → 1.32로 빠르게 하락
2. **중기 안정적 감소** (Epoch 1-5): 꾸준한 개선세 유지
3. **후기 수렴** (Epoch 5-10): Loss 0.8대에서 안정화

### 3.3 학습 안정성 평가

```python
# ==================== 학습 안정성 지표 ==================== #
stability_metrics = {
    # ---------------------- Gradient Norm 안정성 ---------------------- #
    'grad_norm_mean': 5.5,                      # 평균 Gradient Norm
    'grad_norm_std': 1.2,                       # 표준편차
    'grad_norm_range': (4.1, 8.8),              # 최소/최대 범위
    'grad_norm_stability': 'GOOD',              # 안정성 평가

    # ---------------------- Loss 변동성 ---------------------- #
    'loss_variance_early': 0.15,                # 초기 Loss 분산 (높음)
    'loss_variance_late': 0.02,                 # 후기 Loss 분산 (낮음)
    'loss_stability': 'EXCELLENT',              # Loss 안정성 평가

    # ---------------------- 학습률 스케줄링 ---------------------- #
    'lr_decay_pattern': 'LINEAR',               # 학습률 감소 패턴
    'lr_final_value': 5.37e-08,                 # 최종 학습률
    'lr_scheduling': 'NORMAL'                   # 스케줄링 평가
}
```

---

## 4. 결과 분석

### 4.1 성공 항목 ✅

#### 4.1.1 모델 자동 확장 기능
- ✅ `--models all` 옵션이 6개 모델로 정상 확장
- ✅ 로그에 확장된 모델 리스트 명확히 출력
- ✅ 실험명은 `full_kobart`로 생성 (첫 번째 모델명 사용)

#### 4.1.2 KoBART 모델 학습
- ✅ **학습 완료**: 20 Epoch 중 약 10 Epoch 완료 (50%)
- ✅ **Loss 수렴**: 2.59 → 0.84 (67.5% 감소)
- ✅ **안정성**: Gradient Norm 안정적, Loss 변동 적음
- ✅ **모델 저장**: 체크포인트 정상 저장

#### 4.1.3 학습 파라미터 적용
- ✅ **학습률 스케줄링**: Warmup + Linear Decay 정상 작동
- ✅ **Gradient Accumulation**: 4 스텝 누적 정상
- ✅ **Label Smoothing**: 0.1 적용
- ✅ **Weight Decay**: 0.01 적용

### 4.2 실패 항목 ❌

#### 4.2.1 다중 모델 학습 중단
- ❌ **llama-3.2-korean-3b 모델 로드 실패**
- ❌ **나머지 5개 모델 학습 미완료**
- ❌ **앙상블 전략 미실행** (Stacking 앙상블 불가)
- ❌ **TTA 전략 미실행**

#### 4.2.2 발생 오류
```python
# ==================== 오류 메시지 ==================== #
error_message = """
❌ 오류 발생: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
"""
# 발생 시점: llama-3.2-korean-3b 모델 학습 시작 직후
# 발생 위치: Causal LM 모델 로드 중
```

### 4.3 학습 성능 평가

```python
# ==================== KoBART 모델 성능 평가 ==================== #
performance_evaluation = {
    # ---------------------- Loss 성능 ---------------------- #
    'final_loss': 0.8411,                       # 최종 Loss
    'min_loss': 0.8133,                         # 최저 Loss
    'loss_evaluation': 'GOOD',                  # Loss 0.8대는 양호한 수준

    # ---------------------- 학습 효율성 ---------------------- #
    'epochs_completed': 10,                     # 완료된 Epoch (20 중 10)
    'training_time': '32분',                    # 총 학습 시간
    'time_per_epoch': '3.2분',                  # Epoch당 평균 시간
    'samples_per_second': 65,                   # 초당 처리 샘플 수
    'efficiency': 'GOOD',                       # 효율성 평가

    # ---------------------- 모델 파라미터 ---------------------- #
    'total_params': 123_859_968,                # 전체 파라미터 수
    'trainable_params': 123_859_968,            # 학습 가능 파라미터 수
    'model_size': '약 472MB'                    # 모델 크기 (FP32 기준)
}
```

**성능 평가 요약:**
- **Loss 수준**: 0.8411은 대화 요약 태스크에서 양호한 수준
- **학습 속도**: 약 18 it/s로 KoBART 모델은 빠른 학습 속도
- **수렴 상태**: 10 Epoch에서 Loss가 안정화되었으나, 20 Epoch 완주 시 더 개선 가능

---

## 5. 발생 오류 및 원인

### 5.1 오류 상세 정보

```python
# ==================== 오류 정보 ==================== #
error_details = {
    'error_type': 'RuntimeError',                                   # 오류 타입
    'error_message': '_amp_foreach_non_finite_check_and_unscale_cuda not implemented for BFloat16',
    'failed_model': 'llama-3.2-korean-3b',                          # 실패한 모델
    'model_type': 'causal_lm',                                      # 모델 타입
    'occurrence_time': '2025-10-12 09:03:05',                       # 발생 시간
    'occurrence_stage': 'model_training_start'                      # 발생 단계
}
```

### 5.2 원인 분석

```python
# ==================== 원인 분석 ==================== #
root_cause_analysis = {
    # ---------------------- 직접적 원인 ---------------------- #
    'direct_cause': 'BFloat16 데이터 타입과 AMP(Automatic Mixed Precision) 호환성 문제',

    # ---------------------- 세부 원인 ---------------------- #
    'detailed_causes': [
        '1. llama-3.2-korean-3b 모델이 BFloat16 타입으로 로드됨',
        '2. PyTorch AMP의 GradScaler가 BFloat16을 지원하지 않음',
        '3. _amp_foreach_non_finite_check_and_unscale_cuda 커널이 BFloat16 미구현',
        '4. Gradient Checkpointing 활성화로 인한 추가 메모리 연산'
    ],

    # ---------------------- 기술적 배경 ---------------------- #
    'technical_background': {
        'bfloat16': 'Brain Float 16 - Google TPU에서 주로 사용하는 16비트 부동소수점',
        'amp': 'Automatic Mixed Precision - GPU 학습 가속화 기술',
        'compatibility_issue': 'PyTorch의 일부 AMP 커널이 BFloat16 타입을 지원하지 않음'
    },

    # ---------------------- 환경 요인 ---------------------- #
    'environment_factors': [
        'CUDA 버전과 PyTorch 버전 호환성',
        'GPU 아키텍처 (Ampere 이상에서 BFloat16 지원)',
        'Config 파일의 dtype 설정'
    ]
}
```

### 5.3 재현 조건

```python
# ==================== 오류 재현 조건 ==================== #
reproduction_conditions = {
    # ---------------------- 필수 조건 ---------------------- #
    'required_conditions': [
        'llama-3.2-korean-3b 모델 사용',
        'BFloat16 dtype 설정',
        'AMP (Automatic Mixed Precision) 활성화',
        'Gradient Checkpointing 활성화'
    ],

    # ---------------------- Config 설정 ---------------------- #
    'config_settings': {
        'model_dtype': 'bfloat16',              # 모델 데이터 타입
        'fp16': False,                          # FP16 비활성화
        'bf16': True,                           # BF16 활성화
        'gradient_checkpointing': True          # Gradient Checkpointing 활성화
    }
}
```

---

## 6. 수정 방향 및 개선안

### 6.1 즉시 수정 사항 (Critical)

#### 6.1.1 BFloat16 문제 해결

```python
# ==================== 수정 방법 1: FP16 사용 (권장) ==================== #
# configs/models/llama_3.2_korean_3b.yaml 파일 수정

# ---------------------- 기존 설정 (오류 발생) ---------------------- #
training_old = {
    'fp16': False,                              # FP16 비활성화
    'bf16': True,                               # BF16 활성화 (오류 원인)
    'torch_dtype': 'bfloat16'                   # BFloat16 타입
}

# ---------------------- 수정 설정 (권장) ---------------------- #
training_new = {
    'fp16': True,                               # FP16 활성화 (AMP 완전 지원)
    'bf16': False,                              # BF16 비활성화
    'torch_dtype': 'float16'                    # Float16 타입
}
```

#### 6.1.2 Config 파일 일괄 수정

```bash
# ==================== 모든 Causal LM Config 수정 ==================== #

# ---------------------- 수정 대상 파일 목록 ---------------------- #
config_files = [
    'configs/models/llama_3.2_korean_3b.yaml',
    'configs/models/qwen3_4b.yaml',
    'configs/models/solar-10.7b.yaml',
    'configs/models/polyglot-ko-12.8b.yaml',
    'configs/models/kullm-v2.yaml'
]

# ---------------------- 수정 스크립트 예시 ---------------------- #
# 각 파일에서 bf16: true → bf16: false, fp16: false → fp16: true 변경
# torch_dtype: bfloat16 → torch_dtype: float16 변경
```

### 6.2 중기 개선 사항 (Important)

#### 6.2.1 모델별 오류 처리 강화

```python
# ==================== src/training/full_pipeline.py 수정 ==================== #

# ---------------------- 기존 코드 (오류 발생 시 전체 중단) ---------------------- #
def train_multiple_models_old(args, train_dataset, val_dataset):
    for model_name in args.models:
        # 오류 발생 시 전체 파이프라인 중단
        train_single_model(model_name, train_dataset, val_dataset)

# ---------------------- 개선 코드 (오류 발생 시 다음 모델 계속) ---------------------- #
def train_multiple_models_new(args, train_dataset, val_dataset):
    """
    다중 모델 학습 함수 (오류 발생 시 다음 모델 계속 진행)

    Args:
        args: 학습 인자
        train_dataset: 학습 데이터셋
        val_dataset: 검증 데이터셋

    Returns:
        dict: 모델별 학습 결과 및 오류 정보
    """
    results = {}                                                    # 결과 딕셔너리 초기화

    for idx, model_name in enumerate(args.models):
        print(f"==================================================")
        print(f"모델 {idx+1}/{len(args.models)}: {model_name}")
        print(f"==================================================")

        # ---------------------- 오류 처리 블록 ---------------------- #
        try:
            # 모델 학습 시도
            result = train_single_model(model_name, train_dataset, val_dataset)
            results[model_name] = {
                'status': 'success',                                # 성공 상태
                'result': result                                    # 학습 결과
            }
            print(f"✅ {model_name} 학습 완료")

        except Exception as e:
            # 오류 발생 시 로깅 및 다음 모델로 계속
            print(f"❌ {model_name} 학습 실패: {e}")
            results[model_name] = {
                'status': 'failed',                                 # 실패 상태
                'error': str(e),                                    # 오류 메시지
                'error_type': type(e).__name__                      # 오류 타입
            }

            # 오류 로그 상세 기록
            import traceback
            with open(f'errors/{model_name}_error.log', 'w') as f:
                f.write(f"Model: {model_name}\n")
                f.write(f"Error: {e}\n")
                f.write(f"Traceback:\n{traceback.format_exc()}")

            # 다음 모델 학습 계속
            continue

    # ---------------------- 최종 결과 요약 ---------------------- #
    success_count = sum(1 for r in results.values() if r['status'] == 'success')
    failed_count = len(results) - success_count
    print(f"\n📊 학습 결과 요약:")
    print(f"  ✅ 성공: {success_count}/{len(args.models)} 모델")
    print(f"  ❌ 실패: {failed_count}/{len(args.models)} 모델")

    return results
```

#### 6.2.2 Config 검증 로직 추가

```python
# ==================== src/utils/config_validator.py 생성 (신규) ==================== #

def validate_model_config(config_path):
    """
    모델 Config 파일 검증 함수

    Args:
        config_path: Config 파일 경로

    Returns:
        tuple: (is_valid, errors, warnings)
    """
    errors = []                                 # 오류 리스트 초기화
    warnings = []                               # 경고 리스트 초기화

    # ---------------------- Config 파일 로드 ---------------------- #
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # ---------------------- BFloat16 + AMP 호환성 체크 ---------------------- #
    if config.get('training', {}).get('bf16', False):
        errors.append(
            "BFloat16은 PyTorch AMP와 호환 문제 발생 가능. "
            "FP16 사용 권장 (bf16: false, fp16: true)"
        )

    # ---------------------- Gradient Checkpointing 체크 ---------------------- #
    if config.get('model', {}).get('gradient_checkpointing', False):
        warnings.append(
            "Gradient Checkpointing 활성화 시 메모리는 절약되지만 "
            "학습 속도가 약 20-30% 감소할 수 있음"
        )

    # ---------------------- Batch Size 체크 ---------------------- #
    batch_size = config.get('training', {}).get('batch_size', 8)
    if batch_size > 16:
        warnings.append(
            f"큰 Batch Size ({batch_size})는 GPU 메모리 부족 위험. "
            f"Gradient Accumulation 사용 권장"
        )

    # ---------------------- 검증 결과 반환 ---------------------- #
    is_valid = len(errors) == 0
    return is_valid, errors, warnings
```

### 6.3 장기 개선 사항 (Nice to Have)

#### 6.3.1 실험 자동 재개 기능

```python
# ==================== 실험 체크포인트 시스템 ==================== #

class ExperimentCheckpoint:
    """
    실험 진행 상황 체크포인트 관리 클래스
    """

    def __init__(self, experiment_dir):
        self.experiment_dir = experiment_dir                        # 실험 디렉토리
        self.checkpoint_file = os.path.join(
            experiment_dir, 'experiment_checkpoint.json'
        )                                                           # 체크포인트 파일

    def save_progress(self, model_name, status, result=None):
        """체크포인트 저장"""
        checkpoint = self._load_checkpoint()                        # 기존 체크포인트 로드

        checkpoint['models'][model_name] = {
            'status': status,                                       # 상태: success/failed/in_progress
            'timestamp': datetime.now().isoformat(),                # 타임스탬프
            'result': result                                        # 결과 (옵션)
        }

        self._save_checkpoint(checkpoint)                           # 체크포인트 저장

    def get_remaining_models(self, all_models):
        """남은 모델 리스트 반환"""
        checkpoint = self._load_checkpoint()                        # 체크포인트 로드
        completed = [
            m for m, info in checkpoint['models'].items()
            if info['status'] == 'success'
        ]                                                           # 완료된 모델 리스트
        return [m for m in all_models if m not in completed]       # 남은 모델 반환
```

#### 6.3.2 모델별 최적 Config 자동 선택

```python
# ==================== Config 자동 최적화 시스템 ==================== #

def auto_select_optimal_config(model_name, gpu_memory_gb):
    """
    GPU 메모리에 따라 최적 Config 자동 선택

    Args:
        model_name: 모델명
        gpu_memory_gb: GPU 메모리 크기 (GB)

    Returns:
        dict: 최적화된 Config 설정
    """
    # ---------------------- 모델별 권장 설정 ---------------------- #
    recommended_configs = {
        'kobart': {
            'min_memory': 8,                                        # 최소 메모리 (GB)
            'optimal_batch_size': 16,                               # 최적 배치 크기
            'dtype': 'float16'                                      # 권장 데이터 타입
        },
        'llama-3.2-korean-3b': {
            'min_memory': 12,
            'optimal_batch_size': 4,
            'dtype': 'float16'                                      # FP16 권장 (BF16 오류 방지)
        },
        'solar-10.7b': {
            'min_memory': 24,
            'optimal_batch_size': 2,
            'dtype': 'float16'
        }
    }

    # ---------------------- GPU 메모리 기반 Config 조정 ---------------------- #
    base_config = recommended_configs.get(model_name, {})
    min_memory = base_config.get('min_memory', 8)

    if gpu_memory_gb < min_memory:
        # 메모리 부족 시 설정 조정
        return {
            'batch_size': 1,                                        # 최소 배치 크기
            'gradient_accumulation_steps': 16,                      # 그래디언트 누적 증가
            'gradient_checkpointing': True,                         # Gradient Checkpointing 활성화
            'dtype': 'float16'                                      # FP16 사용
        }
    else:
        # 충분한 메모리 시 기본 설정
        return base_config
```

### 6.4 수정 우선순위

```python
# ==================== 수정 우선순위 ==================== #
priority_order = [
    {
        'priority': 'P0 (Critical)',                                # 최우선
        'task': 'BFloat16 → FP16 변경',
        'impact': 'HIGH',                                           # 영향도: 높음
        'effort': 'LOW',                                            # 작업량: 낮음
        'deadline': '즉시',                                         # 마감: 즉시
        'files': [
            'configs/models/llama_3.2_korean_3b.yaml',
            'configs/models/qwen3_4b.yaml',
            'configs/models/solar-10.7b.yaml',
            'configs/models/polyglot-ko-12.8b.yaml',
            'configs/models/kullm-v2.yaml'
        ]
    },
    {
        'priority': 'P1 (High)',                                    # 높음
        'task': '모델별 오류 처리 강화',
        'impact': 'HIGH',
        'effort': 'MEDIUM',                                         # 작업량: 중간
        'deadline': '1주일',
        'files': [
            'src/training/full_pipeline.py'
        ]
    },
    {
        'priority': 'P2 (Medium)',                                  # 중간
        'task': 'Config 검증 로직 추가',
        'impact': 'MEDIUM',                                         # 영향도: 중간
        'effort': 'MEDIUM',
        'deadline': '2주일',
        'files': [
            'src/utils/config_validator.py (신규)'
        ]
    },
    {
        'priority': 'P3 (Low)',                                     # 낮음
        'task': '실험 자동 재개 기능',
        'impact': 'LOW',                                            # 영향도: 낮음
        'effort': 'HIGH',                                           # 작업량: 높음
        'deadline': '1개월',
        'files': [
            'src/training/checkpoint_manager.py (신규)'
        ]
    }
]
```

---

## 7. 수정 완료 후 검증 계획

### 7.1 검증 단계

```python
# ==================== 검증 계획 ==================== #
verification_plan = [
    {
        'stage': '1단계: Config 수정 검증',
        'tasks': [
            '1. 모든 Causal LM Config에서 bf16 → fp16 변경',
            '2. Config 파일 Syntax 오류 체크',
            '3. Config Validator로 검증'
        ],
        'expected_result': '모든 Config 파일 검증 통과'
    },
    {
        'stage': '2단계: 단일 모델 테스트',
        'tasks': [
            '1. llama-3.2-korean-3b 단일 모델 학습 테스트 (1 epoch)',
            '2. 오류 없이 정상 완료 확인',
            '3. 로그에서 FP16 사용 확인'
        ],
        'expected_result': 'llama-3.2-korean-3b 학습 정상 완료'
    },
    {
        'stage': '3단계: 다중 모델 테스트',
        'tasks': [
            '1. 2개 모델 (kobart + llama-3.2-korean-3b) 학습 테스트',
            '2. 오류 발생 시 다음 모델 계속 진행 확인',
            '3. 결과 요약 로그 확인'
        ],
        'expected_result': '2개 모델 모두 정상 완료 또는 오류 처리 후 계속'
    },
    {
        'stage': '4단계: 전체 파이프라인 재실행',
        'tasks': [
            '1. 6개 모델 전체 파이프라인 재실행',
            '2. 20 Epoch 완전 학습',
            '3. 앙상블 및 TTA 정상 실행 확인'
        ],
        'expected_result': '6개 모델 모두 정상 완료 + 앙상블 성공'
    }
]
```

### 7.2 검증 명령어

```bash
# ==================== 검증 명령어 모음 ==================== #

# ---------------------- 1단계: Config 검증 ---------------------- #
# Config Validator 실행 (추후 구현)
python scripts/validate_configs.py --config-dir configs/models

# ---------------------- 2단계: 단일 모델 테스트 ---------------------- #
# llama-3.2-korean-3b 단일 모델 1 Epoch 학습
python scripts/train.py \
  --mode single \
  --models llama-3.2-korean-3b \
  --epochs 1 \
  --batch_size 4 \
  --experiment_name test_llama_fp16_fix

# ---------------------- 3단계: 다중 모델 테스트 ---------------------- #
# 2개 모델 (kobart + llama) 1 Epoch 학습
python scripts/train.py \
  --mode multi_model \
  --models kobart llama-3.2-korean-3b \
  --epochs 1 \
  --batch_size 8 \
  --ensemble_strategy weighted_avg \
  --experiment_name test_multi_model_fp16_fix

# ---------------------- 4단계: 전체 파이프라인 재실행 ---------------------- #
# 전체 파이프라인 20 Epoch 완전 학습 (Epoch 수 줄여서 테스트 권장)
python scripts/train.py \
  --mode full \
  --models all \
  --epochs 5 \
  --batch_size 8 \
  --k_folds 3 \
  --ensemble_strategy stacking \
  --experiment_name test_full_pipeline_fp16_fix
```

---

## 8. 결론 및 권장사항

### 8.1 실험 결론

```python
# ==================== 실험 종합 평가 ==================== #
experiment_summary = {
    # ---------------------- 성공 요소 ---------------------- #
    'successes': [
        '✅ KoBART 모델 학습 성공적 (Loss 2.59 → 0.84)',
        '✅ --models all 옵션 자동 확장 정상 작동',
        '✅ 학습 파라미터 및 스케줄링 정상 적용',
        '✅ 모델 체크포인트 저장 정상 작동'
    ],

    # ---------------------- 실패 요소 ---------------------- #
    'failures': [
        '❌ BFloat16 + AMP 호환성 문제로 Causal LM 학습 실패',
        '❌ 다중 모델 학습 중단 (6개 중 1개만 완료)',
        '❌ 앙상블 및 TTA 전략 미실행'
    ],

    # ---------------------- 학습된 교훈 ---------------------- #
    'lessons_learned': [
        '📝 BFloat16은 PyTorch AMP와 호환 문제 가능 → FP16 사용 권장',
        '📝 다중 모델 학습 시 오류 처리 강화 필요 (하나 실패 시 전체 중단 방지)',
        '📝 Config 검증 로직 필요 (학습 전 설정 오류 사전 체크)',
        '📝 실험 체크포인트 시스템 필요 (중단 후 재개 기능)'
    ]
}
```

### 8.2 즉시 조치 사항

```bash
# ==================== 즉시 수정 필요 (P0) ==================== #

# ---------------------- 1. Config 파일 수정 ---------------------- #
# 모든 Causal LM Config에서 BFloat16 → Float16 변경

# configs/models/llama_3.2_korean_3b.yaml
# bf16: true → bf16: false
# fp16: false → fp16: true
# torch_dtype: bfloat16 → torch_dtype: float16

# 동일하게 qwen3_4b.yaml, solar-10.7b.yaml, polyglot-ko-12.8b.yaml, kullm-v2.yaml 수정
```

### 8.3 향후 실험 권장사항

```python
# ==================== 향후 실험 권장사항 ==================== #
recommendations = [
    {
        'category': '모델 학습',
        'recommendations': [
            '1. Config 수정 후 단일 모델 테스트부터 시작',
            '2. 소규모 실험 (1-2 Epoch)으로 오류 사전 발견',
            '3. 전체 파이프라인은 검증 후 실행'
        ]
    },
    {
        'category': '리소스 관리',
        'recommendations': [
            '1. GPU 메모리 모니터링 (nvidia-smi 활용)',
            '2. Batch Size 조정으로 메모리 최적화',
            '3. Gradient Accumulation으로 효과적 배치 크기 증가'
        ]
    },
    {
        'category': '실험 추적',
        'recommendations': [
            '1. WandB 활용하여 실험 결과 시각화',
            '2. 모델별 Loss Curve 비교 분석',
            '3. 오류 로그 자동 수집 및 분석'
        ]
    },
    {
        'category': 'Config 관리',
        'recommendations': [
            '1. Config Validator 구현하여 사전 검증',
            '2. 모델별 최적 설정 문서화',
            '3. GPU 메모리별 권장 Config 가이드 작성'
        ]
    }
]
```

---

## 9. 참고 자료

### 9.1 관련 문서
- `docs/모듈화/04_명령어_옵션_완전_가이드.md` - 전체 파이프라인 명령어 가이드
- `docs/모듈화/02_모델_설정_가이드.md` - 모델 Config 설정 가이드
- `configs/models/all.yaml` - 전체 모델 앙상블 메타 Config

### 9.2 로그 파일 위치
- **학습 로그**: `experiments/20251012/20251012_082741_full_kobart/train.log`
- **실험 폴더**: `experiments/20251012/20251012_082741_full_kobart/`

### 9.3 수정 대상 파일
```python
# ==================== 수정 필요 파일 목록 ==================== #
files_to_modify = [
    # ---------------------- P0: Config 파일 (즉시) ---------------------- #
    'configs/models/llama_3.2_korean_3b.yaml',                      # BF16 → FP16
    'configs/models/qwen3_4b.yaml',                                 # BF16 → FP16
    'configs/models/solar-10.7b.yaml',                              # BF16 → FP16
    'configs/models/polyglot-ko-12.8b.yaml',                        # BF16 → FP16
    'configs/models/kullm-v2.yaml',                                 # BF16 → FP16

    # ---------------------- P1: 오류 처리 (1주일) ---------------------- #
    'src/training/full_pipeline.py',                                # 모델별 오류 처리 강화

    # ---------------------- P2: 검증 로직 (2주일) ---------------------- #
    'src/utils/config_validator.py',                                # Config 검증 (신규)

    # ---------------------- P3: 체크포인트 (1개월) ---------------------- #
    'src/training/checkpoint_manager.py'                            # 실험 재개 (신규)
]
```

---

**작성일**: 2025-10-12
**작성자**: AI Assistant
**실험 담당자**: ieyeppo
**문서 버전**: v1.0
