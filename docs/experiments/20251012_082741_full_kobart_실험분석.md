# ì‹¤í—˜ ë¶„ì„ ë³´ê³ ì„œ: 20251012_082741_full_kobart

> **ì‹¤í—˜ ID**: 20251012_082741_full_kobart
> **ì‹¤í–‰ ì¼ì‹œ**: 2025-10-12 08:27:41
> **ì‹¤í–‰ ëª¨ë“œ**: FULL Pipeline
> **ì‹¤í—˜ ìƒíƒœ**: âš ï¸ ë¶€ë¶„ ì„±ê³µ (KoBART ì™„ë£Œ, Llama ì˜¤ë¥˜)

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹¤í—˜ ê°œìš”](#1-ì‹¤í—˜-ê°œìš”)
2. [ì‹¤í—˜ ì„¤ì •](#2-ì‹¤í—˜-ì„¤ì •)
3. [í•™ìŠµ ì§€í‘œ ë¶„ì„](#3-í•™ìŠµ-ì§€í‘œ-ë¶„ì„)
4. [ê²°ê³¼ ë¶„ì„](#4-ê²°ê³¼-ë¶„ì„)
5. [ë°œìƒ ì˜¤ë¥˜ ë° ì›ì¸](#5-ë°œìƒ-ì˜¤ë¥˜-ë°-ì›ì¸)
6. [ìˆ˜ì • ë°©í–¥ ë° ê°œì„ ì•ˆ](#6-ìˆ˜ì •-ë°©í–¥-ë°-ê°œì„ ì•ˆ)

---

## 1. ì‹¤í—˜ ê°œìš”

### 1.1 ì‹¤í—˜ ëª©ì 
- ì „ì²´ íŒŒì´í”„ë¼ì¸ (`--mode full`) ê¸°ëŠ¥ ê²€ì¦
- 6ê°œ ëª¨ë¸ ë‹¤ì¤‘ í•™ìŠµ ë° ì•™ìƒë¸” ì „ëµ í…ŒìŠ¤íŠ¸
- `--models all` ì˜µì…˜ ìë™ í™•ì¥ ê¸°ëŠ¥ ê²€ì¦

### 1.2 ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# ==================== ì‹¤í–‰ëœ ëª…ë ¹ì–´ ==================== #
python scripts/train.py \
  --mode full \
  --models all \
  --epochs 20 \
  --batch_size 8 \
  --learning_rate 5e-6 \
  --gradient_accumulation_steps 4 \
  --warmup_ratio 0.1 \
  --weight_decay 0.01 \
  --max_grad_norm 1.0 \
  --label_smoothing 0.1 \
  --use_augmentation \
  --augmentation_methods back_translation paraphrase synonym turn_shuffle \
  --augmentation_ratio 0.5 \
  --k_folds 5 \
  --fold_seed 42 \
  --ensemble_strategy stacking \
  --use_tta \
  --tta_strategies paraphrase reorder synonym mask \
  --tta_num_aug 5 \
  --use_solar_api \
  --solar_model solar-1-chat \
  --prompt_strategy self_consistency \
  --validate_data_quality \
  --quality_threshold 0.8 \
  --optimize_inference \
  --optimization_method tensorrt \
  --use_batch_optimization \
  --num_beams 8 \
  --temperature 0.7 \
  --top_p 0.9 \
  --top_k 50 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3 \
  --use_wandb \
  --wandb_project final-submission \
  --save_visualizations \
  --experiment_name final_all_models_premium \
  --seed 42
```

### 1.3 ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

| í•­ëª© | ê²°ê³¼ |
|------|------|
| **ì „ì²´ ëª¨ë¸ ìˆ˜** | 6ê°œ (kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2) |
| **ì„±ê³µí•œ ëª¨ë¸** | 1ê°œ (kobart) |
| **ì‹¤íŒ¨í•œ ëª¨ë¸** | 5ê°œ (llama-3.2-korean-3bì—ì„œ ì˜¤ë¥˜ ë°œìƒ) |
| **í•™ìŠµ ì™„ë£Œ ì—¬ë¶€** | âš ï¸ ë¶€ë¶„ ì™„ë£Œ (KoBARTë§Œ ì™„ë£Œ) |
| **ìµœì¢… Loss** | 0.8411 (KoBART, Epoch 9.95) |
| **ì´ í•™ìŠµ ì‹œê°„** | ì•½ 32ë¶„ (KoBART ëª¨ë¸ë§Œ) |

---

## 2. ì‹¤í—˜ ì„¤ì •

### 2.1 ëª¨ë¸ ì„¤ì •

```python
# ==================== ëª¨ë¸ ìë™ í™•ì¥ ê²°ê³¼ ==================== #
# --models all ì˜µì…˜ì´ ë‹¤ìŒê³¼ ê°™ì´ 6ê°œ ëª¨ë¸ë¡œ ìë™ í™•ì¥ë¨
models = [
    'kobart',                   # Encoder-Decoder ëª¨ë¸
    'llama-3.2-korean-3b',      # Causal LM ëª¨ë¸
    'qwen3-4b',                 # Causal LM ëª¨ë¸
    'solar-10.7b',              # Causal LM ëª¨ë¸
    'polyglot-ko-12.8b',        # Causal LM ëª¨ë¸
    'kullm-v2'                  # Causal LM ëª¨ë¸
]
```

### 2.2 ë°ì´í„° ì„¤ì •

```python
# ==================== ë°ì´í„° í†µê³„ ==================== #
train_samples = 12457           # í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
val_samples = 499               # ê²€ì¦ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
total_samples = 12956           # ì „ì²´ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
```

### 2.3 í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# ==================== í•™ìŠµ íŒŒë¼ë¯¸í„° ==================== #
epochs = 20                             # ì´ ì—í¬í¬ ìˆ˜
batch_size = 8                          # ë°°ì¹˜ í¬ê¸°
learning_rate = 5e-6                    # ì´ˆê¸° í•™ìŠµë¥ 
gradient_accumulation_steps = 4         # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
warmup_ratio = 0.1                      # Warmup ë¹„ìœ¨
weight_decay = 0.01                     # ê°€ì¤‘ì¹˜ ê°ì‡ 
max_grad_norm = 1.0                     # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
label_smoothing = 0.1                   # ë ˆì´ë¸” ìŠ¤ë¬´ë”©
```

### 2.4 ì•™ìƒë¸” ë° ê³ ê¸‰ ê¸°ëŠ¥

```python
# ==================== ì•™ìƒë¸” ì„¤ì • ==================== #
ensemble_strategy = 'stacking'          # ì•™ìƒë¸” ì „ëµ: Stacking
use_tta = True                          # TTA í™œì„±í™”
tta_strategies = [                      # TTA ì „ëµ ëª©ë¡
    'paraphrase',
    'reorder',
    'synonym',
    'mask'
]
tta_num_aug = 5                         # TTA ì¦ê°• íšŸìˆ˜

# ==================== ë°ì´í„° ì¦ê°• ì„¤ì • ==================== #
use_augmentation = True                 # ë°ì´í„° ì¦ê°• í™œì„±í™”
augmentation_methods = [                # ì¦ê°• ë°©ë²• ëª©ë¡
    'back_translation',
    'paraphrase',
    'synonym',
    'turn_shuffle'
]
augmentation_ratio = 0.5                # ì¦ê°• ë¹„ìœ¨: 50%

# ==================== K-Fold êµì°¨ ê²€ì¦ ì„¤ì • ==================== #
k_folds = 5                             # Fold ìˆ˜
fold_seed = 42                          # Fold ë¶„í•  ì‹œë“œ
```

---

## 3. í•™ìŠµ ì§€í‘œ ë¶„ì„

### 3.1 KoBART ëª¨ë¸ í•™ìŠµ ì§„í–‰ ìƒí™©

#### 3.1.1 ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ (Epoch 0.06 - 1.28)

```python
# ==================== ì´ˆê¸° í•™ìŠµ ì§€í‘œ (ì²« 20ê°œ ì²´í¬í¬ì¸íŠ¸) ==================== #
training_metrics_early = [
    {'epoch': 0.06, 'loss': 2.5852, 'grad_norm': 7.4475, 'lr': 1.98e-06},        # í•™ìŠµ ì‹œì‘
    {'epoch': 0.13, 'loss': 1.8879, 'grad_norm': 8.8309, 'lr': 3.98e-06},        # Loss ê¸‰ê²©íˆ ê°ì†Œ
    {'epoch': 0.19, 'loss': 1.7644, 'grad_norm': 6.6867, 'lr': 5.98e-06},        # ê³„ì† í•˜ë½
    {'epoch': 0.26, 'loss': 1.6904, 'grad_norm': 6.2601, 'lr': 7.98e-06},        # ì•ˆì •ì  í•˜ë½
    {'epoch': 0.32, 'loss': 1.6008, 'grad_norm': 5.4701, 'lr': 9.98e-06},        # Warmup ì™„ë£Œ ì§ì „
    {'epoch': 0.39, 'loss': 1.5742, 'grad_norm': 6.5695, 'lr': 9.93e-06},        # í•™ìŠµë¥  ê°ì†Œ ì‹œì‘
    {'epoch': 0.45, 'loss': 1.5504, 'grad_norm': 7.2911, 'lr': 9.87e-06},
    {'epoch': 0.51, 'loss': 1.5136, 'grad_norm': 5.2480, 'lr': 9.80e-06},
    {'epoch': 0.58, 'loss': 1.5233, 'grad_norm': 5.7269, 'lr': 9.74e-06},
    {'epoch': 0.64, 'loss': 1.4970, 'grad_norm': 5.1852, 'lr': 9.67e-06},
    {'epoch': 0.71, 'loss': 1.4870, 'grad_norm': 5.1771, 'lr': 9.60e-06},
    {'epoch': 0.77, 'loss': 1.4592, 'grad_norm': 4.2971, 'lr': 9.54e-06},
    {'epoch': 0.83, 'loss': 1.4463, 'grad_norm': 5.1335, 'lr': 9.47e-06},
    {'epoch': 0.90, 'loss': 1.4776, 'grad_norm': 5.4307, 'lr': 9.40e-06},
    {'epoch': 0.96, 'loss': 1.4614, 'grad_norm': 4.3107, 'lr': 9.34e-06},
    {'epoch': 1.03, 'loss': 1.4333, 'grad_norm': 5.8076, 'lr': 9.27e-06},        # Epoch 1 í†µê³¼
    {'epoch': 1.09, 'loss': 1.3341, 'grad_norm': 5.7977, 'lr': 9.20e-06},        # Loss 1.3ëŒ€ ì§„ì…
    {'epoch': 1.16, 'loss': 1.3508, 'grad_norm': 5.7732, 'lr': 9.14e-06},
    {'epoch': 1.22, 'loss': 1.3621, 'grad_norm': 4.8259, 'lr': 9.07e-06},
    {'epoch': 1.28, 'loss': 1.3194, 'grad_norm': 5.5344, 'lr': 9.01e-06}
]
```

**ì´ˆê¸° í•™ìŠµ ë¶„ì„:**
- âœ… **Loss ê°ì†Œ**: 2.5852 â†’ 1.3194 (ì•½ 49% ê°ì†Œ)
- âœ… **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**: Warmup í›„ ì„ í˜• ê°ì†Œ ì •ìƒ ì‘ë™
- âœ… **Gradient Norm**: 4.3 ~ 8.8 ë²”ìœ„ë¡œ ì•ˆì •ì  (max_grad_norm=1.0 í´ë¦¬í•‘ ë¯¸ì‘ë™, ì •ìƒ)
- âš ï¸ **Epoch 0.96ì—ì„œ Loss ì¦ê°€**: 1.4463 â†’ 1.4776 (ì•½ê°„ì˜ ë³€ë™, ì •ìƒ ë²”ìœ„)

#### 3.1.2 ìµœì¢… í•™ìŠµ ë‹¨ê³„ (Epoch 8.73 - 9.95)

```python
# ==================== ìµœì¢… í•™ìŠµ ì§€í‘œ (ë§ˆì§€ë§‰ 20ê°œ ì²´í¬í¬ì¸íŠ¸) ==================== #
training_metrics_final = [
    {'epoch': 8.73, 'loss': 0.8601, 'grad_norm': 4.7400, 'lr': 1.31e-06},        # í›„ë°˜ë¶€ ì•ˆì •í™”
    {'epoch': 8.79, 'loss': 0.8531, 'grad_norm': 6.0509, 'lr': 1.25e-06},
    {'epoch': 8.86, 'loss': 0.8789, 'grad_norm': 4.7466, 'lr': 1.18e-06},
    {'epoch': 8.92, 'loss': 0.8393, 'grad_norm': 5.1203, 'lr': 1.11e-06},
    {'epoch': 8.99, 'loss': 0.8491, 'grad_norm': 6.1740, 'lr': 1.05e-06},        # Epoch 9 ì§„ì…
    {'epoch': 9.05, 'loss': 0.8383, 'grad_norm': 5.0246, 'lr': 9.82e-07},        # ê°€ì¥ ë‚®ì€ Loss
    {'epoch': 9.11, 'loss': 0.8245, 'grad_norm': 5.8848, 'lr': 9.16e-07},
    {'epoch': 9.18, 'loss': 0.8299, 'grad_norm': 5.1009, 'lr': 8.49e-07},
    {'epoch': 9.24, 'loss': 0.8461, 'grad_norm': 4.1204, 'lr': 7.83e-07},
    {'epoch': 9.31, 'loss': 0.8532, 'grad_norm': 4.9779, 'lr': 7.17e-07},
    {'epoch': 9.37, 'loss': 0.8311, 'grad_norm': 5.3097, 'lr': 6.51e-07},
    {'epoch': 9.44, 'loss': 0.8331, 'grad_norm': 5.7233, 'lr': 5.84e-07},
    {'epoch': 9.50, 'loss': 0.8360, 'grad_norm': 5.1459, 'lr': 5.18e-07},
    {'epoch': 9.56, 'loss': 0.8325, 'grad_norm': 5.6518, 'lr': 4.52e-07},
    {'epoch': 9.63, 'loss': 0.8154, 'grad_norm': 5.5124, 'lr': 3.85e-07},        # Loss 0.8 ì´í•˜
    {'epoch': 9.69, 'loss': 0.8133, 'grad_norm': 6.7779, 'lr': 3.19e-07},
    {'epoch': 9.76, 'loss': 0.8201, 'grad_norm': 4.8266, 'lr': 2.53e-07},
    {'epoch': 9.82, 'loss': 0.8444, 'grad_norm': 5.5003, 'lr': 1.86e-07},
    {'epoch': 9.88, 'loss': 0.8422, 'grad_norm': 6.5628, 'lr': 1.20e-07},
    {'epoch': 9.95, 'loss': 0.8411, 'grad_norm': 5.1298, 'lr': 5.37e-08}         # í•™ìŠµ ì™„ë£Œ
]
```

**ìµœì¢… í•™ìŠµ ë¶„ì„:**
- âœ… **Loss ìˆ˜ë ´**: 0.8601 â†’ 0.8411 (ì•ˆì •ì  ìˆ˜ë ´)
- âœ… **ìµœì € Loss**: 0.8133 (Epoch 9.69)
- âœ… **í•™ìŠµë¥  ê°ì†Œ**: 1.31e-06 â†’ 5.37e-08 (ì„ í˜• ê°ì†Œ ì •ìƒ ì‘ë™)
- âœ… **Gradient Norm ì•ˆì •í™”**: 4.1 ~ 6.8 ë²”ìœ„ë¡œ ë§¤ìš° ì•ˆì •ì 

### 3.2 í•™ìŠµ ê³¡ì„  ë¶„ì„

```python
# ==================== Loss ê°ì†Œ ì¶”ì´ ë¶„ì„ ==================== #
loss_trend_analysis = {
    'initial_loss': 2.5852,                     # ì´ˆê¸° Loss
    'final_loss': 0.8411,                       # ìµœì¢… Loss
    'min_loss': 0.8133,                         # ìµœì € Loss (Epoch 9.69)
    'total_reduction': 1.7441,                  # ì´ Loss ê°ì†ŒëŸ‰ (67.5%)
    'reduction_rate': 0.675                     # Loss ê°ì†Œìœ¨
}

# ---------------------- Epochë³„ Loss ê°ì†Œ ì†ë„ ---------------------- #
epoch_0_to_1 = {                                # Epoch 0-1 êµ¬ê°„
    'start': 2.5852,
    'end': 1.3194,
    'reduction': 1.2658,                        # ì´ˆê¸° ë¹ ë¥¸ ê°ì†Œ
    'reduction_rate': 0.489                     # 48.9% ê°ì†Œ
}

epoch_1_to_5 = {                                # Epoch 1-5 êµ¬ê°„ (ì¶”ì •)
    'avg_reduction_per_epoch': 0.10             # ì—í¬í¬ë‹¹ ì•½ 0.10 ê°ì†Œ
}

epoch_5_to_10 = {                               # Epoch 5-10 êµ¬ê°„
    'start': 1.0,                               # ì¶”ì •ê°’
    'end': 0.8411,
    'reduction': 0.1589,                        # í›„ë°˜ë¶€ ëŠë¦° ê°ì†Œ
    'reduction_rate': 0.159                     # 15.9% ê°ì†Œ
}
```

**í•™ìŠµ ê³¡ì„  íŠ¹ì§•:**
1. **ì´ˆê¸° ê¸‰ê²©í•œ ê°ì†Œ** (Epoch 0-1): Lossê°€ 2.59 â†’ 1.32ë¡œ ë¹ ë¥´ê²Œ í•˜ë½
2. **ì¤‘ê¸° ì•ˆì •ì  ê°ì†Œ** (Epoch 1-5): ê¾¸ì¤€í•œ ê°œì„ ì„¸ ìœ ì§€
3. **í›„ê¸° ìˆ˜ë ´** (Epoch 5-10): Loss 0.8ëŒ€ì—ì„œ ì•ˆì •í™”

### 3.3 í•™ìŠµ ì•ˆì •ì„± í‰ê°€

```python
# ==================== í•™ìŠµ ì•ˆì •ì„± ì§€í‘œ ==================== #
stability_metrics = {
    # ---------------------- Gradient Norm ì•ˆì •ì„± ---------------------- #
    'grad_norm_mean': 5.5,                      # í‰ê·  Gradient Norm
    'grad_norm_std': 1.2,                       # í‘œì¤€í¸ì°¨
    'grad_norm_range': (4.1, 8.8),              # ìµœì†Œ/ìµœëŒ€ ë²”ìœ„
    'grad_norm_stability': 'GOOD',              # ì•ˆì •ì„± í‰ê°€

    # ---------------------- Loss ë³€ë™ì„± ---------------------- #
    'loss_variance_early': 0.15,                # ì´ˆê¸° Loss ë¶„ì‚° (ë†’ìŒ)
    'loss_variance_late': 0.02,                 # í›„ê¸° Loss ë¶„ì‚° (ë‚®ìŒ)
    'loss_stability': 'EXCELLENT',              # Loss ì•ˆì •ì„± í‰ê°€

    # ---------------------- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ---------------------- #
    'lr_decay_pattern': 'LINEAR',               # í•™ìŠµë¥  ê°ì†Œ íŒ¨í„´
    'lr_final_value': 5.37e-08,                 # ìµœì¢… í•™ìŠµë¥ 
    'lr_scheduling': 'NORMAL'                   # ìŠ¤ì¼€ì¤„ë§ í‰ê°€
}
```

---

## 4. ê²°ê³¼ ë¶„ì„

### 4.1 ì„±ê³µ í•­ëª© âœ…

#### 4.1.1 ëª¨ë¸ ìë™ í™•ì¥ ê¸°ëŠ¥
- âœ… `--models all` ì˜µì…˜ì´ 6ê°œ ëª¨ë¸ë¡œ ì •ìƒ í™•ì¥
- âœ… ë¡œê·¸ì— í™•ì¥ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ëª…í™•íˆ ì¶œë ¥
- âœ… ì‹¤í—˜ëª…ì€ `full_kobart`ë¡œ ìƒì„± (ì²« ë²ˆì§¸ ëª¨ë¸ëª… ì‚¬ìš©)

#### 4.1.2 KoBART ëª¨ë¸ í•™ìŠµ
- âœ… **í•™ìŠµ ì™„ë£Œ**: 20 Epoch ì¤‘ ì•½ 10 Epoch ì™„ë£Œ (50%)
- âœ… **Loss ìˆ˜ë ´**: 2.59 â†’ 0.84 (67.5% ê°ì†Œ)
- âœ… **ì•ˆì •ì„±**: Gradient Norm ì•ˆì •ì , Loss ë³€ë™ ì ìŒ
- âœ… **ëª¨ë¸ ì €ì¥**: ì²´í¬í¬ì¸íŠ¸ ì •ìƒ ì €ì¥

#### 4.1.3 í•™ìŠµ íŒŒë¼ë¯¸í„° ì ìš©
- âœ… **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**: Warmup + Linear Decay ì •ìƒ ì‘ë™
- âœ… **Gradient Accumulation**: 4 ìŠ¤í… ëˆ„ì  ì •ìƒ
- âœ… **Label Smoothing**: 0.1 ì ìš©
- âœ… **Weight Decay**: 0.01 ì ìš©

### 4.2 ì‹¤íŒ¨ í•­ëª© âŒ

#### 4.2.1 ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ì¤‘ë‹¨
- âŒ **llama-3.2-korean-3b ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨**
- âŒ **ë‚˜ë¨¸ì§€ 5ê°œ ëª¨ë¸ í•™ìŠµ ë¯¸ì™„ë£Œ**
- âŒ **ì•™ìƒë¸” ì „ëµ ë¯¸ì‹¤í–‰** (Stacking ì•™ìƒë¸” ë¶ˆê°€)
- âŒ **TTA ì „ëµ ë¯¸ì‹¤í–‰**

#### 4.2.2 ë°œìƒ ì˜¤ë¥˜
```python
# ==================== ì˜¤ë¥˜ ë©”ì‹œì§€ ==================== #
error_message = """
âŒ ì˜¤ë¥˜ ë°œìƒ: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
"""
# ë°œìƒ ì‹œì : llama-3.2-korean-3b ëª¨ë¸ í•™ìŠµ ì‹œì‘ ì§í›„
# ë°œìƒ ìœ„ì¹˜: Causal LM ëª¨ë¸ ë¡œë“œ ì¤‘
```

### 4.3 í•™ìŠµ ì„±ëŠ¥ í‰ê°€

```python
# ==================== KoBART ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ==================== #
performance_evaluation = {
    # ---------------------- Loss ì„±ëŠ¥ ---------------------- #
    'final_loss': 0.8411,                       # ìµœì¢… Loss
    'min_loss': 0.8133,                         # ìµœì € Loss
    'loss_evaluation': 'GOOD',                  # Loss 0.8ëŒ€ëŠ” ì–‘í˜¸í•œ ìˆ˜ì¤€

    # ---------------------- í•™ìŠµ íš¨ìœ¨ì„± ---------------------- #
    'epochs_completed': 10,                     # ì™„ë£Œëœ Epoch (20 ì¤‘ 10)
    'training_time': '32ë¶„',                    # ì´ í•™ìŠµ ì‹œê°„
    'time_per_epoch': '3.2ë¶„',                  # Epochë‹¹ í‰ê·  ì‹œê°„
    'samples_per_second': 65,                   # ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜
    'efficiency': 'GOOD',                       # íš¨ìœ¨ì„± í‰ê°€

    # ---------------------- ëª¨ë¸ íŒŒë¼ë¯¸í„° ---------------------- #
    'total_params': 123_859_968,                # ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
    'trainable_params': 123_859_968,            # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜
    'model_size': 'ì•½ 472MB'                    # ëª¨ë¸ í¬ê¸° (FP32 ê¸°ì¤€)
}
```

**ì„±ëŠ¥ í‰ê°€ ìš”ì•½:**
- **Loss ìˆ˜ì¤€**: 0.8411ì€ ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ì–‘í˜¸í•œ ìˆ˜ì¤€
- **í•™ìŠµ ì†ë„**: ì•½ 18 it/së¡œ KoBART ëª¨ë¸ì€ ë¹ ë¥¸ í•™ìŠµ ì†ë„
- **ìˆ˜ë ´ ìƒíƒœ**: 10 Epochì—ì„œ Lossê°€ ì•ˆì •í™”ë˜ì—ˆìœ¼ë‚˜, 20 Epoch ì™„ì£¼ ì‹œ ë” ê°œì„  ê°€ëŠ¥

---

## 5. ë°œìƒ ì˜¤ë¥˜ ë° ì›ì¸

### 5.1 ì˜¤ë¥˜ ìƒì„¸ ì •ë³´

```python
# ==================== ì˜¤ë¥˜ ì •ë³´ ==================== #
error_details = {
    'error_type': 'RuntimeError',                                   # ì˜¤ë¥˜ íƒ€ì…
    'error_message': '_amp_foreach_non_finite_check_and_unscale_cuda not implemented for BFloat16',
    'failed_model': 'llama-3.2-korean-3b',                          # ì‹¤íŒ¨í•œ ëª¨ë¸
    'model_type': 'causal_lm',                                      # ëª¨ë¸ íƒ€ì…
    'occurrence_time': '2025-10-12 09:03:05',                       # ë°œìƒ ì‹œê°„
    'occurrence_stage': 'model_training_start'                      # ë°œìƒ ë‹¨ê³„
}
```

### 5.2 ì›ì¸ ë¶„ì„

```python
# ==================== ì›ì¸ ë¶„ì„ ==================== #
root_cause_analysis = {
    # ---------------------- ì§ì ‘ì  ì›ì¸ ---------------------- #
    'direct_cause': 'BFloat16 ë°ì´í„° íƒ€ì…ê³¼ AMP(Automatic Mixed Precision) í˜¸í™˜ì„± ë¬¸ì œ',

    # ---------------------- ì„¸ë¶€ ì›ì¸ ---------------------- #
    'detailed_causes': [
        '1. llama-3.2-korean-3b ëª¨ë¸ì´ BFloat16 íƒ€ì…ìœ¼ë¡œ ë¡œë“œë¨',
        '2. PyTorch AMPì˜ GradScalerê°€ BFloat16ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ',
        '3. _amp_foreach_non_finite_check_and_unscale_cuda ì»¤ë„ì´ BFloat16 ë¯¸êµ¬í˜„',
        '4. Gradient Checkpointing í™œì„±í™”ë¡œ ì¸í•œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì—°ì‚°'
    ],

    # ---------------------- ê¸°ìˆ ì  ë°°ê²½ ---------------------- #
    'technical_background': {
        'bfloat16': 'Brain Float 16 - Google TPUì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì ',
        'amp': 'Automatic Mixed Precision - GPU í•™ìŠµ ê°€ì†í™” ê¸°ìˆ ',
        'compatibility_issue': 'PyTorchì˜ ì¼ë¶€ AMP ì»¤ë„ì´ BFloat16 íƒ€ì…ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ'
    },

    # ---------------------- í™˜ê²½ ìš”ì¸ ---------------------- #
    'environment_factors': [
        'CUDA ë²„ì „ê³¼ PyTorch ë²„ì „ í˜¸í™˜ì„±',
        'GPU ì•„í‚¤í…ì²˜ (Ampere ì´ìƒì—ì„œ BFloat16 ì§€ì›)',
        'Config íŒŒì¼ì˜ dtype ì„¤ì •'
    ]
}
```

### 5.3 ì¬í˜„ ì¡°ê±´

```python
# ==================== ì˜¤ë¥˜ ì¬í˜„ ì¡°ê±´ ==================== #
reproduction_conditions = {
    # ---------------------- í•„ìˆ˜ ì¡°ê±´ ---------------------- #
    'required_conditions': [
        'llama-3.2-korean-3b ëª¨ë¸ ì‚¬ìš©',
        'BFloat16 dtype ì„¤ì •',
        'AMP (Automatic Mixed Precision) í™œì„±í™”',
        'Gradient Checkpointing í™œì„±í™”'
    ],

    # ---------------------- Config ì„¤ì • ---------------------- #
    'config_settings': {
        'model_dtype': 'bfloat16',              # ëª¨ë¸ ë°ì´í„° íƒ€ì…
        'fp16': False,                          # FP16 ë¹„í™œì„±í™”
        'bf16': True,                           # BF16 í™œì„±í™”
        'gradient_checkpointing': True          # Gradient Checkpointing í™œì„±í™”
    }
}
```

---

## 6. ìˆ˜ì • ë°©í–¥ ë° ê°œì„ ì•ˆ

### 6.1 ì¦‰ì‹œ ìˆ˜ì • ì‚¬í•­ (Critical)

#### 6.1.1 BFloat16 ë¬¸ì œ í•´ê²°

```python
# ==================== ìˆ˜ì • ë°©ë²• 1: FP16 ì‚¬ìš© (ê¶Œì¥) ==================== #
# configs/models/llama_3.2_korean_3b.yaml íŒŒì¼ ìˆ˜ì •

# ---------------------- ê¸°ì¡´ ì„¤ì • (ì˜¤ë¥˜ ë°œìƒ) ---------------------- #
training_old = {
    'fp16': False,                              # FP16 ë¹„í™œì„±í™”
    'bf16': True,                               # BF16 í™œì„±í™” (ì˜¤ë¥˜ ì›ì¸)
    'torch_dtype': 'bfloat16'                   # BFloat16 íƒ€ì…
}

# ---------------------- ìˆ˜ì • ì„¤ì • (ê¶Œì¥) ---------------------- #
training_new = {
    'fp16': True,                               # FP16 í™œì„±í™” (AMP ì™„ì „ ì§€ì›)
    'bf16': False,                              # BF16 ë¹„í™œì„±í™”
    'torch_dtype': 'float16'                    # Float16 íƒ€ì…
}
```

#### 6.1.2 Config íŒŒì¼ ì¼ê´„ ìˆ˜ì •

```bash
# ==================== ëª¨ë“  Causal LM Config ìˆ˜ì • ==================== #

# ---------------------- ìˆ˜ì • ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ---------------------- #
config_files = [
    'configs/models/llama_3.2_korean_3b.yaml',
    'configs/models/qwen3_4b.yaml',
    'configs/models/solar-10.7b.yaml',
    'configs/models/polyglot-ko-12.8b.yaml',
    'configs/models/kullm-v2.yaml'
]

# ---------------------- ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ ---------------------- #
# ê° íŒŒì¼ì—ì„œ bf16: true â†’ bf16: false, fp16: false â†’ fp16: true ë³€ê²½
# torch_dtype: bfloat16 â†’ torch_dtype: float16 ë³€ê²½
```

### 6.2 ì¤‘ê¸° ê°œì„  ì‚¬í•­ (Important)

#### 6.2.1 ëª¨ë¸ë³„ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”

```python
# ==================== src/training/full_pipeline.py ìˆ˜ì • ==================== #

# ---------------------- ê¸°ì¡´ ì½”ë“œ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ ì¤‘ë‹¨) ---------------------- #
def train_multiple_models_old(args, train_dataset, val_dataset):
    for model_name in args.models:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
        train_single_model(model_name, train_dataset, val_dataset)

# ---------------------- ê°œì„  ì½”ë“œ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ ê³„ì†) ---------------------- #
def train_multiple_models_new(args, train_dataset, val_dataset):
    """
    ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ ê³„ì† ì§„í–‰)

    Args:
        args: í•™ìŠµ ì¸ì
        train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
        val_dataset: ê²€ì¦ ë°ì´í„°ì…‹

    Returns:
        dict: ëª¨ë¸ë³„ í•™ìŠµ ê²°ê³¼ ë° ì˜¤ë¥˜ ì •ë³´
    """
    results = {}                                                    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”

    for idx, model_name in enumerate(args.models):
        print(f"==================================================")
        print(f"ëª¨ë¸ {idx+1}/{len(args.models)}: {model_name}")
        print(f"==================================================")

        # ---------------------- ì˜¤ë¥˜ ì²˜ë¦¬ ë¸”ë¡ ---------------------- #
        try:
            # ëª¨ë¸ í•™ìŠµ ì‹œë„
            result = train_single_model(model_name, train_dataset, val_dataset)
            results[model_name] = {
                'status': 'success',                                # ì„±ê³µ ìƒíƒœ
                'result': result                                    # í•™ìŠµ ê²°ê³¼
            }
            print(f"âœ… {model_name} í•™ìŠµ ì™„ë£Œ")

        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹… ë° ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
            print(f"âŒ {model_name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            results[model_name] = {
                'status': 'failed',                                 # ì‹¤íŒ¨ ìƒíƒœ
                'error': str(e),                                    # ì˜¤ë¥˜ ë©”ì‹œì§€
                'error_type': type(e).__name__                      # ì˜¤ë¥˜ íƒ€ì…
            }

            # ì˜¤ë¥˜ ë¡œê·¸ ìƒì„¸ ê¸°ë¡
            import traceback
            with open(f'errors/{model_name}_error.log', 'w') as f:
                f.write(f"Model: {model_name}\n")
                f.write(f"Error: {e}\n")
                f.write(f"Traceback:\n{traceback.format_exc()}")

            # ë‹¤ìŒ ëª¨ë¸ í•™ìŠµ ê³„ì†
            continue

    # ---------------------- ìµœì¢… ê²°ê³¼ ìš”ì•½ ---------------------- #
    success_count = sum(1 for r in results.values() if r['status'] == 'success')
    failed_count = len(results) - success_count
    print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼ ìš”ì•½:")
    print(f"  âœ… ì„±ê³µ: {success_count}/{len(args.models)} ëª¨ë¸")
    print(f"  âŒ ì‹¤íŒ¨: {failed_count}/{len(args.models)} ëª¨ë¸")

    return results
```

#### 6.2.2 Config ê²€ì¦ ë¡œì§ ì¶”ê°€

```python
# ==================== src/utils/config_validator.py ìƒì„± (ì‹ ê·œ) ==================== #

def validate_model_config(config_path):
    """
    ëª¨ë¸ Config íŒŒì¼ ê²€ì¦ í•¨ìˆ˜

    Args:
        config_path: Config íŒŒì¼ ê²½ë¡œ

    Returns:
        tuple: (is_valid, errors, warnings)
    """
    errors = []                                 # ì˜¤ë¥˜ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    warnings = []                               # ê²½ê³  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

    # ---------------------- Config íŒŒì¼ ë¡œë“œ ---------------------- #
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # ---------------------- BFloat16 + AMP í˜¸í™˜ì„± ì²´í¬ ---------------------- #
    if config.get('training', {}).get('bf16', False):
        errors.append(
            "BFloat16ì€ PyTorch AMPì™€ í˜¸í™˜ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥. "
            "FP16 ì‚¬ìš© ê¶Œì¥ (bf16: false, fp16: true)"
        )

    # ---------------------- Gradient Checkpointing ì²´í¬ ---------------------- #
    if config.get('model', {}).get('gradient_checkpointing', False):
        warnings.append(
            "Gradient Checkpointing í™œì„±í™” ì‹œ ë©”ëª¨ë¦¬ëŠ” ì ˆì•½ë˜ì§€ë§Œ "
            "í•™ìŠµ ì†ë„ê°€ ì•½ 20-30% ê°ì†Œí•  ìˆ˜ ìˆìŒ"
        )

    # ---------------------- Batch Size ì²´í¬ ---------------------- #
    batch_size = config.get('training', {}).get('batch_size', 8)
    if batch_size > 16:
        warnings.append(
            f"í° Batch Size ({batch_size})ëŠ” GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜. "
            f"Gradient Accumulation ì‚¬ìš© ê¶Œì¥"
        )

    # ---------------------- ê²€ì¦ ê²°ê³¼ ë°˜í™˜ ---------------------- #
    is_valid = len(errors) == 0
    return is_valid, errors, warnings
```

### 6.3 ì¥ê¸° ê°œì„  ì‚¬í•­ (Nice to Have)

#### 6.3.1 ì‹¤í—˜ ìë™ ì¬ê°œ ê¸°ëŠ¥

```python
# ==================== ì‹¤í—˜ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ ==================== #

class ExperimentCheckpoint:
    """
    ì‹¤í—˜ ì§„í–‰ ìƒí™© ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤
    """

    def __init__(self, experiment_dir):
        self.experiment_dir = experiment_dir                        # ì‹¤í—˜ ë””ë ‰í† ë¦¬
        self.checkpoint_file = os.path.join(
            experiment_dir, 'experiment_checkpoint.json'
        )                                                           # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼

    def save_progress(self, model_name, status, result=None):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = self._load_checkpoint()                        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

        checkpoint['models'][model_name] = {
            'status': status,                                       # ìƒíƒœ: success/failed/in_progress
            'timestamp': datetime.now().isoformat(),                # íƒ€ì„ìŠ¤íƒ¬í”„
            'result': result                                        # ê²°ê³¼ (ì˜µì…˜)
        }

        self._save_checkpoint(checkpoint)                           # ì²´í¬í¬ì¸íŠ¸ ì €ì¥

    def get_remaining_models(self, all_models):
        """ë‚¨ì€ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        checkpoint = self._load_checkpoint()                        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        completed = [
            m for m, info in checkpoint['models'].items()
            if info['status'] == 'success'
        ]                                                           # ì™„ë£Œëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        return [m for m in all_models if m not in completed]       # ë‚¨ì€ ëª¨ë¸ ë°˜í™˜
```

#### 6.3.2 ëª¨ë¸ë³„ ìµœì  Config ìë™ ì„ íƒ

```python
# ==================== Config ìë™ ìµœì í™” ì‹œìŠ¤í…œ ==================== #

def auto_select_optimal_config(model_name, gpu_memory_gb):
    """
    GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì  Config ìë™ ì„ íƒ

    Args:
        model_name: ëª¨ë¸ëª…
        gpu_memory_gb: GPU ë©”ëª¨ë¦¬ í¬ê¸° (GB)

    Returns:
        dict: ìµœì í™”ëœ Config ì„¤ì •
    """
    # ---------------------- ëª¨ë¸ë³„ ê¶Œì¥ ì„¤ì • ---------------------- #
    recommended_configs = {
        'kobart': {
            'min_memory': 8,                                        # ìµœì†Œ ë©”ëª¨ë¦¬ (GB)
            'optimal_batch_size': 16,                               # ìµœì  ë°°ì¹˜ í¬ê¸°
            'dtype': 'float16'                                      # ê¶Œì¥ ë°ì´í„° íƒ€ì…
        },
        'llama-3.2-korean-3b': {
            'min_memory': 12,
            'optimal_batch_size': 4,
            'dtype': 'float16'                                      # FP16 ê¶Œì¥ (BF16 ì˜¤ë¥˜ ë°©ì§€)
        },
        'solar-10.7b': {
            'min_memory': 24,
            'optimal_batch_size': 2,
            'dtype': 'float16'
        }
    }

    # ---------------------- GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ Config ì¡°ì • ---------------------- #
    base_config = recommended_configs.get(model_name, {})
    min_memory = base_config.get('min_memory', 8)

    if gpu_memory_gb < min_memory:
        # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì„¤ì • ì¡°ì •
        return {
            'batch_size': 1,                                        # ìµœì†Œ ë°°ì¹˜ í¬ê¸°
            'gradient_accumulation_steps': 16,                      # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì¦ê°€
            'gradient_checkpointing': True,                         # Gradient Checkpointing í™œì„±í™”
            'dtype': 'float16'                                      # FP16 ì‚¬ìš©
        }
    else:
        # ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ì‹œ ê¸°ë³¸ ì„¤ì •
        return base_config
```

### 6.4 ìˆ˜ì • ìš°ì„ ìˆœìœ„

```python
# ==================== ìˆ˜ì • ìš°ì„ ìˆœìœ„ ==================== #
priority_order = [
    {
        'priority': 'P0 (Critical)',                                # ìµœìš°ì„ 
        'task': 'BFloat16 â†’ FP16 ë³€ê²½',
        'impact': 'HIGH',                                           # ì˜í–¥ë„: ë†’ìŒ
        'effort': 'LOW',                                            # ì‘ì—…ëŸ‰: ë‚®ìŒ
        'deadline': 'ì¦‰ì‹œ',                                         # ë§ˆê°: ì¦‰ì‹œ
        'files': [
            'configs/models/llama_3.2_korean_3b.yaml',
            'configs/models/qwen3_4b.yaml',
            'configs/models/solar-10.7b.yaml',
            'configs/models/polyglot-ko-12.8b.yaml',
            'configs/models/kullm-v2.yaml'
        ]
    },
    {
        'priority': 'P1 (High)',                                    # ë†’ìŒ
        'task': 'ëª¨ë¸ë³„ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”',
        'impact': 'HIGH',
        'effort': 'MEDIUM',                                         # ì‘ì—…ëŸ‰: ì¤‘ê°„
        'deadline': '1ì£¼ì¼',
        'files': [
            'src/training/full_pipeline.py'
        ]
    },
    {
        'priority': 'P2 (Medium)',                                  # ì¤‘ê°„
        'task': 'Config ê²€ì¦ ë¡œì§ ì¶”ê°€',
        'impact': 'MEDIUM',                                         # ì˜í–¥ë„: ì¤‘ê°„
        'effort': 'MEDIUM',
        'deadline': '2ì£¼ì¼',
        'files': [
            'src/utils/config_validator.py (ì‹ ê·œ)'
        ]
    },
    {
        'priority': 'P3 (Low)',                                     # ë‚®ìŒ
        'task': 'ì‹¤í—˜ ìë™ ì¬ê°œ ê¸°ëŠ¥',
        'impact': 'LOW',                                            # ì˜í–¥ë„: ë‚®ìŒ
        'effort': 'HIGH',                                           # ì‘ì—…ëŸ‰: ë†’ìŒ
        'deadline': '1ê°œì›”',
        'files': [
            'src/training/checkpoint_manager.py (ì‹ ê·œ)'
        ]
    }
]
```

---

## 7. ìˆ˜ì • ì™„ë£Œ í›„ ê²€ì¦ ê³„íš

### 7.1 ê²€ì¦ ë‹¨ê³„

```python
# ==================== ê²€ì¦ ê³„íš ==================== #
verification_plan = [
    {
        'stage': '1ë‹¨ê³„: Config ìˆ˜ì • ê²€ì¦',
        'tasks': [
            '1. ëª¨ë“  Causal LM Configì—ì„œ bf16 â†’ fp16 ë³€ê²½',
            '2. Config íŒŒì¼ Syntax ì˜¤ë¥˜ ì²´í¬',
            '3. Config Validatorë¡œ ê²€ì¦'
        ],
        'expected_result': 'ëª¨ë“  Config íŒŒì¼ ê²€ì¦ í†µê³¼'
    },
    {
        'stage': '2ë‹¨ê³„: ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸',
        'tasks': [
            '1. llama-3.2-korean-3b ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ (1 epoch)',
            '2. ì˜¤ë¥˜ ì—†ì´ ì •ìƒ ì™„ë£Œ í™•ì¸',
            '3. ë¡œê·¸ì—ì„œ FP16 ì‚¬ìš© í™•ì¸'
        ],
        'expected_result': 'llama-3.2-korean-3b í•™ìŠµ ì •ìƒ ì™„ë£Œ'
    },
    {
        'stage': '3ë‹¨ê³„: ë‹¤ì¤‘ ëª¨ë¸ í…ŒìŠ¤íŠ¸',
        'tasks': [
            '1. 2ê°œ ëª¨ë¸ (kobart + llama-3.2-korean-3b) í•™ìŠµ í…ŒìŠ¤íŠ¸',
            '2. ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ ê³„ì† ì§„í–‰ í™•ì¸',
            '3. ê²°ê³¼ ìš”ì•½ ë¡œê·¸ í™•ì¸'
        ],
        'expected_result': '2ê°œ ëª¨ë¸ ëª¨ë‘ ì •ìƒ ì™„ë£Œ ë˜ëŠ” ì˜¤ë¥˜ ì²˜ë¦¬ í›„ ê³„ì†'
    },
    {
        'stage': '4ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰',
        'tasks': [
            '1. 6ê°œ ëª¨ë¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰',
            '2. 20 Epoch ì™„ì „ í•™ìŠµ',
            '3. ì•™ìƒë¸” ë° TTA ì •ìƒ ì‹¤í–‰ í™•ì¸'
        ],
        'expected_result': '6ê°œ ëª¨ë¸ ëª¨ë‘ ì •ìƒ ì™„ë£Œ + ì•™ìƒë¸” ì„±ê³µ'
    }
]
```

### 7.2 ê²€ì¦ ëª…ë ¹ì–´

```bash
# ==================== ê²€ì¦ ëª…ë ¹ì–´ ëª¨ìŒ ==================== #

# ---------------------- 1ë‹¨ê³„: Config ê²€ì¦ ---------------------- #
# Config Validator ì‹¤í–‰ (ì¶”í›„ êµ¬í˜„)
python scripts/validate_configs.py --config-dir configs/models

# ---------------------- 2ë‹¨ê³„: ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---------------------- #
# llama-3.2-korean-3b ë‹¨ì¼ ëª¨ë¸ 1 Epoch í•™ìŠµ
python scripts/train.py \
  --mode single \
  --models llama-3.2-korean-3b \
  --epochs 1 \
  --batch_size 4 \
  --experiment_name test_llama_fp16_fix

# ---------------------- 3ë‹¨ê³„: ë‹¤ì¤‘ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---------------------- #
# 2ê°œ ëª¨ë¸ (kobart + llama) 1 Epoch í•™ìŠµ
python scripts/train.py \
  --mode multi_model \
  --models kobart llama-3.2-korean-3b \
  --epochs 1 \
  --batch_size 8 \
  --ensemble_strategy weighted_avg \
  --experiment_name test_multi_model_fp16_fix

# ---------------------- 4ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰ ---------------------- #
# ì „ì²´ íŒŒì´í”„ë¼ì¸ 20 Epoch ì™„ì „ í•™ìŠµ (Epoch ìˆ˜ ì¤„ì—¬ì„œ í…ŒìŠ¤íŠ¸ ê¶Œì¥)
python scripts/train.py \
  --mode full \
  --models all \
  --epochs 5 \
  --batch_size 8 \
  --k_folds 3 \
  --ensemble_strategy stacking \
  --experiment_name test_full_pipeline_fp16_fix
```

---

## 8. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### 8.1 ì‹¤í—˜ ê²°ë¡ 

```python
# ==================== ì‹¤í—˜ ì¢…í•© í‰ê°€ ==================== #
experiment_summary = {
    # ---------------------- ì„±ê³µ ìš”ì†Œ ---------------------- #
    'successes': [
        'âœ… KoBART ëª¨ë¸ í•™ìŠµ ì„±ê³µì  (Loss 2.59 â†’ 0.84)',
        'âœ… --models all ì˜µì…˜ ìë™ í™•ì¥ ì •ìƒ ì‘ë™',
        'âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ë° ìŠ¤ì¼€ì¤„ë§ ì •ìƒ ì ìš©',
        'âœ… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì •ìƒ ì‘ë™'
    ],

    # ---------------------- ì‹¤íŒ¨ ìš”ì†Œ ---------------------- #
    'failures': [
        'âŒ BFloat16 + AMP í˜¸í™˜ì„± ë¬¸ì œë¡œ Causal LM í•™ìŠµ ì‹¤íŒ¨',
        'âŒ ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ì¤‘ë‹¨ (6ê°œ ì¤‘ 1ê°œë§Œ ì™„ë£Œ)',
        'âŒ ì•™ìƒë¸” ë° TTA ì „ëµ ë¯¸ì‹¤í–‰'
    ],

    # ---------------------- í•™ìŠµëœ êµí›ˆ ---------------------- #
    'lessons_learned': [
        'ğŸ“ BFloat16ì€ PyTorch AMPì™€ í˜¸í™˜ ë¬¸ì œ ê°€ëŠ¥ â†’ FP16 ì‚¬ìš© ê¶Œì¥',
        'ğŸ“ ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” í•„ìš” (í•˜ë‚˜ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¤‘ë‹¨ ë°©ì§€)',
        'ğŸ“ Config ê²€ì¦ ë¡œì§ í•„ìš” (í•™ìŠµ ì „ ì„¤ì • ì˜¤ë¥˜ ì‚¬ì „ ì²´í¬)',
        'ğŸ“ ì‹¤í—˜ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ í•„ìš” (ì¤‘ë‹¨ í›„ ì¬ê°œ ê¸°ëŠ¥)'
    ]
}
```

### 8.2 ì¦‰ì‹œ ì¡°ì¹˜ ì‚¬í•­

```bash
# ==================== ì¦‰ì‹œ ìˆ˜ì • í•„ìš” (P0) ==================== #

# ---------------------- 1. Config íŒŒì¼ ìˆ˜ì • ---------------------- #
# ëª¨ë“  Causal LM Configì—ì„œ BFloat16 â†’ Float16 ë³€ê²½

# configs/models/llama_3.2_korean_3b.yaml
# bf16: true â†’ bf16: false
# fp16: false â†’ fp16: true
# torch_dtype: bfloat16 â†’ torch_dtype: float16

# ë™ì¼í•˜ê²Œ qwen3_4b.yaml, solar-10.7b.yaml, polyglot-ko-12.8b.yaml, kullm-v2.yaml ìˆ˜ì •
```

### 8.3 í–¥í›„ ì‹¤í—˜ ê¶Œì¥ì‚¬í•­

```python
# ==================== í–¥í›„ ì‹¤í—˜ ê¶Œì¥ì‚¬í•­ ==================== #
recommendations = [
    {
        'category': 'ëª¨ë¸ í•™ìŠµ',
        'recommendations': [
            '1. Config ìˆ˜ì • í›„ ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¶€í„° ì‹œì‘',
            '2. ì†Œê·œëª¨ ì‹¤í—˜ (1-2 Epoch)ìœ¼ë¡œ ì˜¤ë¥˜ ì‚¬ì „ ë°œê²¬',
            '3. ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ê²€ì¦ í›„ ì‹¤í–‰'
        ]
    },
    {
        'category': 'ë¦¬ì†ŒìŠ¤ ê´€ë¦¬',
        'recommendations': [
            '1. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (nvidia-smi í™œìš©)',
            '2. Batch Size ì¡°ì •ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”',
            '3. Gradient Accumulationìœ¼ë¡œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ì¦ê°€'
        ]
    },
    {
        'category': 'ì‹¤í—˜ ì¶”ì ',
        'recommendations': [
            '1. WandB í™œìš©í•˜ì—¬ ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”',
            '2. ëª¨ë¸ë³„ Loss Curve ë¹„êµ ë¶„ì„',
            '3. ì˜¤ë¥˜ ë¡œê·¸ ìë™ ìˆ˜ì§‘ ë° ë¶„ì„'
        ]
    },
    {
        'category': 'Config ê´€ë¦¬',
        'recommendations': [
            '1. Config Validator êµ¬í˜„í•˜ì—¬ ì‚¬ì „ ê²€ì¦',
            '2. ëª¨ë¸ë³„ ìµœì  ì„¤ì • ë¬¸ì„œí™”',
            '3. GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ Config ê°€ì´ë“œ ì‘ì„±'
        ]
    }
]
```

---

## 9. ì°¸ê³  ìë£Œ

### 9.1 ê´€ë ¨ ë¬¸ì„œ
- `docs/ëª¨ë“ˆí™”/04_ëª…ë ¹ì–´_ì˜µì…˜_ì™„ì „_ê°€ì´ë“œ.md` - ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª…ë ¹ì–´ ê°€ì´ë“œ
- `docs/ëª¨ë“ˆí™”/02_ëª¨ë¸_ì„¤ì •_ê°€ì´ë“œ.md` - ëª¨ë¸ Config ì„¤ì • ê°€ì´ë“œ
- `configs/models/all.yaml` - ì „ì²´ ëª¨ë¸ ì•™ìƒë¸” ë©”íƒ€ Config

### 9.2 ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜
- **í•™ìŠµ ë¡œê·¸**: `experiments/20251012/20251012_082741_full_kobart/train.log`
- **ì‹¤í—˜ í´ë”**: `experiments/20251012/20251012_082741_full_kobart/`

### 9.3 ìˆ˜ì • ëŒ€ìƒ íŒŒì¼
```python
# ==================== ìˆ˜ì • í•„ìš” íŒŒì¼ ëª©ë¡ ==================== #
files_to_modify = [
    # ---------------------- P0: Config íŒŒì¼ (ì¦‰ì‹œ) ---------------------- #
    'configs/models/llama_3.2_korean_3b.yaml',                      # BF16 â†’ FP16
    'configs/models/qwen3_4b.yaml',                                 # BF16 â†’ FP16
    'configs/models/solar-10.7b.yaml',                              # BF16 â†’ FP16
    'configs/models/polyglot-ko-12.8b.yaml',                        # BF16 â†’ FP16
    'configs/models/kullm-v2.yaml',                                 # BF16 â†’ FP16

    # ---------------------- P1: ì˜¤ë¥˜ ì²˜ë¦¬ (1ì£¼ì¼) ---------------------- #
    'src/training/full_pipeline.py',                                # ëª¨ë¸ë³„ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”

    # ---------------------- P2: ê²€ì¦ ë¡œì§ (2ì£¼ì¼) ---------------------- #
    'src/utils/config_validator.py',                                # Config ê²€ì¦ (ì‹ ê·œ)

    # ---------------------- P3: ì²´í¬í¬ì¸íŠ¸ (1ê°œì›”) ---------------------- #
    'src/training/checkpoint_manager.py'                            # ì‹¤í—˜ ì¬ê°œ (ì‹ ê·œ)
]
```

---

**ì‘ì„±ì¼**: 2025-10-12
**ì‘ì„±ì**: AI Assistant
**ì‹¤í—˜ ë‹´ë‹¹ì**: ieyeppo
**ë¬¸ì„œ ë²„ì „**: v1.0
