# ğŸ­ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì „ëµ

## ğŸ¯ ê°œìš”
ì»´í“¨í„° ë¹„ì „ í”„ë¡œì íŠ¸ì—ì„œ ê²€ì¦ëœ ì•™ìƒë¸” ì „ëµì„ NLP ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì— ìµœì í™”í•˜ì—¬ ì ìš©

## ğŸ—ï¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    subgraph "ì…ë ¥ ë°ì´í„°"
        A[ëŒ€í™” ë°ì´í„°]
    end

    subgraph "ê¸°ë³¸ ëª¨ë¸ë“¤ (Base Models)"
        A --> B1[SOLAR-10.7B<br/>ìµœê³  ì„±ëŠ¥ ì˜ˆìƒ]
        A --> B2[Polyglot-Ko-12.8B<br/>í•œêµ­ì–´ íŠ¹í™”]
        A --> B3[KULLM-v2<br/>instruction ê°•ì ]
        A --> B4[KoAlpaca-LoRA<br/>íš¨ìœ¨ì„±]
        A --> B5[LDCC-SOLAR<br/>ëŒ€í™” íŠ¹í™”]
    end

    subgraph "ìƒì„± ì˜µì…˜"
        B1 --> C1[Temperature 0.3]
        B1 --> C2[Temperature 0.5]
        B1 --> C3[Temperature 0.7]
        B2 --> D1[Top-p 0.8]
        B2 --> D2[Top-p 0.9]
        B3 --> E1[Beam Search 3]
        B3 --> E2[Beam Search 5]
    end

    subgraph "ì•™ìƒë¸” ë°©ë²•"
        C1 --> F[Weighted Voting]
        C2 --> F
        C3 --> F
        D1 --> G[Majority Voting]
        D2 --> G
        E1 --> H[Averaging]
        E2 --> H
        F --> I[Meta Learner<br/>Stacking]
        G --> I
        H --> I
    end

    subgraph "TTA (Text Test Augmentation)"
        I --> J1[ì›ë³¸]
        I --> J2[íŒ¨ëŸ¬í”„ë ˆì´ì§•]
        I --> J3[ìˆœì„œ ë³€ê²½]
        J1 --> K[TTA ì•™ìƒë¸”]
        J2 --> K
        J3 --> K
    end

    K --> L[ìµœì¢… ìš”ì•½]

    style B1 fill:#f96,stroke:#333,stroke-width:4px
    style B2 fill:#9f6,stroke:#333,stroke-width:4px
    style I fill:#ff9,stroke:#333,stroke-width:4px
```

## ğŸ† NLP ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### 1. Top-Tier ëª¨ë¸ (ì»´í“¨í„° ë¹„ì „ì˜ ConvNext/Swin í¬ì§€ì…˜)

#### ğŸ¥‡ SOLAR-10.7B (ìµœìš°ì„  ì¶”ì²œ)
```python
model_config = {
    'name': 'upstage/SOLAR-10.7B-Instruct-v1.0',
    'params': '10.7B',
    'context_length': 4096,
    'expected_rouge': '70-75',
    'pros': [
        'í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜',
        'ëŒ€í™” ì´í•´ë ¥ ë›°ì–´ë‚¨',
        'Instruction following ê°•ë ¥'
    ],
    'memory_required': '24GB (LoRA: 8GB)'
}
```

#### ğŸ¥ˆ Polyglot-Ko-12.8B (ê°•ë ¥ ì¶”ì²œ)
```python
model_config = {
    'name': 'EleutherAI/polyglot-ko-12.8b',
    'params': '12.8B',
    'context_length': 2048,
    'expected_rouge': '68-73',
    'pros': [
        'í•œêµ­ì–´ ì „ìš© í•™ìŠµ',
        'ì•ˆì •ì  ì„±ëŠ¥',
        'ì»¤ë®¤ë‹ˆí‹° ì§€ì› í™œë°œ'
    ],
    'memory_required': '26GB (LoRA: 10GB)'
}
```

### 2. High-Performance ëª¨ë¸

#### KULLM-v2 (ê³ ë ¤ëŒ€ ëª¨ë¸)
```python
model_config = {
    'name': 'nlpai-lab/kullm-v2',
    'params': '7B',
    'context_length': 2048,
    'expected_rouge': '65-70',
    'pros': [
        'Instruction íŠœë‹ ì™„ë£Œ',
        'ëŒ€í™” íŠ¹í™”',
        'ë©”ëª¨ë¦¬ íš¨ìœ¨ì '
    ]
}
```

#### KoAlpaca (LoRA íš¨ìœ¨ì„±)
```python
model_config = {
    'name': 'beomi/KoAlpaca',
    'params': '7B',
    'context_length': 2048,
    'expected_rouge': '63-68',
    'pros': [
        'LoRA ìµœì í™”',
        'ë¹ ë¥¸ ì¶”ë¡ ',
        'ë‚®ì€ ë©”ëª¨ë¦¬'
    ]
}
```

#### LDCC-SOLAR (ëŒ€í™” íŠ¹í™”)
```python
model_config = {
    'name': 'lotte/ldcc-solar',
    'params': '10.7B',
    'context_length': 4096,
    'expected_rouge': '66-71',
    'pros': [
        'ë¡¯ë° ëŒ€í™” ë°ì´í„°',
        'ì¼ìƒ ëŒ€í™” ê°•ì ',
        'Solar ê¸°ë°˜'
    ]
}
```

### 3. ëª¨ë¸ ì¡°í•© ì „ëµ

#### ìµœì  5-ëª¨ë¸ ì•™ìƒë¸” (ì˜ˆìƒ ROUGE: 75-80)
```python
ensemble_config = {
    'models': [
        ('SOLAR-10.7B', 0.30),      # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
        ('Polyglot-Ko-12.8B', 0.25), # ë‘ ë²ˆì§¸ ê°€ì¤‘ì¹˜
        ('KULLM-v2', 0.20),          # ì„¸ ë²ˆì§¸
        ('KoAlpaca-LoRA', 0.15),    # íš¨ìœ¨ì„±
        ('LDCC-SOLAR', 0.10)         # ë³´ì¡°
    ],
    'strategy': 'weighted_average'
}
```

#### íš¨ìœ¨ì  3-ëª¨ë¸ ì•™ìƒë¸” (ì˜ˆìƒ ROUGE: 72-76)
```python
ensemble_config = {
    'models': [
        ('SOLAR-10.7B', 0.45),
        ('Polyglot-Ko-12.8B', 0.35),
        ('KULLM-v2', 0.20)
    ],
    'strategy': 'weighted_voting'
}
```

## ğŸ”„ ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œ

### 1. í•™ìŠµ ëª¨ë“œ ì„ íƒ
```python
class TrainingMode:
    SINGLE_FOLD = "single"          # ë‹¨ì¼ í´ë“œ (ë¹ ë¥¸ ì‹¤í—˜)
    K_FOLD = "kfold"                # K-í´ë“œ êµì°¨ ê²€ì¦
    MULTI_MODEL = "multi_model"     # ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
    OPTUNA = "optuna"               # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    FULL_PIPELINE = "full"          # ëª¨ë“  ì˜µì…˜ í™œì„±í™”

# ì‹¤í–‰ ì˜ˆì‹œ
python train.py \
    --mode multi_model \
    --models "solar,polyglot,kullm" \
    --ensemble_strategy weighted_voting \
    --k_folds 5 \
    --use_tta True \
    --optuna_trials 100
```

### 2. ì•™ìƒë¸” ì „ëµ ì„ íƒ
```python
class EnsembleStrategy:
    # ê¸°ë³¸ ì•™ìƒë¸”
    AVERAGING = "averaging"              # ë‹¨ìˆœ í‰ê· 
    WEIGHTED_AVG = "weighted_avg"        # ê°€ì¤‘ í‰ê· 
    MAJORITY_VOTE = "majority_vote"      # ë‹¤ìˆ˜ê²°

    # ê³ ê¸‰ ì•™ìƒë¸”
    STACKING = "stacking"               # ë©”íƒ€ í•™ìŠµ
    BLENDING = "blending"               # ë¸”ë Œë”©
    BOOSTING = "boosting"               # ë¶€ìŠ¤íŒ…

    # NLP íŠ¹í™”
    ROUGE_WEIGHTED = "rouge_weighted"   # ROUGE ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    LENGTH_ADAPTIVE = "length_adaptive" # ëŒ€í™” ê¸¸ì´ë³„ ì ì‘
    CONFIDENCE_BASED = "confidence"     # ì‹ ë¢°ë„ ê¸°ë°˜
```

### 3. TTA (Text Test Augmentation) ì˜µì…˜
```python
class TextTTA:
    """NLPìš© Test Time Augmentation"""

    def __init__(self, strategies):
        self.strategies = strategies

    def augment(self, text):
        augmented = [text]  # ì›ë³¸

        if 'paraphrase' in self.strategies:
            # íŒ¨ëŸ¬í”„ë ˆì´ì§• (3ê°€ì§€ ë³€í˜•)
            augmented.extend(self.paraphrase(text, n=3))

        if 'reorder' in self.strategies:
            # ëŒ€í™” ìˆœì„œ ë¶€ë¶„ ë³€ê²½
            augmented.append(self.reorder_dialogue(text))

        if 'synonym' in self.strategies:
            # ë™ì˜ì–´ ì¹˜í™˜
            augmented.append(self.synonym_replace(text))

        if 'mask' in self.strategies:
            # ë¶€ë¶„ ë§ˆìŠ¤í‚¹ í›„ ë³µì›
            augmented.append(self.mask_and_restore(text))

        return augmented

# ì‚¬ìš© ì˜ˆì‹œ
tta = TextTTA(['paraphrase', 'reorder'])
augmented_texts = tta.augment(dialogue)
predictions = [model.predict(text) for text in augmented_texts]
final_summary = ensemble(predictions)
```

## ğŸ“Š ì•™ìƒë¸” êµ¬í˜„ ì½”ë“œ

### 1. ê¸°ë³¸ ì•™ìƒë¸” í´ë˜ìŠ¤
```python
class MultiModelEnsemble:
    def __init__(self, model_configs, ensemble_strategy='weighted_avg'):
        self.models = self._load_models(model_configs)
        self.strategy = ensemble_strategy
        self.weights = self._optimize_weights()

    def _load_models(self, configs):
        """ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”"""
        models = []
        for config in configs:
            if config['use_lora']:
                model = self._load_with_lora(config)
            else:
                model = self._load_full_model(config)
            models.append(model)
        return models

    def _optimize_weights(self):
        """ê²€ì¦ ì„¸íŠ¸ì—ì„œ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰"""
        if self.strategy == 'weighted_avg':
            # Optunaë¡œ ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°
            return self._optuna_weight_search()
        return None

    def predict(self, dialogue, use_tta=False):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        all_predictions = []

        for model in self.models:
            if use_tta:
                # TTA ì ìš©
                tta_preds = self._predict_with_tta(model, dialogue)
                prediction = self._aggregate_tta(tta_preds)
            else:
                prediction = model.generate(dialogue)

            all_predictions.append(prediction)

        # ì•™ìƒë¸” ì „ëµ ì ìš©
        if self.strategy == 'weighted_avg':
            return self._weighted_average(all_predictions, self.weights)
        elif self.strategy == 'majority_vote':
            return self._majority_voting(all_predictions)
        elif self.strategy == 'stacking':
            return self._stacking(all_predictions)
        else:
            return self._simple_average(all_predictions)
```

### 2. K-Fold ì•™ìƒë¸”
```python
class KFoldEnsemble:
    def __init__(self, n_folds=5):
        self.n_folds = n_folds
        self.fold_models = []

    def train_fold(self, fold_idx, train_data, val_data):
        """ê° í´ë“œ í•™ìŠµ"""
        model = self._create_model()
        model.train(train_data, val_data)

        # í´ë“œë³„ ìµœì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = f'fold_{fold_idx}_best.pt'
        model.save(checkpoint_path)
        self.fold_models.append(checkpoint_path)

        return model.evaluate(val_data)

    def predict_ensemble(self, test_data):
        """K-Fold ì•™ìƒë¸” ì˜ˆì¸¡"""
        all_predictions = []

        for checkpoint in self.fold_models:
            model = self._load_model(checkpoint)
            pred = model.predict(test_data)
            all_predictions.append(pred)

        # í´ë“œë³„ ì˜ˆì¸¡ í‰ê· 
        return np.mean(all_predictions, axis=0)
```

### 3. ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
```python
class StackingEnsemble:
    def __init__(self, base_models, meta_model):
        self.base_models = base_models
        self.meta_model = meta_model

    def train(self, train_data, val_data):
        """2ë‹¨ê³„ í•™ìŠµ"""
        # 1ë‹¨ê³„: ë² ì´ìŠ¤ ëª¨ë¸ í•™ìŠµ
        base_predictions = []
        for model in self.base_models:
            model.train(train_data)
            pred = model.predict(val_data)
            base_predictions.append(pred)

        # 2ë‹¨ê³„: ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
        meta_features = np.column_stack(base_predictions)
        self.meta_model.train(meta_features, val_data.labels)

    def predict(self, test_data):
        """ìŠ¤íƒœí‚¹ ì˜ˆì¸¡"""
        base_preds = [model.predict(test_data)
                      for model in self.base_models]
        meta_features = np.column_stack(base_preds)
        return self.meta_model.predict(meta_features)
```

## ğŸ¯ ì‹¤í–‰ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ ì‹¤í—˜ (Single Model)
```bash
python train.py \
    --mode single \
    --model solar-10.7b \
    --epochs 3 \
    --batch_size 8
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì•ˆì •ì  ê²€ì¦ (K-Fold)
```bash
python train.py \
    --mode kfold \
    --model polyglot-ko \
    --k_folds 5 \
    --epochs 5
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœê³  ì„±ëŠ¥ (Multi-Model + TTA)
```bash
python train.py \
    --mode multi_model \
    --models "solar,polyglot,kullm" \
    --ensemble_strategy stacking \
    --use_tta True \
    --tta_strategies "paraphrase,reorder"
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ìë™ ìµœì í™” (Optuna)
```bash
python train.py \
    --mode optuna \
    --model solar-10.7b \
    --optuna_trials 100 \
    --optuna_timeout 7200
```

### ì‹œë‚˜ë¦¬ì˜¤ 5: í’€ íŒŒì´í”„ë¼ì¸ (ëŒ€íšŒ ìµœì¢…)
```bash
python train.py \
    --mode full \
    --models "all" \
    --k_folds 5 \
    --ensemble_strategy stacking \
    --use_tta True \
    --optuna_trials 50 \
    --final_submission True
```

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

| ì „ëµ | ëª¨ë¸ êµ¬ì„± | ì˜ˆìƒ ROUGE | í•™ìŠµ ì‹œê°„ | GPU ë©”ëª¨ë¦¬ |
|------|----------|------------|----------|------------|
| Single | SOLAR-10.7B | 70-73 | 3ì‹œê°„ | 24GB |
| Single | Polyglot-Ko | 68-71 | 4ì‹œê°„ | 26GB |
| 3-Model | Top 3 | 72-76 | 10ì‹œê°„ | 32GB |
| 5-Model | Top 5 | 75-80 | 18ì‹œê°„ | 48GB |
| 5-Model + TTA | Top 5 + 3x TTA | 77-82 | 24ì‹œê°„ | 48GB |
| Full Pipeline | All + K-Fold + TTA | 80-85 | 48ì‹œê°„ | 64GB |

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### ì»´í“¨í„° ë¹„ì „ vs NLP ì•™ìƒë¸” ì°¨ì´ì 

| ì¸¡ë©´ | ì»´í“¨í„° ë¹„ì „ | NLP (ëŒ€í™” ìš”ì•½) |
|------|------------|----------------|
| ìµœê³  ëª¨ë¸ | ConvNext, Swin | SOLAR, Polyglot-Ko |
| TTA | íšŒì „, í”Œë¦½, í¬ë¡­ | íŒ¨ëŸ¬í”„ë ˆì´ì§•, ìˆœì„œ ë³€ê²½ |
| ì•™ìƒë¸” íš¨ê³¼ | +3-5% | +5-8% (ë” íš¨ê³¼ì ) |
| ë‹¤ì–‘ì„± ì¤‘ìš”ë„ | ì¤‘ìš” | ë§¤ìš° ì¤‘ìš” |
| ì¶”ë¡  ì‹œê°„ | ë¹ ë¦„ | ëŠë¦¼ (í…ìŠ¤íŠ¸ ìƒì„±) |

### ì„±ê³µ ì „ëµ
1. **ëª¨ë¸ ë‹¤ì–‘ì„±**: ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ì™€ í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì§„ ëª¨ë¸ ì¡°í•©
2. **ìƒì„± ë‹¤ì–‘ì„±**: Temperature, Top-p, Beam Search ë“± ë‹¤ì–‘í•œ ë””ì½”ë”© ì „ëµ
3. **TTA í™œìš©**: íŒ¨ëŸ¬í”„ë ˆì´ì§•ìœ¼ë¡œ 3-5ê°œ ë³€í˜• ìƒì„± í›„ ì•™ìƒë¸”
4. **ì ì‘ì  ê°€ì¤‘ì¹˜**: ëŒ€í™” ê¸¸ì´ë‚˜ ì°¸ì—¬ì ìˆ˜ì— ë”°ë¼ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¡°ì •
5. **ì‹ ë¢°ë„ ê¸°ë°˜**: ê° ëª¨ë¸ì˜ ìƒì„± ì‹ ë¢°ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ í™œìš©