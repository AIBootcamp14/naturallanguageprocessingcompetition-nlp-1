# ✅ 베이스라인 검증 전략

## 🎯 목표
대회 제공 베이스라인 코드를 분석하여 검증된 설정값과 기법을 추출하고, 문장 끊김 문제를 해결

## 📊 베이스라인 성능 분석

### 검증된 베이스라인 성능
| 모델 | Config | 결과 품질 | 비고 |
|------|--------|----------|------|
| KoBART Baseline | config.yaml | ✅ 완벽한 문장 | 끊김 없음 |
| Solar API | Few-shot | ✅ 완벽한 문장 | 일관된 품질 |

### 문제 코드 분석
| 코드 | 결과 | 문제점 |
|------|------|--------|
| Full Pipeline (내 코드) | ❌ 문장 끊김 | "메모를 작성과 배포하라고 지시합니다. 그들은 정책이 부서장에게 문의될 것이라고 생각합니다. #teve는" |
| | | 이상한 토큰: "Mos.Da웨song", "#teve", "##" |

## 🔑 핵심 차이점 분석

### 1. **토큰 제거 방식 (문장 끊김의 근본 원인)**

#### ❌ 잘못된 방식
```python
# 토큰을 그냥 제거 → 문장이 붙어버림
for token in remove_tokens:
    summary = summary.replace(token, "")  # 공백 없이 제거
```

#### ✅ 베이스라인 방식 (정답)
```python
# 베이스라인: 토큰을 공백으로 치환
remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']
preprocessed_summary = summary.copy()
for token in remove_tokens:
    preprocessed_summary = [sentence.replace(token, " ") for sentence in preprocessed_summary]
```

**중요**: 반드시 `replace(token, " ")` - **공백으로 치환**

### 2. **KoBART 학습 파라미터 (검증된 설정)**

#### 베이스라인 Config (성공)
```yaml
training:
  num_train_epochs: 20
  learning_rate: 1.0e-05  # 1e-5 (매우 중요!)
  per_device_train_batch_size: 50  # 큰 배치
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: cosine
  optim: adamw_torch
  gradient_accumulation_steps: 1
  fp16: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100

inference:
  batch_size: 32
  no_repeat_ngram_size: 2  # 2가 최적!
  early_stopping: true
  num_beams: 4
  generate_max_length: 100
```

#### 내 코드 (실패)
```yaml
# 문제점
learning_rate: 5e-5  # 5배 높음!
batch_size: 8  # 6배 작음!
no_repeat_ngram_size: 3  # 너무 높음
```

### 3. **Solar API 프롬프트 전략**

#### Few-shot Learning 구조
```python
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner."

    # Few-shot example을 assistant role로 제공
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Dialogue:\n{sample_dialogue}\nSummary:\n"},
        {"role": "assistant", "content": sample_summary},  # 예시 제공
        {"role": "user", "content": f"Dialogue:\n{dialogue}\nSummary:\n"}
    ]

# API 호출 파라미터
summary = client.chat.completions.create(
    model="solar-1-mini-chat",
    messages=build_prompt(dialogue),
    temperature=0.2,  # 낮은 값 (일관성)
    top_p=0.3         # 낮은 값 (일관성)
)
```

#### 핵심 포인트
- **Role 기반 구조**: system → user → assistant → user
- **Few-shot example**: assistant role로 예시 제공
- **낮은 temperature/top_p**: 0.2/0.3으로 일관성 유지

### 4. **데이터 전처리 파이프라인**

```python
class Preprocess:
    def __init__(self, bos_token: str, eos_token: str):
        self.bos_token = bos_token  # <s>
        self.eos_token = eos_token  # </s>

    def make_input(self, dataset, is_test=False):
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            # Ground truth를 디코더 input으로 사용
            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()
```

### 5. **Dataset 클래스 구조**

```python
class DatasetForTrain(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}

        # decoder_input_ids 설정
        item2['decoder_input_ids'] = item2['input_ids']
        item2.pop('input_ids')
        item2.pop('attention_mask')

        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]

        return item
```

## 🛠️ 즉시 적용 체크리스트

### Phase 1: 긴급 수정 (문장 끊김 해결)
- [ ] **토큰 제거 방식 수정**
  ```python
  # 수정 전
  for token in remove_tokens:
      summary = summary.replace(token, "")

  # 수정 후
  for token in remove_tokens:
      summary = summary.replace(token, " ")  # 공백 추가!
  ```

### Phase 2: 학습 파라미터 최적화
- [ ] **Learning rate 하향 조정**
  - 5e-5 → **1e-5** (베이스라인)

- [ ] **Batch size 상향 조정**
  - 8 → **50** (가능하면)
  - GPU 메모리 부족 시: **16 또는 32**

- [ ] **no_repeat_ngram_size 조정**
  - 3 → **2** (베이스라인)

- [ ] **Early stopping 설정**
  ```yaml
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  ```

### Phase 3: Config 파일 표준화
- [ ] **베이스라인 config 기반 재작성**
  ```yaml
  general:
    model_name: digit82/kobart-summarization

  tokenizer:
    encoder_max_len: 512
    decoder_max_len: 100
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
      - '#PhoneNumber#'
      - '#Address#'
      - '#PassportNumber#'

  training:
    num_train_epochs: 20
    learning_rate: 1.0e-05
    per_device_train_batch_size: 50
    warmup_ratio: 0.1
    weight_decay: 0.01
    lr_scheduler_type: cosine

  inference:
    batch_size: 32
    no_repeat_ngram_size: 2
    num_beams: 4
    remove_tokens:
      - '<usr>'
      - '<s>'
      - '</s>'
      - '<pad>'
  ```

## 📈 예상 성능 개선

| 항목 | 수정 전 | 수정 후 | 개선 |
|------|---------|---------|------|
| 문장 완성도 | ❌ 끊김 발생 | ✅ 완벽 | 100% |
| 이상한 토큰 | ❌ 다수 발생 | ✅ 없음 | 100% |
| ROUGE Score | 낮음 | 베이스라인 수준 | +20~30% |
| 학습 안정성 | 불안정 | 안정적 | ⬆️ |

## 🔍 검증 절차

### 1. 토큰 제거 검증
```python
# 테스트 코드
test_summary = "#Person1#은 <s>안녕하세요</s><pad>"
remove_tokens = ['<s>', '</s>', '<pad>']

# 잘못된 방식
wrong = test_summary
for token in remove_tokens:
    wrong = wrong.replace(token, "")
print(wrong)  # "#Person1#은안녕하세요" - 붙어있음!

# 올바른 방식
correct = test_summary
for token in remove_tokens:
    correct = correct.replace(token, " ")
print(correct)  # "#Person1#은  안녕하세요  " - 공백 유지!
```

### 2. 생성 품질 검증
```python
# 제출 파일 샘플 확인
def validate_submission(csv_path):
    df = pd.read_csv(csv_path)

    issues = []
    for idx, row in df.iterrows():
        summary = row['summary']

        # 문장 끊김 체크
        if summary.endswith(('.', '다', '요', '니다')):
            continue
        else:
            issues.append({
                'idx': idx,
                'summary': summary,
                'issue': '문장 끊김'
            })

    return issues
```

## 🚀 실행 우선순위

### 즉시 (1일 이내)
1. ✅ 토큰 제거 방식 수정 (공백 치환)
2. ✅ Learning rate 1e-5로 변경
3. ✅ no_repeat_ngram_size = 2

### 단기 (3일 이내)
1. ✅ Batch size 최적화 (50 또는 32)
2. ✅ Early stopping 추가
3. ✅ 베이스라인 config 전면 적용

### 중기 (1주 이내)
1. ✅ Solar API Few-shot 구조 적용
2. ✅ 데이터 전처리 파이프라인 재구축
3. ✅ 전체 제출 파일 품질 검증

## 💡 핵심 교훈

### 문장 끊김 방지
1. **절대 규칙**: 토큰 제거 시 반드시 공백으로 치환
2. **검증 필수**: 생성된 모든 문장이 완전한지 확인
3. **remove_tokens 정확히 설정**: `['<usr>', '<s>', '</s>', '<pad>']`

### 학습 안정성
1. **Learning rate**: KoBART는 1e-5가 최적
2. **Batch size**: 클수록 좋음 (50 권장)
3. **Scheduler**: cosine이 linear보다 우수

### 프롬프트 엔지니어링
1. **Few-shot**: assistant role로 예시 제공
2. **Temperature**: 0.2~0.3 (낮게)
3. **Top_p**: 0.3~0.5 (낮게)
