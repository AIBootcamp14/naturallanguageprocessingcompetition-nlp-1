# âœ… ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ ì „ëµ

## ğŸ¯ ëª©í‘œ
ëŒ€íšŒ ì œê³µ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ê²€ì¦ëœ ì„¤ì •ê°’ê³¼ ê¸°ë²•ì„ ì¶”ì¶œí•˜ê³ , ë¬¸ì¥ ëŠê¹€ ë¬¸ì œë¥¼ í•´ê²°

## ğŸ“Š ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ë¶„ì„

### ê²€ì¦ëœ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥
| ëª¨ë¸ | Config | ê²°ê³¼ í’ˆì§ˆ | ë¹„ê³  |
|------|--------|----------|------|
| KoBART Baseline | config.yaml | âœ… ì™„ë²½í•œ ë¬¸ì¥ | ëŠê¹€ ì—†ìŒ |
| Solar API | Few-shot | âœ… ì™„ë²½í•œ ë¬¸ì¥ | ì¼ê´€ëœ í’ˆì§ˆ |

### ë¬¸ì œ ì½”ë“œ ë¶„ì„
| ì½”ë“œ | ê²°ê³¼ | ë¬¸ì œì  |
|------|------|--------|
| Full Pipeline (ë‚´ ì½”ë“œ) | âŒ ë¬¸ì¥ ëŠê¹€ | "ë©”ëª¨ë¥¼ ì‘ì„±ê³¼ ë°°í¬í•˜ë¼ê³  ì§€ì‹œí•©ë‹ˆë‹¤. ê·¸ë“¤ì€ ì •ì±…ì´ ë¶€ì„œì¥ì—ê²Œ ë¬¸ì˜ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. #teveëŠ”" |
| | | ì´ìƒí•œ í† í°: "Mos.Daì›¨song", "#teve", "##" |

## ğŸ”‘ í•µì‹¬ ì°¨ì´ì  ë¶„ì„

### 1. **í† í° ì œê±° ë°©ì‹ (ë¬¸ì¥ ëŠê¹€ì˜ ê·¼ë³¸ ì›ì¸)**

#### âŒ ì˜ëª»ëœ ë°©ì‹
```python
# í† í°ì„ ê·¸ëƒ¥ ì œê±° â†’ ë¬¸ì¥ì´ ë¶™ì–´ë²„ë¦¼
for token in remove_tokens:
    summary = summary.replace(token, "")  # ê³µë°± ì—†ì´ ì œê±°
```

#### âœ… ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹ (ì •ë‹µ)
```python
# ë² ì´ìŠ¤ë¼ì¸: í† í°ì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
remove_tokens = ['<usr>', '<s>', '</s>', '<pad>']
preprocessed_summary = summary.copy()
for token in remove_tokens:
    preprocessed_summary = [sentence.replace(token, " ") for sentence in preprocessed_summary]
```

**ì¤‘ìš”**: ë°˜ë“œì‹œ `replace(token, " ")` - **ê³µë°±ìœ¼ë¡œ ì¹˜í™˜**

### 2. **KoBART í•™ìŠµ íŒŒë¼ë¯¸í„° (ê²€ì¦ëœ ì„¤ì •)**

#### ë² ì´ìŠ¤ë¼ì¸ Config (ì„±ê³µ)
```yaml
training:
  num_train_epochs: 20
  learning_rate: 1.0e-05  # 1e-5 (ë§¤ìš° ì¤‘ìš”!)
  per_device_train_batch_size: 50  # í° ë°°ì¹˜
  per_device_eval_batch_size: 32
  warmup_ratio: 0.1
  weight_decay: 0.01
  lr_scheduler_type: cosine
  optim: adamw_torch
  gradient_accumulation_steps: 1
  fp16: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

tokenizer:
  encoder_max_len: 512
  decoder_max_len: 100

inference:
  batch_size: 32
  no_repeat_ngram_size: 2  # 2ê°€ ìµœì !
  early_stopping: true
  num_beams: 4
  generate_max_length: 100
```

#### ë‚´ ì½”ë“œ (ì‹¤íŒ¨)
```yaml
# ë¬¸ì œì 
learning_rate: 5e-5  # 5ë°° ë†’ìŒ!
batch_size: 8  # 6ë°° ì‘ìŒ!
no_repeat_ngram_size: 3  # ë„ˆë¬´ ë†’ìŒ
```

### 3. **Solar API í”„ë¡¬í”„íŠ¸ ì „ëµ**

#### Few-shot Learning êµ¬ì¡°
```python
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner."

    # Few-shot exampleì„ assistant roleë¡œ ì œê³µ
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"Dialogue:\n{sample_dialogue}\nSummary:\n"},
        {"role": "assistant", "content": sample_summary},  # ì˜ˆì‹œ ì œê³µ
        {"role": "user", "content": f"Dialogue:\n{dialogue}\nSummary:\n"}
    ]

# API í˜¸ì¶œ íŒŒë¼ë¯¸í„°
summary = client.chat.completions.create(
    model="solar-1-mini-chat",
    messages=build_prompt(dialogue),
    temperature=0.2,  # ë‚®ì€ ê°’ (ì¼ê´€ì„±)
    top_p=0.3         # ë‚®ì€ ê°’ (ì¼ê´€ì„±)
)
```

#### í•µì‹¬ í¬ì¸íŠ¸
- **Role ê¸°ë°˜ êµ¬ì¡°**: system â†’ user â†’ assistant â†’ user
- **Few-shot example**: assistant roleë¡œ ì˜ˆì‹œ ì œê³µ
- **ë‚®ì€ temperature/top_p**: 0.2/0.3ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€

### 4. **ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**

```python
class Preprocess:
    def __init__(self, bos_token: str, eos_token: str):
        self.bos_token = bos_token  # <s>
        self.eos_token = eos_token  # </s>

    def make_input(self, dataset, is_test=False):
        if is_test:
            encoder_input = dataset['dialogue']
            decoder_input = [self.bos_token] * len(dataset['dialogue'])
            return encoder_input.tolist(), list(decoder_input)
        else:
            encoder_input = dataset['dialogue']
            # Ground truthë¥¼ ë””ì½”ë” inputìœ¼ë¡œ ì‚¬ìš©
            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))
            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)
            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()
```

### 5. **Dataset í´ë˜ìŠ¤ êµ¬ì¡°**

```python
class DatasetForTrain(Dataset):
    def __init__(self, encoder_input, decoder_input, labels, len):
        self.encoder_input = encoder_input
        self.decoder_input = decoder_input
        self.labels = labels
        self.len = len

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}

        # decoder_input_ids ì„¤ì •
        item2['decoder_input_ids'] = item2['input_ids']
        item2.pop('input_ids')
        item2.pop('attention_mask')

        item.update(item2)
        item['labels'] = self.labels['input_ids'][idx]

        return item
```

## ğŸ› ï¸ ì¦‰ì‹œ ì ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ê¸´ê¸‰ ìˆ˜ì • (ë¬¸ì¥ ëŠê¹€ í•´ê²°)
- [ ] **í† í° ì œê±° ë°©ì‹ ìˆ˜ì •**
  ```python
  # ìˆ˜ì • ì „
  for token in remove_tokens:
      summary = summary.replace(token, "")

  # ìˆ˜ì • í›„
  for token in remove_tokens:
      summary = summary.replace(token, " ")  # ê³µë°± ì¶”ê°€!
  ```

### Phase 2: í•™ìŠµ íŒŒë¼ë¯¸í„° ìµœì í™”
- [ ] **Learning rate í•˜í–¥ ì¡°ì •**
  - 5e-5 â†’ **1e-5** (ë² ì´ìŠ¤ë¼ì¸)

- [ ] **Batch size ìƒí–¥ ì¡°ì •**
  - 8 â†’ **50** (ê°€ëŠ¥í•˜ë©´)
  - GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: **16 ë˜ëŠ” 32**

- [ ] **no_repeat_ngram_size ì¡°ì •**
  - 3 â†’ **2** (ë² ì´ìŠ¤ë¼ì¸)

- [ ] **Early stopping ì„¤ì •**
  ```yaml
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  ```

### Phase 3: Config íŒŒì¼ í‘œì¤€í™”
- [ ] **ë² ì´ìŠ¤ë¼ì¸ config ê¸°ë°˜ ì¬ì‘ì„±**
  ```yaml
  general:
    model_name: digit82/kobart-summarization

  tokenizer:
    encoder_max_len: 512
    decoder_max_len: 100
    special_tokens:
      - '#Person1#'
      - '#Person2#'
      - '#Person3#'
      - '#PhoneNumber#'
      - '#Address#'
      - '#PassportNumber#'

  training:
    num_train_epochs: 20
    learning_rate: 1.0e-05
    per_device_train_batch_size: 50
    warmup_ratio: 0.1
    weight_decay: 0.01
    lr_scheduler_type: cosine

  inference:
    batch_size: 32
    no_repeat_ngram_size: 2
    num_beams: 4
    remove_tokens:
      - '<usr>'
      - '<s>'
      - '</s>'
      - '<pad>'
  ```

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| í•­ëª© | ìˆ˜ì • ì „ | ìˆ˜ì • í›„ | ê°œì„  |
|------|---------|---------|------|
| ë¬¸ì¥ ì™„ì„±ë„ | âŒ ëŠê¹€ ë°œìƒ | âœ… ì™„ë²½ | 100% |
| ì´ìƒí•œ í† í° | âŒ ë‹¤ìˆ˜ ë°œìƒ | âœ… ì—†ìŒ | 100% |
| ROUGE Score | ë‚®ìŒ | ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì¤€ | +20~30% |
| í•™ìŠµ ì•ˆì •ì„± | ë¶ˆì•ˆì • | ì•ˆì •ì  | â¬†ï¸ |

## ğŸ” ê²€ì¦ ì ˆì°¨

### 1. í† í° ì œê±° ê²€ì¦
```python
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
test_summary = "#Person1#ì€ <s>ì•ˆë…•í•˜ì„¸ìš”</s><pad>"
remove_tokens = ['<s>', '</s>', '<pad>']

# ì˜ëª»ëœ ë°©ì‹
wrong = test_summary
for token in remove_tokens:
    wrong = wrong.replace(token, "")
print(wrong)  # "#Person1#ì€ì•ˆë…•í•˜ì„¸ìš”" - ë¶™ì–´ìˆìŒ!

# ì˜¬ë°”ë¥¸ ë°©ì‹
correct = test_summary
for token in remove_tokens:
    correct = correct.replace(token, " ")
print(correct)  # "#Person1#ì€  ì•ˆë…•í•˜ì„¸ìš”  " - ê³µë°± ìœ ì§€!
```

### 2. ìƒì„± í’ˆì§ˆ ê²€ì¦
```python
# ì œì¶œ íŒŒì¼ ìƒ˜í”Œ í™•ì¸
def validate_submission(csv_path):
    df = pd.read_csv(csv_path)

    issues = []
    for idx, row in df.iterrows():
        summary = row['summary']

        # ë¬¸ì¥ ëŠê¹€ ì²´í¬
        if summary.endswith(('.', 'ë‹¤', 'ìš”', 'ë‹ˆë‹¤')):
            continue
        else:
            issues.append({
                'idx': idx,
                'summary': summary,
                'issue': 'ë¬¸ì¥ ëŠê¹€'
            })

    return issues
```

## ğŸš€ ì‹¤í–‰ ìš°ì„ ìˆœìœ„

### ì¦‰ì‹œ (1ì¼ ì´ë‚´)
1. âœ… í† í° ì œê±° ë°©ì‹ ìˆ˜ì • (ê³µë°± ì¹˜í™˜)
2. âœ… Learning rate 1e-5ë¡œ ë³€ê²½
3. âœ… no_repeat_ngram_size = 2

### ë‹¨ê¸° (3ì¼ ì´ë‚´)
1. âœ… Batch size ìµœì í™” (50 ë˜ëŠ” 32)
2. âœ… Early stopping ì¶”ê°€
3. âœ… ë² ì´ìŠ¤ë¼ì¸ config ì „ë©´ ì ìš©

### ì¤‘ê¸° (1ì£¼ ì´ë‚´)
1. âœ… Solar API Few-shot êµ¬ì¡° ì ìš©
2. âœ… ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬êµ¬ì¶•
3. âœ… ì „ì²´ ì œì¶œ íŒŒì¼ í’ˆì§ˆ ê²€ì¦

## ğŸ’¡ í•µì‹¬ êµí›ˆ

### ë¬¸ì¥ ëŠê¹€ ë°©ì§€
1. **ì ˆëŒ€ ê·œì¹™**: í† í° ì œê±° ì‹œ ë°˜ë“œì‹œ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
2. **ê²€ì¦ í•„ìˆ˜**: ìƒì„±ëœ ëª¨ë“  ë¬¸ì¥ì´ ì™„ì „í•œì§€ í™•ì¸
3. **remove_tokens ì •í™•íˆ ì„¤ì •**: `['<usr>', '<s>', '</s>', '<pad>']`

### í•™ìŠµ ì•ˆì •ì„±
1. **Learning rate**: KoBARTëŠ” 1e-5ê°€ ìµœì 
2. **Batch size**: í´ìˆ˜ë¡ ì¢‹ìŒ (50 ê¶Œì¥)
3. **Scheduler**: cosineì´ linearë³´ë‹¤ ìš°ìˆ˜

### í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
1. **Few-shot**: assistant roleë¡œ ì˜ˆì‹œ ì œê³µ
2. **Temperature**: 0.2~0.3 (ë‚®ê²Œ)
3. **Top_p**: 0.3~0.5 (ë‚®ê²Œ)
