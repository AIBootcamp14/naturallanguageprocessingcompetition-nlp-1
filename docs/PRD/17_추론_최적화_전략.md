# âš¡ ì¶”ë¡  ìµœì í™” ì „ëµ

## ğŸ¯ ëª©í‘œ

### ìµœì í™” ëª©í‘œ
- ì¶”ë¡  ì†ë„ 10ë°° í–¥ìƒ (100ms â†’ 10ms/ìƒ˜í”Œ)
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ
- ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”
- ì„±ëŠ¥ ì†ì‹¤ ìµœì†Œí™” (ROUGE 1% ì´ë‚´)

## ğŸ—ï¸ ì¶”ë¡  ìµœì í™” ì•„í‚¤í…ì²˜

```mermaid
graph LR
    subgraph "ì›ë³¸ ëª¨ë¸"
        A[PyTorch Model<br/>FP32]
    end

    subgraph "ìµœì í™” íŒŒì´í”„ë¼ì¸"
        A --> B[Model Optimization]
        B --> C[Quantization<br/>INT8/INT4]
        B --> D[Graph Optimization]
        B --> E[Kernel Fusion]
    end

    subgraph "ë³€í™˜ ì˜µì…˜"
        C --> F[ONNX Runtime]
        D --> G[TensorRT]
        E --> H[TorchScript]
    end

    subgraph "ë°°í¬ í˜•íƒœ"
        F --> I[CPU ì„œë¹™]
        G --> J[GPU ì„œë¹™]
        H --> K[Edge ë””ë°”ì´ìŠ¤]
    end

    subgraph "ì„±ëŠ¥"
        I --> L[10x Speedup<br/>90% Memoryâ†“]
        J --> L
        K --> L
    end

    style B fill:#f9f,stroke:#333,stroke-width:4px
    style L fill:#9ff,stroke:#333,stroke-width:4px
```

## ğŸ”§ ëª¨ë¸ ê²½ëŸ‰í™” ì „ëµ

### 1. ì–‘ìí™” (Quantization)
```python
class ModelQuantizer:
    def __init__(self, model_path):
        self.model = self.load_model(model_path)
        self.calibration_data = None

    def quantize_dynamic(self):
        """ë™ì  ì–‘ìí™” - CPU ì¶”ë¡ ìš©"""
        import torch.quantization as quant

        quantized_model = quant.quantize_dynamic(
            self.model,
            {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},
            dtype=torch.qint8
        )

        # í¬ê¸° ë¹„êµ
        original_size = self.get_model_size(self.model)
        quantized_size = self.get_model_size(quantized_model)

        print(f"Original: {original_size:.2f} MB")
        print(f"Quantized: {quantized_size:.2f} MB")
        print(f"Reduction: {(1 - quantized_size/original_size)*100:.1f}%")

        return quantized_model

    def quantize_static(self, calibration_loader):
        """ì •ì  ì–‘ìí™” - ë” ë†’ì€ ì••ì¶•ë¥ """
        import torch.quantization as quant

        # ì–‘ìí™” ì„¤ì •
        self.model.qconfig = quant.get_default_qconfig('fbgemm')
        quant.prepare(self.model, inplace=True)

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜
        self.calibrate(calibration_loader)

        # ì–‘ìí™” ì ìš©
        quantized_model = quant.convert(self.model, inplace=False)

        return quantized_model

    def quantize_qat(self, train_loader):
        """Quantization-Aware Training"""
        import torch.quantization as quant

        # QAT ì¤€ë¹„
        self.model.qconfig = quant.get_default_qat_qconfig('fbgemm')
        quant.prepare_qat(self.model, inplace=True)

        # ì¶”ê°€ í•™ìŠµ
        for epoch in range(3):
            self.train_one_epoch(train_loader)

        # ì–‘ìí™” ë³€í™˜
        quantized_model = quant.convert(self.model, inplace=False)

        return quantized_model

    def int4_quantization(self):
        """INT4 ê·¹í•œ ì–‘ìí™” (bitsandbytes)"""
        import bitsandbytes as bnb

        # 4-bit ì–‘ìí™” ì„¤ì •
        quantization_config = {
            'load_in_4bit': True,
            'bnb_4bit_compute_dtype': torch.float16,
            'bnb_4bit_quant_type': 'nf4',
            'bnb_4bit_use_double_quant': True
        }

        model_4bit = self.load_model_with_config(quantization_config)

        return model_4bit
```

### 2. ëª¨ë¸ í”„ë£¨ë‹ (Pruning)
```python
class ModelPruner:
    def __init__(self, model):
        self.model = model
        self.importance_scores = {}

    def structured_pruning(self, pruning_ratio=0.3):
        """êµ¬ì¡°ì  í”„ë£¨ë‹ - ì±„ë„/ë ˆì´ì–´ ë‹¨ìœ„"""
        import torch.nn.utils.prune as prune

        for name, module in self.model.named_modules():
            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):
                # ì¤‘ìš”ë„ ê¸°ë°˜ í”„ë£¨ë‹
                prune.ln_structured(
                    module,
                    name='weight',
                    amount=pruning_ratio,
                    n=2,
                    dim=0
                )

        # í”„ë£¨ë‹ ì˜êµ¬ ì ìš©
        for name, module in self.model.named_modules():
            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):
                prune.remove(module, 'weight')

        return self.model

    def unstructured_pruning(self, sparsity=0.5):
        """ë¹„êµ¬ì¡°ì  í”„ë£¨ë‹ - ê°€ì¤‘ì¹˜ ë‹¨ìœ„"""
        import torch.nn.utils.prune as prune

        parameters_to_prune = []
        for name, module in self.model.named_modules():
            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):
                parameters_to_prune.append((module, 'weight'))

        # ì „ì—­ í”„ë£¨ë‹
        prune.global_unstructured(
            parameters_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=sparsity
        )

        return self.model

    def sensitivity_analysis(self, validation_loader):
        """ë ˆì´ì–´ë³„ ë¯¼ê°ë„ ë¶„ì„"""
        sensitivity_results = {}

        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # ë ˆì´ì–´ í”„ë£¨ë‹ ì‹œë®¬ë ˆì´ì…˜
                original_weight = module.weight.clone()

                for ratio in [0.1, 0.3, 0.5, 0.7]:
                    # ì„ì‹œ í”„ë£¨ë‹
                    self.prune_layer(module, ratio)

                    # ì„±ëŠ¥ í‰ê°€
                    performance = self.evaluate(validation_loader)

                    sensitivity_results[name] = {
                        'ratio': ratio,
                        'performance': performance
                    }

                    # ë³µì›
                    module.weight.data = original_weight

        return sensitivity_results
```

## ğŸš€ ONNX ë³€í™˜ ë° ìµœì í™”

### 1. ONNX ë³€í™˜
```python
class ONNXConverter:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def convert_to_onnx(self, output_path, optimize=True):
        """PyTorch â†’ ONNX ë³€í™˜"""
        import torch.onnx

        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = self.create_dummy_input()

        # ONNX ë³€í™˜
        torch.onnx.export(
            self.model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=14,
            do_constant_folding=True,
            input_names=['input_ids', 'attention_mask'],
            output_names=['output'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'output': {0: 'batch_size', 1: 'sequence'}
            }
        )

        if optimize:
            self.optimize_onnx(output_path)

        return output_path

    def optimize_onnx(self, model_path):
        """ONNX ê·¸ë˜í”„ ìµœì í™”"""
        from onnxruntime.transformers import optimizer

        optimized_model = optimizer.optimize_model(
            model_path,
            model_type='bert',  # ë˜ëŠ” 'gpt2', 'bart' ë“±
            num_heads=12,
            hidden_size=768,
            optimization_options={
                'enable_gelu_approximation': True,
                'enable_layer_norm_fusion': True,
                'enable_attention_fusion': True,
                'enable_skip_layer_norm_fusion': True,
                'enable_embed_layer_norm_fusion': True,
                'enable_bias_skip_layer_norm_fusion': True,
                'enable_bias_gelu_fusion': True,
                'enable_gelu_fusion': True,
                'enable_shape_inference': True
            }
        )

        optimized_model.save_model_to_file('optimized_model.onnx')

        return optimized_model

    def benchmark_onnx(self, onnx_path, test_data):
        """ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        import onnxruntime as ort
        import time

        # ONNX Runtime ì„¸ì…˜ ìƒì„±
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        session = ort.InferenceSession(onnx_path, providers=providers)

        # ì›Œë°ì—…
        for _ in range(10):
            _ = session.run(None, test_data)

        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for _ in range(100):
            start = time.perf_counter()
            _ = session.run(None, test_data)
            times.append(time.perf_counter() - start)

        return {
            'mean_latency': np.mean(times) * 1000,  # ms
            'p50_latency': np.percentile(times, 50) * 1000,
            'p95_latency': np.percentile(times, 95) * 1000,
            'p99_latency': np.percentile(times, 99) * 1000,
            'throughput': 1 / np.mean(times)  # samples/sec
        }
```

### 2. ONNX Runtime ìµœì í™”
```python
class ONNXRuntimeOptimizer:
    def __init__(self, model_path):
        self.model_path = model_path
        self.providers = self.get_providers()

    def get_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” í™•ì¸"""
        import onnxruntime as ort

        available = ort.get_available_providers()
        providers = []

        if 'TensorrtExecutionProvider' in available:
            providers.append(('TensorrtExecutionProvider', {
                'trt_max_workspace_size': 2147483648,
                'trt_fp16_enable': True,
                'trt_int8_enable': True,
                'trt_engine_cache_enable': True
            }))

        if 'CUDAExecutionProvider' in available:
            providers.append(('CUDAExecutionProvider', {
                'device_id': 0,
                'arena_extend_strategy': 'kNextPowerOfTwo',
                'gpu_mem_limit': 4 * 1024 * 1024 * 1024,
                'cudnn_conv_algo_search': 'EXHAUSTIVE',
                'do_copy_in_default_stream': True
            }))

        providers.append('CPUExecutionProvider')

        return providers

    def create_optimized_session(self):
        """ìµœì í™”ëœ ì¶”ë¡  ì„¸ì…˜ ìƒì„±"""
        import onnxruntime as ort

        # ì„¸ì…˜ ì˜µì…˜
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.enable_cpu_mem_arena = True
        sess_options.enable_mem_pattern = True
        sess_options.enable_mem_reuse = True
        sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL

        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        sess_options.inter_op_num_threads = 4
        sess_options.intra_op_num_threads = 4

        session = ort.InferenceSession(
            self.model_path,
            sess_options,
            providers=self.providers
        )

        return session
```

## ğŸ”¥ TensorRT ê°€ì†í™”

### 1. TensorRT ë³€í™˜
```python
class TensorRTOptimizer:
    def __init__(self, onnx_path):
        self.onnx_path = onnx_path
        self.engine = None

    def build_engine(self, precision='fp16', max_batch_size=32):
        """TensorRT ì—”ì§„ ë¹Œë“œ"""
        import tensorrt as trt

        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        network = builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        parser = trt.OnnxParser(network, logger)

        # ONNX ëª¨ë¸ íŒŒì‹±
        with open(self.onnx_path, 'rb') as f:
            parser.parse(f.read())

        # ë¹Œë” ì„¤ì •
        config = builder.create_builder_config()
        config.max_workspace_size = 4 << 30  # 4GB

        # ì •ë°€ë„ ì„¤ì •
        if precision == 'fp16':
            config.set_flag(trt.BuilderFlag.FP16)
        elif precision == 'int8':
            config.set_flag(trt.BuilderFlag.INT8)
            config.int8_calibrator = self.create_calibrator()

        # ë™ì  shape ì„¤ì •
        profile = builder.create_optimization_profile()
        profile.set_shape(
            'input_ids',
            (1, 128),         # min
            (max_batch_size//2, 256),  # opt
            (max_batch_size, 512)      # max
        )
        config.add_optimization_profile(profile)

        # ì—”ì§„ ë¹Œë“œ
        self.engine = builder.build_engine(network, config)

        return self.engine

    def optimize_for_inference(self):
        """ì¶”ë¡  ìµœì í™” ì„¤ì •"""
        optimizations = {
            'kernel_auto_tuning': True,
            'fusion_optimization': True,
            'multi_stream_execution': True,
            'cuda_graph_capture': True,
            'persistent_cache': True
        }

        return optimizations

    def benchmark_tensorrt(self, test_data, iterations=1000):
        """TensorRT ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        import time

        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = self.engine.create_execution_context()

        # ë©”ëª¨ë¦¬ í• ë‹¹
        inputs, outputs, bindings = self.allocate_buffers(self.engine)

        # ì›Œë°ì—…
        for _ in range(100):
            self.infer(context, bindings, inputs, outputs, test_data)

        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            self.infer(context, bindings, inputs, outputs, test_data)
            times.append(time.perf_counter() - start)

        return {
            'mean_latency': np.mean(times) * 1000,
            'throughput': 1 / np.mean(times),
            'memory_usage': self.get_memory_usage()
        }
```

## ğŸ“Š ë°°ì¹˜ ì¶”ë¡  ìµœì í™”

### 1. ë™ì  ë°°ì¹˜ ì²˜ë¦¬
```python
class BatchInferenceOptimizer:
    def __init__(self, model, max_batch_size=32):
        self.model = model
        self.max_batch_size = max_batch_size
        self.batch_queue = []

    def dynamic_batching(self, requests, timeout=0.01):
        """ë™ì  ë°°ì¹­ with timeout"""
        import time
        import threading

        batches = []
        current_batch = []
        start_time = time.time()

        for request in requests:
            current_batch.append(request)

            # ë°°ì¹˜ í¬ê¸°ë‚˜ ì‹œê°„ ì œí•œ ë„ë‹¬
            if (len(current_batch) >= self.max_batch_size or
                time.time() - start_time > timeout):
                batches.append(current_batch)
                current_batch = []
                start_time = time.time()

        # ë‚¨ì€ ìš”ì²­ ì²˜ë¦¬
        if current_batch:
            batches.append(current_batch)

        return batches

    def optimize_batch_size(self, model, sample_data):
        """ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ íƒìƒ‰"""
        import torch

        results = []

        for batch_size in [1, 2, 4, 8, 16, 32, 64, 128]:
            try:
                # ë©”ëª¨ë¦¬ ì²´í¬
                torch.cuda.empty_cache()
                batch = self.create_batch(sample_data, batch_size)

                # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                start = time.perf_counter()
                with torch.no_grad():
                    _ = model(batch)
                inference_time = time.perf_counter() - start

                # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
                throughput = batch_size / inference_time

                results.append({
                    'batch_size': batch_size,
                    'latency': inference_time * 1000,
                    'throughput': throughput,
                    'memory': torch.cuda.memory_allocated() / 1024**3
                })

            except RuntimeError as e:
                if 'out of memory' in str(e):
                    break
                raise

        # ìµœì  ë°°ì¹˜ í¬ê¸° ì„ íƒ
        optimal = max(results, key=lambda x: x['throughput'])

        return optimal['batch_size'], results

    def continuous_batching(self):
        """ì—°ì† ë°°ì¹­ (Continuous Batching)"""
        import asyncio
        from collections import deque

        class ContinuousBatcher:
            def __init__(self, model, max_batch_size):
                self.model = model
                self.max_batch_size = max_batch_size
                self.queue = deque()
                self.results = {}

            async def process_requests(self):
                """ìš”ì²­ ì²˜ë¦¬ ë£¨í”„"""
                while True:
                    if len(self.queue) > 0:
                        # ë°°ì¹˜ ìƒì„±
                        batch_size = min(len(self.queue), self.max_batch_size)
                        batch = [self.queue.popleft() for _ in range(batch_size)]

                        # ë°°ì¹˜ ì²˜ë¦¬
                        results = await self.process_batch(batch)

                        # ê²°ê³¼ ì €ì¥
                        for req_id, result in results:
                            self.results[req_id] = result
                    else:
                        await asyncio.sleep(0.001)

            async def process_batch(self, batch):
                """ë°°ì¹˜ ì¶”ë¡ """
                # íŒ¨ë”© ë° í…ì„œ ë³€í™˜
                padded_batch = self.pad_batch(batch)

                # ëª¨ë¸ ì¶”ë¡ 
                with torch.no_grad():
                    outputs = self.model(padded_batch)

                # ê²°ê³¼ ì–¸íŒ¨ë”©
                results = self.unpad_results(outputs, batch)

                return results

        return ContinuousBatcher(self.model, self.max_batch_size)
```

## ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™”

### 1. ë©”ëª¨ë¦¬ í’€ë§
```python
class MemoryOptimizer:
    def __init__(self):
        self.memory_pool = {}
        self.cache = {}

    def setup_memory_pool(self, shapes, dtype=torch.float16):
        """ì‚¬ì „ í• ë‹¹ ë©”ëª¨ë¦¬ í’€"""
        for name, shape in shapes.items():
            self.memory_pool[name] = torch.empty(
                shape,
                dtype=dtype,
                device='cuda',
                requires_grad=False
            )

        return self.memory_pool

    def gradient_checkpointing(self, model):
        """ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…"""
        model.gradient_checkpointing_enable()

        # ë˜ëŠ” ìˆ˜ë™ ì„¤ì •
        for module in model.modules():
            if hasattr(module, 'gradient_checkpointing'):
                module.gradient_checkpointing = True

        return model

    def mixed_precision_inference(self):
        """í˜¼í•© ì •ë°€ë„ ì¶”ë¡ """
        from torch.cuda.amp import autocast

        def inference_with_amp(model, inputs):
            with autocast(dtype=torch.float16):
                with torch.no_grad():
                    outputs = model(**inputs)
            return outputs

        return inference_with_amp

    def memory_efficient_attention(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜"""
        import xformers.ops as xops

        class EfficientAttention(torch.nn.Module):
            def __init__(self, dim, num_heads):
                super().__init__()
                self.dim = dim
                self.num_heads = num_heads

            def forward(self, q, k, v):
                # xFormers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜
                return xops.memory_efficient_attention(
                    q, k, v,
                    attn_bias=None,
                    op=xops.MemoryEfficientAttentionFlashAttentionOp
                )

        return EfficientAttention
```

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 1. ì¶”ë¡  í”„ë¡œíŒŒì¼ë§
```python
class InferenceProfiler:
    def __init__(self):
        self.metrics = defaultdict(list)

    def profile_model(self, model, input_data, num_runs=100):
        """ëª¨ë¸ í”„ë¡œíŒŒì¼ë§"""
        import torch.profiler as profiler

        with profiler.profile(
            activities=[
                profiler.ProfilerActivity.CPU,
                profiler.ProfilerActivity.CUDA
            ],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            with profiler.record_function("model_inference"):
                for _ in range(num_runs):
                    with torch.no_grad():
                        _ = model(input_data)

        # ê²°ê³¼ ë¶„ì„
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

        # ë³‘ëª© ì§€ì  íŒŒì•…
        bottlenecks = self.identify_bottlenecks(prof)

        return {
            'profile': prof,
            'bottlenecks': bottlenecks,
            'recommendations': self.get_optimization_recommendations(bottlenecks)
        }

    def identify_bottlenecks(self, profile):
        """ë³‘ëª© ì§€ì  ì‹ë³„"""
        bottlenecks = []

        for event in profile.key_averages():
            if event.cuda_time_total > 1000:  # 1ms ì´ìƒ
                bottlenecks.append({
                    'operation': event.key,
                    'cuda_time': event.cuda_time_total,
                    'cpu_time': event.cpu_time_total,
                    'calls': event.count
                })

        return sorted(bottlenecks, key=lambda x: x['cuda_time'], reverse=True)
```

## ğŸš€ ì‹¤í–‰ ê³„íš

### Phase 1: ê¸°ë³¸ ìµœì í™”
- [ ] ë™ì  ì–‘ìí™” ì ìš© (INT8)
- [ ] ONNX ë³€í™˜ ë° ìµœì í™”
- [ ] ë°°ì¹˜ í¬ê¸° ìµœì í™”

### Phase 2: ê³ ê¸‰ ìµœì í™”
- [ ] TensorRT ì—”ì§„ êµ¬ì¶•
- [ ] INT4 ì–‘ìí™” ì‹¤í—˜
- [ ] ëª¨ë¸ í”„ë£¨ë‹ ì ìš©

### Phase 3: í”„ë¡œë•ì…˜ ì¤€ë¹„
- [ ] ì—°ì† ë°°ì¹­ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë©”ëª¨ë¦¬ í’€ë§ ì„¤ì •
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

## ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| ìµœì í™” ê¸°ë²• | ì†ë„ í–¥ìƒ | ë©”ëª¨ë¦¬ ê°ì†Œ | ROUGE ì†ì‹¤ |
|-----------|----------|-----------|-----------|
| FP16 ë³€í™˜ | 2x | 50% | 0% |
| INT8 ì–‘ìí™” | 3-4x | 75% | <1% |
| ONNX Runtime | 2-3x | 30% | 0% |
| TensorRT | 5-10x | 60% | <0.5% |
| í”„ë£¨ë‹ (30%) | 1.5x | 30% | <2% |
| **ì¢…í•© ì ìš©** | **8-15x** | **80%** | **<2%** |