# â˜€ï¸ Solar API ìµœì í™” ì „ëµ (ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦)

## âœ… ëŒ€íšŒ ë² ì´ìŠ¤ë¼ì¸ ê²€ì¦ (í•„ìˆ˜)

### Few-shot Learning êµ¬ì¡°
```python
# ë² ì´ìŠ¤ë¼ì¸: Few-shot ì˜ˆì‹œë¥¼ assistant roleë¡œ ì œê³µ
def build_prompt(dialogue):
    system_prompt = "You are a expert in the field of dialogue summarization, summarize the given dialogue in a concise manner. Follow the user's instruction carefully and provide a summary that is relevant to the dialogue."

    few_shot_user = f"Dialogue:\n{sample_dialogue}\nSummary:\n"
    few_shot_assistant = sample_summary  # ì˜ˆì‹œ ìš”ì•½

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": few_shot_user},
        {"role": "assistant", "content": few_shot_assistant},  # Few-shot!
        {"role": "user", "content": f"Dialogue:\n{dialogue}\nSummary:\n"}
    ]

# API í˜¸ì¶œ (ê²€ì¦ëœ íŒŒë¼ë¯¸í„°)
summary = client.chat.completions.create(
    model="solar-1-mini-chat",
    messages=build_prompt(dialogue),
    temperature=0.2,  # ë‚®ì€ ê°’ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€
    top_p=0.3        # ë‚®ì€ ê°’ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€
)
```

### í•µì‹¬ ê²€ì¦ ì‚¬í•­
- **Role êµ¬ì¡°**: system â†’ user (ì˜ˆì‹œ) â†’ assistant (ë‹µë³€) â†’ user (ì‹¤ì œ)
- **Temperature**: 0.2 (ë‚®ê²Œ ì„¤ì • - ì¼ê´€ì„±)
- **Top_p**: 0.3 (ë‚®ê²Œ ì„¤ì • - ì¼ê´€ì„±)
- **Few-shot**: 1ê°œ ì˜ˆì‹œë§Œìœ¼ë¡œ ì¶©ë¶„
- **Rate limit**: 1ë¶„ë‹¹ 100ê°œ ìš”ì²­ ì œí•œ

## ğŸ¯ í•µì‹¬ ë¬¸ì œ
**í† í° ì‚¬ìš©ëŸ‰ í­ì¦ ë¬¸ì œ**: ì „ì²´ ëŒ€í™”ë¥¼ ê·¸ëŒ€ë¡œ APIì— ì „ì†¡í•˜ë©´ í† í° ì†Œë¹„ê°€ ê³¼ë„í•¨

## ğŸ’¡ í•´ê²° ì „ëµ: CSV ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ í†µí•œ í† í° ì ˆì•½

### 1. ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
class SolarAPIOptimizer:
    def __init__(self, api_key):
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.upstage.ai/v1/solar"
        )
        self.token_limit = 512  # í† í° ì œí•œ

    def preprocess_dialogue(self, dialogue):
        """
        ëŒ€í™”ë¬¸ ì „ì²˜ë¦¬ë¡œ í† í° ì ˆì•½
        """
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        dialogue = ' '.join(dialogue.split())

        # 2. Person íƒœê·¸ ê°„ì†Œí™”
        dialogue = dialogue.replace('#Person1#:', 'A:')
        dialogue = dialogue.replace('#Person2#:', 'B:')
        dialogue = dialogue.replace('#Person3#:', 'C:')

        # 3. ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±°
        dialogue = self.remove_repetitions(dialogue)

        # 4. ëŒ€í™” ê¸¸ì´ ì œí•œ (ì¤‘ìš” ë¶€ë¶„ë§Œ ì¶”ì¶œ)
        dialogue = self.extract_key_parts(dialogue)

        return dialogue

    def extract_key_parts(self, dialogue):
        """
        ëŒ€í™”ì˜ í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        """
        sentences = dialogue.split('\n')

        # ì „ëµ 1: ì²˜ìŒê³¼ ë ë¶€ë¶„ ìš°ì„ 
        if len(sentences) > 20:
            key_parts = (
                sentences[:7] +  # ì²˜ìŒ 7ê°œ ë¬¸ì¥
                ['...'] +
                sentences[-7:]   # ë§ˆì§€ë§‰ 7ê°œ ë¬¸ì¥
            )
            return '\n'.join(key_parts)

        # ì „ëµ 2: ê¸´ ë¬¸ì¥ ìš°ì„  (ì •ë³´ëŸ‰ì´ ë§ì„ ê°€ëŠ¥ì„±)
        sorted_sentences = sorted(
            sentences,
            key=lambda x: len(x),
            reverse=True
        )[:15]

        return '\n'.join(sorted_sentences)

    def smart_truncate(self, text, max_tokens=512):
        """
        ìŠ¤ë§ˆíŠ¸ ì ˆë‹¨: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        """
        # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ í‰ê·  2.5ì = 1í† í°)
        estimated_tokens = len(text) / 2.5

        if estimated_tokens <= max_tokens:
            return text

        sentences = text.split('.')
        truncated = []
        current_length = 0

        for sentence in sentences:
            sentence_tokens = len(sentence) / 2.5
            if current_length + sentence_tokens > max_tokens:
                break
            truncated.append(sentence)
            current_length += sentence_tokens

        return '.'.join(truncated) + '.'
```

### 2. í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì „ëµ
```python
def extract_dialogue_essence(dialogue):
    """
    ëŒ€í™”ì˜ í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ í† í° ì ˆì•½
    """
    # 1. í™”ìë³„ ì£¼ìš” ë°œì–¸ ì¶”ì¶œ
    speakers = {}
    lines = dialogue.split('\n')

    for line in lines:
        if ':' in line:
            speaker, content = line.split(':', 1)
            if speaker not in speakers:
                speakers[speaker] = []
            speakers[speaker].append(content)

    # 2. ê° í™”ìì˜ ê°€ì¥ ê¸´ ë°œì–¸ 2ê°œì”©ë§Œ ì„ íƒ
    essence = []
    for speaker, utterances in speakers.items():
        sorted_utterances = sorted(
            utterances,
            key=len,
            reverse=True
        )[:2]
        for utterance in sorted_utterances:
            essence.append(f"{speaker}: {utterance}")

    return '\n'.join(essence)
```

### 3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
class BatchProcessor:
    def __init__(self, api_optimizer):
        self.api = api_optimizer
        self.cache = {}  # ê²°ê³¼ ìºì‹±

    def process_batch(self, dialogues, batch_size=10):
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì†Œí™”
        """
        results = []

        for i in range(0, len(dialogues), batch_size):
            batch = dialogues[i:i+batch_size]

            # ìºì‹œ í™•ì¸
            cached_results = []
            uncached_dialogues = []

            for dialogue in batch:
                dialogue_hash = hashlib.md5(
                    dialogue.encode()
                ).hexdigest()

                if dialogue_hash in self.cache:
                    cached_results.append(self.cache[dialogue_hash])
                else:
                    uncached_dialogues.append(dialogue)

            # ìºì‹œë˜ì§€ ì•Šì€ ê²ƒë§Œ API í˜¸ì¶œ
            if uncached_dialogues:
                api_results = self.call_api_batch(uncached_dialogues)
                results.extend(api_results)

            results.extend(cached_results)

            # Rate limiting
            time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°

        return results

    def call_api_batch(self, dialogues):
        """
        ì—¬ëŸ¬ ëŒ€í™”ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ì²˜ë¦¬
        """
        combined_prompt = self.create_batch_prompt(dialogues)
        response = self.api.client.chat.completions.create(
            model="solar-1-mini-chat",
            messages=[{"role": "user", "content": combined_prompt}],
            max_tokens=100 * len(dialogues),  # ëŒ€í™”ë‹¹ 100í† í°
            temperature=0.3
        )

        # ì‘ë‹µ íŒŒì‹±
        summaries = self.parse_batch_response(
            response.choices[0].message.content
        )
        return summaries

    def create_batch_prompt(self, dialogues):
        """
        ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        """
        prompt = "ë‹¤ìŒ ëŒ€í™”ë“¤ì„ ê°ê° ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n"

        for i, dialogue in enumerate(dialogues, 1):
            # ì „ì²˜ë¦¬ëœ ëŒ€í™” ì‚¬ìš©
            processed = self.api.preprocess_dialogue(dialogue)
            prompt += f"[ëŒ€í™” {i}]\n{processed}\n\n"

        prompt += "ê° ëŒ€í™”ì˜ ìš”ì•½ì„ [ìš”ì•½ 1], [ìš”ì•½ 2] í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
        return prompt
```

### 4. í”„ë¡¬í”„íŠ¸ ìµœì í™”
```python
class PromptOptimizer:
    """
    íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë¡œ í† í° ì ˆì•½
    """

    @staticmethod
    def create_minimal_prompt(dialogue):
        """
        ìµœì†Œí•œì˜ í”„ë¡¬í”„íŠ¸
        """
        return f"ìš”ì•½: {dialogue[:500]}"  # 500ì ì œí•œ

    @staticmethod
    def create_structured_prompt(dialogue_info):
        """
        êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ (í•„ìˆ˜ ì •ë³´ë§Œ)
        """
        return f"""ì£¼ì œ: {dialogue_info['topic']}
ì°¸ì—¬: {dialogue_info['speakers']}
í•µì‹¬: {dialogue_info['key_points']}
ìš”ì•½:"""

    @staticmethod
    def create_template_prompt(dialogue):
        """
        í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
        """
        # ëŒ€í™” ë¶„ì„
        num_speakers = len(set(re.findall(r'#Person\d+#', dialogue)))
        num_turns = dialogue.count('\n')

        # ê°„ì†Œí™”ëœ í…œí”Œë¦¿
        if num_speakers == 2 and num_turns < 10:
            return f"ì§§ì€ ëŒ€í™” ìš”ì•½: {dialogue[:300]}"
        elif num_speakers > 3:
            return f"ë‹¤ì ëŒ€í™” í•µì‹¬: {dialogue[:400]}"
        else:
            return f"ëŒ€í™” ìš”ì•½: {dialogue[:350]}"
```

## ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ë¹„êµ

| ë°©ì‹ | í‰ê·  í† í°/ëŒ€í™” | ë¹„ìš© | í’ˆì§ˆ |
|------|--------------|------|------|
| ì›ë³¸ ì „ì²´ | 800-1200 | ë†’ìŒ | 100% |
| ì „ì²˜ë¦¬ í›„ | 300-400 | ì¤‘ê°„ | 95% |
| í•µì‹¬ ì¶”ì¶œ | 200-300 | ë‚®ìŒ | 90% |
| ë°°ì¹˜ ì²˜ë¦¬ | 150-200 | ë§¤ìš° ë‚®ìŒ | 88% |

## ğŸ”§ í†µí•© êµ¬í˜„
```python
class OptimizedSolarAPI:
    def __init__(self, api_key, token_budget=100000):
        self.optimizer = SolarAPIOptimizer(api_key)
        self.batch_processor = BatchProcessor(self.optimizer)
        self.prompt_optimizer = PromptOptimizer()
        self.token_budget = token_budget
        self.token_used = 0

    def process_dataset(self, df):
        """
        ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬
        """
        results = []

        # 1. ë°ì´í„° ì „ì²˜ë¦¬
        processed_dialogues = []
        for dialogue in df['dialogue']:
            # í† í° ì˜ˆì‚° í™•ì¸
            if self.token_used >= self.token_budget:
                print("í† í° ì˜ˆì‚° ì´ˆê³¼!")
                break

            # ì „ì²˜ë¦¬
            processed = self.optimizer.preprocess_dialogue(dialogue)
            processed = self.optimizer.smart_truncate(processed)
            processed_dialogues.append(processed)

        # 2. ë°°ì¹˜ ì²˜ë¦¬
        summaries = self.batch_processor.process_batch(
            processed_dialogues,
            batch_size=10
        )

        # 3. ê²°ê³¼ ì €ì¥
        df['solar_summary'] = summaries
        return df

    def estimate_tokens(self, text):
        """
        í† í° ìˆ˜ ì¶”ì •
        """
        # í•œê¸€: í‰ê·  2.5ì = 1í† í°
        # ì˜ì–´: í‰ê·  4ì = 1í† í°
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))

        estimated = (korean_chars / 2.5) + (english_chars / 4)
        return int(estimated)
```

## ğŸ’° ë¹„ìš© ìµœì í™” ì „ëµ

### 1. ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì²˜ë¦¬
```python
def prioritize_dialogues(df):
    """
    ì¤‘ìš”ë„ ê¸°ë°˜ ëŒ€í™” ìš°ì„ ìˆœìœ„
    """
    # ê¸¸ì´ê°€ ì ë‹¹í•œ ëŒ€í™” ìš°ì„  (ì •ë³´ ë°€ë„ ë†’ìŒ)
    df['priority'] = df['dialogue'].apply(
        lambda x: 1 / abs(len(x) - 500)  # 500ì ê·¼ì²˜ ìš°ì„ 
    )

    # ì •ë ¬
    return df.sort_values('priority', ascending=False)
```

### 2. ìºì‹± ì „ëµ
```python
import pickle

def save_cache(cache_dict, filename='solar_cache.pkl'):
    with open(filename, 'wb') as f:
        pickle.dump(cache_dict, f)

def load_cache(filename='solar_cache.pkl'):
    try:
        with open(filename, 'rb') as f:
            return pickle.load(f)
    except:
        return {}
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
```python
def hybrid_approach(df, budget_ratio=0.3):
    """
    ëª¨ë¸ + API í•˜ì´ë¸Œë¦¬ë“œ
    """
    total_samples = len(df)
    api_samples = int(total_samples * budget_ratio)

    # ë‚œì´ë„ ë†’ì€ ìƒ˜í”Œë§Œ API ì‚¬ìš©
    difficult_samples = identify_difficult_samples(df)[:api_samples]

    # ë‚˜ë¨¸ì§€ëŠ” íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
    results = {}
    for idx in df.index:
        if idx in difficult_samples:
            results[idx] = use_solar_api(df.loc[idx])
        else:
            results[idx] = use_finetuned_model(df.loc[idx])

    return results
```

## ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼

### í† í° ì ˆì•½
- **ê¸°ì¡´**: ëŒ€í™”ë‹¹ 800-1200 í† í°
- **ìµœì í™” í›„**: ëŒ€í™”ë‹¹ 200-300 í† í°
- **ì ˆì•½ë¥ **: ì•½ 70-75%

### ë¹„ìš© ì ˆê°
- **ê¸°ì¡´**: $0.001 / ëŒ€í™”
- **ìµœì í™” í›„**: $0.0003 / ëŒ€í™”
- **ì ˆê°ë¥ **: ì•½ 70%

### ì„±ëŠ¥ ìœ ì§€
- **ì›ë³¸ í’ˆì§ˆ**: 100%
- **ìµœì í™” í’ˆì§ˆ**: 88-95%
- **í—ˆìš© ê°€ëŠ¥í•œ í’ˆì§ˆ ì €í•˜**: 5-12%

## ğŸš€ ì‹¤í–‰ ê³„íš

1. **Phase 1**: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
2. **Phase 2**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
3. **Phase 3**: ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ì ìš©
4. **Phase 4**: í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í†µí•©