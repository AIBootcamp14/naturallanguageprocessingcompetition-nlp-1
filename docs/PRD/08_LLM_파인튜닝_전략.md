# ğŸ¤– LLM íŒŒì¸íŠœë‹ ì „ëµ

## ğŸ“š ì ‘ê·¼ ë°©ì‹ ë¹„êµ

### 1. ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ (ê¸°ì¡´ ë°©ì‹)
- **ëª¨ë¸**: BART, T5, mBART
- **íŠ¹ì§•**: Seq2Seq êµ¬ì¡°ë¡œ ìš”ì•½ì— íŠ¹í™”
- **ì¥ì **: ìš”ì•½ íƒœìŠ¤í¬ì— ìµœì í™”ë¨
- **ë‹¨ì **: ëª¨ë¸ í¬ê¸° ì œí•œ, ì»¨í…ìŠ¤íŠ¸ ì´í•´ í•œê³„

### 2. ë””ì½”ë” ì „ìš© LLM íŒŒì¸íŠœë‹ (ìƒˆë¡œìš´ ë°©ì‹) âœ¨
- **ëª¨ë¸**: GPT, LLaMA, Polyglot-Ko, SOLAR
- **íŠ¹ì§•**: Causal Language Modelingì„ ìš”ì•½ì— í™œìš©
- **ì¥ì **:
  - ë” ê¹Šì€ ë¬¸ë§¥ ì´í•´
  - ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ ì§€ì‹ í™œìš©
  - Instruction Following ëŠ¥ë ¥
- **ë‹¨ì **:
  - ë” ë§ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”
  - í† í° ì‚¬ìš©ëŸ‰ ì¦ê°€

## ğŸ¯ LLM íŒŒì¸íŠœë‹ êµ¬í˜„ ì „ëµ

### 1. ëª¨ë¸ ì„ íƒ
```python
# ê¶Œì¥ ëª¨ë¸ (í•œêµ­ì–´ íŠ¹í™”)
models = {
    'polyglot-ko-5.8b': {
        'size': '5.8B',
        'context': 2048,
        'korean_optimized': True
    },
    'kogpt': {
        'size': '6B',
        'context': 2048,
        'korean_optimized': True
    },
    'llama-2-ko': {
        'size': '7B',
        'context': 4096,
        'korean_optimized': True
    }
}
```

### 2. ë°ì´í„° í¬ë§·íŒ…
```python
def format_for_llm_finetuning(dialogue, summary):
    """
    LLM íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„° í¬ë§·
    """
    instruction = "ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."

    # Option 1: Instruction Format
    prompt = f"""### Instruction:
{instruction}

### Input:
{dialogue}

### Response:
{summary}"""

    # Option 2: Chat Format
    prompt = f"""<|system|>
ë‹¹ì‹ ì€ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|user|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:
{dialogue}
<|assistant|>
{summary}"""

    return prompt
```

### 3. íŒŒì¸íŠœë‹ ì„¤ì •
```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType

# LoRA ì„¤ì • (íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹)
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./llm_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    learning_rate=2e-5,
    fp16=True,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)
```

### 4. Instruction Tuning
```python
# ë‹¤ì–‘í•œ Instruction í…œí”Œë¦¿
instructions = [
    "ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
    "ì•„ë˜ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
    "ì£¼ì–´ì§„ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš”.",
    "ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ìš”ì•½í•˜ì„¸ìš”.",
    "ëŒ€í™” ë‚´ìš©ì„ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
]

def augment_with_instructions(data):
    """
    ë‹¤ì–‘í•œ instructionìœ¼ë¡œ ë°ì´í„° ì¦ê°•
    """
    augmented = []
    for dialogue, summary in data:
        for instruction in instructions:
            formatted = format_with_instruction(
                instruction, dialogue, summary
            )
            augmented.append(formatted)
    return augmented
```

## ğŸ’¡ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ê¸°ë²•

### 1. LoRA (Low-Rank Adaptation)
- ì „ì²´ ëª¨ë¸ ëŒ€ì‹  ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ê°ì†Œ
- í•™ìŠµ ì†ë„ 3-5ë°° í–¥ìƒ

### 2. QLoRA (Quantized LoRA)
- 4-bit ì–‘ìí™” + LoRA
- ë” í° ëª¨ë¸ì„ ì œí•œëœ GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥

### 3. Gradient Checkpointing
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš© ê°€ëŠ¥

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

| ë°©ì‹ | ëª¨ë¸ | íŒŒë¼ë¯¸í„° | GPU ë©”ëª¨ë¦¬ | ì˜ˆìƒ ROUGE |
|------|------|----------|------------|------------|
| ê¸°ì¡´ | KoBART | 124M | 8GB | 47-50 |
| LLM | Polyglot-Ko | 5.8B | 16GB | 55-60 |
| LLM + LoRA | Polyglot-Ko | 5.8B | 8GB | 53-58 |
| LLM + QLoRA | LLaMA-2-Ko | 7B | 8GB | 58-63 |

## ğŸ”§ êµ¬í˜„ ì˜ˆì œ

### ì „ì²´ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
```python
class LLMFineTuner:
    def __init__(self, model_name, use_lora=True):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_8bit=True if use_lora else False,
            device_map="auto"
        )

        if use_lora:
            self.model = get_peft_model(self.model, lora_config)

    def prepare_dataset(self, data):
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        formatted_data = []
        for item in data:
            text = self.format_for_training(
                item['dialogue'],
                item['summary']
            )
            formatted_data.append(text)
        return formatted_data

    def train(self, train_dataset, eval_dataset):
        """ëª¨ë¸ í•™ìŠµ"""
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        trainer.train()

    def generate_summary(self, dialogue):
        """ìš”ì•½ ìƒì„±"""
        prompt = f"""### Instruction:
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.

### Input:
{dialogue}

### Response:"""

        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        summary = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )
        return summary.split("### Response:")[-1].strip()
```

## ğŸ“ˆ í•™ìŠµ ì „ëµ

### Phase 1: ê¸°ë³¸ íŒŒì¸íŠœë‹
1. ì „ì²´ ë°ì´í„°ë¡œ 3 epochs í•™ìŠµ
2. Learning rate: 2e-5
3. Batch size: 4 (gradient accumulation ì‚¬ìš©)

### Phase 2: Instruction Tuning
1. ë‹¤ì–‘í•œ instruction í…œí”Œë¦¿ ì ìš©
2. ë°ì´í„° ì¦ê°• (5ë°°)
3. ì¶”ê°€ 2 epochs í•™ìŠµ

### Phase 3: RLHF (ì„ íƒì‚¬í•­)
1. ìƒì„±ëœ ìš”ì•½ì— ëŒ€í•œ í’ˆì§ˆ í‰ê°€
2. Reward model í•™ìŠµ
3. PPOë¥¼ í†µí•œ ì¶”ê°€ ìµœì í™”

## âš¡ ìµœì í™” íŒ

### 1. Mixed Precision Training
```python
training_args = TrainingArguments(
    fp16=True,  # ë˜ëŠ” bf16=True
    # ...
)
```

### 2. Gradient Accumulation
```python
# ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = 4 * 4 = 16
per_device_train_batch_size=4,
gradient_accumulation_steps=4,
```

### 3. Learning Rate Schedule
```python
lr_scheduler_type="cosine",
warmup_ratio=0.1,
```

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### ì„±ëŠ¥ í–¥ìƒ ì˜ˆì¸¡
- **ë² ì´ìŠ¤ë¼ì¸ (BART)**: 47.12
- **LLM íŒŒì¸íŠœë‹ (ê¸°ë³¸)**: 55-58
- **LLM + Instruction Tuning**: 58-62
- **LLM + ì•™ìƒë¸”**: 62-65
- **ìµœì¢… (í›„ì²˜ë¦¬ í¬í•¨)**: 65-70

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
- **í•™ìŠµ ì‹œê°„**: 5-10ì‹œê°„ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
- **GPU ë©”ëª¨ë¦¬**: 8-16GB
- **ë””ìŠ¤í¬ ê³µê°„**: 20-50GB (ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)

## ğŸš€ ì‹¤í–‰ ê³„íš

### Week 1
- [ ] LLM ëª¨ë¸ ì„ íƒ ë° í™˜ê²½ êµ¬ì„±
- [ ] ë°ì´í„° í¬ë§·íŒ… ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] LoRA ì„¤ì • ìµœì í™”

### Week 2
- [ ] íŒŒì¸íŠœë‹ ì‹¤í–‰
- [ ] Instruction tuning
- [ ] ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ

### Week 3
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- [ ] ì•™ìƒë¸” ì¤€ë¹„
- [ ] ìµœì¢… ëª¨ë¸ ì„ ì •