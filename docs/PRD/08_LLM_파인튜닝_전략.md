# ğŸ¤– LLM íŒŒì¸íŠœë‹ ì „ëµ

## ğŸ“š ì ‘ê·¼ ë°©ì‹ ë¹„êµ

### 1. ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ (ê¸°ì¡´ ë°©ì‹)
- **ëª¨ë¸**: BART, T5, mBART
- **íŠ¹ì§•**: Seq2Seq êµ¬ì¡°ë¡œ ìš”ì•½ì— íŠ¹í™”
- **ì¥ì **: ìš”ì•½ íƒœìŠ¤í¬ì— ìµœì í™”ë¨
- **ë‹¨ì **: ëª¨ë¸ í¬ê¸° ì œí•œ, ì»¨í…ìŠ¤íŠ¸ ì´í•´ í•œê³„

### 2. ë””ì½”ë” ì „ìš© LLM íŒŒì¸íŠœë‹ (ìƒˆë¡œìš´ ë°©ì‹) âœ¨
- **ëª¨ë¸**: GPT, LLaMA, Polyglot-Ko, SOLAR
- **íŠ¹ì§•**: Causal Language Modelingì„ ìš”ì•½ì— í™œìš©
- **ì¥ì **:
  - ë” ê¹Šì€ ë¬¸ë§¥ ì´í•´
  - ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ ì§€ì‹ í™œìš©
  - Instruction Following ëŠ¥ë ¥
- **ë‹¨ì **:
  - ë” ë§ì€ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ í•„ìš”
  - í† í° ì‚¬ìš©ëŸ‰ ì¦ê°€

## ğŸ¯ LLM íŒŒì¸íŠœë‹ êµ¬í˜„ ì „ëµ

### 1. ëª¨ë¸ ì„ íƒ (ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ)
```python
# ê²€ì¦ëœ ëª¨ë¸ (Zero-shot ì„±ëŠ¥ ê¸°ì¤€)
models = {
    'llama-3.2-korean-3b': {
        'model_name': 'Bllossom/llama-3.2-Korean-Bllossom-3B',
        'size': '3B',
        'zero_shot': 49.52,  # 1ìœ„
        'dtype': 'bf16',
        'chat_template': 'llama'
    },
    'llama-3-korean-8b': {
        'model_name': 'MLP-KTLim/llama-3-Korean-Bllossom-8B',
        'size': '8B',
        'zero_shot': 48.61,  # 2ìœ„
        'dtype': 'bf16',
        'chat_template': 'llama'
    },
    'qwen2.5-7b': {
        'model_name': 'Qwen/Qwen2.5-7B-Instruct',
        'size': '7B',
        'zero_shot': 46.84,  # 3ìœ„
        'dtype': 'fp16',
        'chat_template': 'qwen'
    },
    'qwen3-4b': {
        'model_name': 'Qwen/Qwen3-4B-Instruct-2507',
        'size': '4B',
        'zero_shot': 45.02,  # 4ìœ„
        'dtype': 'fp16',
        'chat_template': 'qwen'
    }
}
```

### 2. ë°ì´í„° í¬ë§·íŒ…
```python
def format_for_llm_finetuning(dialogue, summary):
    """
    LLM íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„° í¬ë§·
    """
    instruction = "ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."

    # Option 1: Instruction Format
    prompt = f"""### Instruction:
{instruction}

### Input:
{dialogue}

### Response:
{summary}"""

    # Option 2: Chat Format
    prompt = f"""<|system|>
ë‹¹ì‹ ì€ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
<|user|>
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:
{dialogue}
<|assistant|>
{summary}"""

    return prompt
```

### 3. íŒŒì¸íŠœë‹ ì„¤ì •
```python
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType

# QLoRA ì„¤ì • (4bit ì–‘ìí™” + LoRA)
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # Llama: bf16, Qwen: fp16
)

# LoRA ì„¤ì • (ê²€ì¦ëœ íŒŒë¼ë¯¸í„°)
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,  # alpha = r * 2
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"       # MLP (ì¤‘ìš”!)
    ],
    lora_dropout=0.05,  # ìµœì í™”: 0.1 â†’ 0.05
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# í•™ìŠµ ì„¤ì • (ê²€ì¦ëœ íŒŒë¼ë¯¸í„°)
training_args = TrainingArguments(
    output_dir="./llm_finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=8,  # ìµœì í™”: 4 â†’ 8
    gradient_accumulation_steps=8,   # effective batch=64
    warmup_ratio=0.1,                # ìµœì í™”: warmup_steps â†’ ratio
    learning_rate=2e-5,
    lr_scheduler_type="cosine",      # ìµœì í™” ì¶”ê°€
    bf16=True,  # Llamaìš© (Qwenì€ fp16=True)
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},  # PyTorch 2.0+
    optim="paged_adamw_32bit",  # QLoRA ìµœì í™”
    max_grad_norm=1.2,          # ìµœì í™”: ê¸°ë³¸ê°’ â†’ 1.2
    weight_decay=0.1,
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",  # LLMì€ loss ì‚¬ìš©
    greater_is_better=False,
    predict_with_generate=True,
    generation_max_length=100,
    generation_num_beams=4
)
```

### 4. Instruction Tuning
```python
# ë‹¤ì–‘í•œ Instruction í…œí”Œë¦¿
instructions = [
    "ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
    "ì•„ë˜ ëŒ€í™”ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
    "ì£¼ì–´ì§„ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•˜ì„¸ìš”.",
    "ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ìš”ì•½í•˜ì„¸ìš”.",
    "ëŒ€í™” ë‚´ìš©ì„ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
]

def augment_with_instructions(data):
    """
    ë‹¤ì–‘í•œ instructionìœ¼ë¡œ ë°ì´í„° ì¦ê°•
    """
    augmented = []
    for dialogue, summary in data:
        for instruction in instructions:
            formatted = format_with_instruction(
                instruction, dialogue, summary
            )
            augmented.append(formatted)
    return augmented
```

## ğŸ’¡ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ê¸°ë²•

### 1. LoRA (Low-Rank Adaptation)
- ì „ì²´ ëª¨ë¸ ëŒ€ì‹  ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ê°ì†Œ
- í•™ìŠµ ì†ë„ 3-5ë°° í–¥ìƒ

### 2. QLoRA (Quantized LoRA)
- 4-bit ì–‘ìí™” + LoRA
- ë” í° ëª¨ë¸ì„ ì œí•œëœ GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥

### 3. Gradient Checkpointing
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš© ê°€ëŠ¥

## ğŸ“Š ì‹¤ì œ ì„±ëŠ¥ ë°ì´í„°

| ë°©ì‹ | ëª¨ë¸ | íŒŒë¼ë¯¸í„° | GPU ë©”ëª¨ë¦¬ | ROUGE Sum | ìƒíƒœ |
|------|------|----------|------------|-----------|------|
| Encoder-Decoder | KoBART | 124M | 8GB | **94.51** | âœ… ì™„ë£Œ |
| LLM Zero-shot | Llama-3.2-Korean | 3B | - | 49.52 | - |
| LLM + QLoRA 4bit (bf16) | Llama-3.2-Korean | 3B | 8GB | 95+ ëª©í‘œ | ğŸ”„ ì§„í–‰ì¤‘ |
| LLM + QLoRA 4bit (fp16) | Llama-3.2-Korean | 3B | 8GB | 95+ ëª©í‘œ | â³ ëŒ€ê¸° |
| LLM + QLoRA 4bit | Qwen3-4B | 4B | 10GB | 95+ ëª©í‘œ | â³ ëŒ€ê¸° |

## ğŸ”§ êµ¬í˜„ ì˜ˆì œ

### ì „ì²´ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸
```python
class LLMFineTuner:
    def __init__(self, model_name, use_lora=True):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_8bit=True if use_lora else False,
            device_map="auto"
        )

        if use_lora:
            self.model = get_peft_model(self.model, lora_config)

    def prepare_dataset(self, data):
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        formatted_data = []
        for item in data:
            text = self.format_for_training(
                item['dialogue'],
                item['summary']
            )
            formatted_data.append(text)
        return formatted_data

    def train(self, train_dataset, eval_dataset):
        """ëª¨ë¸ í•™ìŠµ"""
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
        )
        trainer.train()

    def generate_summary(self, dialogue):
        """ìš”ì•½ ìƒì„±"""
        prompt = f"""### Instruction:
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.

### Input:
{dialogue}

### Response:"""

        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        summary = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )
        return summary.split("### Response:")[-1].strip()
```

## ğŸ“ˆ í•™ìŠµ ì „ëµ

### Phase 1: ê¸°ë³¸ íŒŒì¸íŠœë‹
1. ì „ì²´ ë°ì´í„°ë¡œ 3 epochs í•™ìŠµ
2. Learning rate: 2e-5
3. Batch size: 4 (gradient accumulation ì‚¬ìš©)

### Phase 2: Instruction Tuning
1. ë‹¤ì–‘í•œ instruction í…œí”Œë¦¿ ì ìš©
2. ë°ì´í„° ì¦ê°• (5ë°°)
3. ì¶”ê°€ 2 epochs í•™ìŠµ

### Phase 3: RLHF (ì„ íƒì‚¬í•­)
1. ìƒì„±ëœ ìš”ì•½ì— ëŒ€í•œ í’ˆì§ˆ í‰ê°€
2. Reward model í•™ìŠµ
3. PPOë¥¼ í†µí•œ ì¶”ê°€ ìµœì í™”

## âš¡ ìµœì í™” íŒ

### 1. Mixed Precision Training
```python
training_args = TrainingArguments(
    fp16=True,  # ë˜ëŠ” bf16=True
    # ...
)
```

### 2. Gradient Accumulation
```python
# ì‹¤ì œ ë°°ì¹˜ í¬ê¸° = 4 * 4 = 16
per_device_train_batch_size=4,
gradient_accumulation_steps=4,
```

### 3. Learning Rate Schedule
```python
lr_scheduler_type="cosine",
warmup_ratio=0.1,
```

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### ì„±ëŠ¥ í–¥ìƒ ì˜ˆì¸¡
- **ë² ì´ìŠ¤ë¼ì¸ (BART)**: 47.12
- **LLM íŒŒì¸íŠœë‹ (ê¸°ë³¸)**: 55-58
- **LLM + Instruction Tuning**: 58-62
- **LLM + ì•™ìƒë¸”**: 62-65
- **ìµœì¢… (í›„ì²˜ë¦¬ í¬í•¨)**: 65-70

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
- **í•™ìŠµ ì‹œê°„**: 5-10ì‹œê°„ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
- **GPU ë©”ëª¨ë¦¬**: 8-16GB
- **ë””ìŠ¤í¬ ê³µê°„**: 20-50GB (ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)

## âš ï¸ ì¹˜ëª…ì  ê¸°ìˆ  ì´ìŠˆ ë° í•´ê²°ì±…

### 1. Prompt Truncation ë¬¸ì œ (í•„ìˆ˜ ì²´í¬)
**ë¬¸ì œ**: max_length ì„¤ì •ì´ ë¶€ì ì ˆí•˜ë©´ assistant í—¤ë”ê°€ ì˜ë ¤ ëª¨ë¸ì´ ìƒì„± ìœ„ì¹˜ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•¨

**ì‹¤ì¸¡ ë°ì´í„°**:
- encoder_max_len=512: Prompt ì˜ë¦¼ **6.07%** (756/12,457ê°œ)
- encoder_max_len=1024: Prompt ì˜ë¦¼ **0.11%** (14/12,457ê°œ)

**í•´ê²°ì±…**:
```python
tokenizer_config = {
    'encoder_max_len': 1024,  # 512 â†’ 1024 (í•„ìˆ˜!)
    'decoder_max_len': 200,   # 100 â†’ 200 (ì—¬ìœ )
}

# ì¶”ë¡  ì‹œ Left Truncation ì‚¬ìš© (Assistant í—¤ë” ë³´ì¡´)
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
```

**ì„±ëŠ¥ ì˜í–¥**: Prompt truncation 6% ë°œìƒ ì‹œ **-20~30 ROUGE points**

### 2. Chat Template Tokens (í•„ìˆ˜ ì¶”ê°€)
```python
# Llama ëª¨ë¸
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# Qwen ëª¨ë¸
chat_tokens = ["<|im_start|>", "<|im_end|>"]

tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
model.resize_token_embeddings(len(tokenizer))
```

### 3. QLoRA compute_dtype ë§¤ì¹­
```python
# Llama: bf16
bnb_config = BitsAndBytesConfig(
    bnb_4bit_compute_dtype=torch.bfloat16
)
training_args = TrainingArguments(bf16=True, fp16=False)

# Qwen: fp16
bnb_config = BitsAndBytesConfig(
    bnb_4bit_compute_dtype=torch.float16
)
training_args = TrainingArguments(fp16=True, bf16=False)
```

### 4. metric_for_best_model ì°¨ì´
- **Encoder-Decoder**: `"rouge_sum"` + `greater_is_better=True`
- **Causal LM**: `"eval_loss"` + `greater_is_better=False`

## ğŸš€ ì‹¤í–‰ ê³„íš

### Week 1
- [x] LLM ëª¨ë¸ ì„ íƒ ì™„ë£Œ (Llama-3.2-Korean-3B)
- [x] QLoRA ì„¤ì • ìµœì í™” ì™„ë£Œ
- [x] ì¹˜ëª…ì  ì´ìŠˆ í•´ê²° ì™„ë£Œ

### Week 2
- [x] KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ (ROUGE Sum: 94.51)
- [x] ê¸°ìˆ  ì´ìŠˆ ë¬¸ì„œí™” ì™„ë£Œ
- [ ] Llama-3.2 íŒŒì¸íŠœë‹ ì§„í–‰ì¤‘

### Week 3
- [ ] ë‹¤ì¤‘ ëª¨ë¸ íŒŒì¸íŠœë‹ ì™„ë£Œ
- [ ] ì•™ìƒë¸” ì „ëµ ì ìš©
- [ ] ìµœì¢… ëª¨ë¸ ì„ ì • ë° ì œì¶œ