# 🚀 성능 개선 전략 (v2.0)

## 📈 현재 상황 분석
- **베이스라인 성능**: ROUGE-F1 47.1244 (BART 모델)
- **Solar API 성능**: 약 10~15% (초기 테스트 기준)
- **목표**: 70점 이상

## 🆕 핵심 전략 변경사항
### 팀원 제안 반영 (2025.10.10 업데이트)
1. **LLM 파인튜닝 접근법**: 인코더-디코더 대신 디코더 전용 LLM 사용
2. **Solar API 최적화**: CSV 데이터 전처리로 토큰 70-75% 절약
3. **교차 검증 시스템**: 모델과 API 결과 비교로 최적 요약 선택

## 🏗️ 통합 성능 개선 아키텍처

```mermaid
graph TD
    subgraph "입력 데이터"
        A[원본 대화<br/>12,457개]
    end

    subgraph "데이터 전처리"
        A --> B[노이즈 제거]
        B --> C[토큰 최적화]
        C --> D[데이터 증강]
    end

    subgraph "모델링 전략"
        D --> E[LLM 파인튜닝<br/>Polyglot-Ko + LoRA]
        D --> F[Solar API<br/>토큰 70% 절약]
    end

    subgraph "교차 검증"
        E --> G[모델 요약]
        F --> H[API 요약]
        G --> I[품질 평가기]
        H --> I
        I --> J[최적 선택]
    end

    subgraph "앙상블"
        J --> K[다중 모델<br/>앙상블]
        K --> L[가중 평균]
    end

    subgraph "후처리"
        L --> M[문법 교정]
        M --> N[길이 조절]
        N --> O[최종 요약]
    end

    subgraph "성능 추적"
        O --> P[ROUGE 평가]
        P --> Q[로깅/시각화]
        Q --> R[WandB 대시보드]
    end

    style E fill:#f9f,stroke:#333,stroke-width:4px
    style F fill:#9ff,stroke:#333,stroke-width:4px
    style I fill:#ff9,stroke:#333,stroke-width:4px
```

## 🎯 개선 전략

### 1. 데이터 전처리 최적화
#### 1.1 노이즈 제거
- [ ] `\\n` → `\n` 변환
- [ ] `<br>` 태그 제거
- [ ] 중복 공백 제거
- [ ] 특수문자 정규화
- [ ] 자모음 단독 사용 처리 (ㅋㅋ, ㅇㅇ 등)

#### 1.2 특수 토큰 처리
- [ ] Person 토큰 최적화 (#Person1#, #Person2# 등)
- [ ] 마스킹 토큰 활용 전략
- [ ] 토크나이저 특수 토큰 추가

#### 1.3 데이터 증강
- [ ] 패러프레이징 (KoGPT 활용)
- [ ] 백트랜슬레이션 (한→영→한)
- [ ] 문장 순서 섞기
- [ ] 동의어 치환
- [ ] 대화 턴 샘플링

### 2. 모델 개선 (LLM 파인튜닝 중심)
#### 2.1 모델 선택 전략 변경 ✨
##### 기존 접근법 (인코더-디코더)
| 모델 | 특징 | 예상 성능 |
|------|------|-----------|
| KoBART | 한국어 특화 BART | 50-55 |
| KoT5 | 한국어 T5 | 48-53 |
| mT5 | 다국어 T5 | 45-50 |

##### 새로운 접근법 (디코더 전용 LLM) 🚀
| 모델 | 파라미터 | GPU 메모리 | 예상 성능 |
|------|----------|------------|-----------|
| Polyglot-Ko | 5.8B | 16GB | 55-60 |
| KoGPT | 6B | 16GB | 53-58 |
| LLaMA-2-Ko | 7B | 24GB | 58-63 |
| Polyglot-Ko + LoRA | 5.8B | 8GB | 53-58 |
| LLaMA-2-Ko + QLoRA | 7B | 8GB | 58-63 |

#### 2.2 LLM Fine-tuning 전략
- [ ] **LoRA/QLoRA 적용** (메모리 90% 절약)
- [ ] **Instruction Tuning** (다양한 프롬프트 템플릿)
- [ ] **Mixed Precision Training** (fp16/bf16)
- [ ] **Gradient Accumulation** (큰 배치 효과)
- [ ] **Gradient Checkpointing** (메모리 최적화)

#### 2.3 하이퍼파라미터 최적화
```python
hyperparameters = {
    'learning_rate': [1e-5, 3e-5, 5e-5],
    'batch_size': [16, 32, 64],
    'max_length': [512, 768, 1024],
    'num_beams': [3, 4, 5],
    'warmup_ratio': [0.0, 0.1, 0.2],
    'weight_decay': [0.0, 0.01, 0.1]
}
```

### 3. Solar API 최적화 (토큰 절약 중심) 💰
#### 3.1 토큰 사용량 최적화 전략
- [ ] **CSV 데이터 전처리** (70-75% 토큰 절약)
  - 불필요한 공백 제거
  - Person 태그 간소화 (#Person1# → A:)
  - 반복 패턴 제거
- [ ] **핵심 부분 추출**
  - 처음과 끝 부분 우선
  - 긴 문장 우선 (정보 밀도 높음)
- [ ] **스마트 절단** (문장 단위로 자르기)
- [ ] **배치 처리** (여러 대화 한번에)
- [ ] **캐싱 메커니즘** (중복 호출 방지)

#### 3.2 프롬프트 최적화
```python
# 기존: 800-1200 토큰/대화
# 최적화 후: 200-300 토큰/대화
optimized_params = {
    'max_input_tokens': 512,
    'batch_size': 10,
    'cache_enabled': True,
    'preprocessing': True
}
```

### 4. 교차 검증 시스템 (새로운 핵심 전략) 🔄
#### 4.1 듀얼 생성 시스템
- [ ] **파인튜닝 모델 요약 생성**
- [ ] **Solar API 요약 생성**
- [ ] **품질 평가기로 비교**
- [ ] **최적 요약 선택**

#### 4.2 품질 평가 메트릭
- [ ] **길이 적절성** (원본의 20-30%)
- [ ] **키워드 포함도** (핵심 명사 coverage)
- [ ] **문장 일관성** (coherence)
- [ ] **정보 밀도** (고유 단어 비율)

### 5. 앙상블 전략 (고급)
#### 5.1 모델 앙상블
- [ ] **LLM 모델들 앙상블** (Polyglot + KoGPT + LLaMA)
- [ ] **LLM + Solar API 하이브리드**
- [ ] **다양한 체크포인트 앙상블**
- [ ] **Cross-validation 앙상블**

#### 5.2 앙상블 방법
- [ ] **Voting** (가장 많이 나온 내용)
- [ ] **신뢰도 가중 평균**
- [ ] **최고 품질 선택**
- [ ] **Stacking**

### 6. 후처리 최적화
#### 6.1 규칙 기반 후처리
- [ ] 문장 부호 정규화
- [ ] 주어 생략 복원
- [ ] 조사 교정
- [ ] 길이 조절 (너무 짧거나 긴 요약 처리)

#### 6.2 품질 개선
- [ ] 문법 검사기 적용
- [ ] 중복 문장 제거
- [ ] 핵심 정보 보존 검증

### 7. 평가 및 분석
#### 7.1 에러 분석
- [ ] 낮은 점수 샘플 분석
- [ ] 대화 길이별 성능 분석
- [ ] 참여자 수별 성능 분석
- [ ] 토픽별 성능 분석

#### 7.2 A/B 테스팅
- [ ] 전처리 방법별 비교
- [ ] 모델별 비교
- [ ] 하이퍼파라미터별 비교

## 📅 실행 계획

### Week 1 (09/26 - 10/02)
- [x] 프로젝트 셋업 및 베이스라인 구축
- [ ] EDA 및 데이터 분석
- [ ] 전처리 파이프라인 구축
- [ ] 모델 실험 환경 구축

### Week 2 (10/03 - 10/09)
- [ ] 다양한 모델 실험
- [ ] 하이퍼파라미터 튜닝
- [ ] Solar API 최적화
- [ ] 앙상블 실험

### Week 3 (10/10 - 10/15)
- [ ] 최종 모델 선정
- [ ] 후처리 최적화
- [ ] 최종 제출물 생성
- [ ] 문서화 및 정리

## 🎯 예상 성능 향상 (업데이트)
### 기존 전략 (인코더-디코더)
| 전략 | 예상 향상 | 누적 점수 |
|------|----------|----------|
| 베이스라인 (BART) | - | 47.12 |
| 데이터 전처리 | +3-5 | 50-52 |
| 모델 개선 | +5-8 | 55-60 |
| 앙상블 | +5-7 | 60-67 |
| 후처리 | +2-3 | 62-70 |

### 새로운 전략 (LLM + 교차검증) 🚀
| 전략 | 예상 향상 | 누적 점수 | 비고 |
|------|----------|----------|------|
| 베이스라인 (BART) | - | 47.12 | 시작점 |
| LLM 파인튜닝 | +8-13 | 55-60 | 디코더 전용 |
| Solar API 최적화 | +3-5 | 58-65 | 토큰 절약 |
| 교차 검증 시스템 | +5-7 | 63-72 | 듀얼 생성 |
| 앙상블 (고급) | +3-5 | 66-77 | 다중 모델 |
| 후처리 | +2-3 | 68-80 | 최종 정제 |

## 💡 추가 아이디어 및 핵심 인사이트
### 혁신적 접근법
1. **멀티태스크 러닝**: 요약 + 주제 분류 동시 학습
2. **Curriculum Learning**: 쉬운 샘플부터 점진적 학습
3. **Knowledge Distillation**: 큰 모델에서 작은 모델로 지식 전달
4. **Semi-supervised Learning**: 라벨 없는 대화 데이터 활용
5. **Active Learning**: 불확실한 샘플 우선 학습

### 핵심 인사이트 (팀원 제안)
#### 언제 LLM 모델을 신뢰할까?
- 짧고 명확한 대화 (300-800자)
- 2인 대화
- 학습 데이터와 유사한 패턴

#### 언제 Solar API를 신뢰할까?
- 복잡한 다자 대화 (3인 이상)
- 긴 대화 (1000자 이상)
- 특수한 도메인이나 전문 용어

#### 하이브리드의 강점
- 두 방법의 장점 결합
- 안정적인 성능 보장
- 비용 효율적 (토큰 절약)

## 🔥 최종 권장 구현 순서
1. **LLM 파인튜닝 환경 구축** (Polyglot-Ko + LoRA)
2. **Solar API 토큰 최적화** 구현
3. **교차 검증 시스템** 개발
4. **A/B 테스팅**으로 최적 조합 찾기
5. **앙상블 및 후처리** 적용