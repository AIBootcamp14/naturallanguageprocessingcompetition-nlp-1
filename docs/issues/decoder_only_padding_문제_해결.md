# Decoder-Only ëª¨ë¸ Padding ê²½ê³  ë¬¸ì œ í•´ê²°

## ğŸ“‹ ë¬¸ì œ ê°œìš”

### ë°œìƒ ìœ„ì¹˜
- **ì‹¤í—˜**: `experiments/20251013/20251013_161056_test_strategy3_triple`
- **ë¡œê·¸ íŒŒì¼**: `train.log` (ë¼ì¸ 1683)
- **ë°œìƒ ì‹œì **: ìµœì¢… í‰ê°€ ì¤‘ (Final Evaluation)

### ê²½ê³  ë©”ì‹œì§€
```
A decoder-only architecture is being used, but right-padding was detected!
For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
```

### ì˜í–¥ì„ ë°›ëŠ” ëª¨ë¸
- Llama-3.2-Korean-3B (Decoder-only, Causal LM)
- Qwen3-4B (Decoder-only, Causal LM)
- ê¸°íƒ€ ëª¨ë“  Causal LM ì•„í‚¤í…ì²˜

---

## ğŸ” ì›ì¸ ë¶„ì„

### 1. Decoder-Only ëª¨ë¸ì˜ íŠ¹ì„±
Decoder-only ëª¨ë¸ (Causal LM)ì€ **left-to-right autoregressive** ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤:
- ê° í† í°ì€ **ì´ì „ í† í°ë“¤ë§Œ** ì°¸ì¡° ê°€ëŠ¥ (causal attention mask)
- Paddingì´ ì˜¤ë¥¸ìª½ì— ìˆìœ¼ë©´ ëª¨ë¸ì´ padding í† í°ë„ "ì´ì „ ì»¨í…ìŠ¤íŠ¸"ë¡œ í•™ìŠµ
- ì´ëŠ” ìƒì„± í’ˆì§ˆ ì €í•˜ì™€ ì˜ˆì¸¡ ë¶ˆì¼ì¹˜ë¥¼ ì´ˆë˜

### 2. ì˜¬ë°”ë¥¸ Padding ë°©ì‹
| ëª¨ë¸ íƒ€ì… | Padding ìœ„ì¹˜ | ì´ìœ  |
|-----------|--------------|------|
| **Encoder-Decoder** (Seq2Seq) | Right | EncoderëŠ” ì–‘ë°©í–¥ attention ì‚¬ìš© ê°€ëŠ¥ |
| **Decoder-Only** (Causal LM) | Left | ê³¼ê±° í† í°ë§Œ ì°¸ì¡°í•´ì•¼ í•˜ë¯€ë¡œ paddingì€ ì™¼ìª½ì— ë°°ì¹˜ |

### 3. ë¬¸ì œê°€ ë°œìƒí•œ ì½”ë“œ ìœ„ì¹˜

#### âœ… í•™ìŠµ ì‹œ (ë¬¸ì œ ì—†ìŒ)
`src/models/lora_loader.py:191`ì—ì„œ ì´ë¯¸ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨:
```python
def _configure_tokenizer(self, tokenizer: AutoTokenizer):
    """í† í¬ë‚˜ì´ì € ì„¤ì • (Left padding for Causal LM)"""
    tokenizer.padding_side = "left"
    tokenizer.truncation_side = "left"
```

#### âŒ í‰ê°€/ì¶”ë¡  ì‹œ (ë¬¸ì œ ë°œìƒ)
ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•  ë•Œ tokenizer ì„¤ì •ì´ ì´ˆê¸°í™”ë¨:

1. **`src/trainers/full_pipeline_trainer.py:523`**
   - ì œì¶œ íŒŒì¼ ìƒì„± ì‹œ (`_create_submission` ë©”ì„œë“œ)
   - ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í›„ tokenizer ì¬ì„¤ì • ëˆ„ë½

2. **`src/trainers/full_pipeline_trainer.py:333`**
   - ì•™ìƒë¸” í‰ê°€ ì‹œ (`_create_and_evaluate_ensemble` ë©”ì„œë“œ)
   - ì—¬ëŸ¬ ëª¨ë¸ ë¡œë“œ ì‹œ ê° tokenizer ì¬ì„¤ì • ëˆ„ë½

3. **`src/trainers/multi_model_trainer.py:239`**
   - Multi-model ì•™ìƒë¸” í‰ê°€ ì‹œ (`_evaluate_ensemble` ë©”ì„œë“œ)
   - Encoder-Decoderë§Œ ê°€ì •, Causal LM ë¯¸ì§€ì›

4. **`src/ensemble/manager.py:58`**
   - ModelManagerì˜ `load_model` ë©”ì„œë“œ
   - Encoder-Decoderë§Œ ê°€ì •, Causal LM ë¯¸ì§€ì›

---

## âœ… í•´ê²° ë°©ë²•

### ìˆ˜ì • ì „ëµ
1. **ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€**: `AutoConfig`ë¡œ `is_encoder_decoder` í™•ì¸
2. **ì¡°ê±´ë¶€ Tokenizer ì„¤ì •**: Decoder-only ëª¨ë¸ì—ë§Œ left padding ì ìš©
3. **Pad Token ë³´ì¥**: `pad_token`ì´ ì—†ìœ¼ë©´ `eos_token` ì‚¬ìš©

### ìˆ˜ì •ëœ ì½”ë“œ íŒ¨í„´

```python
from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer

# 1. ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
config = AutoConfig.from_pretrained(model_path)
is_encoder_decoder = config.is_encoder_decoder if hasattr(config, 'is_encoder_decoder') else False

# 2. ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í´ë˜ìŠ¤ ì‚¬ìš©
if is_encoder_decoder:
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
else:
    model = AutoModelForCausalLM.from_pretrained(model_path)

# 3. Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 4. Decoder-only ëª¨ë¸ì˜ ê²½ìš° left padding ì„¤ì •
if not is_encoder_decoder:
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
```

### ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡

#### 1. `src/trainers/full_pipeline_trainer.py`
**ìˆ˜ì • ìœ„ì¹˜ 1**: `_create_submission` ë©”ì„œë“œ (ë¼ì¸ 523 ê·¼ì²˜)
```python
# ìˆ˜ì • ì „
tokenizer = AutoTokenizer.from_pretrained(best_model_path)
if torch.cuda.is_available():
    model = model.cuda()

# ìˆ˜ì • í›„
tokenizer = AutoTokenizer.from_pretrained(best_model_path)

# Decoder-only ëª¨ë¸ì˜ ê²½ìš° left padding ì„¤ì •
if not is_encoder_decoder:
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

if torch.cuda.is_available():
    model = model.cuda()
```

**ìˆ˜ì • ìœ„ì¹˜ 2**: `_create_and_evaluate_ensemble` ë©”ì„œë“œ (ë¼ì¸ 333 ê·¼ì²˜)
```python
# ìˆ˜ì • ì „
tokenizer = AutoTokenizer.from_pretrained(model_path)
if torch.cuda.is_available():
    model = model.cuda()

# ìˆ˜ì • í›„
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Decoder-only ëª¨ë¸ì˜ ê²½ìš° left padding ì„¤ì •
if not is_encoder_decoder:
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

if torch.cuda.is_available():
    model = model.cuda()
```

#### 2. `src/trainers/multi_model_trainer.py`
**ìˆ˜ì • ìœ„ì¹˜**: `_evaluate_ensemble` ë©”ì„œë“œ (ë¼ì¸ 226-266)
```python
# ìˆ˜ì • ì „: Encoder-Decoderë§Œ ì§€ì›
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# ìˆ˜ì • í›„: ëª¨ë“  ëª¨ë¸ íƒ€ì… ì§€ì›
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    AutoTokenizer
)

# ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
config = AutoConfig.from_pretrained(model_path)
is_encoder_decoder = config.is_encoder_decoder if hasattr(config, 'is_encoder_decoder') else False

# ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í´ë˜ìŠ¤ ì‚¬ìš©
if is_encoder_decoder:
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
else:
    model = AutoModelForCausalLM.from_pretrained(model_path)

tokenizer = AutoTokenizer.from_pretrained(model_path)

# Decoder-only ëª¨ë¸ì˜ ê²½ìš° left padding ì„¤ì •
if not is_encoder_decoder:
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
```

#### 3. `src/ensemble/manager.py`
**ìˆ˜ì • ìœ„ì¹˜**: `load_model` ë©”ì„œë“œ (ë¼ì¸ 36-93)
```python
# ìˆ˜ì • ì „: Encoder-Decoderë§Œ ì§€ì›
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# ìˆ˜ì • í›„: ëª¨ë“  ëª¨ë¸ íƒ€ì… ì§€ì›
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    AutoTokenizer
)

# ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
config = AutoConfig.from_pretrained(model_path)
is_encoder_decoder = config.is_encoder_decoder if hasattr(config, 'is_encoder_decoder') else False

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
if is_encoder_decoder:
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
else:
    model = AutoModelForCausalLM.from_pretrained(model_path)

tokenizer = AutoTokenizer.from_pretrained(model_path)

# Decoder-only ëª¨ë¸ì˜ ê²½ìš° left padding ì„¤ì •
if not is_encoder_decoder:
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
```

---

## ğŸ¯ ê°œì„  íš¨ê³¼

### 1. ê²½ê³  ì œê±°
- Decoder-only ëª¨ë¸ í‰ê°€/ì¶”ë¡  ì‹œ padding ê²½ê³  ì™„ì „ ì œê±°
- ë¡œê·¸ê°€ ê¹”ë”í•´ì ¸ ì‹¤ì œ ë¬¸ì œ íŒŒì•… ìš©ì´

### 2. ìƒì„± í’ˆì§ˆ í–¥ìƒ
- **ì˜¬ë°”ë¥¸ attention mask**: Padding í† í°ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í•™ìŠµí•˜ì§€ ì•ŠìŒ
- **ì¼ê´€ëœ ìƒì„±**: í•™ìŠµ ì‹œì™€ ì¶”ë¡  ì‹œ ë™ì¼í•œ padding ì „ëµ ì‚¬ìš©
- **ROUGE ì ìˆ˜ ê°œì„  ê°€ëŠ¥**: ë” ì •í™•í•œ ìš”ì•½ ìƒì„±

### 3. ì½”ë“œ ê²¬ê³ ì„± í–¥ìƒ
- **íƒ€ì… ì•ˆì „ì„±**: `AutoConfig`ë¡œ ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
- **ë²”ìš©ì„±**: Encoder-Decoderì™€ Decoder-only ëª¨ë‘ ì§€ì›
- **ìœ ì§€ë³´ìˆ˜ì„±**: ì¼ê´€ëœ íŒ¨í„´ìœ¼ë¡œ í–¥í›„ ë””ë²„ê¹… ìš©ì´

### 4. í˜¸í™˜ì„± ë³´ì¥
| ëª¨ë¸ | ì•„í‚¤í…ì²˜ | Padding | ìƒíƒœ |
|------|----------|---------|------|
| KoBART | Encoder-Decoder | Right | âœ… ê¸°ì¡´ ìœ ì§€ |
| Llama-3.2-Korean-3B | Decoder-only | Left | âœ… ìˆ˜ì • ì™„ë£Œ |
| Qwen3-4B | Decoder-only | Left | âœ… ìˆ˜ì • ì™„ë£Œ |
| í–¥í›„ LLM ëª¨ë¸ | Decoder-only | Left | âœ… ìë™ ì§€ì› |

---

## ğŸ“Š ê²€ì¦ ë°©ë²•

### 1. ë¡œê·¸ í™•ì¸
ë‹¤ìŒ ì‹¤í—˜ ì‹¤í–‰ ì‹œ ê²½ê³  ë©”ì‹œì§€ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸:
```bash
# Full pipeline ì‹¤í–‰
python main.py --mode full --models kobart llama-3.2-korean-3b qwen3-4b

# ë¡œê·¸ì—ì„œ padding ê²½ê³  ê²€ìƒ‰
grep "right-padding was detected" experiments/*/train.log
```

### 2. Tokenizer ì„¤ì • í™•ì¸
```python
from transformers import AutoTokenizer

# Decoder-only ëª¨ë¸ tokenizer í™•ì¸
tokenizer = AutoTokenizer.from_pretrained("model_path")
print(f"Padding side: {tokenizer.padding_side}")  # "left" ì¶œë ¥ ì˜ˆìƒ
print(f"Pad token: {tokenizer.pad_token}")  # EOS í† í° ì¶œë ¥ ì˜ˆìƒ
```

### 3. ROUGE ì ìˆ˜ ë¹„êµ
- ìˆ˜ì • ì „/í›„ ë™ì¼ ë°ì´í„°ì— ëŒ€í•œ ROUGE ì ìˆ˜ ë¹„êµ
- Decoder-only ëª¨ë¸ì˜ ê²½ìš° ë¯¸ì„¸í•œ ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

### PRD ë¬¸ì„œ
- **PRD 08**: LLM (Decoder-only) ì§€ì›
- **PRD 12**: ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” ì „ëµ
- **PRD 14**: Full Pipeline í†µí•©

### ì°¸ê³  ìë£Œ
- [Hugging Face - Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation)
- [Causal LM vs Seq2Seq](https://huggingface.co/docs/transformers/tasks/language_modeling)

### ê´€ë ¨ ì´ìŠˆ
- `docs/issues/ì‹œìŠ¤í…œ_ë¬¸ì œ_ê°œì„ _ê³¼ì •.md` - BFloat16, Config ìš°ì„ ìˆœìœ„ ë“± ê¸°ì¡´ ë¬¸ì œ í•´ê²°
- `docs/issues/ë¬¸ì¥_ëŠê¹€_ë¬¸ì œ_í•´ê²°_ê³¼ì •.md` - í›„ì²˜ë¦¬ ê°œì„  ê³¼ì •

---

## ğŸ“ ìš”ì•½

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ë¬¸ì œ** | Decoder-only ëª¨ë¸ í‰ê°€/ì¶”ë¡  ì‹œ right-padding ê²½ê³  ë°œìƒ |
| **ì›ì¸** | ëª¨ë¸ ì¬ë¡œë“œ ì‹œ tokenizer left-padding ì„¤ì • ëˆ„ë½ |
| **í•´ê²°** | 4ê°œ íŒŒì¼ì—ì„œ ìë™ íƒ€ì… ê°ì§€ + ì¡°ê±´ë¶€ padding ì„¤ì • ì¶”ê°€ |
| **ìˆ˜ì • íŒŒì¼** | `full_pipeline_trainer.py`, `multi_model_trainer.py`, `manager.py` (3ê°œ íŒŒì¼, 4ê°œ ìœ„ì¹˜) |
| **íš¨ê³¼** | ê²½ê³  ì œê±°, ìƒì„± í’ˆì§ˆ í–¥ìƒ, ì½”ë“œ ê²¬ê³ ì„±/ë²”ìš©ì„± ê°œì„  |
| **ë‚ ì§œ** | 2025-10-14 |

---

**âœ… í•´ê²° ì™„ë£Œ**: ëª¨ë“  Decoder-only ëª¨ë¸ì—ì„œ ì˜¬ë°”ë¥¸ left-paddingì´ ì ìš©ë˜ë©°, í–¥í›„ ìƒˆë¡œìš´ Causal LM ëª¨ë¸ ì¶”ê°€ ì‹œì—ë„ ìë™ìœ¼ë¡œ ì§€ì›ë©ë‹ˆë‹¤.
