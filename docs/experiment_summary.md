# 실험 요약 테이블

**프로젝트**: 일상 대화 요약 모델 성능 개선
**목표**: Baseline 47점 → 50점 이상 달성
**Current Best**: **47.4421점** (Exp #4)

---

## 📊 전체 실험 결과

| Exp | 날짜 | 변경사항 | 점수 | 상승분 | 상태 | 비고 |
|-----|------|----------|------|--------|------|------|
| #0 | 2025-10-12 | Baseline 재현 | **46.9426** | - | ✅ 성공 | 공식 baseline 재현 |
| #0.1 | 2025-10-13 | Baseline 모듈화 | **46.9526** | +0.01 | ✅ 성공 | 7개 모듈 분리 |
| #1 | 2025-10-12 | 증강 데이터 (2배) | 42.7807 | **-4.16** | ❌ 실패 | 스타일 불일치 → 롤백 |
| #2 | 2025-10-13 | 후처리 개선 (v2) | 46.9863 | +0.03 | ❌ 실패 | 무의미한 변화 → 롤백 |
| #3-v1 | 2025-10-13 | LR 2e-5 (ckpt-1750) | 46.6919 | **-0.26** | ❌ 실패 | LR 과도 → 롤백 |
| #3-v2 | 2025-10-13 | LR 2e-5 (ckpt-1000) | 46.6089 | **-0.34** | ❌ 실패 | 더 나쁨 → 롤백 |
| **#4** | **2025-10-14** | **길이 정규화 + Max768** | **47.4421** | **+0.49** | **✅ 성공** | **Current Best** 🏆 |

---

## 🎯 Exp #4 상세 비교

### ROUGE 점수 상세 분석

| 지표 | Baseline | Exp #4 | 상승분 | 상승률 |
|------|----------|--------|--------|--------|
| **ROUGE-1** | 56.31% | **56.71%** | +0.40%p | +0.71% |
| **ROUGE-2** | 36.65% | **37.22%** | +0.57%p | +1.56% |
| **ROUGE-L** | 47.75% | **48.39%** | +0.64%p | +1.34% |
| **Total** | 46.9526 | **47.4421** | **+0.4895** | **+1.04%** |

**분석**:
- 모든 지표에서 일관된 개선 ✅
- ROUGE-L 개선이 가장 큼 (+0.64%p)
- 길이 정규화 효과 확인

---

## 📈 성능 변화 그래프 (텍스트)

```
점수
48.0 ┤                                   ┌─ #4: 47.44 🏆
47.5 ┤                                   │
47.0 ┼─────────────────────────────────┘
46.5 ┤       ╭─ #0: 46.94                   ╭─ #3-v1: 46.69
46.0 ┤       │                               │
45.5 ┤       │                               ├─ #3-v2: 46.61
45.0 ┤       │                               │
44.5 ┤       │                               │
44.0 ┤       │                               │
43.5 ┤       │                               │
43.0 ┤       ╰─ #1: 42.78 (최저)            │
     └───────┴───────────────────────────────┴───────
         #0   #1   #2   #3-v1 #3-v2   #4
```

---

## 💡 주요 교훈

### ✅ 성공 사례

1. **Exp #4**: 길이 정규화 + Max Length 768
   - 메모리 최적화 (Gradient Checkpointing + Batch 조정)
   - OOM 해결하면서도 성능 향상
   - **+0.49점 (가장 큰 개선)** 🏆

2. **Exp #0.1**: Baseline 모듈화
   - 재사용 가능한 7개 모듈 구조
   - 성능 유지 (+0.01점)

### ❌ 실패 사례

1. **Exp #1**: 증강 데이터 (-4.16점)
   - 스타일 불일치 (번역투 vs 자연스러운 한국어)
   - 데이터 많다고 좋은 것 아님

2. **Exp #3**: LR 2e-5 (-0.26~-0.34점)
   - Baseline LR 1e-5가 이미 최적
   - LR 튜닝은 예상보다 민감함

3. **Exp #2**: 후처리 개선 (+0.03점)
   - 이론적 개선 ≠ 실제 성능 향상
   - "당연히 좋을 것" 가정의 위험

---

## 🚀 다음 실험 방향 (우선순위)

### ⭐ 최우선 (즉시 적용 가능)

| 실험 | 변경사항 | 예상 효과 | 소요 시간 | 리스크 |
|------|----------|-----------|-----------|--------|
| **Exp #5** | Length Penalty 튜닝 (0.5, 0.7, 0.8) | +0.3~0.7점 | 각 10분 | ✅ Low |
| **Exp #6** | Warmup Steps 조정 (20→50) | +0.3~0.5점 | 30분 | ✅ Low |
| **Exp #7** | Epochs 연장 (20→30) | +0.5~1점 | 45분 | ✅ Low |

### 🟡 중기 (검토 필요)

| 실험 | 변경사항 | 예상 효과 | 소요 시간 | 리스크 |
|------|----------|-----------|-----------|--------|
| **Exp #8** | Special Tokens (Time, Money) | +0.5~1.5점 | 3-4시간 | ⚠️ Medium |
| **Exp #9** | FlashAttention-2 적용 | encoder→1024 가능 | 10-20분 | ⚠️ Medium |

### ❌ 건너뛰기

- LR 증가 (2e-5 이상): Exp #3에서 실패 확인
- 데이터 증강: Exp #1에서 실패, 스타일 일관성 문제

---

## 📋 제출 횟수 관리

**사용**: 9/12회
**남은 횟수**: 3회

**전략**:
- Length Penalty 튜닝: 1~2회 (최우선)
- 나머지: 1~2회 (신중하게 선택)

---

**상세 로그**: `/Competition/NLP/docs/experiment_logs.md` 참조