# ì‹¤í—˜ ë¡œê·¸

**í”„ë¡œì íŠ¸:** ì¼ìƒ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 
**ëª©í‘œ:** Baseline 47ì  â†’ 50ì  ì´ìƒ ë‹¬ì„±
**ì‹œì‘ì¼:** 2025-10-12

---

## Experiment #0: Baseline ì¬í˜„

**ë‚ ì§œ:** 2025-10-12
**ì‹¤í—˜ì:** Claude + User

### ë³€ê²½ì‚¬í•­
- ì—†ìŒ (ê³µì‹ baseline ì½”ë“œ ê·¸ëŒ€ë¡œ ì‹¤í–‰)

### ì„¤ì •
```yaml
model: digit82/kobart-summarization
data: train.csv (12,457 samples)
epochs: 20
learning_rate: 1e-5
batch_size: 50 (train) / 32 (eval)
optimizer: adamw_torch
scheduler: cosine
warmup_ratio: 0.1
early_stopping: patience=3
generation:
  num_beams: 4
  no_repeat_ngram_size: 2
  max_length: 100
```

### í™˜ê²½
```
GPU: NVIDIA RTX 3090 24GB
CUDA: 12.2
transformers: 4.35.2
tokenizers: 0.15.2
accelerate: 0.25.0
```

### ê²°ê³¼

**ROUGE Scores:**
```
ROUGE-1 F1:  56.43%
ROUGE-2 F1:  36.65%
ROUGE-L F1:  47.75%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score: 46.9426
```

**ë¹„êµ:**
```
ê³µì‹ Baseline: 47.1244
ìš°ë¦¬ ê²°ê³¼:     46.9426
ì°¨ì´:          -0.18ì  (0.38%)
```

### íŒë‹¨
âœ… **ì„±ê³µ** - Baseline ì¬í˜„ ì™„ë£Œ

**ë¶„ì„:**
- ì˜¤ì°¨ 0.18ì ì€ ë¬´ì‹œ ê°€ëŠ¥í•œ ìˆ˜ì¤€ (0.4% ë¯¸ë§Œ)
- í™˜ê²½ ì°¨ì´ (ëœë¤ ì‹œë“œ, GPU ì—°ì‚° ìˆœì„œ ë“±)ë¡œ ì¸í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë³€ë™
- ì•ˆì •ì ì¸ ì¶œë°œì  í™•ë³´

### ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ ì§„í–‰:**
1. ì¦ê°• ë°ì´í„° ì‹¤í—˜ (Experiment #1)
   - ë°°ê²½: ë™ì¼ ëŒ€íšŒ ìˆ˜í–‰ íŒ€ì˜ Solar mini ì¦ê°• ì„±ê³µ ì‚¬ë¡€ í™•ì¸
   - ë°©ë²•: train_with_augmentation.csv (24,914 samples) ì‚¬ìš©
   - ëª©í‘œ: 48-49ì  ë‹¬ì„±

2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Experiment #2)
   - Learning rate: 1e-5 â†’ 5e-5
   - ëª©í‘œ: ì¶”ê°€ 0.5-1ì  ê°œì„ 

**ì£¼ì˜ì‚¬í•­:**
- í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½
- Dev/Test ê²©ì°¨ 5ì  ì´ë‚´ ìœ ì§€
- ë§¤ ì‹¤í—˜ë§ˆë‹¤ Test ì œì¶œë¡œ ê²€ì¦ (12íšŒ/ì¼ ì œí•œ)

---
---

## Experiment #1: ì¦ê°• ë°ì´í„° í•™ìŠµ (2ë°° ë°ì´í„°)

**ë‚ ì§œ**: 2025-10-12 23:15 - 23:48
**ë² ì´ìŠ¤**: Baseline (Experiment #0)

### ë³€ê²½ì‚¬í•­
- ë°ì´í„°: train.csv (12,457) â†’ train_with_augmentation.csv (24,914)
- 2ë°° ì¦ê°• (ì›ë³¸ + ì¦ê°•)

### ì„¤ì •
```yaml
model: digit82/kobart-summarization
data: train_with_augmentation.csv (24,914 samples)
epochs: 20
learning_rate: 1e-5
batch_size: 50 (train) / 32 (eval)
optimizer: adamw_torch
scheduler: cosine
warmup_ratio: 0.1
early_stopping: patience=3
```

### ì¦ê°• ë°ì´í„° í’ˆì§ˆ (EDA ê²°ê³¼)
- ROUGE-L F1 (ì›ë³¸ vs ì¦ê°•): 0.4299 (í‰ê· )
- Dialogue ê¸¸ì´: 406ì â†’ 544ì (+33.9%)
- Summary: ë™ì¼ (86ì)
- í† í”½ ë¶„í¬: ë™ì¼

### ê²°ê³¼

**Dev Set ROUGE (í•™ìŠµ ì¤‘ í‰ê°€)**:
```
Epoch 1:  R-1: 19.2% | R-2:  3.3% | R-L: 18.2%
Epoch 4:  R-1: 27.0% | R-2:  7.3% | R-L: 25.0% â† Best
Epoch 5:  R-1: 19.4% | R-2:  6.0% | R-L: 18.3%
Epoch 8:  R-1: 21.7% | R-2:  7.1% | R-L: 20.3% (Early stopping)
```

**ë¹„êµ**:
```
Baseline (Exp #0): Dev ROUGE-L 47.75% (ì¶”ì •)
Experiment #1:     Dev ROUGE-L 25.00% (Epoch 4)
ë³€í™”: -22.75%p âš ï¸
```

**Test Set**: ì œì¶œ ëŒ€ê¸° ì¤‘
- íŒŒì¼: submission_augmented_exp1.csv
- ìƒ˜í”Œ ìˆ˜: 499ê°œ âœ…
- í¬ë§· ê²€ì¦: í†µê³¼ âœ…

### íŒë‹¨
âš ï¸ **ì£¼ì˜ í•„ìš”**

**ë¶„ì„**:
1. Dev ROUGEê°€ Baseline ëŒ€ë¹„ í¬ê²Œ í•˜ë½ (47.75% â†’ 25.00%)
2. ì¦ê°• ë°ì´í„° í’ˆì§ˆì€ ì–‘í˜¸í–ˆìœ¼ë‚˜ (ROUGE-L 0.43) í•™ìŠµ íš¨ê³¼ëŠ” ë¯¸ë¯¸
3. Early stoppingì´ Epoch 8ì—ì„œ ì‘ë™ (patience=3)
4. ê°€ëŠ¥í•œ ì›ì¸:
   - ì¦ê°• dialogueì˜ ìŠ¤íƒ€ì¼ ì°¨ì´ (ë²ˆì—­íˆ¬ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)
   - ë°ì´í„° ì–‘ ì¦ê°€ë¡œ ì¸í•œ í•™ìŠµ ë¶ˆì¶©ë¶„
   - ì¦ê°• í’ˆì§ˆê³¼ í•™ìŠµ íš¨ê³¼ì˜ ë¶ˆì¼ì¹˜

**ì˜ˆìƒ**:
- Test ì ìˆ˜: 30-40ì  (Baseline 46.94ë³´ë‹¤ ë‚®ì„ ê°€ëŠ¥ì„± ë†’ìŒ)

### ë‹¤ìŒ ë‹¨ê³„
1. Test ì œì¶œ í›„ ì‹¤ì œ ì ìˆ˜ í™•ì¸
2. ì ìˆ˜ ë¶„ì„:
   - **40ì  ì´ìƒ**: ë¶€ë¶„ì  ì„±ê³µ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³ ë ¤
   - **30-40ì **: ì¦ê°• ë°©ë²• ì¬ê²€í†  í•„ìš”
   - **30ì  ë¯¸ë§Œ**: Baselineìœ¼ë¡œ ë¡¤ë°±, ë‹¤ë¥¸ ì ‘ê·¼ ì‹œë„


**Test Set ê²°ê³¼**: 42.7807ì  âŒ
- ROUGE-1: 52.43%
- ROUGE-2: 32.50%
- ROUGE-L: 43.41%
- **Baseline ëŒ€ë¹„: -4.16ì  (-8.9%)**

### ìµœì¢… íŒë‹¨
âŒ **ì‹¤íŒ¨ - Baselineìœ¼ë¡œ ë¡¤ë°±**

**ì‹¤íŒ¨ ì›ì¸ ë¶„ì„**:
1. **ì¦ê°• ë°ì´í„° ìŠ¤íƒ€ì¼ ë¬¸ì œ**
   - ì¦ê°•ì´ ë²ˆì—­íˆ¬ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë³€í™˜
   - ì›ë³¸ ë°ì´í„°ì˜ ë²ˆì—­íˆ¬ ìŠ¤íƒ€ì¼ê³¼ ë¶ˆì¼ì¹˜
   - ëª¨ë¸ì´ í˜¼ë€ì„ ê²ªìŒ

2. **ë°ì´í„° ì–‘ ì¦ê°€ì˜ ì—­íš¨ê³¼**
   - 2ë°° ë°ì´í„°(24,914ê°œ)ê°€ ì˜¤íˆë ¤ ë¶€ì •ì 
   - í•™ìŠµ ì‹œê°„ ë¶€ì¡± (Epoch 8ì—ì„œ ì¡°ê¸° ì¢…ë£Œ)
   - ì¦ê°• í’ˆì§ˆ(ROUGE 0.43)ì€ ê´œì°®ì•˜ìœ¼ë‚˜ ì‹¤ì œ íš¨ê³¼ëŠ” ìŒìˆ˜

3. **Dev/Test ê²©ì°¨**
   - Dev ROUGE-L: 0.25 (ë‚®ìŒ)
   - Test ROUGE-L: 0.43 (ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ)
   - Devì—ì„œ ì´ë¯¸ ë¬¸ì œ ì‹ í˜¸ ìˆì—ˆìŒ

### êµí›ˆ
- âœ… EDAë¡œ ì¦ê°• í’ˆì§ˆ í™•ì¸ (ROUGE 0.43)
- âŒ ì¦ê°• ë°ì´í„°ì˜ ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ë¶€ì¡±
- âŒ ë°ì´í„° ì¦ê°•ì´ ë§ŒëŠ¥ì€ ì•„ë‹˜
- âœ… "í•œ ë²ˆì— í•˜ë‚˜ì”©" ì›ì¹™ ì¤€ìˆ˜ (ì¦ê°•ë§Œ ë³€ê²½)

### ë‹¤ìŒ ì‹¤í—˜ ë°©í–¥
1. **Baseline ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** (ìš°ì„ )
   - Learning rate: 1e-5 â†’ 5e-5
   - ëª©í‘œ: +1~2ì  ê°œì„ 
   
2. **Longer training** (ì°¨ì„ )
   - Epochs: 20 â†’ 30
   - Early stopping patience: 3 â†’ 5
   
3. **ì¦ê°• ì¬ì‹œë„** (ë³´ë¥˜)
   - ë²ˆì—­íˆ¬ ìŠ¤íƒ€ì¼ ìœ ì§€í•˜ëŠ” ì¦ê°• ë°©ë²• ì—°êµ¬
   - ë˜ëŠ” ì¦ê°• ë¹„ìœ¨ ì¶•ì†Œ (50% í˜¼í•©)

---

## Experiment #2: í›„ì²˜ë¦¬ ê°œì„  (Post-processing v2)

**ë‚ ì§œ**: 2025-10-13
**ë² ì´ìŠ¤**: Baseline Modular (Experiment #0.1, 46.9526ì )

### ê°€ì„¤

ìƒì„±ëœ ìš”ì•½ë¬¸ì˜ í’ˆì§ˆ ì €í•˜ ìš”ì¸:
1. ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°±
2. ì¤‘ë³µëœ ë¬¸ì¥ ë°˜ë³µ
3. íŠ¹ìˆ˜ í† í° ì”ì—¬

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1.2ì  (ëª©í‘œ 47.5~48.2ì )
**ë¦¬ìŠ¤í¬**: âœ… Low (í›„ì²˜ë¦¬ë§Œ ë³€ê²½, ëª¨ë¸ ì¬í•™ìŠµ ë¶ˆí•„ìš”)

### ë³€ê²½ì‚¬í•­

**ê¸°ì¡´**: `postprocess_summaries()` - íŠ¹ìˆ˜ í† í°ë§Œ ì œê±°
```python
def postprocess_summaries(summaries, remove_tokens):
    cleaned = summaries.copy()
    for token in remove_tokens:
        cleaned = [s.replace(token, " ") for s in cleaned]
    return cleaned
```

**ê°œì„ **: `postprocess_summaries_v2()` - 3ë‹¨ê³„ ì²˜ë¦¬
```python
def postprocess_summaries_v2(summaries, remove_tokens):
    # 1ë‹¨ê³„: íŠ¹ìˆ˜ í† í° ì œê±°
    cleaned = summaries.copy()
    for token in remove_tokens:
        cleaned = [s.replace(token, " ") for s in cleaned]

    # 2ë‹¨ê³„: ê³µë°± ì •ê·œí™” (ì—°ì† ê³µë°± â†’ ë‹¨ì¼ ìŠ¤í˜ì´ìŠ¤)
    cleaned = [normalize_whitespace(s) for s in cleaned]

    # 3ë‹¨ê³„: ì¤‘ë³µ ë¬¸ì¥ ì œê±°
    cleaned = [remove_duplicate_sentences(s) for s in cleaned]

    return cleaned
```

**ì¶”ê°€ í•¨ìˆ˜**:
1. `normalize_whitespace()`: `re.sub(r'\s+', ' ', text).strip()`
2. `remove_duplicate_sentences()`: ë¬¸ì¥ ë¶€í˜¸(. ! ?)ë¡œ ë¶„ë¦¬ í›„ ì¤‘ë³µ ì œê±°

### ì„¤ì •

**Model**: checkpoint-1750 (ì¬í•™ìŠµ ì—†ìŒ)
```yaml
# ë³€ê²½ ì—†ìŒ (ì¶”ë¡  ë‹¨ê³„ë§Œ ìˆ˜ì •)
checkpoint: submission/checkpoint-1750
postprocessing: postprocess_summaries_v2  # ê¸°ì¡´: postprocess_summaries
```

### êµ¬í˜„

**íŒŒì¼ ìˆ˜ì •**:
- `scripts/inference_utils.py`: 3ê°œ í•¨ìˆ˜ ì¶”ê°€
- `code/test_postprocessing_v2.py`: í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ìƒì„±
- `code/run_exp2.py`: ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
- `code/exp2_postprocessing.ipynb`: ì¬í˜„ìš© ë…¸íŠ¸ë¶ ìƒì„±

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**:
```
âœ… PASS - Import
âœ… PASS - normalize_whitespace (4 passed, 0 failed)
âœ… PASS - remove_duplicate_sentences (4 passed, 0 failed)
âœ… PASS - postprocess_summaries_v2 (3 passed, 0 failed)
âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!
```

**ì¶”ë¡  ì‹¤í–‰**:
- ì¶œë ¥: `code/prediction/output_modular_v2.csv`
- ìƒ˜í”Œ ìˆ˜: 499ê°œ âœ…
- ë³€ê²½ìœ¨: 100% (ëª¨ë“  ìš”ì•½ë¬¸ì´ í›„ì²˜ë¦¬ë¨)
- Baselineê³¼ ë¹„êµ: 0% ì¼ì¹˜ (ëª¨ë“  ìš”ì•½ë¬¸ ë³€ê²½)

### ê²°ê³¼

**Test Set ì ìˆ˜**:
```
ROUGE-1 F1:  56.31%
ROUGE-2 F1:  36.65%
ROUGE-L F1:  48.00%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score: 46.9863
```

**ë¹„êµ**:
```
Baseline Modular: 46.9526
Experiment #2:    46.9863
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë³€í™”: +0.0337ì  (+0.07%)
```

### íŒë‹¨
âŒ **ì‹¤íŒ¨ (ë¡¤ë°±)** - ì‚¬ì‹¤ìƒ ë³€í™” ì—†ìŒ

**ë¶„ì„**:
1. **ì˜ˆìƒ**: +0.5~1.2ì  (47.5~48.2 ëª©í‘œ)
2. **ì‹¤ì œ**: +0.03ì  (ì˜¤ì°¨ ë²”ìœ„ ë‚´)
3. **ê²°ë¡ **: í›„ì²˜ë¦¬ ê°œì„ ì´ ì„±ëŠ¥ì— ê¸°ì—¬í•˜ì§€ ëª»í•¨

### ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

**ì£¼ìš” ì›ì¸: ëª¨ë¸ ì¶œë ¥ì´ ì´ë¯¸ ìµœì í™”ë˜ì–´ ìˆì—ˆìŒ** â­â­â­

**ì¦ê±°**:
1. KoBARTëŠ” ì´ë¯¸ well-trained summarization model
2. Baseline ì½”ë“œê°€ ìµœì†Œí•œì˜ í›„ì²˜ë¦¬ë§Œ í•˜ëŠ” ë°ì—ëŠ” ì´ìœ ê°€ ìˆì—ˆìŒ
3. "ê°œì„ "ì´ë¼ê³  ìƒê°í•œ ê²ƒì´ ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì˜ë„ë¥¼ ë³€ê²½í–ˆì„ ë¿

**ê¸°íƒ€ ê°€ëŠ¥í•œ ì›ì¸**:
- Test setì— ì¤‘ë³µ ë¬¸ì¥ì´ë‚˜ ê³¼ë„í•œ ê³µë°±ì´ ê±°ì˜ ì—†ì—ˆì„ ê°€ëŠ¥ì„±
- ê³µë°± ì •ê·œí™”/ì¤‘ë³µ ì œê±°ê°€ ì ìš©ë  ì¼€ì´ìŠ¤ê°€ ì ì—ˆìŒ
- Dev set ê²€ì¦ì„ í•˜ì§€ ì•Šì•„ íš¨ê³¼ë¥¼ ë¯¸ë¦¬ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨

### êµí›ˆ

1. **"ë‹¹ì—°íˆ ì¢‹ì„ ê²ƒ"ì´ë¼ëŠ” ê°€ì •ì€ ìœ„í—˜í•¨**
   - ì´ë¡ ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ ê°œì„  â‰  ì„±ëŠ¥ í–¥ìƒ
   - ì‹¤ì¦ì  ê²€ì¦ì´ í•„ìˆ˜

2. **Dev set ê²€ì¦ì˜ ì¤‘ìš”ì„±** âš ï¸
   - Test ì œì¶œ ì „ì— Dev setì—ì„œ ë¨¼ì € ê²€ì¦ í•„ìš”
   - 12íšŒ/ì¼ ì œì¶œ ì œí•œ ì ˆì•½

3. **Baselineì˜ ë‹¨ìˆœí•¨ì„ ì¡´ì¤‘**
   - ì£¼ìµœ ì¸¡ baselineì´ ìµœì†Œí•œì˜ í›„ì²˜ë¦¬ë§Œ í•˜ëŠ” ê²ƒì€ ì˜ë„ì 
   - ì„£ë¶€ë¥¸ "ê°œì„ "ë³´ë‹¤ëŠ” ëª¨ë¸ í•™ìŠµ ê°œì„ ì— ì§‘ì¤‘

4. **EDA ë¶„ì„ì˜ í•œê³„**
   - ì´ë¡ ì  ë¶„ì„ìœ¼ë¡œ ì˜ˆì¸¡í•œ íš¨ê³¼ê°€ í•­ìƒ ì‹¤í˜„ë˜ì§€ëŠ” ì•ŠìŒ
   - ë¹ ë¥¸ ì‹¤í—˜ê³¼ ê²€ì¦ì´ ë” ì¤‘ìš”

### ë‹¤ìŒ ë‹¨ê³„

**Exp #3: Learning Rate 2e-5** (ìµœìš°ì„ )
- ëª©í‘œ: +1~2ì  (48~49 ì˜ˆìƒ)
- ë°©ë²•: `config.yaml`ì—ì„œ `learning_rate: 1e-5 â†’ 2e-5`
- ê·¼ê±°:
  - LR íŠœë‹ì€ ì•ˆì „í•œ ì‹¤í—˜ (Low risk)
  - EDA ë¶„ì„ì—ì„œë„ ìµœìš°ì„  ê¶Œì¥
  - í›„ì²˜ë¦¬ ëŒ€ì‹  ëª¨ë¸ í•™ìŠµ ê°œì„ ì— ì§‘ì¤‘
- **ì¤‘ìš”**: ì´ë²ˆì—ëŠ” Dev set ê²€ì¦ ë¨¼ì € ìˆ˜í–‰!

---

## Experiment #3: Learning Rate 2e-5

**ë‚ ì§œ**: 2025-10-13
**ë² ì´ìŠ¤**: Baseline Modular (Experiment #0.1, 46.9526ì )

### ê°€ì„¤

Learning Rateë¥¼ 2ë°° ì¦ê°€ì‹œí‚¤ë©´:
1. ë” ë¹ ë¥¸ ìˆ˜ë ´
2. ë” ë†’ì€ validation ì„±ëŠ¥
3. ìµœì¢… Test ì ìˆ˜ í–¥ìƒ

**ì˜ˆìƒ íš¨ê³¼**: +1~2ì  (ëª©í‘œ 48~49ì )
**ë¦¬ìŠ¤í¬**: âœ… Low (ê³µì‹ ê¶Œì¥ ë²”ìœ„ ë‚´)

### ë³€ê²½ì‚¬í•­

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```yaml
learning_rate: 1e-5 â†’ 2e-5  # 2ë°° ì¦ê°€
epochs: 20
early_stopping_patience: 3
save_strategy: epoch
load_best_model_at_end: true
metric_for_best_model: eval_loss  # ê¸°ë³¸ê°’
```

**ì£¼ì˜ì‚¬í•­**:
- Dev set ê²€ì¦ì„ ë¨¼ì € ìˆ˜í–‰í•˜ê¸°ë¡œ ê²°ì • (Exp #2 êµí›ˆ)
- 3-phase validation ì „ëµ ìˆ˜ë¦½:
  1. Baseline Dev score í™•ì¸
  2. Exp #3 í•™ìŠµ í›„ Dev ë¹„êµ
  3. Dev ê°œì„  ì‹œì—ë§Œ Test ì œì¶œ

### í•™ìŠµ ê³¼ì •

**í•™ìŠµ ì‹œê°„**: ~14ë¶„ (7 epochs, early stopping)

**Epochë³„ ê²°ê³¼**:
```
Epoch  | Train Loss | Eval Loss | Dev ROUGE-1 | Dev ROUGE-2 | Dev ROUGE-L | Dev í‰ê· 
-------|------------|-----------|-------------|-------------|-------------|----------
  1    |   4.6448   |  1.0500   |   26.74%    |    6.70%    |   25.31%    |  19.58%
  2    |   0.6904   |  0.5443   |   35.23%    |   12.20%    |   33.35%    |  26.93%
  3    |   0.5322   |  0.5203   |   36.24%    |   13.06%    |   33.95%    |  27.75%
  4    |   0.4756   |  0.5152   |   36.22%    |   13.17%    |   34.00%    |  27.80% â† Best Loss
  5    |   0.4002   |  0.5155   |   36.96%    |   13.52%    |   34.64%    |  28.37% â† Best ROUGE
  6    |   0.3704   |  0.5205   |   36.70%    |   13.50%    |   34.61%    |  28.27%
  7    |   0.3000   |  0.5269   |   36.30%    |   13.49%    |   34.25%    |  28.01%
```

**Early Stopping**: Epoch 7ì—ì„œ ì¢…ë£Œ (patience=3)

**Best Checkpoints**:
- **checkpoint-1000** (Epoch 4): eval_loss ìµœì € (0.5152)
- **checkpoint-1750** (Epoch 7): Dev ROUGE ìµœê³  (28.37% at Epoch 5)

### Dev Set ê²€ì¦

**Baseline Dev**:
- ROUGE-1: 35.58%, ROUGE-2: 11.82%, ROUGE-L: 32.98%
- **í‰ê· : 26.79%**

#### v1: checkpoint-1750 (Epoch 7, í•™ìŠµ ì¢…ë£Œ checkpoint)

**Dev ROUGE** (ë…ë¦½ í‰ê°€):
- ROUGE-1: 36.33%, ROUGE-2: 12.71%, ROUGE-L: 33.75%
- **í‰ê· : 27.60%** (Baseline ëŒ€ë¹„ +0.81%p) âœ…

**íŒë‹¨**: Dev ê°œì„  í™•ì¸ â†’ Test ì œì¶œ ì§„í–‰

#### v2: checkpoint-1000 (Epoch 4, Best eval_loss)

**Dev ROUGE** (ë…ë¦½ í‰ê°€):
- ROUGE-1: 35.78%, ROUGE-2: 11.78%, ROUGE-L: 32.77%
- **í‰ê· : 26.78%** (Baseline ëŒ€ë¹„ -0.01%p) â‰ˆ Baseline

**íŠ¹ì§•**: LossëŠ” ìµœì €ì´ì§€ë§Œ Dev ROUGEëŠ” Baselineê³¼ ê±°ì˜ ë™ì¼

### ê²°ê³¼

#### v1 Results (checkpoint-1750)

**Test Set ì ìˆ˜**:
```
ROUGE-1 F1:  56.19%
ROUGE-2 F1:  36.32%
ROUGE-L F1:  47.57%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score: 46.6919
```

**ë¹„êµ**:
```
Baseline:      46.9526
Exp #3-v1:     46.6919
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë³€í™”: -0.2607ì  (-0.56%) âŒ
```

**Dev vs Test ê´´ë¦¬**:
- Dev: +0.81%p ê°œì„  âœ…
- Test: -0.26ì  í•˜ë½ âŒ

#### v2 Results (checkpoint-1000)

**ê°€ì„¤**: checkpoint-1750ì´ overfittingì´ë¼ë©´, checkpoint-1000 (best loss)ì´ ë” ë‚˜ì„ ê²ƒ

**Test Set ì ìˆ˜**:
```
ROUGE-1 F1:  55.93%
ROUGE-2 F1:  36.72%
ROUGE-L F1:  47.17%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score: 46.6089
```

**ë¹„êµ**:
```
Baseline:      46.9526
Exp #3-v2:     46.6089
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë³€í™”: -0.3437ì  (-0.73%) âŒâŒ
```

**ì¶©ê²©ì  ë°œê²¬**: checkpoint-1000ì´ ì˜¤íˆë ¤ **ë” ë‚˜ì¨**

### íŒë‹¨
âŒ **ì‹¤íŒ¨ (ì–‘ìª½ ëª¨ë‘ í•˜ë½)** - Baselineìœ¼ë¡œ ë¡¤ë°±

### ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

#### 1. Learning Rate 2e-5ê°€ ê·¼ë³¸ì ìœ¼ë¡œ ë¬¸ì œ â­â­â­

**ì¦ê±°**:
- **ëª¨ë“  checkpointì—ì„œ ì¼ê´€ëœ í•˜ë½**:
  - checkpoint-1750 (Best Dev ROUGE): -0.26ì 
  - checkpoint-1000 (Best Loss): -0.34ì  (ë” ë‚˜ì¨!)
- **checkpoint ì„ íƒê³¼ ë¬´ê´€**: ì–´ë–¤ checkpointë¥¼ ì¨ë„ Baselineë³´ë‹¤ ë‚˜ì¨
- **ê²°ë¡ **: LR 2e-5 ìì²´ê°€ ì˜ëª»ëœ ì„ íƒ

**ì›ì¸ ì¶”ì •**:
- LR 2e-5ëŠ” Baseline 1e-5ì˜ 2ë°°
- ë„ˆë¬´ ë†’ì€ LRë¡œ ì¸í•´ Test ë°ì´í„° ë¶„í¬ì—ì„œ **ì¼ë°˜í™” ì‹¤íŒ¨**
- Train/Devì—ëŠ” ë§ì§€ë§Œ Testì—ëŠ” ê³¼ì í•© (Dev/Test ë¶ˆì¼ì¹˜)

#### 2. checkpoint ì„ íƒì˜ ì—­ì„¤

**ì˜ˆìƒ**:
- Best eval_loss (ckpt-1000)ê°€ ë” ë‚˜ì„ ê²ƒ
- Best Dev ROUGE (ckpt-1750)ê°€ overfittingì¼ ê²ƒ

**ì‹¤ì œ**:
- checkpoint-1000ì´ **ì˜¤íˆë ¤ ë” ë‚˜ì¨** (-0.34 vs -0.26)
- checkpoint-1750ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚˜ìŒ

**êµí›ˆ**:
- **ì˜ëª»ëœ LRë¡œëŠ” ì–´ë–¤ checkpointë„ ì¢‹ì§€ ì•ŠìŒ**
- checkpoint ì„ íƒ < LR ì„ íƒ
- Best loss â‰  Best Test score

#### 3. Dev/Test ê´´ë¦¬ ì‹¬í™”

**Baseline Dev/Test ê²©ì°¨**:
- Dev: 26.79%
- Test: 46.95%
- **ê²©ì°¨: 20.16%p** (ë§¤ìš° í¼)

**Exp #3 Dev/Test ê´´ë¦¬**:
- v1 Dev +0.81%p â†’ Test -0.26ì 
- v2 Dev -0.01%p â†’ Test -0.34ì 

**ê²°ë¡ **: **Dev ì ìˆ˜ë¡œ Test ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥**

#### 4. 3-phase validationì˜ ì‹¤íŒ¨

**Phase 2 íŒë‹¨ ì°©ì˜¤**:
- Dev +0.81%p ê°œì„  ë³´ê³  "ì„±ê³µ"ìœ¼ë¡œ íŒë‹¨
- Test ì œì¶œ ì§„í–‰ ê²°ì •
- í•˜ì§€ë§Œ Testì—ì„œëŠ” -0.26ì  í•˜ë½

**Phase 2 ì¬ê²€ì¦ ì°©ì˜¤**:
- "checkpoint-1750ì´ overfit"ì´ë¼ê³  ê°€ì •
- checkpoint-1000ìœ¼ë¡œ ì¬ì‹œë„
- í•˜ì§€ë§Œ ì˜¤íˆë ¤ ë” ë‚˜ì¨ (-0.34ì )

**ê·¼ë³¸ ë¬¸ì œ**: Dev ê¸°ì¤€ ìì²´ê°€ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ

### êµí›ˆ

#### 1. Learning Rate íŠœë‹ì˜ í•¨ì • âš ï¸

**ì˜ëª»ëœ ê°€ì •**:
- "LR 2ë°° = í•™ìŠµ 2ë°° ë¹ ë¦„ = ì„±ëŠ¥ í–¥ìƒ"
- "ê³µì‹ ê¶Œì¥ ë²”ìœ„ ë‚´ = ì•ˆì „í•¨"

**í˜„ì‹¤**:
- LR 2e-5ëŠ” ì´ ë°ì´í„°/ëª¨ë¸ì—ëŠ” **ê³¼ë„í•¨**
- Baseline 1e-5ê°€ ì´ë¯¸ ìµœì 
- 2ë°° ì¦ê°€ì˜ ì˜í–¥ì´ ì˜ˆìƒë³´ë‹¤ í¼

**êµí›ˆ**:
- LR íŠœë‹ì€ ì˜ˆìƒë³´ë‹¤ **í›¨ì”¬ ë¯¼ê°**
- Baseline í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—ëŠ” ì´ìœ ê°€ ìˆìŒ
- ë³´ìˆ˜ì  ì ‘ê·¼ í•„ìš” (1e-5 â†’ 1.5e-5 ë“±)

#### 2. checkpoint ì„ íƒë³´ë‹¤ Learning Rateê°€ ì¤‘ìš”

**ë°œê²¬**:
- ê°™ì€ LRë¡œ í•™ìŠµí•œ ëª¨ë“  checkpointê°€ ë‚˜ì¨
- checkpoint ì„ íƒìœ¼ë¡œëŠ” ë¬¸ì œ í•´ê²° ë¶ˆê°€
- **ê·¼ë³¸ ì›ì¸(LR)ì„ ê³ ì³ì•¼ í•¨**

**êµí›ˆ**:
- checkpoint ìµœì í™”ëŠ” 2ì°¨ ë¬¸ì œ
- ë¨¼ì € ì˜¬ë°”ë¥¸ LR ì°¾ê¸°

#### 3. Dev ì ìˆ˜ì˜ ì‹ ë¢° í•œê³„

**Dev/Test ê²©ì°¨**: 20.16%p (ë§¤ìš° ë¹„ì •ìƒì )

**í˜„ìƒ**:
- Dev +0.81%p â†’ Test -0.26ì 
- Dev -0.01%p â†’ Test -0.34ì 

**êµí›ˆ**:
- **Dev ì ìˆ˜ë¡œ Test ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥**
- DevëŠ” ì°¸ê³ ë§Œ, Test ì œì¶œì´ ìœ ì¼í•œ ì§„ì‹¤
- ì œì¶œ íšŸìˆ˜(12/day) ì•„ê»´ ì“°ê¸°

#### 4. "ë‹¹ì—°íˆ ì¢‹ì„ ê²ƒ" ê°€ì •ì˜ ìœ„í—˜

**Exp #2**: "í›„ì²˜ë¦¬ ê°œì„ ì€ ë‹¹ì—°íˆ ì¢‹ì„ ê²ƒ" â†’ +0.03ì  (ë¬´ì˜ë¯¸)
**Exp #3**: "LR 2ë°°ëŠ” ë‹¹ì—°íˆ ì¢‹ì„ ê²ƒ" â†’ -0.26ì  (í•˜ë½)

**êµí›ˆ**:
- ì´ë¡ ì  ê°œì„  â‰  ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒ
- ëª¨ë“  ë³€ê²½ì€ ê²€ì¦ í•„ìš”
- Baseline ì¡´ì¤‘

### ë‹¤ìŒ ë‹¨ê³„

#### âŒ ê±´ë„ˆë›°ê¸°

**Exp #4: Learning Rate 3e-5**
- LR íŠœë‹ ë°©í–¥ì´ ì˜ëª»ë¨ í™•ì¸
- ë” ë†’ì€ LRì€ ë” ë‚˜ì  ê²ƒ

**Exp #5: Learning Rate 5e-5**
- ë™ì¼í•œ ì´ìœ ë¡œ ê±´ë„ˆë›°ê¸°

#### âœ… ìƒˆë¡œìš´ ë°©í–¥

**1. Epochs ì—°ì¥** (ìµœìš°ì„  ì¶”ì²œ)
- `num_train_epochs: 20 â†’ 30`
- ë¦¬ìŠ¤í¬: âœ… Low (early stopping ìœ ì§€)
- ì˜ˆìƒ: +0.5~1ì 
- ê·¼ê±°: LRì€ ê·¸ëŒ€ë¡œ, í•™ìŠµ ì‹œê°„ë§Œ ì¦ê°€

**2. Warmup Steps ì¡°ì •**
- `warmup_ratio: 0.1 â†’ 0.15` ë˜ëŠ” `warmup_steps: 20 â†’ 50`
- ë¦¬ìŠ¤í¬: âœ… Low
- ì˜ˆìƒ: +0.3~0.7ì 

**3. Special Tokens ì¶”ê°€**
- Time Token (`#Time#`)
- Money Token (`#Money#`)
- ë¦¬ìŠ¤í¬: âš ï¸ Medium (ì¬ì „ì²˜ë¦¬ í•„ìš”)
- ì˜ˆìƒ: +0.5~1.5ì 

### ê²°ë¡ 

**Exp #3 ì¢…í•© í‰ê°€**:
- âŒ ë‘ checkpoint ëª¨ë‘ ì‹¤íŒ¨
- âŒ LR 2e-5 ìì²´ê°€ ë¬¸ì œ
- âŒ Dev ì ìˆ˜ ì‹ ë¢°ë„ ë‚®ìŒ ì¬í™•ì¸
- âœ… ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ pivot ê²°ì •

**Current Best**: 46.9526 (Baseline Modular) ìœ ì§€

**ì œì¶œ íšŸìˆ˜ ì‚¬ìš©**: 8/12 (ë‚¨ì€ íšŸìˆ˜: 4íšŒ)

---

## Experiment #4: ê¸¸ì´ ì •ê·œí™” + Max Length 768 + ë©”ëª¨ë¦¬ ìµœì í™”

**ë‚ ì§œ**: 2025-10-14
**ë² ì´ìŠ¤**: Baseline Modular (Experiment #0.1, 46.9526ì )

### ê°€ì„¤

1. **encoder_max_len ì¦ê°€** (512 â†’ 768): ê¸´ ëŒ€í™” ì²˜ë¦¬ ê°œì„ 
2. **GNMT ê¸¸ì´ ì •ê·œí™”** (length_penalty=0.6): ì ì ˆí•œ ìš”ì•½ ê¸¸ì´ ìœ ë„
3. **ë©”ëª¨ë¦¬ ìµœì í™”**: OOM ë¬¸ì œ í•´ê²°

**ì˜ˆìƒ íš¨ê³¼**: +1~2ì  (ëª©í‘œ 48~49ì )
**ë¦¬ìŠ¤í¬**: âš ï¸ Medium (OOM ê°€ëŠ¥ì„±)

### ë³€ê²½ì‚¬í•­

#### 1. í† í¬ë‚˜ì´ì € ì„¤ì •
```yaml
encoder_max_len: 512 â†’ 768  # +50% ì¦ê°€
decoder_max_len: 100  # ìœ ì§€
```

#### 2. Generation íŒŒë¼ë¯¸í„°
```yaml
length_penalty: 0.6  # GNMT ê¸¸ì´ ì •ê·œí™” ì¶”ê°€
num_beams: 4  # ìœ ì§€
no_repeat_ngram_size: 2  # ìœ ì§€
```

#### 3. ë©”ëª¨ë¦¬ ìµœì í™” (Tier 1)
```yaml
# OOM í•´ê²° ì „ëµ
per_device_train_batch_size: 50 â†’ 24  # -52% (ë©”ëª¨ë¦¬ 2.08ë°° ì ˆê°)
gradient_accumulation_steps: 1 â†’ 3  # Effective batch: 24Ã—3=72
gradient_checkpointing: true  # ë©”ëª¨ë¦¬ ~30% ì ˆê°
per_device_eval_batch_size: 32 â†’ 24  # í‰ê°€ ë©”ëª¨ë¦¬ ì ˆì•½
```

**ë©”ëª¨ë¦¬ ì ˆê° íš¨ê³¼**: ~60% (OOM ì™„ì „ í•´ê²°)

### í•™ìŠµ ê³¼ì •

**í•™ìŠµ ì‹œê°„**: 1751ì´ˆ (~29ë¶„)
**Total Epochs**: 10 (Early stopping at Epoch 10)

**Epochë³„ Dev ROUGE**:
```
Epoch  | Train Loss | Eval Loss | Dev ROUGE-1 | Dev ROUGE-2 | Dev ROUGE-L
-------|------------|-----------|-------------|-------------|-------------
  1    |   6.157    |  3.214    |   6.13%     |   1.14%     |   5.93%
  2    |   1.873    |  0.688    |  28.90%     |   7.80%     |  27.54%
  3    |   0.633    |  0.550    |  34.36%     |  11.00%     |  32.15%
  4    |   0.552    |  0.534    |  34.94%     |  11.84%     |  32.84%
  5    |   0.519    |  0.527    |  35.69%     |  12.43%     |  33.66%
  6    |   0.493    |  0.522    |  36.18%     |  12.88%     |  33.95%
  7    |   0.476    |  0.520    |  36.16%     |  12.71%     |  33.72%
  8    |   0.459    |  0.519    |  36.34%     |  13.33%     |  34.25%
  9    |   0.443    |  0.520    |  36.80%     |  13.42%     |  34.29% â† Best
 10    |   0.432    |  0.521    |  36.73%     |  13.49%     |  34.67%
```

**Best Checkpoint**: checkpoint-1733 (Epoch 9)
**Train samples/sec**: 142.27

### ê²°ê³¼

**Test Set ì ìˆ˜**:
```
ROUGE-1 F1:  56.71%  (Baseline: 56.31% | +0.40%p)
ROUGE-2 F1:  37.22%  (Baseline: 36.65% | +0.57%p)
ROUGE-L F1:  48.39%  (Baseline: 47.75% | +0.64%p)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score: 47.4421
```

**ë¹„êµ**:
```
Baseline Modular: 46.9526
Experiment #4:    47.4421
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ë³€í™”: +0.4895ì  (+1.04%) âœ…
```

### íŒë‹¨
âœ… **ì„±ê³µ** - ì†Œí­ ê°œì„  í™•ì¸

**ë¶„ì„**:
1. **ì˜ˆìƒ**: +1~2ì  (48~49 ëª©í‘œ)
2. **ì‹¤ì œ**: +0.49ì  (47.44)
3. **í‰ê°€**: ëª©í‘œ ë¯¸ë‹¬ì´ì§€ë§Œ **ë°©í–¥ì€ ì˜¬ë°”ë¦„**

**ì„¸ë¶€ ë¶„ì„**:
- ëª¨ë“  ROUGE ì§€í‘œì—ì„œ ì¼ê´€ëœ ìƒìŠ¹ (+0.40~0.64%p)
- Dev ROUGEë„ ê°œì„  (35.58% â†’ 36.80%)
- OOM ë¬¸ì œ ì™„ì „ í•´ê²°í•˜ë©´ì„œë„ ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±

### ì„±ê³µ ìš”ì¸

#### 1. ë©”ëª¨ë¦¬ ìµœì í™”ì˜ ì„±ê³µ â­â­â­

**ë¬¸ì œ**: encoder_max_len=768ì—ì„œ OOM ë°œìƒ
```
torch.OutOfMemoryError: CUDA out of memory
GPU 22.05 GiB / 23.69 GiB (ê±°ì˜ ê°€ë“)
```

**í•´ê²°ì±… (Tier 1 ìµœì í™”)**:
- Gradient Checkpointing: ë©”ëª¨ë¦¬ 30% ì ˆê°
- Batch size ê°ì†Œ (50â†’24): ë©”ëª¨ë¦¬ 2.08ë°° ì ˆê°
- Gradient Accumulation (Ã—3): Effective batch 72 ìœ ì§€
- **ì´ ë©”ëª¨ë¦¬ ì ˆê°**: ~60%

**ê²°ê³¼**: OOM ì™„ì „ í•´ê²° + í•™ìŠµ ì‹œê°„ 29ë¶„ (ì˜ˆìƒ 50ë¶„ë³´ë‹¤ 21ë¶„ ë‹¨ì¶•!)

#### 2. ê¸¸ì´ ì •ê·œí™”ì˜ ê¸ì •ì  íš¨ê³¼

**ROUGE-L ê°œì„ **: 47.75% â†’ 48.39% (+0.64%p, ê°€ì¥ í° ê°œì„ )
- GNMT length_penalty=0.6ì´ ì ì ˆí•œ ìš”ì•½ ê¸¸ì´ ìœ ë„
- ê¸´ ëŒ€í™”(768 tokens)ì—ì„œ ì •ë³´ ì†ì‹¤ ê°ì†Œ

#### 3. ì•ˆì •ì ì¸ í•™ìŠµ

- Early stopping at Epoch 10 (ê³¼ì í•© ë°©ì§€)
- Dev/Test ê´´ë¦¬ ì™„í™” (Dev 36.80% â†’ Test 47.44%)

### í•œê³„ ë° ê°œì„  ë°©í–¥

#### 1. ëª©í‘œ ë¯¸ë‹¬ (+0.49 < +1~2)

**ì›ì¸ ì¶”ì •**:
- encoder_max_len=768ì´ ëª¨ë“  ìƒ˜í”Œì— ì ìš©ë˜ì§€ ì•ŠìŒ
  - 512 tokens ì´í•˜ ìƒ˜í”Œì€ í˜œíƒ ì—†ìŒ
  - ê¸´ ëŒ€í™” ë¹„ìœ¨ì´ ì˜ˆìƒë³´ë‹¤ ë‚®ì•˜ì„ ê°€ëŠ¥ì„±
- length_penalty=0.6ì´ ìµœì ê°’ì´ ì•„ë‹ ìˆ˜ ìˆìŒ
  - 0.5, 0.7, 0.8 ë“± ì¶”ê°€ ì‹¤í—˜ í•„ìš”

#### 2. ë©”ëª¨ë¦¬ ìµœì í™”ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„

**Batch size ê°ì†Œì˜ ì˜í–¥**:
- Effective batch: 50 â†’ 72 (ìœ ì§€í–ˆì§€ë§Œ)
- Gradient accumulationìœ¼ë¡œ ì¸í•œ ì•½ê°„ì˜ ë¶ˆì•ˆì •ì„±
- í•™ìŠµ ì‹œê°„ì€ ì˜¤íˆë ¤ ë‹¨ì¶•ë¨ (29ë¶„)

#### 3. ì¶”ê°€ ìµœì í™” ì—¬ì§€

**ì ìš© ê°€ëŠ¥í•œ Tier 2 ìµœì í™”**:
- FlashAttention-2: ë©”ëª¨ë¦¬ 10-20Ã— ì ˆê°, ì†ë„ 2-3Ã— í–¥ìƒ
- 8-bit Optimizer: ì˜µí‹°ë§ˆì´ì € ë©”ëª¨ë¦¬ 75% ì ˆê°
- ë™ì  íŒ¨ë”©: ë¶ˆí•„ìš”í•œ íŒ¨ë”© ì œê±°

### êµí›ˆ

#### 1. Tier 1 ìµœì í™”ì˜ íš¨ê³¼ì„± â­â­â­

**Polars ëŒ€ì‹  ì„ íƒí•œ ì „ëµ**:
- Gradient Checkpointing + Batch size ì¡°ì •
- ì„¤ì¹˜ ë¶ˆí•„ìš”, ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
- ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ ì ˆê° (60%)
- FlashAttention-2ëŠ” ë‚˜ì¤‘ìœ¼ë¡œ ìœ ë³´

**êµí›ˆ**:
- "ì €ë¹„ìš© ë¨¼ì €" ì „ëµì˜ ì„±ê³µ
- ë³µì¡í•œ ìµœì í™” ì „ì— ê°„ë‹¨í•œ ê²ƒë¶€í„°
- ROI ì¤‘ì‹¬ ì˜ì‚¬ê²°ì •

#### 2. ë°©í–¥ì„±ì˜ ì¤‘ìš”ì„±

**Exp #2**: í›„ì²˜ë¦¬ (+0.03ì , ë¬´ì˜ë¯¸)
**Exp #3**: LR 2e-5 (-0.26ì , í•˜ë½)
**Exp #4**: ê¸¸ì´ ì •ê·œí™” + Max Length (+0.49ì , ìƒìŠ¹) âœ…

**êµí›ˆ**:
- ì˜ëª»ëœ ë°©í–¥ì—ì„œì˜ ìµœì í™”ëŠ” ë¬´ì˜ë¯¸
- ì˜¬ë°”ë¥¸ ë°©í–¥ì—ì„œì˜ ì†Œí­ ê°œì„ ì´ ë” ê°€ì¹˜ìˆìŒ
- Exp #4ì˜ +0.49ì  > Exp #2ì˜ +0.03ì 

#### 3. ì „ë¬¸ê°€ ì œì•ˆì˜ ê°€ì¹˜

**ì ìš©í•œ ì „ë¬¸ê°€ ì œì•ˆ**:
- ì¹´ë§ˆë„ íƒ„ì§€ë¡œ (Data Analyst): Gradient Checkpointing, ë™ì  íŒ¨ë”©
- ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸  (Creative Strategist): FlashAttention-2 (ë³´ë¥˜)
- ì¹´ë§ˆë„ ë„¤ì¦ˆì½” (Practitioner): AMP, Batch size ì¡°ì •

**êµí›ˆ**:
- ì „ë¬¸ê°€ ìš°ì„ ìˆœìœ„ê°€ ì •í™•í–ˆìŒ (Tier 1 ë¨¼ì €)
- FlashAttention-2ëŠ” Phase 2ì—ì„œ ê²€í†  ê°€ëŠ¥

### ë‹¤ìŒ ë‹¨ê³„

#### âœ… ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (Low-hanging fruit)

**1. Length Penalty íŠœë‹** (ìµœìš°ì„  ì¶”ì²œ)
- `length_penalty: 0.6 â†’ 0.5, 0.7, 0.8` ì‹¤í—˜
- ì˜ˆìƒ: +0.3~0.7ì 
- ë¦¬ìŠ¤í¬: âœ… Low (ì¶”ë¡ ë§Œ ë³€ê²½)
- ì†Œìš”: ê° 10ë¶„

**2. Warmup Steps ì¡°ì •**
- `warmup_ratio: 0.1 â†’ 0.15` ë˜ëŠ” `warmup_steps: 20 â†’ 50`
- ì˜ˆìƒ: +0.3~0.5ì 
- ë¦¬ìŠ¤í¬: âœ… Low

**3. Epochs ì—°ì¥**
- `num_train_epochs: 20 â†’ 25 or 30`
- ì˜ˆìƒ: +0.5~1ì 
- ë¦¬ìŠ¤í¬: âœ… Low (early stopping ìœ ì§€)

#### ğŸŸ¡ ì¤‘ê¸° ê²€í†  (Medium effort)

**4. Special Tokens ì¶”ê°€**
- Time Token (`#Time#`), Money Token (`#Money#`)
- ì˜ˆìƒ: +0.5~1.5ì 
- ë¦¬ìŠ¤í¬: âš ï¸ Medium (ì¬ì „ì²˜ë¦¬)

**5. FlashAttention-2**
- ë©”ëª¨ë¦¬ 10-20Ã— ì ˆê°, ì†ë„ 2-3Ã— í–¥ìƒ
- encoder_max_lenì„ 1024ê¹Œì§€ í™•ì¥ ê°€ëŠ¥
- ì„¤ì¹˜ ì‹œê°„: 10-20ë¶„

#### âŒ ê±´ë„ˆë›°ê¸°

**6. Learning Rate ì¦ê°€**
- Exp #3ì—ì„œ LR 2e-5 ì‹¤íŒ¨ í™•ì¸
- Baseline LR 1e-5ê°€ ì´ë¯¸ ìµœì 

### ê²°ë¡ 

**Exp #4 ì¢…í•© í‰ê°€**:
- âœ… OOM ë¬¸ì œ ì™„ì „ í•´ê²°
- âœ… ì„±ëŠ¥ í–¥ìƒ (+0.49ì , +1.04%)
- âœ… ì˜¬ë°”ë¥¸ ë°©í–¥ì„± í™•ì¸
- âš ï¸ ëª©í‘œ ë¯¸ë‹¬ (48~49ì  ì˜ˆìƒ â†’ 47.44ì  ì‹¤ì œ)

**Current Best**: **47.4421** (Exp #4)

**ì œì¶œ íšŸìˆ˜ ì‚¬ìš©**: 9/12 (ë‚¨ì€ íšŸìˆ˜: 3íšŒ)

---
