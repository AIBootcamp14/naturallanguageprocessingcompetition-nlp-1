2025-10-13 13:07:33 | >> í‘œì¤€ ì¶œë ¥ ë° ì˜¤ë¥˜ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ë¦¬ë””ë ‰ì…˜ ì‹œì‘
2025-10-13 13:07:36 | ğŸ“Š FULL ëª¨ë“œ ì‹¤í–‰ ì¤‘...
2025-10-13 13:07:36 | ============================================================
2025-10-13 13:07:36 | = FULL PIPELINE ì‹¤í–‰ ì‹œì‘
2025-10-13 13:07:36 | =ëŒ€ìƒ ëª¨ë¸: llama-3.2-korean-3b
2025-10-13 13:07:36 | =ì•™ìƒë¸” ì•™ìƒë¸” ì „ëµ: weighted_avg
2025-10-13 13:07:36 | = TTA ì‚¬ìš©: True
2025-10-13 13:07:36 | ============================================================
2025-10-13 13:07:36 | [1/6] ë°ì´í„° ë¡œë”©...
2025-10-13 13:07:36 | âœ… í•™ìŠµ ë°ì´í„°: 12457ê°œ
2025-10-13 13:07:36 | âœ… ê²€ì¦ ë°ì´í„°: 499ê°œ
2025-10-13 13:07:36 | âš™ï¸ max_train_samples ì ìš©: í•™ìŠµ ë°ì´í„° 3000ê°œë¡œ ì œí•œ
2025-10-13 13:07:36 | [2/6] ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ (1 ëª¨ë¸)...
2025-10-13 13:07:36 | ==================================================
2025-10-13 13:07:36 | ëª¨ë¸ 1/1: llama-3.2-korean-3b
2025-10-13 13:07:36 | ==================================================
2025-10-13 13:07:36 | ëª¨ë¸ íƒ€ì…: causal_lm
2025-10-13 13:07:36 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-13 13:07:36 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-13 13:07:36 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-13 13:07:37 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-13 13:07:39 | Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.82s/it]
2025-10-13 13:07:40 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.64s/it]
2025-10-13 13:07:41 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-13 13:07:41 | íŒ¨ë”© í† í° ì„¤ì •: <|eot_id|>
2025-10-13 13:07:41 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-13 13:07:41 | ğŸ” ìë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-13 13:07:42 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-13 13:07:42 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 24,313,856 (0.75%)
2025-10-13 13:07:42 | ì „ì²´ íŒŒë¼ë¯¸í„°: 3,237,063,680
2025-10-13 13:07:42 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-13 13:07:42 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-13 13:07:42 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-13 13:07:42 | ============================================================
2025-10-13 13:07:42 | ëª¨ë¸ í•™ìŠµ ì‹œì‘
2025-10-13 13:07:42 | ============================================================
2025-10-13 13:07:42 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-13 13:07:43 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-13 13:07:43 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-13 13:07:43 | wandb: setting up run 1hr1hl1n
2025-10-13 13:07:44 | wandb: Tracking run with wandb version 0.22.2
2025-10-13 13:07:44 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251013_130743-1hr1hl1n
wandb: Run `wandb offline` to turn off syncing.
2025-10-13 13:07:44 | wandb: Syncing run 1013-1307-llama_3.2_3b_qlora
2025-10-13 13:07:44 | wandb: â­ï¸ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-13 13:07:44 | wandb: ğŸš€ View run at https://wandb.ai/ieyeppo/nlp-competition/runs/1hr1hl1n
2025-10-13 13:07:44 | wandb: Detected [openai] in use.
2025-10-13 13:07:44 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-13 13:07:44 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-13 13:07:44 | ğŸ“‹ ì‹¤í—˜ëª…: 1013-1307-llama_3.2_3b_qlora
2025-10-13 13:07:44 | ğŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/1hr1hl1n
2025-10-13 13:07:44 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 13:07:44 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 13:07:44 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-13 13:07:44 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-13 13:07:44 | 0%|          | 0/750 [00:00<?, ?it/s]
2025-10-13 13:07:57 | 1%|          | 4/750 [00:12<40:22,  3.25s/it]
2025-10-13 13:08:14 | {'loss': 1.5799, 'grad_norm': 2.6425631046295166, 'learning_rate': 3.6e-07, 'epoch': 0.03}
2025-10-13 13:08:14 | 1%|â–         | 10/750 [00:30<35:44,  2.90s/it]
2025-10-13 13:08:20 | 2%|â–         | 12/750 [00:36<36:04,  2.93s/it]
2025-10-13 13:08:40 | 3%|â–         | 19/750 [00:56<34:02,  2.79s/it]
2025-10-13 13:08:43 | {'loss': 1.5259, 'grad_norm': 1.9275765419006348, 'learning_rate': 7.6e-07, 'epoch': 0.05}
2025-10-13 13:08:43 | 3%|â–         | 20/750 [00:58<33:41,  2.77s/it]
2025-10-13 13:09:03 | 4%|â–         | 27/750 [01:19<37:07,  3.08s/it]
2025-10-13 13:09:12 | {'loss': 1.628, 'grad_norm': 2.0326173305511475, 'learning_rate': 1.1600000000000001e-06, 'epoch': 0.08}
2025-10-13 13:09:12 | 4%|â–         | 30/750 [01:27<34:18,  2.86s/it]
2025-10-13 13:09:23 | 5%|â–         | 34/750 [01:38<33:50,  2.84s/it]
2025-10-13 13:09:41 | {'loss': 1.5471, 'grad_norm': 2.0455472469329834, 'learning_rate': 1.56e-06, 'epoch': 0.11}
2025-10-13 13:09:41 | 5%|â–Œ         | 40/750 [01:56<34:32,  2.92s/it]
2025-10-13 13:09:46 | 6%|â–Œ         | 42/750 [02:02<32:48,  2.78s/it]
2025-10-13 13:10:05 | 7%|â–‹         | 49/750 [02:20<31:51,  2.73s/it]
2025-10-13 13:10:09 | {'loss': 1.6197, 'grad_norm': 2.012053966522217, 'learning_rate': 1.9600000000000003e-06, 'epoch': 0.13}
2025-10-13 13:10:09 | 7%|â–‹         | 50/750 [02:24<36:13,  3.11s/it]
2025-10-13 13:10:29 | 8%|â–Š         | 57/750 [02:45<35:11,  3.05s/it]
2025-10-13 13:10:37 | {'loss': 1.535, 'grad_norm': 1.6359083652496338, 'learning_rate': 2.3600000000000003e-06, 'epoch': 0.16}
2025-10-13 13:10:37 | 8%|â–Š         | 60/750 [02:53<32:53,  2.86s/it]
2025-10-13 13:10:49 | 9%|â–Š         | 64/750 [03:05<33:25,  2.92s/it]
2025-10-13 13:11:06 | {'loss': 1.4934, 'grad_norm': 1.6054855585098267, 'learning_rate': 2.7600000000000003e-06, 'epoch': 0.19}
2025-10-13 13:11:06 | 9%|â–‰         | 70/750 [03:21<30:39,  2.70s/it]
2025-10-13 13:11:11 | 10%|â–‰         | 72/750 [03:27<31:20,  2.77s/it]
2025-10-13 13:11:33 | 11%|â–ˆ         | 79/750 [03:49<33:44,  3.02s/it]
2025-10-13 13:11:36 | {'loss': 1.443, 'grad_norm': 1.5794692039489746, 'learning_rate': 3.1600000000000002e-06, 'epoch': 0.21}
2025-10-13 13:11:36 | 11%|â–ˆ         | 80/750 [03:52<33:35,  3.01s/it]
