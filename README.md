# ì¼ìƒ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 

[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/HS6nBbT4)

**ëŒ€íšŒ**: NIKLuge 2024 - ì¼ìƒ ëŒ€í™” ìš”ì•½
**í˜„ì¬ Best Score**: 46.94ì  (Baseline)
**ëª©í‘œ**: 50ì  ì´ìƒ ë‹¬ì„±

---

## í”„ë¡œì íŠ¸ ì†Œê°œ

í•œêµ­ì–´ ì¼ìƒ ëŒ€í™”ë¥¼ ì…ë ¥ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. KoBART ê¸°ë°˜ Seq2Seq ëª¨ë¸ì„ baselineìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ì ì§„ì ì¸ ê°œì„ ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- âœ… **ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬**: í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½, Test setìœ¼ë¡œë§Œ ê²€ì¦
- âœ… **ì½”ë“œ ëª¨ë“ˆí™”**: 7ê°œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ Python ëª¨ë“ˆ (1,745ì¤„)
- âœ… **ì™„ë²½í•œ ì¬í˜„ì„±**: ëª¨ë“  ì‹¤í—˜ì„ Jupyter Notebookìœ¼ë¡œ ê¸°ë¡
- âœ… **ìë™í™”ëœ ê²€ì¦**: CSV í¬ë§· ê²€ì¦, ROUGE ê³„ì‚°

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
naturallanguageprocessingcompetition-nlp-1/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ baseline.ipynb                   # ì›ë³¸ Baseline (47.12ì )
â”‚   â”œâ”€â”€ baseline_modular.ipynb          # ëª¨ë“ˆí™” Baseline âœ¨
â”‚   â”œâ”€â”€ config.yaml                      # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scripts/                             # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ âœ¨
â”‚   â”œâ”€â”€ utils.py                        # Config, ì‹œë“œ, CSV ê²€ì¦
â”‚   â”œâ”€â”€ data_loader.py                  # ë°ì´í„° ë¡œë”©, Preprocess
â”‚   â”œâ”€â”€ tokenizer_utils.py              # Tokenizer ì„¤ì •
â”‚   â”œâ”€â”€ model_utils.py                  # ëª¨ë¸ ë¡œë”© (í•™ìŠµ/ì¶”ë¡ )
â”‚   â”œâ”€â”€ dataset.py                      # PyTorch Dataset í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ trainer_utils.py                # Trainer, compute_metrics
â”‚   â””â”€â”€ inference_utils.py              # ì¶”ë¡  íŒŒì´í”„ë¼ì¸
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                       # í•™ìŠµ ë°ì´í„° (12,457ê°œ)
â”‚   â”œâ”€â”€ dev.csv                         # ê²€ì¦ ë°ì´í„° (499ê°œ)
â”‚   â””â”€â”€ test.csv                        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (499ê°œ)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiment_logs.md              # ì‹¤í—˜ ê¸°ë¡
â”‚   â”œâ”€â”€ RESTART_GUIDE.md                # ì¬ì‹œì‘ ì „ëµ
â”‚   â””â”€â”€ baseline_code_summary.md        # ì½”ë“œ ì„¤ëª…
â”‚
â””â”€â”€ tasks/
    â””â”€â”€ tasks-prd-*.md                  # Task List
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
pip install -r requirements.txt
```

### 2. Baseline ì‹¤í–‰ (ì›ë³¸)

```bash
jupyter notebook baseline.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output.csv ìƒì„±
```

**ê¸°ëŒ€ ê²°ê³¼**: 46-47ì 

### 3. ëª¨ë“ˆí™” Baseline ì‹¤í–‰ âœ¨ NEW

```bash
jupyter notebook baseline_modular.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output_modular.csv ìƒì„±
```

**ì¥ì **:
- ì½”ë“œ ì¬ì‚¬ìš© (ì‹¤í—˜ë§ˆë‹¤ ëª¨ë“ˆë§Œ import)
- ëª…í™•í•œ êµ¬ì¡° (ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì¸ íŒŒì¼)
- ì‰¬ìš´ ë””ë²„ê¹… (ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)

---

## ëª¨ë“ˆ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©

```python
# 1. ëª¨ë“ˆ import
import sys
sys.path.append('../scripts')

from utils import load_config, get_device, set_seed
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from model_utils import load_model_for_train
from dataset import prepare_train_dataset
from trainer_utils import get_trainer
from inference_utils import run_inference

# 2. Config ë¡œë“œ
config = load_config('./config.yaml')
device = get_device()
set_seed(42)

# 3. Tokenizer
tokenizer = load_tokenizer(
    config['general']['model_name'],
    config['tokenizer']['special_tokens']
)

# 4. ë°ì´í„°ì…‹
preprocessor = Preprocess(
    bos_token=config['tokenizer']['bos_token'],
    eos_token=config['tokenizer']['eos_token']
)
train_ds, val_ds = prepare_train_dataset(config, preprocessor, data_path, tokenizer)

# 5. ëª¨ë¸ & í•™ìŠµ
model = load_model_for_train(config, tokenizer, device)
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

# 6. ì¶”ë¡ 
result = run_inference(model, tokenizer, test_loader, config, device, save_path='./output.csv')
```

### ì‹¤í—˜ ì§„í–‰ ë°©ë²•

**Experiment #2: Learning Rate íŠœë‹ ì˜ˆì‹œ**

```python
# baseline_modular.ipynbë¥¼ ë³µì‚¬
cp baseline_modular.ipynb exp2_lr_tuning.ipynb

# config ìˆ˜ì •
config['training']['learning_rate'] = 5e-5  # 1e-5 â†’ 5e-5
config['wandb']['name'] = 'kobart-lr-5e-5'

# í•™ìŠµ & ì¶”ë¡ 
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

result = run_inference(model, tokenizer, test_loader, config, device,
                      save_path='./prediction/output_exp2.csv')
```

---

## ì£¼ìš” ê¸°ëŠ¥

### 1. ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ

ëª¨ë“  ì‹¤í—˜ì€ `docs/experiment_logs.md`ì— ìë™ ê¸°ë¡ë©ë‹ˆë‹¤.

```markdown
## Experiment #N: ì‹¤í—˜ ì´ë¦„

**ë‚ ì§œ**: YYYY-MM-DD
**ë³€ê²½ì‚¬í•­**: Learning rate 1e-5 â†’ 5e-5

### ì„¤ì •
(config.yaml ë‚´ìš©)

### ê²°ê³¼
ROUGE-1: XX.XX%
ROUGE-2: XX.XX%
ROUGE-L: XX.XX%
Final Score: XX.XX

### íŒë‹¨
âœ…/âŒ + ë¶„ì„

### ë‹¤ìŒ ë‹¨ê³„
...
```

### 2. CSV ê²€ì¦

```python
from utils import validate_csv

result = validate_csv('./prediction/output.csv')

if not result['valid']:
    print("ì˜¤ë¥˜:", result['errors'])
else:
    print(f"âœ… ìœ íš¨í•œ CSV (ìƒ˜í”Œ ìˆ˜: {result['num_samples']})")
```

### 3. Config ê´€ë¦¬

```python
from utils import load_config, save_config

# ë¡œë“œ
config = load_config('./config.yaml')

# ìˆ˜ì •
config['training']['learning_rate'] = 5e-5

# ì €ì¥
save_config(config, './config_exp2.yaml')
```

---

## ì‹¤í—˜ ê²°ê³¼

| ì‹¤í—˜ | ROUGE-1 | ROUGE-2 | ROUGE-L | Final | ë³€ê²½ì‚¬í•­ | ìƒíƒœ |
|------|---------|---------|---------|-------|---------|------|
| **Exp #0** | 56.43% | 36.65% | 47.75% | **46.94** | Baseline ì¬í˜„ | âœ… ì„±ê³µ |
| **Exp #1** | 52.43% | 32.50% | 43.41% | **42.78** | ì¦ê°• ë°ì´í„° (2ë°°) | âŒ ì‹¤íŒ¨ |

### Exp #1 ì‹¤íŒ¨ ë¶„ì„

**ì›ì¸**:
1. ì¦ê°• ë°ì´í„°ì˜ ìŠ¤íƒ€ì¼ ë¶ˆì¼ì¹˜ (ë²ˆì—­íˆ¬ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)
2. ì›ë³¸ ë°ì´í„° ìŠ¤íƒ€ì¼ê³¼ ì¶©ëŒ
3. ëª¨ë¸ í˜¼ë€ ë°œìƒ

**êµí›ˆ**:
- âœ… ë°ì´í„° ì¦ê°• != ë¬´ì¡°ê±´ ì¢‹ìŒ
- âœ… ìŠ¤íƒ€ì¼ ì¼ê´€ì„±ì´ ì–‘ë³´ë‹¤ ì¤‘ìš”
- âœ… Dev ROUGEê°€ ë‚®ìœ¼ë©´ Testë„ ë‚®ìŒ

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Dev/Test ê²©ì°¨ ë¬¸ì œ

**ì¦ìƒ**: Dev set 94ì , Test set 20ì 

**ì›ì¸**: ê³¼ì í•© ë˜ëŠ” ë°ì´í„° ë¶ˆì¼ì¹˜

**í•´ê²°ì±…**:
1. Baselineë¶€í„° ì¬í˜„ (ìƒëµ ê¸ˆì§€)
2. í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ë³€ê²½
3. Testë¡œë§Œ ê²€ì¦ (DevëŠ” ì°¸ê³ ìš©)
4. ê²©ì°¨ 5ì  ì´ë‚´ ìœ ì§€

### 2. CSV ì œì¶œ ì˜¤ë¥˜

**ì¦ìƒ**: í”Œë«í¼ì—ì„œ í˜•ì‹ ì˜¤ë¥˜

**ì›ì¸**: Index ì»¬ëŸ¼ ëˆ„ë½

**í•´ê²°ì±…**:
```python
# âŒ ì˜ëª»ëœ ë°©ë²•
df.to_csv('output.csv', index=False)

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²• (index í¬í•¨)
df.to_csv('output.csv', index=True)
# ë˜ëŠ”
validate_csv('output.csv')  # ìë™ ê²€ì¦
```

### 3. íŠ¹ìˆ˜ í† í° ë¯¸ì œê±°

**ì¦ìƒ**: ìš”ì•½ë¬¸ì— `<s>`, `</s>`, `<pad>` í¬í•¨

**í•´ê²°ì±…**:
```python
from utils import remove_special_tokens

cleaned = remove_special_tokens(
    summaries,
    tokens=['<s>', '</s>', '<usr>', '<pad>']
)
```

---

## ê°œë°œ ì›ì¹™

### 1. í•œ ë²ˆì— í•˜ë‚˜ì”©
- ë§¤ ì‹¤í—˜ë§ˆë‹¤ **í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ** ë³€ê²½
- ì—¬ëŸ¬ ê°œ ë™ì‹œ ë³€ê²½ ì‹œ ì›ì¸ íŒŒì•… ë¶ˆê°€

### 2. Testë¡œë§Œ ê²€ì¦
- Dev setì€ ì°¸ê³ ìš©
- **Test ì œì¶œë¡œë§Œ ìµœì¢… íŒë‹¨** (12íšŒ/ì¼ ì œí•œ)

### 3. ëª¨ë“  ê²ƒì„ ê¸°ë¡
- Notebookìœ¼ë¡œ ì „ì²´ íë¦„ ë³´ì¡´
- `experiment_logs.md`ì— ê²°ê³¼ ê¸°ë¡
- Git ì»¤ë°‹ ë©”ì‹œì§€ì— ì ìˆ˜ í¬í•¨

### 4. ì¬í˜„ ê°€ëŠ¥ì„±
- ì‹œë“œ ê³ ì • (`set_seed(42)`)
- Config íŒŒì¼ë¡œ ì„¤ì • ê´€ë¦¬
- Notebookìœ¼ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¶”ì 

---

## ë¡œë“œë§µ

### âœ… Phase 1: Baseline ì¬í˜„ ë° ì¸í”„ë¼ êµ¬ì¶• (ì™„ë£Œ)
- [x] Baseline ì¬í˜„ (46.94ì )
- [x] ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ
- [x] ì½”ë“œ ëª¨ë“ˆí™” (7ê°œ ëª¨ë“ˆ, 1,745ì¤„)
- [x] Git ê´€ë¦¬

### ğŸ”„ Phase 2: ë‹¨ê³„ë³„ ì„±ëŠ¥ ê°œì„  (ì§„í–‰ ì¤‘)
- [ ] Exp #2: Learning Rate íŠœë‹ (1e-5 â†’ 5e-5)
- [ ] Exp #3: Longer Training (20 â†’ 30 epochs)
- [ ] Exp #4: Generation íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] Exp #5: í›„ì²˜ë¦¬ ë¡œì§

### ğŸ“‹ Phase 3: ë°ì´í„° ì¦ê°• (ë³´ë¥˜)
- Exp #1 ì‹¤íŒ¨ í›„ ë³´ë¥˜
- ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ìœ ì§€ ë°©ë²• ì—°êµ¬ í•„ìš”

### ğŸ¯ Phase 4: ìµœì¢… ì œì¶œ
- Best ëª¨ë¸ ì„ íƒ
- ì¬í˜„ì„± í…ŒìŠ¤íŠ¸ (3íšŒ í•™ìŠµ, í‘œì¤€í¸ì°¨ Â±0.5)
- ìµœì¢… ì œì¶œ ë° ë¬¸ì„œ ì •ë¦¬

---

## ì„±ëŠ¥ ëª©í‘œ

- **í˜„ì¬**: 46.94ì  (Baseline)
- **Phase 2 ëª©í‘œ**: 48-50ì 
- **ìµœì¢… ëª©í‘œ**: 50-55ì 
- **ë„ì „ ëª©í‘œ**: 55-60ì 

---

## ê¸°ìˆ  ìŠ¤íƒ

- **ì–¸ì–´**: Python 3.10
- **í”„ë ˆì„ì›Œí¬**: PyTorch 2.5.1, Transformers 4.46.3
- **ëª¨ë¸**: KoBART (digit82/kobart-summarization)
- **í‰ê°€**: ROUGE (rouge 1.0.1)
- **ì‹¤í—˜ ì¶”ì **: Weights & Biases (ì„ íƒì )
- **ë²„ì „ ê´€ë¦¬**: Git, GitHub

---

## ì°¸ê³  ìë£Œ

- [ëŒ€íšŒ í˜ì´ì§€](ëŒ€íšŒ URL)
- [KoBART ëª¨ë¸](https://huggingface.co/digit82/kobart-summarization)
- [ROUGE í‰ê°€ ë°©ë²•](docs/Competition_Overview/evaluation_method.md)
- [Baseline ì½”ë“œ ì„¤ëª…](docs/baseline_code_summary.md)
- [ì¬ì‹œì‘ ê°€ì´ë“œ](docs/RESTART_GUIDE.md)

---

## ë¼ì´ì„ ìŠ¤

MIT License

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-13
**Current Best**: 46.94ì  (Baseline)
**Git Commit**: 3ac2b65