# ì¼ìƒ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 

[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/HS6nBbT4)

**ëŒ€íšŒ**: NIKLuge 2024 - ì¼ìƒ ëŒ€í™” ìš”ì•½
**ìƒíƒœ**: âœ… **ëŒ€íšŒ ì¢…ë£Œ** (2025-10-15)
**ìµœê³  ì ìˆ˜**: 47.47ì  (Phase 1: LP=0.5)
**ê°œì„ í­**: +0.35ì  (Baseline 47.12ì  ëŒ€ë¹„)
**ì œì¶œ**: 12/12 ì‚¬ìš© ì™„ë£Œ

---

## í”„ë¡œì íŠ¸ ì†Œê°œ

í•œêµ­ì–´ ì¼ìƒ ëŒ€í™”ë¥¼ ì…ë ¥ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. KoBART ê¸°ë°˜ Seq2Seq ëª¨ë¸ì„ baselineìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

### ìµœì¢… ì„±ê³¼

- ğŸ† **ìµœê³  ì ìˆ˜**: 47.47ì  (Phase 1: LP=0.5, +0.35ì )
- ğŸ“Š **ì´ ì‹¤í—˜**: Baseline ~ Exp #7ê¹Œì§€ 12íšŒ ì œì¶œ
- ğŸ“ˆ **ê°œì„  íš¨ìœ¨**: ê°„ë‹¨í•œ ë³€ê²½(LP=0.5)ìœ¼ë¡œ ìµœê³  íš¨ê³¼
- ğŸ“š **êµí›ˆ í™•ë¦½**: Loss Gap ë¶„ì„, WeightedSampler í•¨ì • ë°œê²¬

### ì£¼ìš” íŠ¹ì§•

- âœ… **ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬**: í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½, Test setìœ¼ë¡œë§Œ ê²€ì¦
- âœ… **Loss Gap ë¶„ì„**: ê³¼ì í•© ì¡°ê¸° íƒì§€ ê¸°ë²• í™•ë¦½
- âœ… **ì™„ë²½í•œ ë¬¸ì„œí™”**: ì¬í˜„ ê°€ëŠ¥í•œ ìƒì„¸ ê¸°ë¡ ë° êµí›ˆ ì •ë¦¬
- âœ… **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬**: CLI ìŠ¤í¬ë¦½íŠ¸ ë° ëª¨ë“ˆí™” ì½”ë“œ

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
naturallanguageprocessingcompetition-nlp-1/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ baseline.ipynb                   # ì›ë³¸ Baseline (47.12ì )
â”‚   â”œâ”€â”€ baseline_modular.ipynb          # ëª¨ë“ˆí™” Baseline âœ¨
â”‚   â”œâ”€â”€ config.yaml                      # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scripts/                             # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ âœ¨
â”‚   â”œâ”€â”€ utils.py                        # Config, ì‹œë“œ, CSV ê²€ì¦
â”‚   â”œâ”€â”€ data_loader.py                  # ë°ì´í„° ë¡œë”©, Preprocess
â”‚   â”œâ”€â”€ tokenizer_utils.py              # Tokenizer ì„¤ì •
â”‚   â”œâ”€â”€ model_utils.py                  # ëª¨ë¸ ë¡œë”© (í•™ìŠµ/ì¶”ë¡ )
â”‚   â”œâ”€â”€ dataset.py                      # PyTorch Dataset í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ trainer_utils.py                # Trainer, compute_metrics
â”‚   â””â”€â”€ inference_utils.py              # ì¶”ë¡  íŒŒì´í”„ë¼ì¸
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                       # í•™ìŠµ ë°ì´í„° (12,457ê°œ)
â”‚   â”œâ”€â”€ dev.csv                         # ê²€ì¦ ë°ì´í„° (499ê°œ)
â”‚   â””â”€â”€ test.csv                        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (499ê°œ)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiment_logs.md              # ì‹¤í—˜ ê¸°ë¡
â”‚   â”œâ”€â”€ RESTART_GUIDE.md                # ì¬ì‹œì‘ ì „ëµ
â”‚   â””â”€â”€ baseline_code_summary.md        # ì½”ë“œ ì„¤ëª…
â”‚
â””â”€â”€ tasks/
    â””â”€â”€ tasks-prd-*.md                  # Task List
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
pip install -r requirements.txt
```

### 2. Baseline ì‹¤í–‰ (ì›ë³¸)

```bash
jupyter notebook baseline.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output.csv ìƒì„±
```

**ê¸°ëŒ€ ê²°ê³¼**: 46-47ì 

### 3. ëª¨ë“ˆí™” Baseline ì‹¤í–‰ âœ¨ NEW

```bash
jupyter notebook baseline_modular.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output_modular.csv ìƒì„±
```

**ì¥ì **:
- ì½”ë“œ ì¬ì‚¬ìš© (ì‹¤í—˜ë§ˆë‹¤ ëª¨ë“ˆë§Œ import)
- ëª…í™•í•œ êµ¬ì¡° (ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì¸ íŒŒì¼)
- ì‰¬ìš´ ë””ë²„ê¹… (ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)

---

## ëª¨ë“ˆ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©

```python
# 1. ëª¨ë“ˆ import
import sys
sys.path.append('../scripts')

from utils import load_config, get_device, set_seed
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from model_utils import load_model_for_train
from dataset import prepare_train_dataset
from trainer_utils import get_trainer
from inference_utils import run_inference

# 2. Config ë¡œë“œ
config = load_config('./config.yaml')
device = get_device()
set_seed(42)

# 3. Tokenizer
tokenizer = load_tokenizer(
    config['general']['model_name'],
    config['tokenizer']['special_tokens']
)

# 4. ë°ì´í„°ì…‹
preprocessor = Preprocess(
    bos_token=config['tokenizer']['bos_token'],
    eos_token=config['tokenizer']['eos_token']
)
train_ds, val_ds = prepare_train_dataset(config, preprocessor, data_path, tokenizer)

# 5. ëª¨ë¸ & í•™ìŠµ
model = load_model_for_train(config, tokenizer, device)
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

# 6. ì¶”ë¡ 
result = run_inference(model, tokenizer, test_loader, config, device, save_path='./output.csv')
```

### ì‹¤í—˜ ì§„í–‰ ë°©ë²•

**Experiment #2: Learning Rate íŠœë‹ ì˜ˆì‹œ**

```python
# baseline_modular.ipynbë¥¼ ë³µì‚¬
cp baseline_modular.ipynb exp2_lr_tuning.ipynb

# config ìˆ˜ì •
config['training']['learning_rate'] = 5e-5  # 1e-5 â†’ 5e-5
config['wandb']['name'] = 'kobart-lr-5e-5'

# í•™ìŠµ & ì¶”ë¡ 
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

result = run_inference(model, tokenizer, test_loader, config, device,
                      save_path='./prediction/output_exp2.csv')
```

---

## ì£¼ìš” ê¸°ëŠ¥

### 1. ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ

ëª¨ë“  ì‹¤í—˜ì€ `docs/experiment_logs.md`ì— ìë™ ê¸°ë¡ë©ë‹ˆë‹¤.

```markdown
## Experiment #N: ì‹¤í—˜ ì´ë¦„

**ë‚ ì§œ**: YYYY-MM-DD
**ë³€ê²½ì‚¬í•­**: Learning rate 1e-5 â†’ 5e-5

### ì„¤ì •
(config.yaml ë‚´ìš©)

### ê²°ê³¼
ROUGE-1: XX.XX%
ROUGE-2: XX.XX%
ROUGE-L: XX.XX%
Final Score: XX.XX

### íŒë‹¨
âœ…/âŒ + ë¶„ì„

### ë‹¤ìŒ ë‹¨ê³„
...
```

### 2. CSV ê²€ì¦

```python
from utils import validate_csv

result = validate_csv('./prediction/output.csv')

if not result['valid']:
    print("ì˜¤ë¥˜:", result['errors'])
else:
    print(f"âœ… ìœ íš¨í•œ CSV (ìƒ˜í”Œ ìˆ˜: {result['num_samples']})")
```

### 3. Config ê´€ë¦¬

```python
from utils import load_config, save_config

# ë¡œë“œ
config = load_config('./config.yaml')

# ìˆ˜ì •
config['training']['learning_rate'] = 5e-5

# ì €ì¥
save_config(config, './config_exp2.yaml')
```

---

## ì‹¤í—˜ ê²°ê³¼

### ì™„ë£Œëœ ì‹¤í—˜ ìš”ì•½

| ì‹¤í—˜ëª… | ì„¤ëª… | Test ì ìˆ˜ | ë³€í™” | ë‚ ì§œ | ìƒíƒœ |
|--------|------|-----------|------|------|------|
| **Baseline** | ê³µì‹ ì½”ë“œ ì¬í˜„ | 47.12 | - | 10/12 | âœ… ì„±ê³µ |
| **Phase 1: LP=0.5** | Length Penalty ìµœì í™” | **47.47** | **+0.35** ğŸ† | 10/13 | âœ… **ìµœê³ ** |
| Phase 1: LP=0.3 | LP ì¶”ê°€ ì‹¤í—˜ | 47.15 | +0.03 | 10/13 | âœ… ì„±ê³µ |
| Phase 1: LP=0.7 | LP ì¶”ê°€ ì‹¤í—˜ | 47.22 | +0.10 | 10/13 | âœ… ì„±ê³µ |
| **Exp #5** | no_repeat_ngram=3 | 47.03 | -0.44 | 10/14 | âŒ ì‹¤íŒ¨ |
| **Exp #6** | Learning Rate 3e-5 | - | - | - | â¸ï¸ ë³´ë¥˜ |
| **Exp #7-A** | ì¦ê°• ë°ì´í„° (ê°€ì¤‘ì¹˜ ì—†ìŒ) | 47.41 | -0.06 | 10/15 | âœ… ì•ˆì • |
| Exp #7-C | ê°€ì¤‘ì¹˜ max=5.0 | - | - | 10/15 | â­ï¸ ìŠ¤í‚µ |
| **Exp #7-F** | ê°€ì¤‘ì¹˜ ì¡°ì • (ìµœì¢…) | 46.62 | -0.79 | 10/15 | âŒ **ì‹¤íŒ¨** |

**ì œì¶œ í†µê³„**: 12/12 ì‚¬ìš© ì™„ë£Œ (100%)
**ì ìˆ˜ ë²”ìœ„**: 46.62~47.47 (0.85ì  ì°¨ì´)
**í‰ê·  ê°œì„ **: +0.35ì  (Baseline ëŒ€ë¹„)

**ìƒì„¸ ê¸°ë¡**: `docs/EXPERIMENT_LOG.md` ì°¸ì¡°

### í•µì‹¬ êµí›ˆ (Lessons Learned)

#### 1. Loss Gapì´ ì§„ì‹¤ â­â­â­

**ì •ì˜**: `Loss Gap = Train Loss - Eval Loss`

**í•´ì„**:
- **ì–‘ìˆ˜ (+0.15 ì´ìƒ)**: ê±´ê°•í•œ í•™ìŠµ, ì œì¶œ ê³ ë ¤ âœ…
- **ìŒìˆ˜ (-)**: ê³¼ì í•©, ì œì¶œ ê¸ˆì§€ âŒ

**ì‹¤í—˜ ì¦ê±°**:
- Exp #7-A: Loss Gap **+0.50** â†’ Test **47.41** âœ…
- Exp #7-F: Loss Gap +0.47 â†’ Test 46.62 âŒ (ë¶„í¬ ì™œê³¡)

#### 2. WeightedRandomSamplerì˜ í•¨ì • â­â­â­

**ë¬¸ì œ íŒ¨í„´**:
```
ì¦ê°• ì—†ëŠ” ì¹´í…Œê³ ë¦¬ Ã— ê°€ì¤‘ì¹˜ = ê°™ì€ ìƒ˜í”Œ ë°˜ë³µ
â†’ ëª¨ë¸ ì•”ê¸° â†’ Test ì‹¤íŒ¨
```

**Exp #7-F ì‹¤íŒ¨ ì›ì¸**:
- ë…¸ë™/ê³ ìš©(135ê°œ) Ã— 3.70ë°° = 500íšŒ ë°˜ë³µ
- Dev ROUGE 36.43% â†‘ (ë†’ìŒ)
- Test Score 46.62 â†“ (ë‚®ìŒ)
- **Devë„ í•™ìŠµ ë¶„í¬ ì˜í–¥ ë°›ìŒ**

**êµí›ˆ**: ì¦ê°• ì—†ìœ¼ë©´ ê°€ì¤‘ì¹˜ ì ˆëŒ€ ê¸ˆì§€!

#### 3. Dev ROUGE â‰  Test Score â­â­

| ì‹¤í—˜ | Dev ROUGE-1 | Test Score | ìƒê´€ê´€ê³„ |
|------|-------------|------------|----------|
| Exp #7-A | 36.18% | 47.41 | Dev ë‚®ìŒ, Test ë†’ìŒ |
| Exp #7-F | 36.43% | 46.62 | Dev ë†’ìŒ, Test ë‚®ìŒ âŒ |

**êµí›ˆ**: Test ì œì¶œë§Œì´ ìœ ì¼í•œ ì§„ì‹¤!

#### 4. ë‹¨ìˆœí•¨ì˜ ê°€ì¹˜ (KISS ì›ì¹™) â­â­â­

| ì ‘ê·¼ë²• | ë³µì¡ë„ | ì†Œìš”ì‹œê°„ | ì ìˆ˜ ë³€í™” | íš¨ìœ¨ì„± |
|--------|--------|----------|-----------|--------|
| **LP=0.5** | â­ ë‚®ìŒ | 12ì´ˆ | **+0.35** | â­â­â­ ìµœê³  |
| ë°ì´í„° ì¦ê°• | â­â­â­ ë†’ìŒ | 3ì‹œê°„ | -0.06 | â­ ë‚®ìŒ |
| ê°€ì¤‘ì¹˜ ì¡°ì • | â­â­ ì¤‘ê°„ | 3ì‹œê°„ | -0.79 | âŒ ì—­íš¨ê³¼ |

**êµí›ˆ**: ê°€ì¥ ê°„ë‹¨í•œ ë³€ê²½ì´ ê°€ì¥ í° íš¨ê³¼!

**ìƒì„¸ êµí›ˆ**: `docs/LESSONS_LEARNED.md` ì°¸ì¡°

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Dev/Test ê²©ì°¨ ë¬¸ì œ

**ì¦ìƒ**: Dev set 94ì , Test set 20ì 

**ì›ì¸**: ê³¼ì í•© ë˜ëŠ” ë°ì´í„° ë¶ˆì¼ì¹˜

**í•´ê²°ì±…**:
1. Baselineë¶€í„° ì¬í˜„ (ìƒëµ ê¸ˆì§€)
2. í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ë³€ê²½
3. Testë¡œë§Œ ê²€ì¦ (DevëŠ” ì°¸ê³ ìš©)
4. ê²©ì°¨ 5ì  ì´ë‚´ ìœ ì§€

### 2. CSV ì œì¶œ ì˜¤ë¥˜

**ì¦ìƒ**: í”Œë«í¼ì—ì„œ í˜•ì‹ ì˜¤ë¥˜

**ì›ì¸**: Index ì»¬ëŸ¼ ëˆ„ë½

**í•´ê²°ì±…**:
```python
# âŒ ì˜ëª»ëœ ë°©ë²•
df.to_csv('output.csv', index=False)

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²• (index í¬í•¨)
df.to_csv('output.csv', index=True)
# ë˜ëŠ”
validate_csv('output.csv')  # ìë™ ê²€ì¦
```

### 3. íŠ¹ìˆ˜ í† í° ë¯¸ì œê±°

**ì¦ìƒ**: ìš”ì•½ë¬¸ì— `<s>`, `</s>`, `<pad>` í¬í•¨

**í•´ê²°ì±…**:
```python
from utils import remove_special_tokens

cleaned = remove_special_tokens(
    summaries,
    tokens=['<s>', '</s>', '<usr>', '<pad>']
)
```

---

## ê°œë°œ ì›ì¹™

### 1. í•œ ë²ˆì— í•˜ë‚˜ì”©
- ë§¤ ì‹¤í—˜ë§ˆë‹¤ **í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ** ë³€ê²½
- ì—¬ëŸ¬ ê°œ ë™ì‹œ ë³€ê²½ ì‹œ ì›ì¸ íŒŒì•… ë¶ˆê°€

### 2. Testë¡œë§Œ ê²€ì¦
- Dev setì€ ì°¸ê³ ìš©
- **Test ì œì¶œë¡œë§Œ ìµœì¢… íŒë‹¨** (12íšŒ/ì¼ ì œí•œ)

### 3. ëª¨ë“  ê²ƒì„ ê¸°ë¡
- Notebookìœ¼ë¡œ ì „ì²´ íë¦„ ë³´ì¡´
- `experiment_logs.md`ì— ê²°ê³¼ ê¸°ë¡
- Git ì»¤ë°‹ ë©”ì‹œì§€ì— ì ìˆ˜ í¬í•¨

### 4. ì¬í˜„ ê°€ëŠ¥ì„±
- ì‹œë“œ ê³ ì • (`set_seed(42)`)
- Config íŒŒì¼ë¡œ ì„¤ì • ê´€ë¦¬
- Notebookìœ¼ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¶”ì 

---

## ëŒ€íšŒ ê²°ê³¼ ë° í–¥í›„ ë°©í–¥

### âœ… ì™„ë£Œëœ ì‘ì—… (2025-10-12 ~ 10-15)

**Phase 1: Baseline ì¬í˜„ ë° ìµœì í™”**
- [x] Baseline ì¬í˜„ (47.12ì )
- [x] Length Penalty ìµœì í™” (47.47ì , +0.35ì ) ğŸ†
- [x] ë°ì´í„° ì¦ê°• í”„ë ˆì„ì›Œí¬ êµ¬ì¶• (1,009ê°œ)
- [x] Loss Gap ë¶„ì„ ê¸°ë²• í™•ë¦½

**Phase 2: ë°ì´í„° ì¦ê°• ì‹¤í—˜**
- [x] Exp #7-A: ì¦ê°• + ê°€ì¤‘ì¹˜ ì—†ìŒ (47.41ì , ì•ˆì •ì )
- [x] Exp #7-C: ê°€ì¤‘ì¹˜ max=5.0 (Loss Gapìœ¼ë¡œ ì‚¬ì „ ì‹¤íŒ¨ ì˜ˆì¸¡)
- [x] Exp #7-F: ê°€ì¤‘ì¹˜ ì¡°ì • (46.62ì , ì‹¤íŒ¨ êµí›ˆ)

**Phase 3: ë¬¸ì„œí™” ì™„ë£Œ**
- [x] ì „ì²´ ì‹¤í—˜ ê¸°ë¡ (EXPERIMENT_LOG.md)
- [x] ìµœì¢… ë¦¬í¬íŠ¸ (COMPETITION_FINAL_REPORT.md)
- [x] êµí›ˆ ì •ë¦¬ (LESSONS_LEARNED.md)
- [x] ì•„ì¹´ì´ë¸Œ ê°€ì´ë“œ (ARCHIVE.md)

**ì œì¶œ í†µê³„**: 12/12 ì‚¬ìš© ì™„ë£Œ (100%)
**ìµœê³  ì ìˆ˜**: 47.47ì  (Phase 1: LP=0.5)
**ê°œì„ í­**: +0.35ì  (+0.74%)

### ğŸ“‹ í–¥í›„ ê°œì„  ë°©í–¥ (Future Work)

**ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (High Priority)**:
1. **ê· ë“± ì¦ê°•** (Balanced Augmentation) - ì˜ˆìƒ: +0.8~2.0ì 
   - ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ 300~500ê°œë¡œ ê· ë“±í™”
   - WeightedSampler ì—†ì´ ìì—° ë¶„í¬ í•™ìŠµ
2. **Learning Rate íŠœë‹** (1e-5 â†’ 3e-5) - ì˜ˆìƒ: +0.5~1.5ì 
3. **Extended Training** (Epochs 20 â†’ 30) - ì˜ˆìƒ: +0.3~0.8ì 

**ì¥ê¸° ì „ëµ (Lower Priority)**:
4. **Larger Models** (gogamza/kobart-base-v2, KoT5) - ì˜ˆìƒ: +1.0~3.0ì 
5. **Ensemble Methods** - ì˜ˆìƒ: +0.3~1.0ì 

**ìƒì„¸ ê³„íš**: `docs/NEXT_STEPS.md` ì°¸ì¡°

---

## ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¬í˜„

### ğŸ† Phase 1: LP=0.5 (47.47ì )

**ì„¤ì •**:
```yaml
model: digit82/kobart-summarization
length_penalty: 0.5  # Baseline: 1.0
num_beams: 4
no_repeat_ngram_size: 2
learning_rate: 1e-5
batch_size: 50/32
epochs: 20
```

**ì¬í˜„ ë°©ë²•**:
```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code

# 1. config.yaml ìˆ˜ì •: length_penalty=0.5
# 2. ì¶”ë¡  ì‹¤í–‰
python inference.py --checkpoint checkpoint-XXXX
```

### 2ìœ„: Exp #7-A (47.41ì )

**ì„¤ì •**:
- ì¦ê°• ë°ì´í„°: 13,465ê°œ (ì›ë³¸ 12,457 + ì¦ê°• 1,009)
- WeightedSampler: **ì‚¬ìš© ì•ˆ í•¨** (í•µì‹¬!)
- Loss Gap: +0.50 (ì•ˆì •ì )

**ì¬í˜„ ë°©ë²•**:
```bash
python inference.py --experiment exp7a --checkpoint checkpoint-2068
```

**Checkpoint ê²½ë¡œ**: `submission_exp7a/checkpoint-2068/` (1.4GB)

---

## ê¸°ìˆ  ìŠ¤íƒ

- **ì–¸ì–´**: Python 3.10
- **í”„ë ˆì„ì›Œí¬**: PyTorch 2.5.1, Transformers 4.46.3
- **ëª¨ë¸**: KoBART (digit82/kobart-summarization)
- **í‰ê°€**: ROUGE (rouge 1.0.1)
- **ì‹¤í—˜ ì¶”ì **: Weights & Biases (ì„ íƒì )
- **ë²„ì „ ê´€ë¦¬**: Git, GitHub

---

## ì°¸ê³  ë¬¸ì„œ

### ëŒ€íšŒ ê²°ê³¼ ë¬¸ì„œ
- [**COMPETITION_FINAL_REPORT.md**](docs/COMPETITION_FINAL_REPORT.md) - ëŒ€íšŒ ìµœì¢… ë¦¬í¬íŠ¸
- [**LESSONS_LEARNED.md**](docs/LESSONS_LEARNED.md) - ì‹¤í—˜ êµí›ˆ ë° Best Practices
- [**EXPERIMENT_LOG.md**](docs/EXPERIMENT_LOG.md) - ì „ì²´ ì‹¤í—˜ ìƒì„¸ ê¸°ë¡
- [**ARCHIVE.md**](docs/ARCHIVE.md) - í”„ë¡œì íŠ¸ ì•„ì¹´ì´ë¸Œ ê°€ì´ë“œ

### ê¸°ìˆ  ë¬¸ì„œ
- [RESTART_GUIDE.md](docs/RESTART_GUIDE.md) - ì¬ì‹œì‘ ê°€ì´ë“œ
- [NEXT_STEPS.md](docs/NEXT_STEPS.md) - í–¥í›„ ê°œì„  ë°©í–¥
- [code/README.md](code/README.md) - í”„ë ˆì„ì›Œí¬ ì‚¬ìš©ë²•
- [Competition_Overview/](docs/Competition_Overview/) - ëŒ€íšŒ ê·œì¹™

### ì™¸ë¶€ ë§í¬
- [KoBART ëª¨ë¸](https://huggingface.co/digit82/kobart-summarization)
- [ROUGE í‰ê°€ ë°©ë²•](docs/Competition_Overview/evaluation_method.md)

---

## ë¼ì´ì„ ìŠ¤

MIT License

---

**í”„ë¡œì íŠ¸ ê¸°ê°„**: 2025-10-12 ~ 2025-10-15 (4ì¼)
**ìµœê³  ì ìˆ˜**: 47.47ì  (Phase 1: LP=0.5)
**ì œì¶œ**: 12/12 ì‚¬ìš© ì™„ë£Œ
**ìƒíƒœ**: âœ… ëŒ€íšŒ ì¢…ë£Œ, ë¬¸ì„œí™” ì™„ë£Œ
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-15