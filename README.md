# ì¼ìƒ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 

[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/HS6nBbT4)

**ëŒ€íšŒ**: NIKLuge 2024 - ì¼ìƒ ëŒ€í™” ìš”ì•½
**í˜„ì¬ Best Score**: 47.47ì  (Phase 1, LP=0.5)
**ìµœê·¼ ì‹¤í—˜**: 47.41ì  (Exp #7-A, ì¦ê°• ë°ì´í„°)
**ëª©í‘œ**: 50ì  ì´ìƒ ë‹¬ì„±

---

## í”„ë¡œì íŠ¸ ì†Œê°œ

í•œêµ­ì–´ ì¼ìƒ ëŒ€í™”ë¥¼ ì…ë ¥ë°›ì•„ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. KoBART ê¸°ë°˜ Seq2Seq ëª¨ë¸ì„ baselineìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ì ì§„ì ì¸ ê°œì„ ì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- âœ… **ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬**: í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½, Test setìœ¼ë¡œë§Œ ê²€ì¦
- âœ… **ì½”ë“œ ëª¨ë“ˆí™”**: 7ê°œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ Python ëª¨ë“ˆ (1,745ì¤„)
- âœ… **ì™„ë²½í•œ ì¬í˜„ì„±**: ëª¨ë“  ì‹¤í—˜ì„ Jupyter Notebookìœ¼ë¡œ ê¸°ë¡
- âœ… **ìë™í™”ëœ ê²€ì¦**: CSV í¬ë§· ê²€ì¦, ROUGE ê³„ì‚°

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
naturallanguageprocessingcompetition-nlp-1/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ baseline.ipynb                   # ì›ë³¸ Baseline (47.12ì )
â”‚   â”œâ”€â”€ baseline_modular.ipynb          # ëª¨ë“ˆí™” Baseline âœ¨
â”‚   â”œâ”€â”€ config.yaml                      # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ scripts/                             # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ âœ¨
â”‚   â”œâ”€â”€ utils.py                        # Config, ì‹œë“œ, CSV ê²€ì¦
â”‚   â”œâ”€â”€ data_loader.py                  # ë°ì´í„° ë¡œë”©, Preprocess
â”‚   â”œâ”€â”€ tokenizer_utils.py              # Tokenizer ì„¤ì •
â”‚   â”œâ”€â”€ model_utils.py                  # ëª¨ë¸ ë¡œë”© (í•™ìŠµ/ì¶”ë¡ )
â”‚   â”œâ”€â”€ dataset.py                      # PyTorch Dataset í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ trainer_utils.py                # Trainer, compute_metrics
â”‚   â””â”€â”€ inference_utils.py              # ì¶”ë¡  íŒŒì´í”„ë¼ì¸
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                       # í•™ìŠµ ë°ì´í„° (12,457ê°œ)
â”‚   â”œâ”€â”€ dev.csv                         # ê²€ì¦ ë°ì´í„° (499ê°œ)
â”‚   â””â”€â”€ test.csv                        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (499ê°œ)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ experiment_logs.md              # ì‹¤í—˜ ê¸°ë¡
â”‚   â”œâ”€â”€ RESTART_GUIDE.md                # ì¬ì‹œì‘ ì „ëµ
â”‚   â””â”€â”€ baseline_code_summary.md        # ì½”ë“œ ì„¤ëª…
â”‚
â””â”€â”€ tasks/
    â””â”€â”€ tasks-prd-*.md                  # Task List
```

---

## ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
pip install -r requirements.txt
```

### 2. Baseline ì‹¤í–‰ (ì›ë³¸)

```bash
jupyter notebook baseline.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output.csv ìƒì„±
```

**ê¸°ëŒ€ ê²°ê³¼**: 46-47ì 

### 3. ëª¨ë“ˆí™” Baseline ì‹¤í–‰ âœ¨ NEW

```bash
jupyter notebook baseline_modular.ipynb
# ëª¨ë“  ì…€ ì‹¤í–‰ â†’ prediction/output_modular.csv ìƒì„±
```

**ì¥ì **:
- ì½”ë“œ ì¬ì‚¬ìš© (ì‹¤í—˜ë§ˆë‹¤ ëª¨ë“ˆë§Œ import)
- ëª…í™•í•œ êµ¬ì¡° (ê° ê¸°ëŠ¥ì´ ë…ë¦½ì ì¸ íŒŒì¼)
- ì‰¬ìš´ ë””ë²„ê¹… (ëª¨ë“ˆ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)

---

## ëª¨ë“ˆ ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‚¬ìš©

```python
# 1. ëª¨ë“ˆ import
import sys
sys.path.append('../scripts')

from utils import load_config, get_device, set_seed
from data_loader import Preprocess
from tokenizer_utils import load_tokenizer
from model_utils import load_model_for_train
from dataset import prepare_train_dataset
from trainer_utils import get_trainer
from inference_utils import run_inference

# 2. Config ë¡œë“œ
config = load_config('./config.yaml')
device = get_device()
set_seed(42)

# 3. Tokenizer
tokenizer = load_tokenizer(
    config['general']['model_name'],
    config['tokenizer']['special_tokens']
)

# 4. ë°ì´í„°ì…‹
preprocessor = Preprocess(
    bos_token=config['tokenizer']['bos_token'],
    eos_token=config['tokenizer']['eos_token']
)
train_ds, val_ds = prepare_train_dataset(config, preprocessor, data_path, tokenizer)

# 5. ëª¨ë¸ & í•™ìŠµ
model = load_model_for_train(config, tokenizer, device)
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

# 6. ì¶”ë¡ 
result = run_inference(model, tokenizer, test_loader, config, device, save_path='./output.csv')
```

### ì‹¤í—˜ ì§„í–‰ ë°©ë²•

**Experiment #2: Learning Rate íŠœë‹ ì˜ˆì‹œ**

```python
# baseline_modular.ipynbë¥¼ ë³µì‚¬
cp baseline_modular.ipynb exp2_lr_tuning.ipynb

# config ìˆ˜ì •
config['training']['learning_rate'] = 5e-5  # 1e-5 â†’ 5e-5
config['wandb']['name'] = 'kobart-lr-5e-5'

# í•™ìŠµ & ì¶”ë¡ 
trainer = get_trainer(config, model, tokenizer, train_ds, val_ds)
trainer.train()

result = run_inference(model, tokenizer, test_loader, config, device,
                      save_path='./prediction/output_exp2.csv')
```

---

## ì£¼ìš” ê¸°ëŠ¥

### 1. ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ

ëª¨ë“  ì‹¤í—˜ì€ `docs/experiment_logs.md`ì— ìë™ ê¸°ë¡ë©ë‹ˆë‹¤.

```markdown
## Experiment #N: ì‹¤í—˜ ì´ë¦„

**ë‚ ì§œ**: YYYY-MM-DD
**ë³€ê²½ì‚¬í•­**: Learning rate 1e-5 â†’ 5e-5

### ì„¤ì •
(config.yaml ë‚´ìš©)

### ê²°ê³¼
ROUGE-1: XX.XX%
ROUGE-2: XX.XX%
ROUGE-L: XX.XX%
Final Score: XX.XX

### íŒë‹¨
âœ…/âŒ + ë¶„ì„

### ë‹¤ìŒ ë‹¨ê³„
...
```

### 2. CSV ê²€ì¦

```python
from utils import validate_csv

result = validate_csv('./prediction/output.csv')

if not result['valid']:
    print("ì˜¤ë¥˜:", result['errors'])
else:
    print(f"âœ… ìœ íš¨í•œ CSV (ìƒ˜í”Œ ìˆ˜: {result['num_samples']})")
```

### 3. Config ê´€ë¦¬

```python
from utils import load_config, save_config

# ë¡œë“œ
config = load_config('./config.yaml')

# ìˆ˜ì •
config['training']['learning_rate'] = 5e-5

# ì €ì¥
save_config(config, './config_exp2.yaml')
```

---

## ì‹¤í—˜ ê²°ê³¼

### Completed Experiments

| Exp # | Description | ROUGE-1 | ROUGE-2 | ROUGE-L | Score | Date | Status |
|-------|-------------|---------|---------|---------|-------|------|--------|
| #0 | Baseline (Original) | 56.43% | 36.65% | 47.75% | **46.9426** | 2025-10-12 | âœ… |
| #0.1 | Baseline (Modular) | 56.28% | 36.65% | 47.93% | **46.9526** | 2025-10-13 | âœ… (+0.01) |
| #1 | Augmented Data (LLM) | 52.43% | 32.50% | 43.41% | **42.7807** | 2025-10-12 | âŒ Failed (-4.16) |
| #2 | Post-processing v2 | 56.31% | 36.65% | 48.00% | **46.9863** | 2025-10-13 | âŒ Rollback (+0.03) |
| #3 | Learning Rate 2e-5 (v1) | 56.19% | 36.32% | 47.57% | **46.6919** | 2025-10-13 | âŒ Failed (-0.26) |
| #3 | Learning Rate 2e-5 (v2) | 55.93% | 36.72% | 47.17% | **46.6089** | 2025-10-13 | âŒ Failed (-0.34) |

### Planned Experiments

| Exp # | Description | Target | Risk | Priority | Status |
|-------|-------------|--------|------|----------|--------|
| #4 | Learning Rate 3e-5 | +1~2 | âœ… Low | Day 3 | âŒ Skipped (LR ë°©í–¥ ì˜ëª»ë¨) |
| #5 | Learning Rate 5e-5 | +2~3 | âš ï¸ Medium | Day 4 | âŒ Skipped (LR ë°©í–¥ ì˜ëª»ë¨) |
| #6 | Time Token | +0.5~1 | âš ï¸ Medium | Day 4-5 | ğŸ“‹ Planned |
| #7 | Money Token | +0.3~0.7 | âš ï¸ Medium | Day 5-6 | ğŸ“‹ Planned |
| #8 | Warmup Steps 50/100 | +0.5~1 | âœ… Low | Week 2 | ğŸ“‹ Planned |
| #9 | Epochs 30 | +0.5~1 | âœ… Low | Week 2 | ğŸ“‹ Planned |
| #10 | Data Aug (Filtered) | +0.5~1 | âš ï¸ Medium | Week 2+ | ğŸ“‹ Planned |
| #11 | Data Aug (LLM Style) | +1~2 | âš ï¸ Medium | Week 2+ | ğŸ“‹ Planned |

**ì°¸ê³ **: ìƒì„¸ ê³„íšì€ `tasks/eda-findings.md` ì°¸ì¡°

### ì‹¤íŒ¨ ì‹¤í—˜ ë¶„ì„

#### Exp #1: ì¦ê°• ë°ì´í„° í•™ìŠµ

**ì›ì¸**:
1. ì¦ê°• ë°ì´í„°ì˜ ìŠ¤íƒ€ì¼ ë¶ˆì¼ì¹˜ (ë²ˆì—­íˆ¬ â†’ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)
2. ì›ë³¸ ë°ì´í„° ìŠ¤íƒ€ì¼ê³¼ ì¶©ëŒ
3. ëª¨ë¸ í˜¼ë€ ë°œìƒ

**êµí›ˆ**:
- âœ… ë°ì´í„° ì¦ê°• != ë¬´ì¡°ê±´ ì¢‹ìŒ
- âœ… ìŠ¤íƒ€ì¼ ì¼ê´€ì„±ì´ ì–‘ë³´ë‹¤ ì¤‘ìš”
- âœ… Dev ROUGEê°€ ë‚®ìœ¼ë©´ Testë„ ë‚®ìŒ

**ê°œì„  ë°©ì•ˆ** (Agent 3 ê¶Œì¥):
- ë°©ì•ˆ 1: í•„í„°ë§ëœ ì¬ì‚¬ìš© (ìŠ¤íƒ€ì¼ ì¼ì¹˜ ìƒ˜í”Œë§Œ)
- ë°©ì•ˆ 2: LLM ê¸°ë°˜ ìŠ¤íƒ€ì¼ ë³´ì¡´ ì¦ê°•
- ìƒì„¸: `tasks/eda-findings.md` ì°¸ì¡°

#### Exp #2: í›„ì²˜ë¦¬ ê°œì„  (Post-processing v2)

**ì›ì¸**:
1. ëª¨ë¸ ì¶œë ¥ì´ ì´ë¯¸ ìµœì í™”ë˜ì–´ ìˆì—ˆìŒ
2. Baselineì˜ ë‹¨ìˆœí•¨ì—ëŠ” ì´ìœ ê°€ ìˆì—ˆìŒ
3. Dev set ê²€ì¦ ì—†ì´ Testë¡œ ì§í–‰ â†’ ì˜ˆì¸¡ ë¶ˆê°€

**êµí›ˆ**:
- âœ… "ë‹¹ì—°íˆ ì¢‹ì„ ê²ƒ"ì´ë¼ëŠ” ê°€ì •ì€ ìœ„í—˜í•¨
- âœ… ì´ë¡ ì  ê°œì„  â‰  ì„±ëŠ¥ í–¥ìƒ (ì‹¤ì¦ í•„ìˆ˜)
- âœ… Dev set ê²€ì¦ ë¨¼ì € ìˆ˜í–‰í•˜ê¸°
- âœ… Baselineì˜ ë‹¨ìˆœí•¨ì„ ì¡´ì¤‘, ëª¨ë¸ í•™ìŠµ ê°œì„ ì— ì§‘ì¤‘

#### Exp #3: Learning Rate 2e-5

**ì›ì¸**:
1. **LR 2e-5ê°€ ê³¼ë„í•¨** - Baseline 1e-5ê°€ ì´ë¯¸ ìµœì 
2. **ëª¨ë“  checkpointì—ì„œ ì¼ê´€ëœ í•˜ë½** - checkpoint ì„ íƒ ë¬´ê´€
3. **Dev/Test ê´´ë¦¬ ì‹¬í™”** - Dev +0.81%p â†’ Test -0.26ì 
4. **checkpoint ì„ íƒì˜ ì—­ì„¤** - Best loss(ckpt-1000)ê°€ ì˜¤íˆë ¤ ë” ë‚˜ì¨ (-0.34)

**êµí›ˆ**:
- âœ… LR íŠœë‹ì€ ì˜ˆìƒë³´ë‹¤ **í›¨ì”¬ ë¯¼ê°í•¨**
- âœ… Baseline í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—ëŠ” ì´ìœ ê°€ ìˆìŒ
- âœ… ì˜ëª»ëœ LRë¡œëŠ” ì–´ë–¤ checkpointë„ ì¢‹ì§€ ì•ŠìŒ
- âœ… checkpoint ìµœì í™” < LR ì„ íƒì˜ ì¤‘ìš”ì„±
- âœ… Dev ì ìˆ˜(20%p ê²©ì°¨)ë¡œ Test ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥

**ìƒˆë¡œìš´ ë°©í–¥**:
- Epochs ì—°ì¥ (20 â†’ 30)
- Warmup Steps ì¡°ì •
- Special Tokens ì¶”ê°€

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Dev/Test ê²©ì°¨ ë¬¸ì œ

**ì¦ìƒ**: Dev set 94ì , Test set 20ì 

**ì›ì¸**: ê³¼ì í•© ë˜ëŠ” ë°ì´í„° ë¶ˆì¼ì¹˜

**í•´ê²°ì±…**:
1. Baselineë¶€í„° ì¬í˜„ (ìƒëµ ê¸ˆì§€)
2. í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ë³€ê²½
3. Testë¡œë§Œ ê²€ì¦ (DevëŠ” ì°¸ê³ ìš©)
4. ê²©ì°¨ 5ì  ì´ë‚´ ìœ ì§€

### 2. CSV ì œì¶œ ì˜¤ë¥˜

**ì¦ìƒ**: í”Œë«í¼ì—ì„œ í˜•ì‹ ì˜¤ë¥˜

**ì›ì¸**: Index ì»¬ëŸ¼ ëˆ„ë½

**í•´ê²°ì±…**:
```python
# âŒ ì˜ëª»ëœ ë°©ë²•
df.to_csv('output.csv', index=False)

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²• (index í¬í•¨)
df.to_csv('output.csv', index=True)
# ë˜ëŠ”
validate_csv('output.csv')  # ìë™ ê²€ì¦
```

### 3. íŠ¹ìˆ˜ í† í° ë¯¸ì œê±°

**ì¦ìƒ**: ìš”ì•½ë¬¸ì— `<s>`, `</s>`, `<pad>` í¬í•¨

**í•´ê²°ì±…**:
```python
from utils import remove_special_tokens

cleaned = remove_special_tokens(
    summaries,
    tokens=['<s>', '</s>', '<usr>', '<pad>']
)
```

---

## ê°œë°œ ì›ì¹™

### 1. í•œ ë²ˆì— í•˜ë‚˜ì”©
- ë§¤ ì‹¤í—˜ë§ˆë‹¤ **í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ** ë³€ê²½
- ì—¬ëŸ¬ ê°œ ë™ì‹œ ë³€ê²½ ì‹œ ì›ì¸ íŒŒì•… ë¶ˆê°€

### 2. Testë¡œë§Œ ê²€ì¦
- Dev setì€ ì°¸ê³ ìš©
- **Test ì œì¶œë¡œë§Œ ìµœì¢… íŒë‹¨** (12íšŒ/ì¼ ì œí•œ)

### 3. ëª¨ë“  ê²ƒì„ ê¸°ë¡
- Notebookìœ¼ë¡œ ì „ì²´ íë¦„ ë³´ì¡´
- `experiment_logs.md`ì— ê²°ê³¼ ê¸°ë¡
- Git ì»¤ë°‹ ë©”ì‹œì§€ì— ì ìˆ˜ í¬í•¨

### 4. ì¬í˜„ ê°€ëŠ¥ì„±
- ì‹œë“œ ê³ ì • (`set_seed(42)`)
- Config íŒŒì¼ë¡œ ì„¤ì • ê´€ë¦¬
- Notebookìœ¼ë¡œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¶”ì 

---

## ë¡œë“œë§µ

### âœ… Phase 1: Baseline ì¬í˜„ ë° ì¸í”„ë¼ êµ¬ì¶• (ì™„ë£Œ)
- [x] Baseline ì¬í˜„ (46.9426ì )
- [x] ì½”ë“œ ëª¨ë“ˆí™” (7ê°œ ëª¨ë“ˆ, 1,745ì¤„)
- [x] ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ
- [x] Git ê´€ë¦¬

### âœ… Phase 1.5: EDA ë¶„ì„ (ì™„ë£Œ, 2025-10-13)
- [x] 5ê°œ agents ë³‘ë ¬ ë¶„ì„
- [x] í›„ì²˜ë¦¬ ê°œì„  ë°©ì•ˆ ë„ì¶œ
- [x] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš°ì„ ìˆœìœ„ ê²°ì •
- [x] Special Token ìµœì í™” ë°©ì•ˆ
- [x] ë°ì´í„° ì¦ê°• ì¬ì‹œë„ ì „ëµ
- [x] `tasks/eda-findings.md` ë¬¸ì„œí™”

### ğŸ”„ Week 1 (2025-10-13 ~ 10-19) - 50ì  ëŒíŒŒ
- [x] **Day 1 (2025-10-13)**: Exp #2 (í›„ì²˜ë¦¬ v2) â†’ âŒ ì‹¤íŒ¨ (46.99, ë³€í™” ì—†ìŒ)
- [x] **Day 1 (2025-10-13)**: Exp #3 (LR 2e-5) â†’ âŒ ì‹¤íŒ¨ (46.69, -0.26)
- [ ] **Day 2**: Exp #9 (Epochs 30) ë˜ëŠ” Warmup ì¡°ì • â†’ 47~48ì  ëª©í‘œ
- [ ] **Day 3-4**: Time Token ë˜ëŠ” ë‹¤ë¥¸ ì•ˆì „í•œ ê°œì„ 
- [ ] **Day 5-7**: ì¡°í•© ìµœì í™” â†’ 50ì  ëŒíŒŒ ì‹œë„

**ëª©í‘œ**: **50ì  ì´ìƒ ë‹¬ì„±**
**í˜„ì¬ Best**: 46.9526ì  (Baseline Modular)
**ì œì¶œ íšŸìˆ˜**: 8/12 ì‚¬ìš© (4íšŒ ë‚¨ìŒ)

### ğŸ“‹ Week 2 (2025-10-20 ~ 10-26) - 52~54ì 
- [ ] Special Token ì¶”ê°€ (Time/Money)
- [ ] Warmup Steps ì¡°ì • (50, 100)
- [ ] Epochs ì—°ì¥ (30)
- [ ] ê³ ê¸‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©

**ëª©í‘œ**: **52~54ì  ë‹¬ì„±**

### ğŸš€ Week 3+ (Long-term) - 55ì  ì´ìƒ
- [ ] ë°ì´í„° ì¦ê°• ì¬ì‹œë„ (í•„í„°ë§/LLM ìŠ¤íƒ€ì¼ ë³´ì¡´)
- [ ] Ensemble ê¸°ë²• (ì„ íƒì )
- [ ] ë” í° ëª¨ë¸ ì‹¤í—˜ (mBART, KoT5)

**ëª©í‘œ**: **55ì  ì´ìƒ**

---

## ì„±ëŠ¥ ëª©í‘œ

- **í˜„ì¬**: 46.9526ì  (Baseline Modular)
- **Week 1 ëª©í‘œ**: 50ì  ëŒíŒŒ
- **Week 2 ëª©í‘œ**: 52~54ì 
- **ìµœì¢… ëª©í‘œ**: 55ì  ì´ìƒ

### ì˜ˆìƒ ì„±ê³¼ (EDA ë¶„ì„ ê¸°ë°˜)

| Timeline | Target | Key Improvements |
|----------|--------|------------------|
| Day 1 | 48ì  | í›„ì²˜ë¦¬ ê°œì„  |
| Day 2 | 49ì  | LR 2e-5 |
| Week 1 | 50ì  | LR íŠœë‹ ì™„ë£Œ |
| Week 2 | 52~54ì  | Special Token + Warmup |
| Week 3+ | 55ì + | ë°ì´í„° ì¦ê°• ì¬ì‹œë„ |

---

## ê¸°ìˆ  ìŠ¤íƒ

- **ì–¸ì–´**: Python 3.10
- **í”„ë ˆì„ì›Œí¬**: PyTorch 2.5.1, Transformers 4.46.3
- **ëª¨ë¸**: KoBART (digit82/kobart-summarization)
- **í‰ê°€**: ROUGE (rouge 1.0.1)
- **ì‹¤í—˜ ì¶”ì **: Weights & Biases (ì„ íƒì )
- **ë²„ì „ ê´€ë¦¬**: Git, GitHub

---

## ì°¸ê³  ìë£Œ

- [ëŒ€íšŒ í˜ì´ì§€](ëŒ€íšŒ URL)
- [KoBART ëª¨ë¸](https://huggingface.co/digit82/kobart-summarization)
- [ROUGE í‰ê°€ ë°©ë²•](docs/Competition_Overview/evaluation_method.md)
- [Baseline ì½”ë“œ ì„¤ëª…](docs/baseline_code_summary.md)
- [ì¬ì‹œì‘ ê°€ì´ë“œ](docs/RESTART_GUIDE.md)

---

## ë¼ì´ì„ ìŠ¤

MIT License

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-13
**Current Best**: 46.94ì  (Baseline)
**Git Commit**: 3ac2b65