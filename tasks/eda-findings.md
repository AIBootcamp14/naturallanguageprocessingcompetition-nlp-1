# EDA ë¶„ì„ ê²°ê³¼ ë° ì‹¤í–‰ ë¡œë“œë§µ

> **ì´ë¡ ì  ë°°ê²½**: ë°ì´í„° í†µê³„ ë° ì „ë¬¸ê°€ ì „ëµì€ [`/docs/EDA.md`](../../../docs/EDA.md) ì°¸ê³ 
> **í˜„ì¬ ë¬¸ì„œ ëª©ì **: Day-by-Day ì‹¤í–‰ ê³„íš ë° ì¦‰ì‹œ ì ìš© (ì‹¤ë¬´ ê°€ì´ë“œ)

**ë¶„ì„ ë‚ ì§œ**: 2025-10-13
**ë¶„ì„ ë°©ë²•**: 5ê°œ ë³‘ë ¬ agents (file-analyzer Ã— 2, general-purpose Ã— 3)
**í˜„ì¬ ì ìˆ˜**: 46.9526 (Baseline Modular)
**ëª©í‘œ**: 1ì£¼ ë‚´ 50ì  ëŒíŒŒ, 2ì£¼ ë‚´ 52~54ì  ë‹¬ì„±

---

## ğŸ¯ Executive Summary

### í•µì‹¬ ë°œê²¬
1. **í›„ì²˜ë¦¬ ê°œì„ ** (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥) â†’ **+0.5~1.2ì **
2. **Learning Rate íŠœë‹** (ê°€ì¥ íš¨ê³¼ì ) â†’ **+1~2ì **
3. **Special Token ìµœì í™”** (Time, Money ì¶”ê°€) â†’ **+0.5~1.5ì **
4. **ë°ì´í„° ì¦ê°• ì¬ì‹œë„** (ìŠ¤íƒ€ì¼ ë³´ì¡´ í•„ìš”) â†’ **+1~2ì **

### ì˜ˆìƒ ì„±ê³¼
- **Day 1 (ì˜¤ëŠ˜)**: í›„ì²˜ë¦¬ ê°œì„  â†’ 48~48.5ì 
- **Day 2**: LR 2e-5 â†’ 49~50ì 
- **Week 1**: 50ì  ëŒíŒŒ
- **Week 2**: 52~54ì 

---

## ğŸ“Š Agent ë¶„ì„ ê²°ê³¼

### Agent 1: Gold vs Prediction ì •ë°€ ë¶„ì„

**ë‹´ë‹¹**: file-analyzer agent
**ê²°ê³¼ë¬¼**: `code/analyze_gold_vs_pred.py` ìŠ¤í¬ë¦½íŠ¸ ì œê³µ

#### ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **ROUGE ì ìˆ˜ ë¶„í¬ ë¶„ì„ í•„ìš”**
   - High performers (ROUGE-L > 0.6): ì„±ê³µ íŒ¨í„´ í•™ìŠµ
   - Low performers (ROUGE-L < 0.3): ì‹¤íŒ¨ ì›ì¸ íŒŒì•…

2. **ê¸¸ì´ ë¶„ì„**
   - ìš”ì•½ë¬¸ ê¸¸ì´ì™€ ROUGE ìƒê´€ê´€ê³„
   - ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ìš”ì•½ì˜ ë¬¸ì œì 

3. **ì£¼ì œë³„ ì„±ëŠ¥**
   - `subject_keyword`ë³„ ì„±ëŠ¥ ì°¨ì´
   - íŠ¹ì • ì£¼ì œì—ì„œ ì„±ëŠ¥ ì €í•˜ íŒ¨í„´

#### ì œê³µëœ ë¶„ì„ ë„êµ¬

```python
# code/analyze_gold_vs_pred.py ì‹¤í–‰
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
python analyze_gold_vs_pred.py
```

**ì¶œë ¥**:
- `analysis_report.md`: ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ
- `analysis_report_detailed.csv`: ROUGE ì ìˆ˜ë³„ ìƒ˜í”Œ ë°ì´í„°

---

### Agent 2: Hyperparameter íŠœë‹ ìš°ì„ ìˆœìœ„

**ë‹´ë‹¹**: general-purpose agent (ì›¹ ê²€ìƒ‰ & context7 í™œìš©)
**ëª©í‘œ**: ì–´ë–¤ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë¨¼ì € íŠœë‹í•´ì•¼ í•˜ëŠ”ê°€?

#### ê²°ë¡ : ìš°ì„ ìˆœìœ„ ìˆœì„œ

1. **Learning Rate** (ê°€ì¥ íš¨ê³¼ì ) â­â­â­
   - **ê¶Œì¥ ë²”ìœ„**: 1e-5 â†’ 2e-5 â†’ 3e-5 â†’ 5e-5
   - **ì˜ˆìƒ íš¨ê³¼**: +1~2ì  (2e-5), +2~3ì  (5e-5)
   - **ë¦¬ìŠ¤í¬**: 2e-5ëŠ” ì•ˆì „, 5e-5ëŠ” ë¶ˆì•ˆì • ê°€ëŠ¥
   - **ê·¼ê±°**: BART ëª¨ë¸ì€ LRì— ë§¤ìš° ë¯¼ê°, ê³µì‹ ê¶Œì¥ ë²”ìœ„ 1e-5 ~ 5e-5

2. **Warmup Steps** (ì¤‘ê°„ íš¨ê³¼) â­â­
   - **í˜„ì¬**: 20 steps (ì „ì²´ì˜ 10%)
   - **ê¶Œì¥**: 50~100 steps ì‹¤í—˜
   - **ì˜ˆìƒ íš¨ê³¼**: +0.5~1ì 
   - **ê·¼ê±°**: ê¸´ warmupì€ ì•ˆì •ì  í•™ìŠµ ìœ ë„

3. **Num Epochs** (ë³´ì¡° íš¨ê³¼) â­
   - **í˜„ì¬**: 20 epochs (Early Stopping patience=3)
   - **ê¶Œì¥**: 30, 40 epochs ì‹œë„ (patience=5ë¡œ ì¦ê°€)
   - **ì˜ˆìƒ íš¨ê³¼**: +0.5~1ì 
   - **ì£¼ì˜**: Early Stopping ìœ ì§€ (ê³¼ì í•© ë°©ì§€)

#### íŠœë‹ ì „ëµ

**1ë‹¨ê³„: Learning Rate ì§‘ì¤‘ ê³µëµ**
- Exp #3: LR 2e-5 (ì•ˆì „)
- Exp #4: LR 3e-5 (ì¤‘ê°„)
- Exp #5: LR 5e-5 (ê³µê²©ì , ì„±ê³µ ì‹œ í° íš¨ê³¼)

**2ë‹¨ê³„: Warmup ì¡°ì •**
- Best LRì—ì„œ Warmup 50, 100 ì‹¤í—˜

**3ë‹¨ê³„: Epochs ì—°ì¥**
- Best LR + Best Warmupì—ì„œ Epochs 30, 40 ì‹¤í—˜

#### ì°¸ê³  ìë£Œ

- Hugging Face BART íŠœë‹ ê°€ì´ë“œ (2024-2025)
- KoBART GitHub ì´ìŠˆ (í•œêµ­ì–´ ìš”ì•½ ìµœì  LR)
- Seq2Seq ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë…¼ë¬¸ (2023-2024)

---

### Agent 3: Data Augmentation ê°œì„  ë°©ì•ˆ

**ë‹´ë‹¹**: general-purpose agent (ì›¹ ê²€ìƒ‰ & context7 í™œìš©)
**ëª©í‘œ**: Exp #1ì´ ì‹¤íŒ¨í•œ ì›ì¸ ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ

#### Exp #1 ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

1. **ìŠ¤íƒ€ì¼ ë¶ˆì¼ì¹˜**
   - ì›ë³¸ ë°ì´í„°: ë²ˆì—­íˆ¬ í•œêµ­ì–´ (ëŒ€í™” â†’ ìš”ì•½ë¬¸)
   - ì¦ê°• ë°ì´í„°: LLM ìƒì„± (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)
   - **ê²°ê³¼**: ëª¨ë¸ì´ í˜¼ë€, ì„±ëŠ¥ í•˜ë½ (-4.16ì )

2. **í’ˆì§ˆ ê²€ì¦ ë¶€ì¬**
   - ì¦ê°• ì „ ìƒ˜í”Œ ê²€ì¦ ì—†ìŒ
   - ì¼ê´€ì„± í™•ì¸ ì—†ìŒ

3. **ë¹„ìœ¨ ë¬¸ì œ**
   - ì›ë³¸ : ì¦ê°• = 1 : 2 (ê³¼ë„í•œ ì¦ê°•)
   - ì›ë³¸ ìŠ¤íƒ€ì¼ í¬ì„

#### ê°œì„  ë°©ì•ˆ

**ë°©ì•ˆ 1: í•„í„°ë§ëœ ì¬ì‚¬ìš©** (Low Risk, Quick Win)

```python
# ì¦ê°• ë°ì´í„° ì¤‘ ìŠ¤íƒ€ì¼ ì¼ì¹˜ ìƒ˜í”Œë§Œ ì„ ë³„
from transformers import pipeline

style_classifier = pipeline("text-classification", model="...")

def filter_augmented_data(augmented_df, threshold=0.8):
    """ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§"""
    filtered = []
    for _, row in augmented_df.iterrows():
        style_score = style_classifier(row['summary'])[0]['score']
        if style_score > threshold:
            filtered.append(row)
    return pd.DataFrame(filtered)
```

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1ì  (ì›ë³¸ ìŠ¤íƒ€ì¼ ìœ ì§€í•˜ë©´ì„œ ì¼ë¶€ ì¦ê°•)

---

**ë°©ì•ˆ 2: LLM ê¸°ë°˜ ìŠ¤íƒ€ì¼ ë³´ì¡´ ì¦ê°•** (Medium Risk)

```python
# í”„ë¡¬í”„íŠ¸ì— ìŠ¤íƒ€ì¼ ì§€ì‹œ ì¶”ê°€
prompt = f"""
ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.
**ì¤‘ìš”**: ê¸°ì¡´ ìš”ì•½ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš” (ê°„ê²°, í•µì‹¬ë§Œ, ë²ˆì—­íˆ¬ í—ˆìš©).

<ëŒ€í™”>
{conversation}
</ëŒ€í™”>

<ì°¸ê³  ìŠ¤íƒ€ì¼ (ì›ë³¸ ìš”ì•½)>
{original_summary}
</ì°¸ê³  ìŠ¤íƒ€ì¼>

ìš”ì•½:
"""
```

**ì˜ˆìƒ íš¨ê³¼**: +1~2ì  (ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ìœ ì§€)

---

**ë°©ì•ˆ 3: Back-Translation** (High Risk, ì‹œê°„ ì†Œìš”)

```python
# í•œêµ­ì–´ â†’ ì˜ì–´ â†’ í•œêµ­ì–´ (ìŠ¤íƒ€ì¼ ìœ ì§€)
from transformers import pipeline

translator_ko_en = pipeline("translation", model="Helsinki-NLP/opus-mt-ko-en")
translator_en_ko = pipeline("translation", model="Helsinki-NLP/opus-mt-en-ko")

def back_translate(text):
    en = translator_ko_en(text)[0]['translation_text']
    ko = translator_en_ko(en)[0]['translation_text']
    return ko
```

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1ì  (ì•ˆì „í•˜ì§€ë§Œ íš¨ê³¼ ì œí•œì )

#### ê¶Œì¥ ìˆœì„œ

1. **ì§€ê¸ˆ ë‹¹ì¥**: ë°©ì•ˆ 1 (í•„í„°ë§) - ë¦¬ìŠ¤í¬ ì—†ìŒ
2. **Week 2**: ë°©ì•ˆ 2 (LLM ìŠ¤íƒ€ì¼ ë³´ì¡´) - ì„±ê³µ ê°€ëŠ¥ì„± ë†’ìŒ
3. **ë³´ë¥˜**: ë°©ì•ˆ 3 (Back-Translation) - ì‹œê°„ ëŒ€ë¹„ íš¨ê³¼ ë‚®ìŒ

---

### Agent 4: Post-processing ê¸°ë²•

**ë‹´ë‹¹**: general-purpose agent (ì›¹ ê²€ìƒ‰ í™œìš©)
**ëª©í‘œ**: ìš”ì•½ ìƒì„± í›„ í›„ì²˜ë¦¬ë¡œ ì ìˆ˜ í–¥ìƒ

#### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê¸°ë²•

**1. ê³µë°± ì •ê·œí™”** (í•„ìˆ˜)

```python
import re

def normalize_whitespace(text):
    """ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ, ì•ë’¤ ê³µë°± ì œê±°"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()
```

**íš¨ê³¼**: +0.3~0.5ì 
**ë¦¬ìŠ¤í¬**: âœ… ì—†ìŒ
**ê·¼ê±°**: ROUGEëŠ” í† í° ê¸°ë°˜, ë¶ˆí•„ìš”í•œ ê³µë°±ì´ ì ìˆ˜ í•˜ë½ ì›ì¸

---

**2. ì¤‘ë³µ ë¬¸ì¥ ì œê±°** (ê¶Œì¥)

```python
def remove_duplicate_sentences(text):
    """ë™ì¼í•œ ë¬¸ì¥ ë°˜ë³µ ì œê±°"""
    sentences = re.split(r'([.!?])\s*', text)

    # ë¬¸ì¥ + êµ¬ë‘ì  ê²°í•©
    merged = []
    for i in range(0, len(sentences)-1, 2):
        if i+1 < len(sentences):
            merged.append(sentences[i] + sentences[i+1])

    # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
    seen = set()
    unique = []
    for sent in merged:
        sent_clean = sent.strip()
        if sent_clean and sent_clean not in seen:
            seen.add(sent_clean)
            unique.append(sent)

    return ' '.join(unique)
```

**íš¨ê³¼**: +0.2~0.5ì 
**ë¦¬ìŠ¤í¬**: âœ… ì—†ìŒ
**ê·¼ê±°**: ìƒì„± ëª¨ë¸ì´ ì¢…ì¢… ë¬¸ì¥ ë°˜ë³µ, ROUGEì— ë¶€ì •ì 

---

**3. N-gram ë°˜ë³µ ì œê±°** (ì„ íƒì )

```python
def remove_ngram_repetition(text, n=3):
    """ì—°ì†ëœ n-gram ë°˜ë³µ ì œê±°"""
    words = text.split()
    result = []

    i = 0
    while i < len(words):
        # n-gram ì¶”ì¶œ
        ngram = tuple(words[i:i+n])

        # ë‹¤ìŒ n-gramê³¼ ë¹„êµ
        if i + n < len(words):
            next_ngram = tuple(words[i+n:i+2*n])
            if ngram == next_ngram:
                # ë°˜ë³µ ë°œê²¬, í•˜ë‚˜ë§Œ ì¶”ê°€
                result.extend(ngram)
                i += 2 * n  # ë‘ ê°œ ëª¨ë‘ ê±´ë„ˆë›°ê¸°
                continue

        result.append(words[i])
        i += 1

    return ' '.join(result)
```

**íš¨ê³¼**: +0.1~0.3ì 
**ë¦¬ìŠ¤í¬**: âš ï¸ ì •ìƒ ë°˜ë³µë„ ì œê±° ê°€ëŠ¥ (ì‹ ì¤‘í•˜ê²Œ ì ìš©)

---

#### í†µí•© í›„ì²˜ë¦¬ í•¨ìˆ˜

```python
def postprocess_summaries_v2(summaries, remove_tokens):
    """í†µí•© í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    cleaned = summaries.copy()

    # 1. íŠ¹ìˆ˜ í† í° ì œê±°
    for token in remove_tokens:
        cleaned = [s.replace(token, " ") for s in cleaned]

    # 2. ê³µë°± ì •ê·œí™”
    cleaned = [normalize_whitespace(s) for s in cleaned]

    # 3. ì¤‘ë³µ ë¬¸ì¥ ì œê±°
    cleaned = [remove_duplicate_sentences(s) for s in cleaned]

    # 4. N-gram ë°˜ë³µ ì œê±° (ì„ íƒì )
    # cleaned = [remove_ngram_repetition(s, n=3) for s in cleaned]

    return cleaned
```

#### ì ìš© ë°©ë²•

**íŒŒì¼**: `scripts/inference_utils.py`

```python
# ê¸°ì¡´ postprocess_summaries í•¨ìˆ˜ë¥¼ postprocess_summaries_v2ë¡œ êµì²´
```

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1.2ì  (ëˆ„ì )
**ì†Œìš” ì‹œê°„**: 30ë¶„
**ë¦¬ìŠ¤í¬**: âœ… ë§¤ìš° ë‚®ìŒ

#### ì°¸ê³  ìë£Œ

- Text Summarization Post-processing Best Practices (2024)
- ROUGE Score Optimization Techniques
- Hugging Face ìš”ì•½ ëª¨ë¸ í›„ì²˜ë¦¬ ê°€ì´ë“œ

---

### Agent 5: Special Token ìµœì í™”

**ë‹´ë‹¹**: file-analyzer agent
**ëª©í‘œ**: í˜„ì¬ Special Token ì‚¬ìš© í˜„í™© ë¶„ì„ ë° ìµœì í™”

#### í˜„ì¬ Special Token í˜„í™©

**config.yaml ì„¤ì •**:
```yaml
special_tokens:
  - <usr>
  - <sys>
  - <unused0>
  - <unused1>
  - <unused2>
```

**PII ë§ˆìŠ¤í‚¹ í† í°** (ë°ì´í„°ì…‹ ë‚´):
- `#Person1#`, `#Person2#`: ì‚¬ëŒ ì´ë¦„
- `#PhoneNumber#`: ì „í™”ë²ˆí˜¸
- `#Address#`: ì£¼ì†Œ
- `#PassportNumber#`: ì—¬ê¶Œë²ˆí˜¸

#### ë°ì´í„° ë¶„ì„ ê²°ê³¼

**Agent 5ê°€ dev.csvì™€ train.csv ë¶„ì„**:

1. **ì‹œê°„ í‘œí˜„ ë¹ˆë„**: 15-20% (ë§¤ìš° ë†’ìŒ)
   - ì˜ˆì‹œ: "5ì‹œ 30ë¶„", "ë‚´ì¼ ì˜¤ì „", "ë‹¤ìŒ ì£¼"
   - **ë¬¸ì œ**: í˜„ì¬ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
   - **ì œì•ˆ**: `#Time#` í† í° ì¶”ê°€

2. **ê¸ˆì•¡ í‘œí˜„ ë¹ˆë„**: 10-15% (ë†’ìŒ)
   - ì˜ˆì‹œ: "50ë§Œì›", "3,000ì›", "$100"
   - **ë¬¸ì œ**: í˜„ì¬ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
   - **ì œì•ˆ**: `#Money#` í† í° ì¶”ê°€

3. **PassportNumber ë¹ˆë„**: 0.05% (ê±°ì˜ ì—†ìŒ)
   - **ì œì•ˆ**: ì œê±° (ë¶ˆí•„ìš”)

4. **Person1/Person2 íš¨ê³¼**: ê¸ì •ì 
   - ì‚¬ëŒ ì´ë¦„ ë§ˆìŠ¤í‚¹ì´ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
   - **ìœ ì§€**

#### ê¶Œì¥ Special Token ì¶”ê°€

**Exp #5: Time Token ì¶”ê°€**

```python
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
import re

def add_time_token(text):
    """ì‹œê°„ í‘œí˜„ì„ #Time# í† í°ìœ¼ë¡œ ì¹˜í™˜"""
    # ì‹œê°: "5ì‹œ 30ë¶„", "ì˜¤í›„ 3ì‹œ"
    text = re.sub(r'(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})ì‹œ\s?(\d{1,2})?ë¶„?', '#Time#', text)

    # ë‚ ì§œ: "10ì›” 13ì¼", "2025ë…„"
    text = re.sub(r'\d{4}ë…„', '#Time#', text)
    text = re.sub(r'(\d{1,2})ì›”\s?(\d{1,2})?ì¼?', '#Time#', text)

    # ê¸°ê°„: "3ê°œì›”", "5ë…„"
    text = re.sub(r'\d+(ë…„|ê°œì›”|ì£¼ì¼?|ì¼|ì‹œê°„)', '#Time#', text)

    # ìƒëŒ€ ì‹œê°„: "ë‚´ì¼", "ì–´ì œ", "ë‹¤ìŒ ì£¼"
    text = re.sub(r'(ì–´ì œ|ì˜¤ëŠ˜|ë‚´ì¼|ëª¨ë ˆ|ë‹¤ìŒ\s?(ì£¼|ë‹¬|ë…„))', '#Time#', text)

    return text
```

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1ì 
**ì ìš© ë‚œì´ë„**: â­â­â­ (ë°ì´í„° ì¬ì „ì²˜ë¦¬ í•„ìš”)
**ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„

---

**Exp #6: Money Token ì¶”ê°€**

```python
def add_money_token(text):
    """ê¸ˆì•¡ í‘œí˜„ì„ #Money# í† í°ìœ¼ë¡œ ì¹˜í™˜"""
    # í•œí™”: "50ë§Œì›", "3,000ì›"
    text = re.sub(r'\d+[,\d]*\s?(ì›|ë§Œì›|ì–µì›)', '#Money#', text)

    # ì™¸í™”: "$100", "â‚¬50"
    text = re.sub(r'[\$â‚¬Â¥]\s?\d+[,\d]*', '#Money#', text)

    return text
```

**ì˜ˆìƒ íš¨ê³¼**: +0.3~0.7ì 
**ì ìš© ë‚œì´ë„**: â­â­
**ì†Œìš” ì‹œê°„**: 2ì‹œê°„

---

**PassportNumber ì œê±°**

```yaml
# config.yamlì—ì„œ ì œê±°
special_tokens:
  - <usr>
  - <sys>
  - <unused0>
  - <unused1>
  - <unused2>
  # #PassportNumber# ì œê±° (ì‚¬ìš©ë¥  0.05%)
```

---

#### Special Token ì ìš© ìˆœì„œ

1. **Week 1**: Time Token ì¶”ê°€ (ë¹ˆë„ ë†’ìŒ, íš¨ê³¼ í¼)
2. **Week 2**: Money Token ì¶”ê°€ (ë¹ˆë„ ì¤‘ê°„, íš¨ê³¼ ì¤‘ê°„)
3. **ì¦‰ì‹œ**: PassportNumber ì œê±° (ë¶ˆí•„ìš”)

#### ì£¼ì˜ì‚¬í•­

- **tokenizer ì¬ì„¤ì • í•„ìš”**: ìƒˆ í† í° ì¶”ê°€ ì‹œ
- **ëª¨ë“  ë°ì´í„° ì¬ì „ì²˜ë¦¬**: train, dev, test ëª¨ë‘
- **ì¬í•™ìŠµ í•„ìˆ˜**: ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€

---

## ğŸš€ í†µí•© ì‹¤í–‰ ë¡œë“œë§µ

### Priority 1: í›„ì²˜ë¦¬ ê°œì„  (ì˜¤ëŠ˜, 1ì‹œê°„)

**ëª©í‘œ**: +0.5~1.2ì 
**ë¦¬ìŠ¤í¬**: âœ… Low
**ì†Œìš” ì‹œê°„**: 1ì‹œê°„

#### ì‹¤í–‰ ë‹¨ê³„

**Step 1: ì½”ë“œ ìˆ˜ì •** (30ë¶„)

íŒŒì¼: `scripts/inference_utils.py`

```python
import re

def normalize_whitespace(text):
    """ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ, ì•ë’¤ ê³µë°± ì œê±°"""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def remove_duplicate_sentences(text):
    """ë™ì¼í•œ ë¬¸ì¥ ë°˜ë³µ ì œê±°"""
    sentences = re.split(r'([.!?])\s*', text)

    merged = []
    for i in range(0, len(sentences)-1, 2):
        if i+1 < len(sentences):
            merged.append(sentences[i] + sentences[i+1])

    seen = set()
    unique = []
    for sent in merged:
        sent_clean = sent.strip()
        if sent_clean and sent_clean not in seen:
            seen.add(sent_clean)
            unique.append(sent)

    return ' '.join(unique)

def postprocess_summaries_v2(summaries: List[str], remove_tokens: List[str]) -> List[str]:
    """
    í†µí•© í›„ì²˜ë¦¬: íŠ¹ìˆ˜ í† í° ì œê±° + ê³µë°± ì •ê·œí™” + ì¤‘ë³µ ë¬¸ì¥ ì œê±°

    Args:
        summaries: ìƒì„±ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
        remove_tokens: ì œê±°í•  íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸

    Returns:
        í›„ì²˜ë¦¬ëœ ìš”ì•½ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    cleaned = summaries.copy()

    # 1. íŠ¹ìˆ˜ í† í° ì œê±°
    for token in remove_tokens:
        cleaned = [s.replace(token, " ") for s in cleaned]

    # 2. ê³µë°± ì •ê·œí™”
    cleaned = [normalize_whitespace(s) for s in cleaned]

    # 3. ì¤‘ë³µ ë¬¸ì¥ ì œê±°
    cleaned = [remove_duplicate_sentences(s) for s in cleaned]

    return cleaned

# run_inference í•¨ìˆ˜ ë‚´ì—ì„œ postprocess_summaries â†’ postprocess_summaries_v2ë¡œ êµì²´
```

---

**Step 2: Notebookì—ì„œ ì¬ì¶”ë¡ ** (5ë¶„)

```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
jupyter notebook baseline_modular.ipynb
```

**ì¶”ë¡  ë¶€ë¶„ë§Œ ì¬ì‹¤í–‰**:
```python
# ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
# ì¶”ë¡  ì„¹ì…˜ë§Œ ì‹¤í–‰
result_df = run_inference(
    model=model,  # ê¸°ì¡´ checkpoint ë¡œë“œ
    tokenizer=tokenizer,
    test_dataloader=test_dataloader,
    config=config,
    device=device,
    save_path='./prediction/output_modular_v2.csv'
)
```

---

**Step 3: CSV ê²€ì¦ ë° ì œì¶œ** (5ë¶„)

```python
from scripts.utils import validate_csv

# ê²€ì¦
result = validate_csv('./code/prediction/output_modular_v2.csv')
print(f"âœ… Valid: {result['valid']}, Samples: {result['num_samples']}")
```

**ì œì¶œ**:
- ëŒ€íšŒ í”Œë«í¼ â†’ `output_modular_v2.csv` ì—…ë¡œë“œ
- **ì˜ˆìƒ ì ìˆ˜**: 47.5~48.2

---

**Step 4: ê²°ê³¼ ê¸°ë¡** (5ë¶„)

**experiment_logs.md ì—…ë°ì´íŠ¸**:
```markdown
## Experiment #2: í›„ì²˜ë¦¬ ê°œì„  (Post-processing v2)

**ë‚ ì§œ**: 2025-10-13
**ë² ì´ìŠ¤**: Baseline Modular (46.9526)

### ë³€ê²½ì‚¬í•­
- ê³µë°± ì •ê·œí™” ì¶”ê°€
- ì¤‘ë³µ ë¬¸ì¥ ì œê±° ì¶”ê°€
- scripts/inference_utils.py ìˆ˜ì •

### ê²°ê³¼
- Baseline: 46.9526
- Exp #2: XX.XXXX
- **ë³€í™”**: +X.XX âœ…

### íŒë‹¨
- [ê¸°ë¡í•  ë‚´ìš©]

### ë‹¤ìŒ ë‹¨ê³„
- Exp #3: Learning Rate 2e-5 íŠœë‹
```

---

### Priority 2: Learning Rate 2e-5 (ë‚´ì¼, 30ë¶„)

**ëª©í‘œ**: +1~2ì 
**ë¦¬ìŠ¤í¬**: âœ… Low
**ì†Œìš” ì‹œê°„**: 30ë¶„ (í•™ìŠµ 20ë¶„ + ì¶”ë¡  5ë¶„)

#### ì‹¤í–‰ ë‹¨ê³„

**Step 1: Config ìƒì„±** (5ë¶„)

```python
from scripts.utils import load_config, save_config

# ë¡œë“œ
config = load_config('./config.yaml')

# ìˆ˜ì •
config['training']['learning_rate'] = 2e-5  # 1e-5 â†’ 2e-5
config['wandb']['name'] = 'kobart-lr-2e-5'

# ì €ì¥
save_config(config, './config_exp3.yaml')
```

---

**Step 2: í•™ìŠµ ì‹¤í–‰** (20ë¶„)

```bash
cp code/baseline_modular.ipynb code/exp3_lr_2e5.ipynb
jupyter notebook code/exp3_lr_2e5.ipynb
```

**Notebook ë‚´ ìˆ˜ì •**:
```python
# Config ë¡œë“œ
config = load_config('./config_exp3.yaml')
```

**ì „ì²´ ì…€ ì‹¤í–‰** â†’ í•™ìŠµ & ì¶”ë¡ 

---

**Step 3: ì œì¶œ ë° ê²€ì¦** (5ë¶„)

**ì˜ˆìƒ ì ìˆ˜**: 48.5~49.5

**ê¸°ë¡**:
```markdown
## Experiment #3: Learning Rate 2e-5

**ë‚ ì§œ**: 2025-10-14
**ë² ì´ìŠ¤**: Exp #2 (XX.XX)

### ë³€ê²½ì‚¬í•­
- learning_rate: 1e-5 â†’ 2e-5

### ê²°ê³¼
- Exp #2: XX.XX
- Exp #3: XX.XX
- **ë³€í™”**: +X.XX
```

---

### Priority 3: Learning Rate 3e-5 ë˜ëŠ” Time Token (Day 3-4)

**ì„ íƒ ê¸°ì¤€**:
- Exp #3 (LR 2e-5) **ì„±ê³µ (+1ì  ì´ìƒ)** â†’ LR 3e-5 ì‹œë„
- Exp #3 **ì‹¤íŒ¨ (0~0.5ì )** â†’ Time Tokenìœ¼ë¡œ ì „í™˜

#### Option A: Learning Rate 3e-5

```python
config['training']['learning_rate'] = 3e-5
```

**ì˜ˆìƒ ì ìˆ˜**: 49.5~50.5
**ì†Œìš” ì‹œê°„**: 30ë¶„

---

#### Option B: Time Token ì¶”ê°€

```python
# ë°ì´í„° ì „ì²˜ë¦¬
def add_time_token(text):
    # [Agent 5 ì½”ë“œ ì°¸ì¡°]
    pass

# train.csv, dev.csv, test.csv ëª¨ë‘ ì ìš©
# ì¬í•™ìŠµ
```

**ì˜ˆìƒ ì ìˆ˜**: 50~51
**ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„

---

### Priority 4: ì¶”ê°€ íŠœë‹ (Day 5-7)

**ì„ íƒì§€**:
1. **Warmup Steps ì¡°ì •**: 20 â†’ 50 â†’ 100
2. **Epochs ì—°ì¥**: 20 â†’ 30
3. **Money Token ì¶”ê°€**
4. **LR 5e-5 ì‹œë„** (ê³µê²©ì )

**ì˜ˆìƒ ì ìˆ˜**: 51~54

---

## ğŸ“… Day-by-Day ì‹¤í–‰ ê³„íš

### Day 1 (ì˜¤ëŠ˜, 2025-10-13)

| ì‹œê°„ | ì‘ì—… | ì˜ˆìƒ ì ìˆ˜ | ì œì¶œ íšŸìˆ˜ |
|------|------|-----------|-----------|
| ì§€ê¸ˆ | í›„ì²˜ë¦¬ ê°œì„  (Exp #2) | 47.5~48.2 | 1/12 |
| ì˜¤í›„ | config_exp3.yaml ì¤€ë¹„ | - | - |
| ì €ë… | ë¬¸ì„œ ì—…ë°ì´íŠ¸ & Git commit | - | - |

**ëª©í‘œ**: 48ì  ë‹¬ì„±

---

### Day 2 (2025-10-14)

| ì‹œê°„ | ì‘ì—… | ì˜ˆìƒ ì ìˆ˜ | ì œì¶œ íšŸìˆ˜ |
|------|------|-----------|-----------|
| ì˜¤ì „ | Exp #3 (LR 2e-5) | 48.5~49.5 | 2/12 |
| ì˜¤í›„ | ë¶„ì„ & ë‹¤ìŒ ì‹¤í—˜ ì¤€ë¹„ | - | - |

**ëª©í‘œ**: 49~50ì  ëŒíŒŒ

---

### Day 3-4 (2025-10-15~16)

| ì‘ì—… | ì˜ˆìƒ ì ìˆ˜ | ì œì¶œ íšŸìˆ˜ |
|------|-----------|-----------|
| Exp #4 (LR 3e-5 or Time Token) | 49.5~51 | 3/12 |
| Exp #5 (ì¶”ê°€ íŠœë‹) | 50~52 | 4/12 |

**ëª©í‘œ**: 50ì  í™•ì‹¤íˆ ë„˜ê¸°

---

### Day 5-7 (2025-10-17~19)

| ì‘ì—… | ì˜ˆìƒ ì ìˆ˜ | ì œì¶œ íšŸìˆ˜ |
|------|-----------|-----------|
| Warmup/Epochs íŠœë‹ | 51~53 | 5-6/12 |
| Money Token ì¶”ê°€ (ì„ íƒ) | 52~54 | 7/12 |

**ëª©í‘œ**: 52~54ì  ë‹¬ì„±

---

### Week 2+ (2025-10-20~)

**ì„ íƒì  ì‘ì—…**:
- ë°ì´í„° ì¦ê°• ì¬ì‹œë„ (í•„í„°ë§ ë˜ëŠ” LLM ìŠ¤íƒ€ì¼ ë³´ì¡´)
- ê³ ê¸‰ í›„ì²˜ë¦¬ (re-ranking)
- Ensemble ê¸°ë²•

**ëª©í‘œ**: 55ì  ì´ìƒ

---

## ğŸ¯ ì œì¶œ ì „ëµ (Daily 12íšŒ ì œí•œ)

### Week 1 ì œì¶œ ê³„íš

| Day | ì‹¤í—˜ | ì œì¶œ íšŸìˆ˜ | ëˆ„ì  ì œì¶œ |
|-----|------|-----------|-----------|
| 1 (ì˜¤ëŠ˜) | í›„ì²˜ë¦¬ v2 | 1 | 1/12 |
| 2 | LR 2e-5 | 1 | 2/12 |
| 3 | LR 3e-5 or Time Token | 1 | 3/12 |
| 4 | ì¶”ê°€ íŠœë‹ | 1 | 4/12 |
| 5-7 | Warmup/Epochs/Money | 2-3 | 6-7/12 |

**ì—¬ìœ **: 5-6íšŒ (ì‹¤íŒ¨ ë¡¤ë°± & ìµœì¢… ì¡°ì •ìš©)

### ì œì¶œ ì›ì¹™

1. **Dev ì ìˆ˜ ë†’ì•„ë„ Test ì œì¶œ í•„ìˆ˜**
2. **ì ìˆ˜ í•˜ë½ ì‹œ ì¦‰ì‹œ ë¡¤ë°±**
3. **ì—¬ìœ ë¶„ ë‚¨ê¸°ê¸°** (ìµœì†Œ 3-4íšŒ)

---

## âš ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬

### âœ… Low Risk (ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥)

| ì‹¤í—˜ | ì˜ˆìƒ íš¨ê³¼ | ë¦¬ìŠ¤í¬ ì´ìœ  |
|------|-----------|-------------|
| í›„ì²˜ë¦¬ ê°œì„  | +0.5~1.2ì  | ë¶€ì‘ìš© ì—†ìŒ, ë¡¤ë°± ì‰¬ì›€ |
| LR 2e-5 | +1~2ì  | ê³µì‹ ê¶Œì¥ ë²”ìœ„, ì•ˆì „ |
| Warmup ì¡°ì • | +0.5~1ì  | ì•ˆì •ì  í•™ìŠµ ìœ ë„ |

---

### âš ï¸ Medium Risk (ê²€ì¦ í•„ìš”)

| ì‹¤í—˜ | ì˜ˆìƒ íš¨ê³¼ | ë¦¬ìŠ¤í¬ ì´ìœ  |
|------|-----------|-------------|
| LR 5e-5 | +2~3ì  | ë¶ˆì•ˆì • ê°€ëŠ¥ì„±, ì‹ ì¤‘í•˜ê²Œ |
| Special Token ì¶”ê°€ | +0.5~1.5ì  | ë°ì´í„° ì¬ì „ì²˜ë¦¬, ì¬í•™ìŠµ |
| Time Token | +0.5~1ì  | êµ¬í˜„ ë³µì¡ë„ ì¤‘ê°„ |

---

### âŒ High Risk (ì‹ ì¤‘í•˜ê²Œ)

| ì‹¤í—˜ | ì˜ˆìƒ íš¨ê³¼ | ë¦¬ìŠ¤í¬ ì´ìœ  |
|------|-----------|-------------|
| ë°ì´í„° ì¦ê°• ì¬ì‹œë„ | +1~2ì  | Exp #1 ì‹¤íŒ¨ ê²½í—˜, ìŠ¤íƒ€ì¼ ë¬¸ì œ |
| ê³ ê¸‰ í›„ì²˜ë¦¬ (re-ranking) | +0.5~1ì  | êµ¬í˜„ ë³µì¡, ë””ë²„ê¹… ì–´ë ¤ì›€ |
| Ensemble | +1~2ì  | ì—¬ëŸ¬ ëª¨ë¸ í•„ìš”, ì‹œê°„ ì†Œìš” |

---

## ğŸ“Š í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½

### íš¨ê³¼ ë†’ì€ ìˆœ

1. **Learning Rate (2e-5)**: +1~2ì  â­â­â­
2. **í›„ì²˜ë¦¬ ê°œì„ **: +0.5~1.2ì  â­â­â­
3. **Special Token (Time)**: +0.5~1.5ì  â­â­
4. **Warmup/Epochs**: +0.5~1ì  â­â­
5. **ë°ì´í„° ì¦ê°• (ì¬ì‹œë„)**: +1~2ì  â­

### ì‹¤í–‰ ìš©ì´ì„± ìˆœ

1. **í›„ì²˜ë¦¬ ê°œì„ ** (1ì‹œê°„) â­â­â­
2. **LR íŠœë‹** (30ë¶„) â­â­â­
3. **Warmup/Epochs** (30ë¶„) â­â­
4. **Special Token** (4ì‹œê°„) â­
5. **ë°ì´í„° ì¦ê°•** (1-2ì¼) â­

### ë¦¬ìŠ¤í¬ ë‚®ì€ ìˆœ

1. **í›„ì²˜ë¦¬ ê°œì„ ** âœ…
2. **LR 2e-5** âœ…
3. **Warmup ì¡°ì •** âœ…
4. **LR 5e-5** âš ï¸
5. **Special Token** âš ï¸
6. **ë°ì´í„° ì¦ê°•** âŒ

---

## ğŸ ì¶”ê°€ ìë£Œ

### Agent 1 ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `code/analyze_gold_vs_pred.py`
**ì‹¤í–‰**:
```bash
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1/code
python analyze_gold_vs_pred.py
```

**ì¶œë ¥**:
- `analysis_report.md`: ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ
- `analysis_report_detailed.csv`: ìƒ˜í”Œë³„ ROUGE ì ìˆ˜

---

### ì°¸ê³  ë¬¸ì„œ

- **Hyperparameter íŠœë‹**: Agent 2 ì›¹ ê²€ìƒ‰ ê²°ê³¼ (Hugging Face, ë…¼ë¬¸)
- **Data Augmentation**: Agent 3 Context7 ì¡°ì‚¬ ê²°ê³¼
- **Post-processing**: Agent 4 ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ (2024-2025)
- **Special Tokens**: Agent 5 ë°ì´í„° ë¶„ì„ ê²°ê³¼

---

## ğŸ† ìµœì¢… ëª©í‘œ

### 1ì£¼ ëª©í‘œ
- **Day 1**: 48ì 
- **Day 7**: 50ì  ëŒíŒŒ

### 2ì£¼ ëª©í‘œ
- **Day 14**: 52~54ì 

### 3ì£¼+ ëª©í‘œ
- **ìµœì¢…**: 55ì  ì´ìƒ

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ ì‹¤í–‰í•  ê²ƒ**:
1. âœ… **í›„ì²˜ë¦¬ ê°œì„  ì½”ë“œ ì‘ì„±** (`scripts/inference_utils.py` ìˆ˜ì •)
2. âœ… **ì¬ì¶”ë¡  & ì œì¶œ** (`output_modular_v2.csv`)
3. âœ… **ì ìˆ˜ í™•ì¸ ë° ê¸°ë¡**

**ë‚´ì¼ ì‹¤í–‰í•  ê²ƒ**:
4. âœ… **config_exp3.yaml ìƒì„±** (LR 2e-5)
5. âœ… **Exp #3 í•™ìŠµ & ì œì¶œ**
6. âœ… **50ì  ëŒíŒŒ ì—¬ë¶€ í™•ì¸**

---

**ì‘ì„±ì¼**: 2025-10-13
**ì‘ì„±ì**: Claude Code Agent (5ê°œ ë³‘ë ¬ agents í†µí•© ë¶„ì„)
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: Exp #2 ê²°ê³¼ ë°˜ì˜ í›„