# PRD: ì¼ìƒ ëŒ€í™” ìš”ì•½ ëª¨ë¸ ì„±ëŠ¥ ê°œì„  (v2.0)

> **ë²„ì „**: 2.0 (EDA ì‹¬ì¸µ ë¶„ì„ ë°˜ì˜)
> **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-14
> **ê¸°ë°˜**: `/Competition/NLP/docs/EDA.md` v2.0 (ì „ë¬¸ê°€ í‰ê°€ ë°˜ì˜)

---

## 1. Introduction/Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ì¼ìƒ ëŒ€í™” í•œêµ­ì–´ ìš”ì•½ ê²½ì§„ëŒ€íšŒ**ì—ì„œ í˜„ì¬ Baseline ì„±ëŠ¥(47ì  ROUGE-F1)ì„ **53.7~60.6ì ìœ¼ë¡œ í–¥ìƒ**ì‹œì¼œ **1ë“±ì„ ë‹¬ì„±**í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### í•µì‹¬ ë¬¸ì œ
- ì´ì „ ì„¸ì…˜ì—ì„œ Dev set ê³¼ì í•© ë¬¸ì œ ë°œìƒ (Dev 94ì  â†’ Test 20ì )
- ë³µì¡í•œ ê°œì„  ì‹œë„ë¡œ ì¸í•œ ë””ë²„ê¹… ë¶ˆê°€ëŠ¥ ìƒíƒœ
- CausalLM ë°©ì‹ ì‹¤íŒ¨ (Dev 76.30 â†’ Test 27.21)
- **í† í° ì œì•½ ì´ˆê³¼ ë¬¸ì œ**: 1.09% (136ê°œ) ìƒ˜í”Œì—ì„œ ì •ë³´ ì†ì‹¤
- **ìˆ«ì/ì‹œê°„ í™˜ê° ë¬¸ì œ**: 87.88% ìƒ˜í”Œì— ìˆ«ì, 63.35% ìƒ˜í”Œì— ì‹œê°„ í‘œí˜„
- **í…œí”Œë¦¿ ê³¼ì í•©**: "~ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤" 526íšŒ ë°˜ë³µ

### í•´ê²° ë°©ì•ˆ
**"í•œ ë²ˆì— í•˜ë‚˜ì”©, Testë¡œ ê²€ì¦"** ì›ì¹™ì„ ë°”íƒ•ìœ¼ë¡œ **EDA ê¸°ë°˜ì˜ êµ¬ì²´ì ì´ê³  ê²€ì¦ëœ ì „ëµ**ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ì—¬, ì¬í˜„ ê°€ëŠ¥í•˜ê³  ë°ì´í„° ê¸°ë°˜ì˜ ì„±ëŠ¥ ê°œì„ ì„ ë‹¬ì„±

---

## 2. Goals

### ì£¼ìš” ëª©í‘œ ğŸ”„ **ì—…ë°ì´íŠ¸**
1. **Baseline 47ì  â†’ 53.7~60.6ì  ë‹¬ì„±** (ROUGE-F1 ê¸°ì¤€) - **ëª©í‘œ ìƒí–¥**
   - Phase 1: 47 â†’ 49.4~51.6ì 
   - Phase 2: 49.4~51.6 â†’ 51.4~55.8ì 
   - Phase 3: 51.4~55.8 â†’ 53.7~60.6ì 
2. **Dev/Test ê²©ì°¨ 5ì  ì´ë‚´ ìœ ì§€** (ê³¼ì í•© ë°©ì§€)
3. **ì¬í˜„ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•** (ëª¨ë“  ë‹¨ê³„ ë¬¸ì„œí™”)
4. **ì œì¶œ íšŸìˆ˜ 12íšŒ ë‚´ íš¨ìœ¨ì  í™œìš©** (Daily ì œí•œ)

### ë¶€ê°€ ëª©í‘œ
5. W&B ê¸°ë°˜ ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•
6. KoBART Baseline ì•ˆì •í™” í›„ CausalLM ë°©ì‹ ì¬ê²€ì¦
7. Git/ë¬¸ì„œ ìë™ ìµœì‹ í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
8. ëŒ€íšŒ 1ë“± ë‹¬ì„±

---

## 3. User Stories

### Story 1: ì—°êµ¬ì/ê°œë°œìë¡œì„œì˜ ì‹¤í—˜ ê´€ë¦¬
```
As a: ëŒ€íšŒ ì°¸ê°€ì (ì—°êµ¬ì/ê°œë°œì)
I want to: ê° ê°œì„ ì‚¬í•­ì˜ íš¨ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ì¶”ì 
So that: ë¬´ì—‡ì´ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í–ˆëŠ”ì§€ ëª…í™•íˆ íŒŒì•…í•˜ê³  ì¬í˜„í•  ìˆ˜ ìˆë‹¤
```

### Story 2: ì•ˆì •ì ì¸ ì„±ëŠ¥ ê°œì„ 
```
As a: ëŒ€íšŒ ì°¸ê°€ì
I want to: Dev set ì ìˆ˜ë¥¼ ì°¸ê³ ë§Œ í•˜ê³  Test setìœ¼ë¡œ ìµœì¢… ê²€ì¦
So that: ì´ì „ì²˜ëŸ¼ Dev ê³¼ì í•© ë¬¸ì œë¥¼ í”¼í•˜ê³  ì‹¤ì „ ì„±ëŠ¥ì„ ì •í™•íˆ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤
```

### Story 3: íš¨ìœ¨ì ì¸ ì œì¶œ ê´€ë¦¬
```
As a: ëŒ€íšŒ ì°¸ê°€ì
I want to: Dev setìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜ í›„ 3-4ê°œ ê°œì„ ì‚¬í•­ë§ˆë‹¤ Test ì œì¶œ
So that: ì œì¶œ íšŸìˆ˜ 12íšŒ ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ê²€ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
```

### Story 4: ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • â­ **ì¶”ê°€**
```
As a: ì—°êµ¬ì
I want to: EDA ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ì „ëµë¶€í„° ì ìš©
So that: ì œí•œëœ ì‹œê°„ê³¼ ì œì¶œ íšŸìˆ˜ì—ì„œ ìµœëŒ€ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤
```

### Story 5: ìë™í™”ëœ ì›Œí¬í”Œë¡œìš°
```
As a: ê°œë°œì
I want to: í•™ìŠµ/ì¶”ë¡ /CSV ìƒì„±ì„ ìë™í™”í•˜ë˜ ì œì¶œ ì „ ìˆ˜ë™ ê²€ì¦
So that: ì‹œê°„ì„ ì ˆì•½í•˜ë©´ì„œë„ ì—ëŸ¬ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆë‹¤
```

---

## 4. Functional Requirements ğŸ”„ **ì „ë©´ ê°œí¸**

### Phase 1: ì¦‰ì‹œ ì‹¤í–‰ âš¡ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1.5ì¼)

**ëª©í‘œ**: Baseline 47ì  â†’ **49.4~51.6ì **

#### FR1.1: Max Token Length í™•ì¥ â­ **EDA ë°˜ì˜**

**ëª©ì **: 512 í† í° ì´ˆê³¼ ìƒ˜í”Œ (136ê°œ, 1.09%) ì •ë³´ ì†ì‹¤ ë°©ì§€

**êµ¬í˜„**:
```yaml
# config.yaml ìˆ˜ì •
encoder_max_length: 768  # 512 â†’ 768
decoder_max_length: 128  # 100 â†’ 128
```

**ê·¼ê±°**:
- Train 136ê°œ ìƒ˜í”Œì—ì„œ truncation ë°œìƒ â†’ ëŒ€í™” ë’·ë¶€ë¶„ ì •ë³´ ì™„ì „ ì†ì‹¤
- Dev 8ê°œ, Test 18ê°œë„ 512 í† í° ì´ˆê³¼
- Baseline ì˜ˆì¸¡ ë¶„ì„ ê²°ê³¼: 512 í† í° ì´ˆê³¼ ìƒ˜í”Œì˜ ROUGEê°€ í‰ê· ë³´ë‹¤ 15-20% ë‚®ìŒ

**ê²€ì¦**:
- Dev í‰ê°€ â†’ 512 í† í° ì´ˆê³¼ ìƒ˜í”Œ ì„±ëŠ¥ í™•ì¸
- Test ì œì¶œ (Phase 1 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1.0 ROUGE

**ë¦¬ìŠ¤í¬**: ë©”ëª¨ë¦¬ 1.5ë°° ì¦ê°€ â†’ batch size 50â†’32 ì¡°ì •

**ì°¸ê³ **: EDA.md Section 2.2, ì „ëµ 1

---

#### FR1.2: ì œì•½ ë””ì½”ë”© ì ìš© â­â­ **ì „ë¬¸ê°€ ìµœìš°ì„  ê¶Œì¥, EDA ë°˜ì˜**

**ëª©ì **: ìˆ«ì/ì‹œê°„/PII í† í° í™˜ê° ë°©ì§€ (87.88% ìƒ˜í”Œ ì˜í–¥)

**êµ¬í˜„**:
```python
# constrained_decoding.py (ìƒˆ íŒŒì¼)
import re
import torch
from transformers import LogitsProcessor

class NumberTimeConstrainedLogits(LogitsProcessor):
    """ìˆ«ì, ì‹œê°„, PII í† í°ë§Œ í—ˆìš©í•˜ëŠ” ì œì•½ ë””ì½”ë”©"""

    def __init__(self, tokenizer, input_text):
        self.tokenizer = tokenizer

        # ì…ë ¥ì—ì„œ í—ˆìš©í•  íŒ¨í„´ ì¶”ì¶œ
        self.allowed_numbers = set(re.findall(r'\d+', input_text))
        self.allowed_times = set(re.findall(r'\d+ì‹œ|\d+ë¶„|\d+ì´ˆ', input_text))
        self.allowed_pii = set(re.findall(r'#\w+#', input_text))

        # í—ˆìš© í† í° ID ë³€í™˜
        self.allowed_token_ids = self._convert_to_ids()

    def _convert_to_ids(self):
        allowed = []
        for token in self.allowed_numbers | self.allowed_times | self.allowed_pii:
            ids = self.tokenizer.encode(token, add_special_tokens=False)
            allowed.extend(ids)
        return set(allowed)

    def __call__(self, input_ids, scores):
        # í—ˆìš© í† í° ì™¸ì—ëŠ” í™•ë¥  ë‚®ì¶¤ (ë¶€ë¶„ ì œì•½, ì™„ì „ ì°¨ë‹¨ ê¸ˆì§€)
        mask = torch.ones_like(scores) * float('-inf')
        mask[:, list(self.allowed_token_ids)] = 0
        scores = scores + mask * 0.3  # 30% íŒ¨ë„í‹°
        return scores

# baseline.ipynbì— í†µí•©
logits_processor = NumberTimeConstrainedLogits(tokenizer, input_text)
outputs = model.generate(
    input_ids=input_ids,
    attention_mask=attention_mask,
    logits_processor=[logits_processor],  # ì¶”ê°€
    **generation_config
)
```

**ê·¼ê±°**:
- ìˆ«ì íŒ¨í„´ 87.88%, ì‹œê°„ íŒ¨í„´ 63.35% ì¶œí˜„
- ì •í™•íˆ ë³µì‚¬ë˜ì–´ì•¼ í•˜ëŠ” factual ì •ë³´
- ëª¨ë¸ì´ í™˜ê° ë°œìƒ ê°€ëŠ¥ (ì˜ˆ: "3ì‹œ" â†’ "4ì‹œ")
- **Copy Mechanism ëŒ€ì‹  ì¦‰ì‹œ ì ìš© ê°€ëŠ¥** (ëª¨ë¸ êµ¬ì¡° ë³€ê²½ ë¶ˆí•„ìš”)

**ê²€ì¦**:
- Dev í‰ê°€ â†’ ìˆ«ì/ì‹œê°„ ì •í™•ë„ ì¸¡ì •
- Test ì œì¶œ (Phase 1 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1.0 ROUGE

**ë¦¬ìŠ¤í¬**: ì™„ì „ ì°¨ë‹¨ ì‹œ í•„ìš”í•œ ìƒì„± ë¶ˆê°€ â†’ ë¶€ë¶„ ì œì•½ (30% íŒ¨ë„í‹°) ì ìš©

**ì°¸ê³ **: EDA.md Section 2.3, ì „ëµ 4

---

#### FR1.3: ë””ì½”ë”© ê¸¸ì´ ì •ê·œí™” + ì»¤ë²„ë¦¬ì§€ íŒ¨ë„í‹° â­ **ì „ë¬¸ê°€ ê¶Œì¥, EDA ë°˜ì˜**

**ëª©ì **: ì ì • ê¸¸ì´ ìœ ì§€, ì •ë³´ ëˆ„ë½ ë°©ì§€

**êµ¬í˜„**:
```python
# config.yaml ë˜ëŠ” generation_config ìˆ˜ì •
generation_config = {
    "max_length": 128,
    "num_beams": 4,
    "no_repeat_ngram_size": 2,

    # GNMT ê¸¸ì´ ì •ê·œí™” (Google Neural Machine Translation í‘œì¤€ ë°©ì‹)
    "length_penalty": 0.6,  # Î±=0.6 (0.6~1.0 ì‹¤í—˜)
    # score = log_prob / ((5 + length) / 6) ** Î±

    # ì»¤ë²„ë¦¬ì§€ ê´€ë ¨
    "repetition_penalty": 1.2,  # ë°˜ë³µ ì–µì œ
    "diversity_penalty": 0.5,   # ë‹¤ì–‘ì„± ì´‰ì§„ (beam group)

    "early_stopping": True
}

# baseline.ipynbì—ì„œ ì ìš©
outputs = model.generate(
    input_ids=input_ids,
    attention_mask=attention_mask,
    **generation_config
)
```

**ê·¼ê±°**:
- ì••ì¶• ë¹„ìœ¨ í¸ì°¨ í¼ (1.01~19.85ë°°) â†’ ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ìš”ì•½ ë°©ì§€ í•„ìš”
- GNMT í‘œì¤€ ë°©ì‹ìœ¼ë¡œ ê²€ì¦ë¨
- ì •ë³´ ëˆ„ë½ ë¬¸ì œ ê°œì„  (ì»¤ë²„ë¦¬ì§€ íŒ¨ë„í‹°)

**ê²€ì¦**:
- Dev í‰ê°€ â†’ ìš”ì•½ í‰ê·  ê¸¸ì´ ë³€í™” í™•ì¸
- Test ì œì¶œ (Phase 1 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.3~0.8 ROUGE

**ì°¸ê³ **: EDA.md Section 2.5, ì „ëµ 5

---

#### FR1.4: Learning Rate ìµœì í™”

**ëª©ì **: ë¹ ë¥¸ ìˆ˜ë ´, í•™ìŠµ ì†ë„ í–¥ìƒ

**êµ¬í˜„**:
```yaml
# config.yaml
learning_rate: 3e-5  # 1e-5 â†’ 3e-5

# Cosine annealing with warmup
scheduler_type: "cosine"
warmup_ratio: 0.1
num_cycles: 0.5
```

**ê·¼ê±°**:
- í˜„ì¬ 1e-5ëŠ” ì•ˆì „í•˜ì§€ë§Œ ëŠë¦° í•™ìŠµ
- í† í° ê¸¸ì´ ë¶„í¬ê°€ ë„“ê³  ì••ì¶• ë¹„ìœ¨ í¸ì°¨ê°€ í¼ â†’ adaptive learning í•„ìš”

**ê²€ì¦**:
- Dev í‰ê°€ â†’ Loss ìˆ˜ë ´ ì†ë„ í™•ì¸
- Test ì œì¶œ (Phase 1 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.3~0.8 ROUGE

**ë¦¬ìŠ¤í¬**: 3e-5ì—ì„œ ë°œì‚° ê°€ëŠ¥ â†’ Warmup ratio 0.1 ìœ ì§€, gradient clipping ê°•í™” (1.0)

**ì°¸ê³ **: EDA.md ì „ëµ 2

---

#### FR1.5: Hard Sample Mining & Oversampling â­ **EDA ë°˜ì˜**

**ëª©ì **: ë³µí•© ë¬¸ì œ ìƒ˜í”Œ (683ê°œ, 5.48%) ì§‘ì¤‘ ê°œì„ 

**êµ¬í˜„**:
```python
# prepare_hard_samples.py (ìƒˆ íŒŒì¼)
import pandas as pd

def is_hard_sample(row):
    """ë³µí•© ë¬¸ì œ ìƒ˜í”Œ íŒë³„"""
    return (row['dialogue_tokens'] > 512 or
            row['compression_ratio'] < 3.18 or
            row['dialogue_len'] > 700)

# 1. Hard sample íŒë³„
train_df = pd.read_csv('data/train.csv')
hard_samples = train_df[train_df.apply(is_hard_sample, axis=1)]
easy_samples = train_df[~train_df.apply(is_hard_sample, axis=1)]

print(f"Hard samples: {len(hard_samples)} ({len(hard_samples)/len(train_df)*100:.2f}%)")
print(f"Easy samples: {len(easy_samples)}")

# 2. Hard sample 2ë°° oversampling (2ë°° ì´ìƒ ê¸ˆì§€)
train_dataset = pd.concat([
    easy_samples,
    hard_samples,
    hard_samples  # ì¤‘ë³µ
]).sample(frac=1, random_state=42)  # ì…”í”Œ

train_dataset.to_csv('data/train_hard_sampled.csv', index=False)
print(f"Total samples: {len(train_dataset)}")
```

**ê·¼ê±°**:
- ë³µí•© ë¬¸ì œ ìƒ˜í”Œ 683ê°œê°€ ì„±ëŠ¥ ì €í•˜ì˜ ì£¼ë²”
- Baseline ì˜ˆì¸¡ì—ì„œ ì €ì„±ëŠ¥ ìƒ˜í”Œì˜ íŠ¹ì§•: ê¸´ ëŒ€í™” + ë‚®ì€ ì••ì¶• ë¹„ìœ¨ + 512 í† í° ì´ˆê³¼

**ê²€ì¦**:
- Dev í‰ê°€ â†’ Hard sample ì„±ëŠ¥ ê°œì„  í™•ì¸
- Test ì œì¶œ (Phase 1 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.8~1.5 ROUGE

**ë¦¬ìŠ¤í¬**: ì–´ë ¤ìš´ ìƒ˜í”Œ ê³¼ì í•© â†’ ì‰¬ìš´ ìƒ˜í”Œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ â†’ validation ë©´ë°€íˆ ëª¨ë‹ˆí„°ë§

**ì°¸ê³ **: EDA.md Section 2.8, ì „ëµ 3

---

#### Phase 1 ì‹¤í–‰ ë°©ë²•

```bash
# 1. ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸
du -sh /Competition/NLP

# 2. Git ì €ì¥ì†Œë¡œ ì´ë™
cd /Competition/NLP/naturallanguageprocessingcompetition-nlp-1

# 3. config.yaml ìˆ˜ì • (FR1.1, FR1.3, FR1.4)
# encoder_max_length: 768
# decoder_max_length: 128
# learning_rate: 3e-5
# generation_config ì—…ë°ì´íŠ¸

# 4. Hard sample ì¤€ë¹„ (FR1.5)
python prepare_hard_samples.py

# 5. ì œì•½ ë””ì½”ë”© êµ¬í˜„ (FR1.2)
# baseline.ipynbì— constrained_decoding.py í†µí•©

# 6. í•™ìŠµ ì‹¤í–‰
jupyter notebook code/baseline.ipynb

# 7. Test ì œì¶œ ë° ê²€ì¦
```

**Phase 1 ì˜ˆìƒ ì´ íš¨ê³¼**: **+2.4~4.6 ROUGE** (47 â†’ 49.4~51.6ì )

---

### Phase 2: ë‹¨ê¸° ğŸš€ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ì£¼)

**ëª©í‘œ**: 49.4~51.6ì  â†’ **51.4~55.8ì **

#### FR2.1: Longer Training â­ **EDA ë°˜ì˜**

**ëª©ì **: ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í™•ë³´

**êµ¬í˜„**:
```yaml
# config.yaml
num_train_epochs: 30  # 20 â†’ 30
early_stopping_patience: 5  # 3 â†’ 5
```

**ê·¼ê±°**:
- í˜„ì¬ 20 epochs, patience=3
- ë³µì¡í•œ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë” ê¸´ í•™ìŠµ ê°€ëŠ¥

**ê²€ì¦**:
- Dev í‰ê°€ â†’ Loss ìˆ˜ë ´ í™•ì¸
- Test ì œì¶œ (Phase 2 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.3~0.7 ROUGE

**ì°¸ê³ **: EDA.md ì „ëµ 8

---

#### FR2.2: IWCV ë¶„ì„ ë° ì ìš© â­â­ **ì „ë¬¸ê°€ ê¶Œì¥, EDA ë°˜ì˜**

**ëª©ì **: Dev/Test ë¶„í¬ ê´´ë¦¬ ì™„í™”, 12íšŒ ì œì¶œ ì œí•œ í™˜ê²½ ìµœì í™”

**êµ¬í˜„**:
```python
# iwcv_analysis.py (ìƒˆ íŒŒì¼)
from sklearn.linear_model import LogisticRegression
from torch.utils.data import WeightedRandomSampler
import pandas as pd
import numpy as np

def extract_features(df):
    """Dev/Test ë¶„í¬ ì°¨ì´ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ"""
    features = df[[
        'dialogue_len',
        'summary_len',
        'dialogue_tokens',
        'summary_tokens',
        'compression_ratio',
        'num_speakers'  # í•„ìš” ì‹œ ì¶”ê°€
    ]].values
    return features

# 1. Dev/Test ë¶„í¬ ì°¨ì´ ë¶„ì„
dev_df = pd.read_csv('data/dev.csv')
test_df = pd.read_csv('data/test.csv')  # Label ì—†ìŒ, featureë§Œ ì‚¬ìš©

# Feature ì¶”ì¶œ
dev_features = extract_features(dev_df)
test_features = extract_features(test_df)

# Dev(0), Test(1) ë ˆì´ë¸”ë¡œ ë¶„ë¥˜ê¸° í•™ìŠµ
features = np.vstack([dev_features, test_features])
labels = [0] * len(dev_df) + [1] * len(test_df)

clf = LogisticRegression(max_iter=1000)
clf.fit(features, labels)

# 2. Dev ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ê³„ì‚° (Test ë¶„í¬ì™€ ìœ ì‚¬í• ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
dev_weights = clf.predict_proba(dev_features)[:, 1]  # Testì¼ í™•ë¥ 

# 3. Weighted sampling
sampler = WeightedRandomSampler(
    weights=dev_weights,
    num_samples=len(dev_df),
    replacement=True
)

# baseline.ipynbì— í†µí•©
# train_dataloader = DataLoader(
#     dev_dataset,
#     batch_size=50,
#     sampler=sampler  # weighted sampling
# )

print(f"Dev weights: min={dev_weights.min():.3f}, max={dev_weights.max():.3f}, mean={dev_weights.mean():.3f}")
```

**ê·¼ê±°**:
- RESTART_GUIDE.mdì—ì„œ Dev/Test gap ë¬¸ì œ ì‹¬ê°ì„± í™•ì¸
- 12íšŒ ì œì¶œ ì œí•œ â†’ íš¨ìœ¨ì  ê²€ì¦ ë°©ë²• í•„ìˆ˜
- **Importance-Weighted Cross-Validation** (JMLR ë…¼ë¬¸ ê²€ì¦)

**ê²€ì¦**:
- Phase 1 ì™„ë£Œ í›„ IWCV ë¶„ì„ ìˆ˜í–‰
- Dev/Test gap ë³€í™” ëª¨ë‹ˆí„°ë§
- Test ì œì¶œ (Phase 2 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: ì§ì ‘ì  ROUGE í–¥ìƒ ì—†ìŒ, **ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒ**

**ì°¸ê³ **: EDA.md ì „ëµ 13

---

#### FR2.3: LED ëª¨ë¸ íƒìƒ‰ ë° ì‹¤í—˜ â­â­ **ì „ë¬¸ê°€ ìµœìš°ì„  ê¶Œì¥ (ì¡°ê±´ë¶€), EDA ë°˜ì˜**

**ì¡°ê±´**: Phase 1ì˜ Max Length 768 íš¨ê³¼ ë¶ˆì¶©ë¶„ ì‹œ

**ëª©ì **: 512 í† í° ì´ˆê³¼ ìƒ˜í”Œ (136ê°œ, 1.09%) ì™„ë²½ ì²˜ë¦¬

**êµ¬í˜„**:
```python
# led_model.py (ìƒˆ íŒŒì¼, ì¡°ê±´ë¶€)
from transformers import LEDForConditionalGeneration, LEDTokenizer

# 1. í•œêµ­ì–´ LED ëª¨ë¸ íƒìƒ‰
# HuggingFaceì—ì„œ "LED Korean" ê²€ìƒ‰
# ì˜ˆ: "klue/led-base-korean" (ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í•„ìš”)

# 2. ì—†ìœ¼ë©´ ì˜ì–´ LED ëª¨ë¸ì— í•œêµ­ì–´ fine-tuning
model_name = "allenai/led-base-16384"  # 16,384 í† í° ì§€ì›
model = LEDForConditionalGeneration.from_pretrained(model_name)
tokenizer = LEDTokenizer.from_pretrained(model_name)

# 3. Global attention ì„¤ì • (ì¤‘ìš” í† í° í•€)
import torch

def set_global_attention(input_ids, tokenizer):
    """ì¤‘ìš” í† í°ì— global attention ì„¤ì •"""
    global_attention_mask = torch.zeros_like(input_ids)
    global_attention_mask[:, 0] = 1  # [CLS]

    # #Person#, ì‹œê°„, ìˆ«ì ë“± ì¤‘ìš” í† í° ìœ„ì¹˜ ì°¾ê¸°
    for i in range(input_ids.size(1)):
        token = tokenizer.decode(input_ids[:, i])
        if '#' in token or any(char.isdigit() for char in token):
            global_attention_mask[:, i] = 1

    return global_attention_mask

# í•™ìŠµ ì‹œ ì ìš©
global_attention_mask = set_global_attention(input_ids, tokenizer)
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    global_attention_mask=global_attention_mask,
    labels=labels
)
```

**ê·¼ê±°**:
- 512 í† í° ì´ˆê³¼ 1.09% ìƒ˜í”Œ â†’ ì •ë³´ ì†ì‹¤ ë°©ì§€
- LEDëŠ” **16,384 í† í°**ê¹Œì§€ íš¨ìœ¨ì  ì²˜ë¦¬ (window + global attention)
- mBART/KoT5ë³´ë‹¤ ì¥ë¬¸ ì²˜ë¦¬ ìµœì 

**ê²€ì¦**:
1. Phase 1 ì™„ë£Œ í›„ Max Length 768 íš¨ê³¼ ê²€ì¦
2. ë¶ˆì¶©ë¶„ ì‹œ í•œêµ­ì–´ LED ëª¨ë¸ íƒìƒ‰
3. 512 í† í° ì´ˆê³¼ ìƒ˜í”Œ ì„±ëŠ¥ ê°œì„  í™•ì¸
4. Test ì œì¶œ (Phase 2 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +1.0~2.0 ROUGE

**ë¦¬ìŠ¤í¬**: í•œêµ­ì–´ ëª¨ë¸ ì—†ìœ¼ë©´ 1-2ì£¼ ì†Œìš”

**ì°¸ê³ **: EDA.md ì „ëµ 14

---

#### FR2.4: Dynamic Batch Size êµ¬í˜„

**ëª©ì **: GPU íš¨ìœ¨ ì¦ê°€, ê¸´ ìƒ˜í”Œ ì²˜ë¦¬ ìµœì í™”

**êµ¬í˜„**:
```python
# dynamic_batch.py (ìƒˆ íŒŒì¼)

def dynamic_batch_size(num_tokens):
    """í† í° ê¸¸ì´ ê¸°ì¤€ ë™ì  ë°°ì¹˜ í¬ê¸°"""
    if num_tokens < 256:
        return 64
    elif num_tokens < 512:
        return 50
    else:
        return 32

# config.yamlì— gradient accumulation ì¶”ê°€
# gradient_accumulation_steps: 2
# effective_batch_size: 100  # 50 * 2
```

**ê·¼ê±°**:
- ê¸´ ìƒ˜í”Œê³¼ ì§§ì€ ìƒ˜í”Œì´ ì„ì—¬ GPU í™œìš©ë„ ë¹„íš¨ìœ¨
- í˜„ì¬ batch size 50ì€ ì§§ì€ ìƒ˜í”Œì— ìµœì í™”

**ê²€ì¦**:
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- Dev í‰ê°€ â†’ Test ì œì¶œ (Phase 2 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.2~0.5 ROUGE

**ì°¸ê³ **: EDA.md ì „ëµ 7

---

#### FR2.5: Unlikelihood Training ì‹¤í—˜ â­ **ì „ë¬¸ê°€ ê¶Œì¥ (ì‹¤í—˜ì ), EDA ë°˜ì˜**

**ëª©ì **: í…œí”Œë¦¿ ê³¼ì í•© í•´ê²° ("~ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤" 526íšŒ ë°˜ë³µ)

**êµ¬í˜„**:
```python
# unlikelihood_trainer.py (ìƒˆ íŒŒì¼, ì‹¤í—˜ì )
import torch
import torch.nn.functional as F

class UnlikelihoodTrainer:
    """N-gram ë°˜ë³µ ì–µì œ í•™ìŠµ"""

    def __init__(self, model, alpha=0.05):
        self.model = model
        self.alpha = alpha  # unlikelihood weight (0.05~0.1, ì ˆëŒ€ 0.1 ì´ˆê³¼ ê¸ˆì§€)

    def compute_loss(self, logits, labels, prev_ngrams):
        """MLE loss + Unlikelihood loss"""
        # 1. ê¸°ë³¸ cross-entropy loss
        mle_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)

        # 2. Unlikelihood loss (ì´ë¯¸ ìƒì„±ëœ n-gram ì–µì œ)
        ul_loss = 0
        for ngram in prev_ngrams:
            ngram_prob = torch.exp(logits[:, ngram])
            ul_loss += torch.log(1 - ngram_prob + 1e-8)

        ul_loss = -ul_loss.mean()

        # 3. ê²°í•©
        total_loss = mle_loss + self.alpha * ul_loss
        return total_loss

# í•™ìŠµ ì‹œ ì ìš©
trainer = UnlikelihoodTrainer(model, alpha=0.05)

for batch in train_dataloader:
    outputs = model(**batch)
    logits = outputs.logits
    labels = batch['labels']

    # ì´ë¯¸ ìƒì„±ëœ tri-gram ì¶”ì 
    prev_ngrams = extract_trigrams(outputs.sequences)

    loss = trainer.compute_loss(logits, labels, prev_ngrams)
    loss.backward()
```

**ê·¼ê±°**:
- "~ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤" 526íšŒ ë°˜ë³µ (Tri-gram top 1)
- í…œí”Œë¦¿ ê³¼ì í•© ë¬¸ì œ ì‹¬ê° â†’ ì •ë³´ ëˆ„ë½ ê°€ëŠ¥ì„±
- **Unlikelihood Loss** (Welleck et al., 2019 arXiv)

**ê²€ì¦**:
- Alpha 0.05ë¡œ ì‹œì‘ (ì ˆëŒ€ 0.1 ì´ˆê³¼ ê¸ˆì§€)
- Dev ROUGE ì ì§„ì  ëª¨ë‹ˆí„°ë§ (epochë§ˆë‹¤)
- 2 epoch ì—°ì† ì €í•˜ ì‹œ ì¦‰ì‹œ ë¡¤ë°±
- A/B í…ŒìŠ¤íŠ¸: Unlikelihood æœ‰/ç„¡ ë³‘í–‰ í•™ìŠµ
- Test ì œì¶œ (Phase 2 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.3~0.6 ROUGE (ë¶ˆí™•ì‹¤)

**ë¦¬ìŠ¤í¬**: Alpha ê³¼ë‹¤ ì‹œ ROUGE ì €í•˜

**ì°¸ê³ **: EDA.md Section 2.7, ì „ëµ 15

---

#### FR2.6: ì²« Test ì œì¶œ

- Phase 1+2 ëˆ„ì  íš¨ê³¼ í™•ì¸
- Devì™€ Test ì ìˆ˜ gap ë¶„ì„ (IWCV íš¨ê³¼ ê²€ì¦)

**Phase 2 ì˜ˆìƒ ì´ íš¨ê³¼**: **+2.0~4.2 ROUGE** (49.4~51.6 â†’ 51.4~55.8ì )

---

### Phase 3: ì¤‘ì¥ê¸° ğŸ¯ (ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ì£¼)

**ëª©í‘œ**: 51.4~55.8ì  â†’ **53.7~60.6ì **

#### FR3.1: Ensemble ì „ëµ êµ¬ì¶• â­â­ **ìµœê³  íš¨ê³¼**

**ëª©ì **: ì—¬ëŸ¬ checkpoint ì¡°í•©ìœ¼ë¡œ ìµœëŒ€ ì„±ëŠ¥ ë‹¬ì„±

**êµ¬í˜„**:
```python
# ensemble.py (ìƒˆ íŒŒì¼)
import torch
from transformers import BartForConditionalGeneration

# 1. ì—¬ëŸ¬ epoch checkpoint ì €ì¥
top_checkpoints = [
    'checkpoints/epoch_15',
    'checkpoints/epoch_18',
    'checkpoints/epoch_22'
]

# 2. Top-3 checkpoint ì•™ìƒë¸”
predictions = []
for model_path in top_checkpoints:
    model = BartForConditionalGeneration.from_pretrained(model_path)
    model.to('cuda')
    model.eval()

    pred = model.generate(input_ids, **generation_config)
    predictions.append(pred)

# 3. ROUGE ê¸°ë°˜ ì„ íƒ (ê° ìƒ˜í”Œë§ˆë‹¤ ìµœê³  ROUGE ì˜ˆì¸¡ ì„ íƒ)
from rouge import Rouge
rouge = Rouge()

final_summaries = []
for i in range(len(test_dataset)):
    sample_preds = [tokenizer.decode(pred[i], skip_special_tokens=True) for pred in predictions]

    # Dev set ê¸°ì¤€ ìµœê³  ROUGE ëª¨ë¸ ì„ íƒ (ë˜ëŠ” ë‹¤ìˆ˜ê²°)
    best_pred = max(sample_preds, key=lambda x: rouge.get_scores(x, reference[i])['rouge-l']['f'])
    final_summaries.append(best_pred)
```

**ê·¼ê±°**:
- ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„ ê·¹ë³µ
- ì—¬ëŸ¬ checkpoint ì¡°í•©ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ

**ê²€ì¦**:
- Dev í‰ê°€ â†’ Ensemble vs Single model ë¹„êµ
- Test ì œì¶œ (Phase 3 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +1.5~3.0 ROUGE (ê°€ì¥ í° íš¨ê³¼)

**ë¦¬ìŠ¤í¬**: ì¶”ë¡  ì‹œê°„ 3ë°° â†’ submission íŒŒì¼ ìƒì„± ëŠë¦¼ (ìµœì¢… ì œì¶œì—ë§Œ ì‚¬ìš©)

**ì°¸ê³ **: EDA.md ì „ëµ 10

---

#### FR3.2: Copy Mechanism ì •ë°€ êµ¬í˜„ (ì¡°ê±´ë¶€) ğŸ”„ **ìš°ì„ ìˆœìœ„ í•˜í–¥**

**ì¡°ê±´**: ì œì•½ ë””ì½”ë”© íš¨ê³¼ ë¶ˆì¶©ë¶„ ì‹œì—ë§Œ êµ¬í˜„

**ëª©ì **: ìˆ«ì/ì‹œê°„/PII í† í° ì •í™• ë³µì‚¬

**êµ¬í˜„**:
```python
# Pointer-Generator Network êµ¬í˜„ (ë³µì¡)
# 1. Encoder attention ì¶”ê°€
# 2. Copy probability ê³„ì‚°
# 3. Generation vs Copy ì„ íƒ ë©”ì»¤ë‹ˆì¦˜

# ë³µì¡ë„ ë†’ìŒ - ì œì•½ ë””ì½”ë”© íš¨ê³¼ ë¶ˆì¶©ë¶„ ì‹œì—ë§Œ ê³ ë ¤
```

**ê·¼ê±°**:
- ì œì•½ ë””ì½”ë”©ìœ¼ë¡œ 80% ë¬¸ì œ í•´ê²° ê°€ëŠ¥
- ëª¨ë¸ êµ¬ì¡° ë³€ê²½ í•„ìš” (Pointer-Generator) â†’ êµ¬í˜„ ë¹„ìš© ë†’ìŒ (1ì£¼ì¼)

**ê²€ì¦**:
- ì œì•½ ë””ì½”ë”© í›„ ìˆ«ì/ì‹œê°„ ì •í™•ë„ í™•ì¸
- ë¶ˆì¶©ë¶„ ì‹œì—ë§Œ êµ¬í˜„ ì°©ìˆ˜
- Test ì œì¶œ (Phase 3 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: ì œì•½ ë””ì½”ë”© í›„ ì¶”ê°€ ê°œì„ ë¶„ë§Œ +0.3~0.8 ROUGE

**ì°¸ê³ **: EDA.md ì „ëµ 6 (ìš°ì„ ìˆœìœ„ í•˜í–¥)

---

#### FR3.3: Paraphrase Augmentation

**ëª©ì **: ë°ì´í„° ë‹¤ì–‘ì„± ì¦ê°€, í‘œí˜„ íšì¼í™” ë¬¸ì œ í•´ê²°

**êµ¬í˜„**:
```python
# paraphrase_augment.py (ìƒˆ íŒŒì¼)

# 1. Back-translationìœ¼ë¡œ paraphrase ìƒì„±
# í•œêµ­ì–´ â†’ ì˜ì–´ â†’ í•œêµ­ì–´

# 2. Summaryë¥¼ ë‹¤ì–‘í•˜ê²Œ ì¬ì‘ì„±
augmented_data = []
for row in train_df.iterrows():
    original = row['summary']
    paraphrased = paraphrase_model(original)  # Solar Mini ë˜ëŠ” Llama-3.2-Korean-3B
    augmented_data.append({
        'dialogue': row['dialogue'],
        'summary': paraphrased
    })

# 3. ê·œì • ì¤€ìˆ˜ (DialogSum ë¯¸ì‚¬ìš©)
# 4. 2-3ë°° ì¦ê°• (12,457 â†’ 25,000~37,000)
```

**ê·¼ê±°**:
- ìš”ì•½ í‘œí˜„ì´ ë„ˆë¬´ íšì¼í™”
- ë‹¤ì–‘í•œ í‘œí˜„ í•™ìŠµ í•„ìš”

**ê²€ì¦**:
- ì†ŒëŸ‰ í…ŒìŠ¤íŠ¸ (1,000ê°œ) â†’ íš¨ê³¼ ê²€ì¦ â†’ ì „ì²´ ì¦ê°•
- Dev í‰ê°€ â†’ Test ì œì¶œ (Phase 3 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +0.5~1.0 ROUGE

**ì°¸ê³ **: EDA.md ì „ëµ 11

---

#### FR3.4: Model Upgrade (ì¡°ê±´ë¶€) ğŸ”„ **ì¡°ê±´ ë³€ê²½**

**ì¡°ê±´**: LED íš¨ê³¼ ë¶ˆì¶©ë¶„ ì‹œì—ë§Œ mBART/KoT5 ì‹œë„

**ëª©ì **: ë” í° ëª¨ë¸ë¡œ ì„±ëŠ¥ ì²œì¥ ë†’ì´ê¸°

**êµ¬í˜„**:
```python
# Option 1: mBART-large (611M)
model_name = "facebook/mbart-large-cc25"

# Option 2: KoT5-large (770M)
model_name = "KETI-AIR/ke-t5-large"

# Option 3: LED + Ensemble ì¡°í•© (ìµœìš°ì„ )
```

**ê·¼ê±°**:
- KoBARTëŠ” ì‘ì€ ëª¨ë¸ (123M parameters)
- LED Phase 2ì—ì„œ ìš°ì„  ì ìš© â†’ Model Upgrade í•„ìš”ì„± ê°ì†Œ

**ê²€ì¦**:
- Phase 2ê¹Œì§€ íš¨ê³¼ í™•ì¸ í›„ ê²°ì •
- Dev í‰ê°€ â†’ Test ì œì¶œ (Phase 3 í†µí•©)

**ì˜ˆìƒ íš¨ê³¼**: +1.0~2.5 ROUGE

**ì°¸ê³ **: EDA.md ì „ëµ 12

---

**Phase 3 ì˜ˆìƒ ì´ íš¨ê³¼**: **+2.3~4.8 ROUGE** (51.4~55.8 â†’ 53.7~60.6ì )

---

### Phase 4: W&B ì¶”ì  ì‹œìŠ¤í…œ

#### FR4.1: W&B ê¸°ë³¸ ì„¤ì •
- í”„ë¡œì íŠ¸: `dialogue-summarization`
- Entity: íŒ€/ê°œì¸ ê³„ì •
- Run naming: `{date}_{experiment_name}_{lr}`

#### FR4.2: ì¶”ì  ë©”íŠ¸ë¦­
**ê¸°ë³¸ ë©”íŠ¸ë¦­**:
- Train/Val Loss
- ROUGE-1, ROUGE-2, ROUGE-L (F1/Precision/Recall)
- Learning Rate (epochë³„)
- Gradient Norm

**ìƒì„¸ ë©”íŠ¸ë¦­**:
- ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (Table í˜•ì‹, ê° epochë§ˆë‹¤ 5ê°œ)
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ ëŒ€ì‹œë³´ë“œ
- Confusion Matrix (ê¸¸ì´ë³„ ì„±ëŠ¥)
- **ìˆ«ì/ì‹œê°„ ì •í™•ë„** (ì œì•½ ë””ì½”ë”© íš¨ê³¼ ì¸¡ì •) â­ **ì¶”ê°€**

#### FR4.3: ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- Best model (ROUGE ê¸°ì¤€)
- Last checkpoint
- Epochë³„ ì²´í¬í¬ì¸íŠ¸ (save_total_limit=5)

---

### Phase 5: ìë™í™” íŒŒì´í”„ë¼ì¸

#### FR5.1: í•™ìŠµ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/train.py
- Config ë¡œë“œ
- ë°ì´í„° ì „ì²˜ë¦¬ (Hard sample mining í¬í•¨)
- ëª¨ë¸ í•™ìŠµ (W&B ì¶”ì )
- Best model ì €ì¥
- ê²°ê³¼ ë¡œê¹…
```

#### FR5.2: ì¶”ë¡  ìë™í™” ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/inference.py
- Best model ë¡œë“œ
- Test ë°ì´í„° ì¶”ë¡  (ì œì•½ ë””ì½”ë”© ì ìš©)
- CSV ìƒì„± (,fname,summary í˜•ì‹)
- ê²€ì¦ (499 samples, index í¬í•¨)
```

#### FR5.3: ìˆ˜ë™ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] CSV í¬ë§· í™•ì¸ (`head -5 prediction/output.csv`)
- [ ] ìƒ˜í”Œ ìˆ˜ í™•ì¸ (`wc -l prediction/output.csv` = 500)
- [ ] Index ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
- [ ] íŠ¹ìˆ˜ í† í° ì œê±° í™•ì¸
- [ ] ìˆ˜ë™ìœ¼ë¡œ 5ê°œ ìƒ˜í”Œ í™•ì¸
- [ ] **ìˆ«ì/ì‹œê°„ ì •í™•ë„ í™•ì¸** â­ **ì¶”ê°€**

---

### Phase 6: Git/ë¬¸ì„œ ìë™ ìµœì‹ í™”

#### FR6.1: Git ìë™ ì»¤ë°‹ (ì„ íƒì )
- ì‹¤í—˜ ì™„ë£Œ ì‹œ ìë™ ì»¤ë°‹ ì˜µì…˜
- ì»¤ë°‹ ë©”ì‹œì§€: `Experiment #{N}: {description} - {score}`
- ìˆ˜ë™ í™•ì¸ í›„ Push

#### FR6.2: ì‹¤í—˜ ë¡œê·¸ ìë™ ì—…ë°ì´íŠ¸
- `experiment_logs.md`ì— ìë™ ì¶”ê°€
- í˜•ì‹: ì‹¤í—˜ ë²ˆí˜¸, ë³€ê²½ì‚¬í•­, ì„¤ì •, ê²°ê³¼, ë‹¤ìŒ ë‹¨ê³„

#### FR6.3: README ì—…ë°ì´íŠ¸
- Best score ê¸°ë¡
- ì„±ëŠ¥ ê°œì„  ê·¸ë˜í”„ (ì„ íƒì )

---

### Phase 7: CausalLM ì¬ê²€ì¦ (Baseline 50ì  ë‹¬ì„± í›„)

#### FR7.1: ê²€ì¦ëœ ì „ì²˜ë¦¬ ì ìš©
- Phase 2-3ì—ì„œ ê²€ì¦ëœ ì „ì²˜ë¦¬ë¥¼ Korean_DCS_2024ì— ì ìš©
- ë…¸ì´ì¦ˆ ì²˜ë¦¬, íŠ¹ìˆ˜ í† í° ìµœì í™”, ì œì•½ ë””ì½”ë”©

#### FR7.2: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹ ì¤‘í•œ íŠœë‹
- Learning rate: 2e-5 ê¸°ì¤€ìœ¼ë¡œ ì‘ì€ ë²”ìœ„ íƒìƒ‰
- Epoch: 5 â†’ 3, 7 ì‹¤í—˜
- Batch size/Gradient accumulation ì¡°ì •

#### FR7.3: A/B í…ŒìŠ¤íŠ¸
- KoBART best model vs CausalLM best model
- Dev í‰ê°€ â†’ ê²©ì°¨ ë¶„ì„ â†’ ìœ ë§ ì‹œ Test ì œì¶œ

---

### Phase 8: ìµœì¢… ì œì¶œ

#### FR8.1: ìµœì¢… ëª¨ë¸ ì„ íƒ
- Dev/Test ê²©ì°¨ 5ì  ì´ë‚´
- Test 53.7ì  ì´ìƒ ëª©í‘œ â­ **ì—…ë°ì´íŠ¸**
- ì¬í˜„ ê°€ëŠ¥ì„± í™•ì¸

#### FR8.2: ì œì¶œ íŒŒì¼ ìƒì„±
- ìµœì¢… CSV ìƒì„± ë° ë‹¤ì¤‘ ê²€ì¦
- sample_submission.csvì™€ í¬ë§· ì¼ì¹˜ í™•ì¸

#### FR8.3: ì½”ë“œ ì •ë¦¬
- ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- ìš”êµ¬ì‚¬í•­ ë¬¸ì„œ ì—…ë°ì´íŠ¸
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì‘ì„±

---

## 5. Non-Goals (Out of Scope) ğŸ”„ **ì—…ë°ì´íŠ¸**

### ëª…ì‹œì ìœ¼ë¡œ ì œì™¸ë˜ëŠ” í•­ëª©

âŒ **Phase 1 ì™„ë£Œ ì „ ë³µì¡í•œ ê°œì„  ì‹œë„**
- Baseline 47ì  ë‹¬ì„± ì „ LLM, ì•™ìƒë¸”, ìƒˆ ëª¨ë¸ ì‹œë„ ê¸ˆì§€

âŒ **í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì„ ì‚¬í•­ ì ìš©**
- ë°˜ë“œì‹œ í•˜ë‚˜ì”© í…ŒìŠ¤íŠ¸ (ë””ë²„ê¹… ê°€ëŠ¥ì„± ìœ ì§€)

âŒ **Dev set ì ìˆ˜ë§Œ ë³´ê³  ë§Œì¡±**
- Test ê²€ì¦ ì—†ì´ ì§„í–‰ ê¸ˆì§€

âŒ **DialogSum ë°ì´í„°ì…‹ ì‚¬ìš©**
- ì§ì ‘/ê°„ì ‘ ì‚¬ìš© ëª¨ë‘ ê¸ˆì§€ (ëŒ€íšŒ ê·œì •)

âŒ **ìœ ë£Œ API ì‚¬ìš©**
- Solar ì œì™¸, ë‹¤ë¥¸ ìœ ë£Œ API ê¸ˆì§€

âŒ **í‰ê°€ ë°ì´í„°ë¥¼ í•™ìŠµì— í™œìš©**
- ë¶„ì„ë§Œ ê°€ëŠ¥, Label ìƒì„± í•™ìŠµ ê¸ˆì§€

âŒ **ì™„ì „ ìë™í™” ì œì¶œ**
- ì œì¶œ ì „ ë°˜ë“œì‹œ ìˆ˜ë™ ê²€ì¦

âŒ **CheckList í”„ë ˆì„ì›Œí¬ (ëŒ€íšŒ ì¤‘ ë³´ë¥˜)** â­ **ìƒˆ ì¶”ê°€**
- ì „ë¬¸ê°€ í‰ê°€: ëŒ€íšŒ ì¤‘ êµ¬í˜„ ì‹œê°„ ëŒ€ë¹„ íš¨ê³¼ ë‚®ìŒ

âŒ **MTLD/MATTR ì§€í‘œ (ROUGE ì™¸ í‰ê°€)** â­ **ìƒˆ ì¶”ê°€**
- ëŒ€íšŒ í‰ê°€ ì§€í‘œ: ROUGE-F1ë§Œ ì‚¬ìš©

---

## 6. Design Considerations ğŸ”„ **ë³´ê°•**

### 6.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
/Competition/NLP/
â”œâ”€â”€ naturallanguageprocessingcompetition-nlp-1/  # Git ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”œâ”€â”€ baseline.ipynb
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â”œâ”€â”€ constrained_decoding.py          # ìƒˆë¡œ ì¶”ê°€ â­
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ scripts/                               # ìƒˆë¡œ ì¶”ê°€
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ prepare_hard_samples.py           # ìƒˆë¡œ ì¶”ê°€ â­
â”‚   â”‚   â”œâ”€â”€ iwcv_analysis.py                  # ìƒˆë¡œ ì¶”ê°€ â­
â”‚   â”‚   â”œâ”€â”€ led_model.py                      # ìƒˆë¡œ ì¶”ê°€ (ì¡°ê±´ë¶€) â­
â”‚   â”‚   â”œâ”€â”€ unlikelihood_trainer.py           # ìƒˆë¡œ ì¶”ê°€ (ì‹¤í—˜ì ) â­
â”‚   â”‚   â”œâ”€â”€ ensemble.py                       # ìƒˆë¡œ ì¶”ê°€ â­
â”‚   â”‚   â””â”€â”€ paraphrase_augment.py
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ prediction/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ EDA.md                                 # v2.0 (ì „ë¬¸ê°€ í‰ê°€ ë°˜ì˜)
â”‚   â”œâ”€â”€ experiment_logs.md                     # ìƒˆë¡œ ì¶”ê°€
â”‚   â””â”€â”€ ...
â””â”€â”€ tasks/
    â”œâ”€â”€ prd-dialogue-summarization-performance-improvement.md  # ë³¸ ë¬¸ì„œ (v2.0)
    â””â”€â”€ tasks-prd-dialogue-summarization.md                    # ìƒì„± ì˜ˆì •
```

### 6.2 ì‹¤í—˜ ë¡œê·¸ í…œí”Œë¦¿
```markdown
## ì‹¤í—˜ #N: [ì‹¤í—˜ëª…]

**ë‚ ì§œ**: 2025-10-__
**ë² ì´ìŠ¤**: Baseline / ì‹¤í—˜ #(N-1)

### ë³€ê²½ì‚¬í•­
- [í•œ ê°€ì§€ë§Œ ëª…ì‹œ]

### ì„¤ì •
```yaml
[ë³€ê²½ëœ íŒŒë¼ë¯¸í„°ë§Œ]
```

### ê²°ê³¼
- Baseline/ì´ì „: XX.XX
- í˜„ì¬ Dev: XX.XX
- í˜„ì¬ Test: XX.XX (ì œì¶œí•œ ê²½ìš°)
- **Dev/Test ê²©ì°¨**: XX.XX
- **ë³€í™”**: +X.XX âœ…/âŒ
- **ìˆ«ì/ì‹œê°„ ì •í™•ë„**: XX.XX% â­ ì¶”ê°€ (ì œì•½ ë””ì½”ë”© ì ìš© ì‹œ)

### íŒë‹¨
- [ìœ ì§€/ë¡¤ë°±/ì¬ì‹œë„]
- [ì´ìœ ]

### ë‹¤ìŒ ë‹¨ê³„
- [ë‹¤ìŒì— ì‹œë„í•  ê²ƒ]
```

### 6.3 W&B ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- **Overview**: ì „ì²´ ì‹¤í—˜ ë¹„êµ (ROUGE ì ìˆ˜)
- **Training**: Loss, Learning rate, Gradient norm
- **Evaluation**: ROUGE-1/2/L, Dev/Test ê²©ì°¨
- **Samples**: ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (5ê°œ)
- **Hyperparameters**: íŒŒë¼ë¯¸í„° ë¹„êµ í…Œì´ë¸”
- **ìˆ«ì/ì‹œê°„ ì •í™•ë„**: ì œì•½ ë””ì½”ë”© íš¨ê³¼ ì¸¡ì • â­ **ì¶”ê°€**

### 6.4 ì œì•½ ë””ì½”ë”© ì„¤ê³„ â­ **ìƒˆ ì¶”ê°€**

**í•µì‹¬ ì›ì¹™**:
- ë¶€ë¶„ ì œì•½ (30% íŒ¨ë„í‹°) ì ìš©, ì™„ì „ ì°¨ë‹¨ ì§€ì–‘
- í—ˆìš© í† í° ì™¸ì—ë„ ìƒì„± ê°€ëŠ¥í•˜ë„ë¡ ìœ ì—°ì„± ìœ ì§€

**ì•Œê³ ë¦¬ì¦˜**:
1. ì…ë ¥ ëŒ€í™”ì—ì„œ ìˆ«ì/ì‹œê°„/PII íŒ¨í„´ ì¶”ì¶œ
2. í—ˆìš© í† í° ID ë¦¬ìŠ¤íŠ¸ ìƒì„±
3. Generation ì‹œ LogitsProcessorë¡œ í—ˆìš© í† í° ì™¸ í™•ë¥  ë‚®ì¶¤ (30% íŒ¨ë„í‹°)
4. Beam searchì™€ ê²°í•©í•˜ì—¬ ìµœì  ìš”ì•½ ìƒì„±

### 6.5 IWCV ì•Œê³ ë¦¬ì¦˜ ê°œìš” â­ **ìƒˆ ì¶”ê°€**

**í•µì‹¬ ì›ì¹™**:
- Dev/Test ë¶„í¬ ì°¨ì´ë¥¼ í•™ìŠµìœ¼ë¡œ ì™„í™”
- 12íšŒ ì œì¶œ ì œí•œ í™˜ê²½ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥ ì˜ˆì¸¡

**ì•Œê³ ë¦¬ì¦˜**:
1. Dev/Test íŠ¹ì„± ì¶”ì¶œ (ê¸¸ì´, ì••ì¶•ë¹„, í† í° ìˆ˜ ë“±)
2. Logistic Regressionìœ¼ë¡œ Dev(0) vs Test(1) ë¶„ë¥˜
3. Dev ìƒ˜í”Œì˜ Test ìœ ì‚¬ë„ ê³„ì‚° (í™•ë¥ )
4. WeightedRandomSamplerë¡œ í•™ìŠµ ì‹œ ê°€ì¤‘ì¹˜ ì ìš©

### 6.6 LED ëª¨ë¸ ì „í™˜ ì „ëµ â­ **ìƒˆ ì¶”ê°€**

**ì „í™˜ ì¡°ê±´**:
- Phase 1 ì™„ë£Œ í›„ Max Length 768 íš¨ê³¼ ê²€ì¦
- 512 í† í° ì´ˆê³¼ ìƒ˜í”Œ ì„±ëŠ¥ ê°œì„  ë¶ˆì¶©ë¶„ ì‹œ

**ì „í™˜ ë‹¨ê³„**:
1. í•œêµ­ì–´ LED ëª¨ë¸ íƒìƒ‰ (HuggingFace)
2. ì—†ìœ¼ë©´ ì˜ì–´ LED (allenai/led-base-16384) fine-tuning
3. Global attention ì„¤ì • (#Person#, ì‹œê°„, ìˆ«ì ë“±)
4. Baselineê³¼ A/B í…ŒìŠ¤íŠ¸

---

## 7. Technical Considerations ğŸ”„ **ì—…ë°ì´íŠ¸**

### 7.1 í™˜ê²½
- GPU: RTX 3090 24GB
- Python: 3.10
- PyTorch: 2.5.1
- Transformers: 4.46.3 (Baseline)

### 7.2 ì£¼ìš” ì˜ì¡´ì„±
```
transformers==4.46.3
rouge==1.0.1
wandb==0.16.1
pandas==2.1.4
torch==2.5.1
tqdm==4.66.1
scikit-learn==1.3.2  # IWCV ë¶„ì„ â­ ì¶”ê°€
```

### 7.3 ë””ìŠ¤í¬ ìš©ëŸ‰ ê´€ë¦¬
- âš ï¸ **150GB ì œí•œ ì ˆëŒ€ ì¤€ìˆ˜**
- ëª¨ë“  run ì „ `du -sh / 2>/dev/null` í™•ì¸
- ì²´í¬í¬ì¸íŠ¸: save_total_limit=5
- ì˜ˆì¸¡ ê²°ê³¼: ì´ì „ ë²„ì „ ì‚­ì œ

### 7.4 ì œì¶œ íšŸìˆ˜ ê´€ë¦¬
- Daily 12íšŒ ì œí•œ
- ì „ëµ: Devë¡œ 3-4ê°œ ì‹¤í—˜ â†’ Test 1íšŒ
- ì¶”ì : ìŠ¤í”„ë ˆë“œì‹œíŠ¸/ë¬¸ì„œë¡œ ì œì¶œ ì´ë ¥ ê´€ë¦¬

### 7.5 Dev/Test ê²©ì°¨ ëª¨ë‹ˆí„°ë§
- ëª©í‘œ: ê²©ì°¨ 5ì  ì´ë‚´
- ê²½ê³ : ê²©ì°¨ 10ì  ì´ìƒ ì‹œ ê³¼ì í•© ì˜ì‹¬
- ì¡°ì¹˜: ì •ê·œí™”, Dropout, Early stopping ê°•í™”, **IWCV ì ìš©** â­ **ì¶”ê°€**

### 7.6 ì¬í˜„ì„± ë³´ì¥
- Random seed: 42 (ê³ ì •)
- ëª¨ë“  ì„¤ì • YAML/config íŒŒì¼ë¡œ ê´€ë¦¬
- Git commit hash ê¸°ë¡
- í™˜ê²½ ì •ë³´ ì €ì¥ (`pip freeze > requirements_frozen.txt`)

### 7.7 LED ëª¨ë¸ ìš”êµ¬ì‚¬í•­ â­ **ìƒˆ ì¶”ê°€**
- ë©”ëª¨ë¦¬: 16,384 í† í° ì§€ì› â†’ ë©”ëª¨ë¦¬ ì•½ 2ë°° ì¦ê°€
- Batch size: 32 â†’ 16 ê°ì†Œ í•„ìš”
- Global attention: ì¤‘ìš” í† í° ìœ„ì¹˜ ì„¤ì • í•„ìˆ˜

### 7.8 Unlikelihood Training êµ¬í˜„ ê³ ë ¤ì‚¬í•­ â­ **ìƒˆ ì¶”ê°€**
- Alpha: 0.05ë¡œ ì‹œì‘, ì ˆëŒ€ 0.1 ì´ˆê³¼ ê¸ˆì§€
- ëª¨ë‹ˆí„°ë§: Epochë§ˆë‹¤ Dev ROUGE í™•ì¸
- ë¡¤ë°±: 2 epoch ì—°ì† ì €í•˜ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
- A/B í…ŒìŠ¤íŠ¸: Unlikelihood æœ‰/ç„¡ ë³‘í–‰ í•™ìŠµ

### 7.9 ì œì•½ ë””ì½”ë”© ì„±ëŠ¥ ìµœì í™” â­ **ìƒˆ ì¶”ê°€**
- í—ˆìš© í† í° ë¦¬ìŠ¤íŠ¸ ì‚¬ì „ ê³„ì‚° (ë°°ì¹˜ ë‹¨ìœ„)
- LogitsProcessor ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- Beam searchì™€ ê²°í•© ì‹œ ì„±ëŠ¥ ì˜í–¥ ëª¨ë‹ˆí„°ë§

---

## 8. Success Metrics ğŸ”„ **ì¡°ì •**

### 8.1 ì£¼ìš” ì„±ê³µ ì§€í‘œ

**Metric 1: Test ROUGE-F1 ì ìˆ˜** â­ **ì—…ë°ì´íŠ¸**
- ëª©í‘œ: **53.7ì  ì´ìƒ** (ìµœì¢… 60.6ì  ëª©í‘œ)
- ì¸¡ì •: ëŒ€íšŒ í”Œë«í¼ ì œì¶œ
- ë¹ˆë„: 3-4 ì‹¤í—˜ë§ˆë‹¤ 1íšŒ

**Metric 2: Dev/Test ê²©ì°¨**
- ëª©í‘œ: **5ì  ì´ë‚´**
- ì¸¡ì •: Dev ì ìˆ˜ - Test ì ìˆ˜ (ì ˆëŒ€ê°’)
- ë¹ˆë„: ë§¤ Test ì œì¶œ ì‹œ

**Metric 3: ì œì¶œ íš¨ìœ¨ì„±**
- ëª©í‘œ: **12íšŒ ì œì¶œ ë‚´ 53.7ì  ë‹¬ì„±**
- ì¸¡ì •: ì œì¶œ íšŸìˆ˜ ì¶”ì 
- í‰ê°€: ì œì¶œë‹¹ í‰ê·  ê°œì„ í­ (+0.5ì  ì´ìƒ)

**Metric 4: ìˆ«ì/ì‹œê°„ ì •í™•ë„** â­ **ìƒˆ ì¶”ê°€**
- ëª©í‘œ: **95% ì´ìƒ**
- ì¸¡ì •: ìƒì„± ìš”ì•½ì˜ ìˆ«ì/ì‹œê°„ì´ ì›ë³¸ ëŒ€í™”ì™€ ì¼ì¹˜í•˜ëŠ” ë¹„ìœ¨
- ë¹ˆë„: ì œì•½ ë””ì½”ë”© ì ìš© í›„ ë§¤ í‰ê°€ ì‹œ

### 8.2 ë¶€ê°€ ì„±ê³µ ì§€í‘œ

**Metric 5: ì¬í˜„ì„±**
- ëª©í‘œ: ë™ì¼ ì„¤ì •ìœ¼ë¡œ Â±0.5ì  ì´ë‚´ ì¬í˜„
- ì¸¡ì •: 3íšŒ ì¬í•™ìŠµ í›„ í‘œì¤€í¸ì°¨
- ë¹ˆë„: ìµœì¢… ëª¨ë¸ ì„ ì • ì‹œ

**Metric 6: ì‹¤í—˜ ì†ë„**
- ëª©í‘œ: 1íšŒ ì‹¤í—˜(í•™ìŠµ+í‰ê°€) 30ë¶„ ì´ë‚´
- ì¸¡ì •: W&B run time
- ê°œì„ : ë¶ˆí•„ìš”í•œ ë¡œê¹… ì œê±°, ë°°ì¹˜ ìµœì í™”

**Metric 7: ë¬¸ì„œí™” ì™„ì„±ë„**
- ëª©í‘œ: ëª¨ë“  ì‹¤í—˜ ë¡œê·¸ ê¸°ë¡
- ì¸¡ì •: `experiment_logs.md` í•­ëª© ìˆ˜
- í‰ê°€: ëˆ„ë½ ì—†ìŒ

**Metric 8: Phaseë³„ ì¤‘ê°„ ëª©í‘œ** â­ **ìƒˆ ì¶”ê°€**
- Phase 1: 49.4~51.6ì 
- Phase 2: 51.4~55.8ì 
- Phase 3: 53.7~60.6ì 

### 8.3 ìµœì¢… ëª©í‘œ
- **ëŒ€íšŒ ìˆœìœ„**: 1ë“± (ë˜ëŠ” ìƒìœ„ 3ìœ„)
- **í•™ìŠµ ì„±ê³¼**: ì¬í˜„ ê°€ëŠ¥í•œ ì„±ëŠ¥ ê°œì„  íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- **ì§€ì‹ ì¶•ì **: íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ë° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ë¬¸ì„œí™”

---

## 9. Open Questions ğŸ”„ **ì—…ë°ì´íŠ¸**

### Q1: LED í•œêµ­ì–´ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€? â­ **ìƒˆ ì¶”ê°€**
- í•œêµ­ì–´ ì‚¬ì „í•™ìŠµ LED ëª¨ë¸ì´ HuggingFaceì— ìˆëŠ”ì§€?
- ì—†ìœ¼ë©´ ì˜ì–´ LED fine-tuning (1-2ì£¼ ì†Œìš”) ë˜ëŠ” Longformer-BART ëŒ€ì•ˆ
- â†’ ë‹µë³€: Phase 1 ì™„ë£Œ í›„ íƒìƒ‰

### Q2: Unlikelihood Alpha ìµœì ê°’? â­ **ìƒˆ ì¶”ê°€**
- 0.05 vs 0.08 vs 0.1 ì¤‘ ì„ íƒ?
- ROUGE ì €í•˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ë²•?
- â†’ ë‹µë³€: 0.05ë¡œ ì‹œì‘, A/B í…ŒìŠ¤íŠ¸ë¡œ ê²°ì •

### Q3: ì œì•½ ë””ì½”ë”© ì™„ë²½ì„± vs ìœ ì—°ì„± ê· í˜•? â­ **ìƒˆ ì¶”ê°€**
- 30% íŒ¨ë„í‹°ê°€ ìµœì ì¸ì§€?
- ì™„ì „ ì°¨ë‹¨ vs ë¶€ë¶„ ì œì•½ ì‹¤í—˜ í•„ìš”?
- â†’ ë‹µë³€: Phase 1ì—ì„œ ì‹¤í—˜

### Q4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„
- Learning rate: 3e-5 ì™¸ì— 2e-5, 5e-5ë„ ì‹œë„?
- Epoch: 30 ì™¸ì— 40ê¹Œì§€ ëŠ˜ë ¤ë³¼ì§€?
- â†’ ë‹µë³€: Phase 2.1 ê²°ê³¼ ë³´ê³  ê²°ì •

### Q5: ë°ì´í„° ì¦ê°• ê·œëª¨
- 2ë°° vs 3ë°° ì¦ê°• ì¤‘ ì„ íƒ?
- ì¦ê°• í’ˆì§ˆ ê²€ì¦ ë°©ë²•?
- â†’ ë‹µë³€: ì†ŒëŸ‰ í…ŒìŠ¤íŠ¸(1,000ê°œ) í›„ ê²°ì •

### Q6: CausalLM ì¬ì‹œë„ ì¡°ê±´
- KoBART 53.7ì  ë‹¬ì„± ì‹œ ë¬´ì¡°ê±´ ì‹œë„?
- ì‹œê°„ ë¶€ì¡± ì‹œ ìŠ¤í‚µ?
- â†’ ë‹µë³€: ëŒ€íšŒ ë§ˆê° 7ì¼ ì „ê¹Œì§€ 53.7ì  ë‹¬ì„± ì‹œ ì‹œë„

### Q7: ì•™ìƒë¸” ì „ëµ
- KoBART + CausalLM ì•™ìƒë¸” ê³ ë ¤?
- ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬ + ëª¨ë¸ ì¡°í•©?
- â†’ ë‹µë³€: 53.7ì  ë‹¬ì„± í›„ ê²€í† 

### Q8: W&B ê³µê°œ ì„¤ì •
- Public í”„ë¡œì íŠ¸ë¡œ ì„¤ì •? (ë‹¤ë¥¸ ì°¸ê°€ì ë³¼ ìˆ˜ ìˆìŒ)
- Private ìœ ì§€?
- â†’ ë‹µë³€: Private ìœ ì§€ (ëŒ€íšŒ ì¢…ë£Œ í›„ ê³µê°œ ê³ ë ¤)

### Q9: Git ë¸Œëœì¹˜ ì „ëµ
- main: ì•ˆì • ë²„ì „
- experiment: ì‹¤í—˜ìš© ë¸Œëœì¹˜ ë¶„ë¦¬?
- â†’ ë‹µë³€: ë‹¨ìˆœí•˜ê²Œ mainë§Œ ì‚¬ìš© (ê° ì‹¤í—˜ì€ commitìœ¼ë¡œ ê´€ë¦¬)

### Q10: ë¹„ìƒ ì‹œë‚˜ë¦¬ì˜¤
- 53.7ì  ë‹¬ì„± ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ?
  - A: 51ì  ì•ˆì •í™” + ì½”ë“œ í’ˆì§ˆ í–¥ìƒ
  - B: ì•™ìƒë¸”ë¡œ 52-53ì  ëª©í‘œ
  - C: ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ (mBART ë“±)
- â†’ ë‹µë³€: A ìš°ì„  (ì¬í˜„ ê°€ëŠ¥ì„± ìµœìš°ì„ )

---

## 10. Implementation Timeline (ì˜ˆìƒ) ğŸ”„ **ì¬ì¡°ì •**

### Week 1: Foundation + Phase 1 (1.5ì¼)
- **Day 1-2**: Phase 1 ì‹¤í–‰ (Max Length, ì œì•½ ë””ì½”ë”©, ê¸¸ì´ ì •ê·œí™”, Learning Rate, Hard Sample Mining)
  - ì˜ˆìƒ íš¨ê³¼: 47 â†’ 49.4~51.6ì 
  - Test ì œì¶œ (1íšŒ)
- **Day 3-4**: Phase 1 ê²°ê³¼ ë¶„ì„ ë° Phase 2 ì¤€ë¹„
  - IWCV ë¶„ì„ ìˆ˜í–‰
  - LED ëª¨ë¸ íƒìƒ‰ (í•„ìš” ì‹œ)

### Week 2: Phase 2 (1-2ì£¼)
- **Day 5-8**: Longer Training, IWCV ì ìš©, Dynamic Batch Size
- **Day 9-12**: LED ëª¨ë¸ ì‹¤í—˜ (ì¡°ê±´ë¶€, í•œêµ­ì–´ ëª¨ë¸ ì¡´ì¬ ì‹œ)
- **Day 13-14**: Unlikelihood Training ì‹¤í—˜ (ì‹¤í—˜ì , A/B í…ŒìŠ¤íŠ¸)
  - ì˜ˆìƒ íš¨ê³¼: 49.4~51.6 â†’ 51.4~55.8ì 
  - Test ì œì¶œ (2-3íšŒ)

### Week 3: Phase 3 + Finalization (1-2ì£¼)
- **Day 15-17**: Ensemble ì „ëµ êµ¬ì¶• (ìµœìš°ì„ )
- **Day 18-19**: Paraphrase Augmentation
- **Day 20-21**: Copy Mechanism (ì¡°ê±´ë¶€), Model Upgrade (ì¡°ê±´ë¶€)
  - ì˜ˆìƒ íš¨ê³¼: 51.4~55.8 â†’ 53.7~60.6ì 
  - Test ì œì¶œ (2-3íšŒ)

### Week 4: Advanced & Finalization
- **Day 22-23**: CausalLM ì¬ê²€ì¦ (ì¡°ê±´ë¶€, 53.7ì  ë‹¬ì„± ì‹œ)
- **Day 24-25**: ìµœì¢… ëª¨ë¸ ì„ ì • ë° ê²€ì¦
- **Day 26-28**: ì½”ë“œ ì •ë¦¬, ë¬¸ì„œ ì™„ì„±, ìµœì¢… ì œì¶œ

---

## Appendix: ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤í—˜ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë””ìŠ¤í¬ ìš©ëŸ‰ 150GB ë¯¸ë§Œ í™•ì¸
- [ ] Git ìµœì‹  ìƒíƒœ í™•ì¸
- [ ] ì´ì „ ì‹¤í—˜ ë¡œê·¸ ì‘ì„± ì™„ë£Œ
- [ ] Config íŒŒì¼ ë°±ì—…
- [ ] ì œì¶œ íšŸìˆ˜ í™•ì¸ (12íšŒ ì´ë‚´)
- [ ] **EDA ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í™•ì¸** â­ **ì¶”ê°€**

### ì‹¤í—˜ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] W&B run ì‹œì‘ í™•ì¸
- [ ] GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
- [ ] Loss ìˆ˜ë ´ í™•ì¸
- [ ] ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
- [ ] **ìˆ«ì/ì‹œê°„ ì •í™•ë„ ëª¨ë‹ˆí„°ë§** (ì œì•½ ë””ì½”ë”© ì ìš© ì‹œ) â­ **ì¶”ê°€**

### ì‹¤í—˜ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Dev ROUGE ì ìˆ˜ ê¸°ë¡
- [ ] Dev/Test ê²©ì°¨ ë¶„ì„ (Test ì œì¶œ ì‹œ)
- [ ] ì‹¤í—˜ ë¡œê·¸ ì‘ì„±
- [ ] Best model ì²´í¬í¬ì¸íŠ¸ í™•ì¸
- [ ] Git ì»¤ë°‹ (ë³€ê²½ì‚¬í•­ ì„¤ëª…)
- [ ] ë‹¤ìŒ ì‹¤í—˜ ê³„íš ìˆ˜ë¦½
- [ ] **Phaseë³„ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸** â­ **ì¶”ê°€**

### Test ì œì¶œ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] CSV í¬ë§· ê²€ì¦ (,fname,summary)
- [ ] ìƒ˜í”Œ ìˆ˜ í™•ì¸ (500 lines = header + 499)
- [ ] Index ì»¬ëŸ¼ ì¡´ì¬
- [ ] íŠ¹ìˆ˜ í† í° ì œê±° í™•ì¸
- [ ] 5ê°œ ìƒ˜í”Œ ìˆ˜ë™ ê²€í† 
- [ ] **ìˆ«ì/ì‹œê°„ ì •í™•ë„ í™•ì¸** (ì›ë³¸ ëŒ€í™”ì™€ ì¼ì¹˜ ì—¬ë¶€) â­ **ì¶”ê°€**
- [ ] Dev ì ìˆ˜ì™€ ë¹„êµ ë¶„ì„
- [ ] ì œì¶œ íšŸìˆ˜ ì²´í¬

### Phaseë³„ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸ â­ **ìƒˆ ì¶”ê°€**

**Phase 1 ì™„ë£Œ ì²´í¬**:
- [ ] Max Length 768 ì ìš© ì™„ë£Œ
- [ ] ì œì•½ ë””ì½”ë”© êµ¬í˜„ ì™„ë£Œ
- [ ] ê¸¸ì´ ì •ê·œí™” ì ìš© ì™„ë£Œ
- [ ] Learning Rate 3e-5 ì ìš© ì™„ë£Œ
- [ ] Hard Sample Mining ì ìš© ì™„ë£Œ
- [ ] Test ì œì¶œ ì™„ë£Œ (ëª©í‘œ: 49.4~51.6ì )
- [ ] íš¨ê³¼ ê²€ì¦ ì™„ë£Œ (LED í•„ìš”ì„± íŒë‹¨)

**Phase 2 ì™„ë£Œ ì²´í¬**:
- [ ] Longer Training ì ìš© ì™„ë£Œ
- [ ] IWCV ë¶„ì„ ë° ì ìš© ì™„ë£Œ
- [ ] LED ëª¨ë¸ ì‹¤í—˜ ì™„ë£Œ (ì¡°ê±´ë¶€)
- [ ] Dynamic Batch Size ì ìš© ì™„ë£Œ
- [ ] Unlikelihood Training ì‹¤í—˜ ì™„ë£Œ (ì‹¤í—˜ì )
- [ ] Test ì œì¶œ ì™„ë£Œ (ëª©í‘œ: 51.4~55.8ì )

**Phase 3 ì™„ë£Œ ì²´í¬**:
- [ ] Ensemble ì „ëµ êµ¬ì¶• ì™„ë£Œ
- [ ] Paraphrase Augmentation ì ìš© ì™„ë£Œ
- [ ] Copy Mechanism êµ¬í˜„ ì™„ë£Œ (ì¡°ê±´ë¶€)
- [ ] Model Upgrade ì‹¤í—˜ ì™„ë£Œ (ì¡°ê±´ë¶€)
- [ ] Test ì œì¶œ ì™„ë£Œ (ëª©í‘œ: 53.7~60.6ì )

---

**ì‘ì„±ì¼**: 2025-10-12
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-14
**ë²„ì „**: 2.0 (EDA ì‹¬ì¸µ ë¶„ì„ ë°˜ì˜)
**ì‘ì„±ì**: Claude Code + User
**ê¸°ë°˜ ë¬¸ì„œ**: `/Competition/NLP/docs/EDA.md` v2.0
**ë‹¤ìŒ ë‹¨ê³„**: `generate-tasks.md`ë¡œ ìƒì„¸ Task List ìƒì„±

---

## ë³€ê²½ ì´ë ¥ (v2.0)

### ì£¼ìš” ë³€ê²½ì‚¬í•­
1. â­ **5ê°œ ìƒˆ ì „ëµ ì¶”ê°€** (ì „ë¬¸ê°€ ê¶Œì¥):
   - FR1.2: ì œì•½ ë””ì½”ë”© (Phase 1)
   - FR1.3: ë””ì½”ë”© ê¸¸ì´ ì •ê·œí™” (Phase 1)
   - FR2.2: IWCV ë¶„ì„ (Phase 2)
   - FR2.3: LED ëª¨ë¸ (Phase 2, ì¡°ê±´ë¶€)
   - FR2.5: Unlikelihood Training (Phase 2, ì‹¤í—˜ì )

2. ğŸ”„ **ëª©í‘œ ì ìˆ˜ ìƒí–¥**: 50ì  â†’ **53.7~60.6ì **

3. ğŸ”„ **Phaseë³„ êµ¬ì¡° ê°œí¸**:
   - Phase 1: 5ê°œ ì „ëµ (ê¸°ì¡´ 3ê°œ â†’ 5ê°œ)
   - Phase 2: 6ê°œ ì „ëµ (IWCV, LED, Unlikelihood ì¶”ê°€)
   - Phase 3: Copy Mechanism ìš°ì„ ìˆœìœ„ í•˜í–¥ (ì¡°ê±´ë¶€)

4. â­ **Design Considerations 3ê°œ ì„¹ì…˜ ì¶”ê°€**:
   - 6.4: ì œì•½ ë””ì½”ë”© ì„¤ê³„
   - 6.5: IWCV ì•Œê³ ë¦¬ì¦˜ ê°œìš”
   - 6.6: LED ëª¨ë¸ ì „í™˜ ì „ëµ

5. ğŸ”„ **Non-Goals 2ê°œ ì¶”ê°€**:
   - CheckList í”„ë ˆì„ì›Œí¬ (ë³´ë¥˜)
   - MTLD/MATTR ì§€í‘œ (ì œì™¸)

6. â­ **Success Metrics ì—…ë°ì´íŠ¸**:
   - Metric 1: 50ì  â†’ **53.7ì ** ëª©í‘œ
   - Metric 4: ìˆ«ì/ì‹œê°„ ì •í™•ë„ (ìƒˆ ì¶”ê°€)
   - Metric 8: Phaseë³„ ì¤‘ê°„ ëª©í‘œ (ìƒˆ ì¶”ê°€)

7. ğŸ”„ **Open Questions 3ê°œ ì¶”ê°€**:
   - Q1: LED í•œêµ­ì–´ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€
   - Q2: Unlikelihood Alpha ìµœì ê°’
   - Q3: ì œì•½ ë””ì½”ë”© ì™„ë²½ì„± vs ìœ ì—°ì„±

8. ğŸ”„ **Implementation Timeline ì¬ì¡°ì •**:
   - Week 1: 1.5ì¼ (Phase 1 í™•ì¥)
   - Week 2: Phase 2 (1-2ì£¼)
   - Week 3-4: Phase 3 + Finalization

### ìœ ì§€ëœ ë‚´ìš©
- Section 1, 3: Introduction, User Stories (í•µì‹¬ ìœ ì§€)
- "í•œ ë²ˆì— í•˜ë‚˜ì”©, Testë¡œ ê²€ì¦" ì›ì¹™
- Baseline ì¬í˜„ í•„ìˆ˜ (Phase 1 ì‹œì‘)
- ì‹¤í—˜ ë¡œê·¸ ì‹œìŠ¤í…œ (Section 6.2)
- W&B ì¶”ì  ì‹œìŠ¤í…œ (Phase 4)
- ìë™í™” íŒŒì´í”„ë¼ì¸ (Phase 5)
- Git/ë¬¸ì„œ ìµœì‹ í™” (Phase 6)
- CausalLM ì¬ê²€ì¦ (Phase 7, ì¡°ê±´ë¶€)
- ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Appendix)