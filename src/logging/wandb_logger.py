# ---------------------- WandB ë¡œê¹… ëª¨ë“ˆ ---------------------- #
"""
WandB ë¡œê¹… ìœ í‹¸ë¦¬í‹°
íŒ€ í”„ë¡œì íŠ¸ìš© WandB í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ
"""

import os                                                   # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥
import wandb                                                # Weights & Biases ë¡œê¹… ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime                               # í˜„ì¬ ì‹œê°„ ì²˜ë¦¬
from typing import Dict, Any, Optional                      # íƒ€ì… íŒíŠ¸
import torch                                                # PyTorch í…ì„œ ì²˜ë¦¬


# ---------------------- WandB Logger í´ë˜ìŠ¤ ---------------------- #
# WandbLogger í´ë˜ìŠ¤ ì •ì˜
class WandbLogger:
    # ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
    def __init__(
        self,
        project_name: str = "document-classification-team", # í”„ë¡œì íŠ¸ ì´ë¦„
        entity: Optional[str] = None,                       # WandB ì—”í‹°í‹° (íŒ€/ì‚¬ìš©ì)
        experiment_name: str = "experiment",                # ì‹¤í—˜ ì´ë¦„
        config: Optional[Dict[str, Any]] = None,            # ì„¤ì • ë”•ì…”ë„ˆë¦¬
        tags: Optional[list] = None,                        # íƒœê·¸ ë¦¬ìŠ¤íŠ¸
    ):
        self.project_name = project_name                    # í”„ë¡œì íŠ¸ ì´ë¦„ ì €ì¥
        self.entity = entity                                # ì—”í‹°í‹° ì €ì¥
        self.experiment_name = experiment_name              # ì‹¤í—˜ ì´ë¦„ ì €ì¥
        self.config = config or {}                          # ì„¤ì • ì €ì¥ (ê¸°ë³¸ê°’: ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        self.tags = tags or []                              # íƒœê·¸ ì €ì¥ (ê¸°ë³¸ê°’: ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%m%d-%H%M")    # í˜„ì¬ ì‹œê°„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        self.run_name = f"{timestamp}-{experiment_name}"    # ì‹¤í–‰ ì´ë¦„ ìƒì„±
        
        self.run = None                                     # WandB ì‹¤í–‰ ê°ì²´ ì´ˆê¸°í™”
        self.is_initialized = False                         # ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸
    
    
    # WandB ë¡œê·¸ì¸ í•¨ìˆ˜ ì •ì˜
    def login(self):
        try:
            # API í‚¤ê°€ ì—†ëŠ” ê²½ìš°
            if wandb.api.api_key is None:
                print("WandBì— ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")          # ë¡œê·¸ì¸ í•„ìš” ë©”ì‹œì§€
                wandb.login()                               # WandB ë¡œê·¸ì¸ ìˆ˜í–‰
            # API í‚¤ê°€ ìˆëŠ” ê²½ìš°
            else:
                print(f"WandB ë¡œê·¸ì¸ ìƒíƒœ: {wandb.api.viewer()['username']}")   # ë¡œê·¸ì¸ ìƒíƒœ ì¶œë ¥
                
        # ì˜ˆì™¸ ë°œìƒ ì‹œ
        except:
            print("WandB ë¡œê·¸ì¸ì„ ì§„í–‰í•©ë‹ˆë‹¤...")              # ë¡œê·¸ì¸ ì§„í–‰ ë©”ì‹œì§€
            wandb.login()                                   # WandB ë¡œê·¸ì¸ ìˆ˜í–‰
    
    
    # ì‹¤í–‰ ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
    def init_run(self, fold: Optional[int] = None):
        # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
        if self.is_initialized:
            return      # í•¨ìˆ˜ ì¢…ë£Œ

        self.login()    # WandB ë¡œê·¸ì¸

        # foldê°€ ì§€ì •ëœ ê²½ìš° run nameì— ì¶”ê°€
        run_name = self.run_name                 # ê¸°ë³¸ ì‹¤í–‰ ì´ë¦„

        # í´ë“œê°€ ì§€ì •ëœ ê²½ìš°
        if fold is not None:
            run_name = f"fold-{fold}-{run_name}" # í´ë“œ ë²ˆí˜¸ ì¶”ê°€

        # WandB ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì„¤ì •
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        wandb_dir = os.path.join(project_root, "wandb")
        os.environ["WANDB_DIR"] = wandb_dir

        # WandB run ì´ˆê¸°í™”
        self.run = wandb.init(
            project=self.project_name,           # í”„ë¡œì íŠ¸ ì´ë¦„
            entity=self.entity,                  # ì—”í‹°í‹°
            name=run_name,                       # ì‹¤í–‰ ì´ë¦„
            config=self.config,                  # ì„¤ì •
            tags=self.tags,                      # íƒœê·¸
            dir=wandb_dir,                       # WandB ë””ë ‰í† ë¦¬ ì§€ì •
            reinit=True                          # ì¬ì´ˆê¸°í™” í—ˆìš©
        )
        
        self.is_initialized = True               # ì´ˆê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        print(f"ğŸ“‹ ì‹¤í—˜ëª…: {run_name}")           # ì‹¤í—˜ëª… ì¶œë ¥
        print(f"ğŸ”— WandB URL: {self.run.url}")   # WandB URL ì¶œë ¥
    
    
    # ë©”íŠ¸ë¦­ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
    def log_metrics(self, metrics: Dict[str, Any], step: Optional[int] = None):
        # ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.is_initialized:
            return  # í•¨ìˆ˜ ì¢…ë£Œ

        wandb.log(metrics, step=step)            # WandBì— ë©”íŠ¸ë¦­ ë¡œê¹…


    # í…ìŠ¤íŠ¸ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
    def log_text(self, message: str, level: str = "info"):
        """
        í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ë¡œê¹… (WandBì—ëŠ” ë¡œê¹…í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€ìš©)

        Args:
            message: ë¡œê·¸ ë©”ì‹œì§€
            level: ë¡œê·¸ ë ˆë²¨ (info, warning, error ë“±)
        """
        # WandBì—ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë¡œê¹…í•˜ì§€ ì•ŠìŒ
        # ì´ ë©”ì„œë“œëŠ” base_trainer.pyì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ì¡´ì¬
        pass
    
    
    # ëª¨ë¸ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
    def log_model(self, model_path: str, name: str = "model"):
        # ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.is_initialized:
            return  # í•¨ìˆ˜ ì¢…ë£Œ

        artifact = wandb.Artifact(name, type="model")   # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„±
        artifact.add_file(model_path)                   # ëª¨ë¸ íŒŒì¼ ì¶”ê°€
        wandb.log_artifact(artifact)                    # ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
    
    
    # í˜¼ë™ í–‰ë ¬ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
    def log_confusion_matrix(self, y_true, y_pred, class_names=None):
        # ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.is_initialized:
            return  # í•¨ìˆ˜ ì¢…ë£Œ
        
        # WandBì— í˜¼ë™ í–‰ë ¬ ë¡œê¹…
        wandb.log({
            "confusion_matrix": wandb.plot.confusion_matrix(# í˜¼ë™ í–‰ë ¬ í”Œë¡¯ ìƒì„±
                y_true=y_true,                              # ì‹¤ì œ ê°’
                preds=y_pred,                               # ì˜ˆì¸¡ ê°’
                class_names=class_names                     # í´ë˜ìŠ¤ ì´ë¦„
            )
        })

    
    # ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
    def log_predictions(self, images, predictions, targets, class_names=None):
        # ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not self.is_initialized:
            return  # í•¨ìˆ˜ ì¢…ë£Œ

        # ìµœëŒ€ 100ê°œ ìƒ˜í”Œë§Œ ë¡œê¹…
        max_samples = min(100, len(images))      # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ

        data = []                                # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

        # ìƒ˜í”Œ ìˆ˜ë§Œí¼ ë°˜ë³µ
        for i in range(max_samples):
            img = images[i]                      # ië²ˆì§¸ ì´ë¯¸ì§€
            pred = predictions[i]                # ië²ˆì§¸ ì˜ˆì¸¡
            target = targets[i]                  # ië²ˆì§¸ ì •ë‹µ

            # ì´ë¯¸ì§€ë¥¼ wandb Imageë¡œ ë³€í™˜
            if torch.is_tensor(img):             # í…ì„œì¸ ê²½ìš°
                # numpyë¡œ ë³€í™˜ í›„ ì°¨ì› ìˆœì„œ ë³€ê²½
                img = img.cpu().numpy().transpose(1, 2, 0)

            # ì˜ˆì¸¡ í´ë˜ìŠ¤ëª…
            pred_class = class_names[pred] if class_names else str(pred)
            # ì •ë‹µ í´ë˜ìŠ¤ëª…
            target_class = class_names[target] if class_names else str(target)

            # ë°ì´í„° ì¶”ê°€
            data.append([
                wandb.Image(img),                # WandB ì´ë¯¸ì§€ ê°ì²´
                pred_class,                      # ì˜ˆì¸¡ í´ë˜ìŠ¤
                target_class,                    # ì •ë‹µ í´ë˜ìŠ¤
                pred == target                   # ì •ë‹µ ì—¬ë¶€
            ])

        # WandB í…Œì´ë¸” ìƒì„±
        table = wandb.Table(
            data=data,                           # í…Œì´ë¸” ë°ì´í„°
            columns=["Image", "Prediction", "Target", "Correct"]  # ì»¬ëŸ¼ëª…
        )

        wandb.log({"predictions": table})        # ì˜ˆì¸¡ í…Œì´ë¸” ë¡œê¹…


    # ========== PRD 11: ì¶”ê°€ ì‹œê°í™” 5ê°€ì§€ ========== #

    # 1. í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë¡œê¹…
    def log_learning_rate_schedule(self, step: int, learning_rate: float):
        """
        í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë¡œê¹…

        Args:
            step: í˜„ì¬ ìŠ¤í…
            learning_rate: í˜„ì¬ í•™ìŠµë¥ 
        """
        if not self.is_initialized:
            return

        wandb.log({
            "learning_rate": learning_rate,
            "step": step
        }, step=step)


    # 2. ê·¸ë˜ë””ì–¸íŠ¸ norm ë¡œê¹…
    def log_gradient_norms(self, model, step: int):
        """
        ê·¸ë˜ë””ì–¸íŠ¸ norm ë¡œê¹…

        Args:
            model: PyTorch ëª¨ë¸
            step: í˜„ì¬ ìŠ¤í…
        """
        if not self.is_initialized:
            return

        # ì „ì²´ ê·¸ë˜ë””ì–¸íŠ¸ norm ê³„ì‚°
        total_norm = 0.0
        layer_norms = {}

        for name, param in model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2).item()
                total_norm += param_norm ** 2

                # ë ˆì´ì–´ë³„ norm ì €ì¥ (ìƒìœ„ 10ê°œë§Œ)
                layer_norms[f"grad_norm/{name}"] = param_norm

        total_norm = total_norm ** 0.5

        # ë¡œê¹…
        log_data = {
            "gradient/total_norm": total_norm,
            "step": step
        }

        # ìƒìœ„ 10ê°œ ë ˆì´ì–´ë§Œ ë¡œê¹… (ë„ˆë¬´ ë§ìœ¼ë©´ ëŠë ¤ì§)
        sorted_layers = sorted(layer_norms.items(), key=lambda x: x[1], reverse=True)[:10]
        for layer_name, norm_value in sorted_layers:
            log_data[layer_name] = norm_value

        wandb.log(log_data, step=step)


    # 3. Loss curve ë¡œê¹…
    def log_loss_curve(self, train_loss: float, val_loss: float = None, step: int = None):
        """
        Loss curve ë¡œê¹…

        Args:
            train_loss: í•™ìŠµ ì†ì‹¤
            val_loss: ê²€ì¦ ì†ì‹¤ (ì„ íƒ)
            step: ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
        """
        if not self.is_initialized:
            return

        log_data = {
            "loss/train": train_loss
        }

        if val_loss is not None:
            log_data["loss/val"] = val_loss

            # Loss ì°¨ì´ë„ ë¡œê¹… (overfitting ëª¨ë‹ˆí„°ë§)
            log_data["loss/train_val_diff"] = train_loss - val_loss

        wandb.log(log_data, step=step)


    # 4. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
    def log_gpu_memory(self, step: int = None):
        """
        GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…

        Args:
            step: ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
        """
        if not self.is_initialized:
            return

        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not torch.cuda.is_available():
            return

        # GPU ê°œìˆ˜ë§Œí¼ ë°˜ë³µ
        log_data = {}

        for i in range(torch.cuda.device_count()):
            # ë©”ëª¨ë¦¬ ì •ë³´ (ë°”ì´íŠ¸ -> GB)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(i) / 1024**3

            log_data[f"gpu_{i}/memory_allocated_gb"] = allocated
            log_data[f"gpu_{i}/memory_reserved_gb"] = reserved
            log_data[f"gpu_{i}/memory_max_allocated_gb"] = max_allocated

            # ì‚¬ìš©ë¥  (%)
            if reserved > 0:
                log_data[f"gpu_{i}/memory_utilization"] = (allocated / reserved) * 100

        wandb.log(log_data, step=step)


    # 5. í•™ìŠµ ì†ë„ ë¡œê¹…
    def log_training_speed(
        self,
        samples_per_second: float = None,
        steps_per_second: float = None,
        epoch_time: float = None,
        step: int = None
    ):
        """
        í•™ìŠµ ì†ë„ ë¡œê¹…

        Args:
            samples_per_second: ì´ˆë‹¹ ìƒ˜í”Œ ìˆ˜
            steps_per_second: ì´ˆë‹¹ ìŠ¤í… ìˆ˜
            epoch_time: ì—í¬í¬ ì†Œìš” ì‹œê°„ (ì´ˆ)
            step: ìŠ¤í… ë²ˆí˜¸ (ì„ íƒ)
        """
        if not self.is_initialized:
            return

        log_data = {}

        if samples_per_second is not None:
            log_data["speed/samples_per_second"] = samples_per_second

        if steps_per_second is not None:
            log_data["speed/steps_per_second"] = steps_per_second

        if epoch_time is not None:
            log_data["speed/epoch_time_seconds"] = epoch_time
            log_data["speed/epoch_time_minutes"] = epoch_time / 60

        wandb.log(log_data, step=step)
    
    
    # ì‹¤í–‰ ì¢…ë£Œ í•¨ìˆ˜ ì •ì˜
    def finish(self):
        # ì‹¤í–‰ ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        if self.run is not None:
            wandb.finish()                       # WandB ì‹¤í–‰ ì¢…ë£Œ
            self.is_initialized = False          # ì´ˆê¸°í™” ìƒíƒœ ë¦¬ì…‹
    
    
    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… í•¨ìˆ˜ ì •ì˜
    def __enter__(self):
        return self                              # ìê¸° ìì‹  ë°˜í™˜
    
    
    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ í•¨ìˆ˜ ì •ì˜
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.finish()                            # ì‹¤í–‰ ì¢…ë£Œ


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_wandb_config(
    model_name: str,                            # ëª¨ë¸ ì´ë¦„
    img_size: int,                              # ì´ë¯¸ì§€ í¬ê¸°
    batch_size: int,                            # ë°°ì¹˜ í¬ê¸°
    learning_rate: float,                       # í•™ìŠµë¥ 
    epochs: int,                                # ì—í¬í¬ ìˆ˜
    **kwargs                                    # ì¶”ê°€ ì¸ì
) -> Dict[str, Any]:                            # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    """WandB config ìƒì„± í•¨ìˆ˜"""
    config = {
        "architecture": model_name,             # ëª¨ë¸ êµ¬ì¡°
        "image_size": img_size,                 # ì´ë¯¸ì§€ í¬ê¸°
        "batch_size": batch_size,               # ë°°ì¹˜ í¬ê¸°
        "learning_rate": learning_rate,         # í•™ìŠµë¥ 
        "epochs": epochs,                       # ì—í¬í¬ ìˆ˜
        "framework": "PyTorch",                 # í”„ë ˆì„ì›Œí¬
        "dataset": "Document Classification",   # ë°ì´í„°ì…‹
    }
    
    config.update(kwargs)                       # ì¶”ê°€ ì„¤ì • ì—…ë°ì´íŠ¸
    return config                               # ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë°˜í™˜


# í´ë“œ ê²°ê³¼ ë¡œê¹… í•¨ìˆ˜ ì •ì˜
def log_fold_results(logger: WandbLogger, fold: int, metrics: Dict[str, float]):
    # í´ë“œë³„ ë©”íŠ¸ë¦­ ë¡œê¹…
    logger.log_metrics({
        f"fold_{fold}_train_f1": metrics.get("train_f1", 0),        # í´ë“œë³„ í•™ìŠµ F1
        f"fold_{fold}_val_f1": metrics.get("val_f1", 0),            # í´ë“œë³„ ê²€ì¦ F1
        f"fold_{fold}_train_loss": metrics.get("train_loss", 0),    # í´ë“œë³„ í•™ìŠµ ì†ì‹¤
        f"fold_{fold}_val_loss": metrics.get("val_loss", 0),        # í´ë“œë³„ ê²€ì¦ ì†ì‹¤
    })
