# ==================== Predictor í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. Predictor ì´ˆê¸°í™”
2. ë‹¨ì¼ ì˜ˆì¸¡
3. ë°°ì¹˜ ì˜ˆì¸¡
4. DataFrame ì˜ˆì¸¡
5. ì œì¶œ íŒŒì¼ ìƒì„±
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
from pathlib import Path
import tempfile

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import pandas as pd

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.inference import Predictor, create_predictor


# ==================== í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ ==================== #
# ---------------------- Predictor ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ---------------------- #
def test_predictor_initialization():
    """Predictor ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 1: Predictor ì´ˆê¸°í™”")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")             # ë² ì´ìŠ¤ë¼ì¸ Config
    print(f"  Config ë¡œë“œ ì™„ë£Œ")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)  # ëª¨ë¸ ë¡œë“œ
    print(f"  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # Predictor ì´ˆê¸°í™”
    predictor = Predictor(                              # Predictor ìƒì„±
        model=model,
        tokenizer=tokenizer,
        config=config
    )

    # ê²€ì¦
    assert predictor.model is not None                  # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    assert predictor.tokenizer is not None              # í† í¬ë‚˜ì´ì € í™•ì¸
    assert predictor.generation_config is not None      # ìƒì„± ì„¤ì • í™•ì¸
    assert predictor.device is not None                 # ë””ë°”ì´ìŠ¤ í™•ì¸

    print(f"\n  ë””ë°”ì´ìŠ¤: {predictor.device}")
    print(f"  ìƒì„± ì„¤ì •: {predictor.generation_config}")

    print("\nâœ… Predictor ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_single_prediction():
    """ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 2: ë‹¨ì¼ ì˜ˆì¸¡")
    print("="*60)

    # Config ë° ëª¨ë¸ ë¡œë“œ
    config = load_config("baseline_kobart")
    model, tokenizer = load_model_and_tokenizer(config)

    # Predictor ìƒì„±
    predictor = Predictor(model, tokenizer, config)

    # í…ŒìŠ¤íŠ¸ ëŒ€í™”
    dialogue = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”.\\n#Person2#: ë„¤, ì •ë§ í™”ì°½í•˜ë„¤ìš”. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì˜ˆìš”."

    # ì˜ˆì¸¡ ì‹¤í–‰
    print(f"\n  ì…ë ¥ ëŒ€í™”: {dialogue[:50]}...")
    summary = predictor.predict_single(dialogue)        # ë‹¨ì¼ ì˜ˆì¸¡

    # ê²°ê³¼ ì¶œë ¥
    print(f"  ì˜ˆì¸¡ ìš”ì•½: {summary}")

    # ê²€ì¦
    assert isinstance(summary, str)                     # ë¬¸ìì—´ íƒ€ì… í™•ì¸
    assert len(summary) > 0                             # ë¹„ì–´ìˆì§€ ì•ŠìŒ í™•ì¸

    print("\nâœ… ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_batch_prediction():
    """ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 3: ë°°ì¹˜ ì˜ˆì¸¡")
    print("="*60)

    # Config ë° ëª¨ë¸ ë¡œë“œ
    config = load_config("baseline_kobart")
    model, tokenizer = load_model_and_tokenizer(config)

    # Predictor ìƒì„±
    predictor = Predictor(model, tokenizer, config)

    # í…ŒìŠ¤íŠ¸ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    dialogues = [
        "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤",
        "#Person1#: ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”\\n#Person2#: ë„¤, ì‚°ì±…í•˜ê¸° ì¢‹ì•„ìš”",
        "#Person1#: ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”?\\n#Person2#: ê¹€ì¹˜ì°Œê°œ ì–´ë•Œìš”?"
    ]

    # ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰
    print(f"\n  ìƒ˜í”Œ ê°œìˆ˜: {len(dialogues)}")
    summaries = predictor.predict_batch(                # ë°°ì¹˜ ì˜ˆì¸¡
        dialogues,
        batch_size=2,
        show_progress=True
    )

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n  ì˜ˆì¸¡ ê²°ê³¼:")
    for i, summary in enumerate(summaries, 1):
        print(f"    {i}. {summary}")

    # ê²€ì¦
    assert len(summaries) == len(dialogues)             # ê°œìˆ˜ ì¼ì¹˜ í™•ì¸
    assert all(isinstance(s, str) for s in summaries)   # ëª¨ë‘ ë¬¸ìì—´ í™•ì¸
    assert all(len(s) > 0 for s in summaries)           # ëª¨ë‘ ë¹„ì–´ìˆì§€ ì•ŠìŒ

    print("\nâœ… ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- DataFrame ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_dataframe_prediction():
    """DataFrame ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 4: DataFrame ì˜ˆì¸¡")
    print("="*60)

    # Config ë° ëª¨ë¸ ë¡œë“œ
    config = load_config("baseline_kobart")
    model, tokenizer = load_model_and_tokenizer(config)

    # Predictor ìƒì„±
    predictor = Predictor(model, tokenizer, config)

    # í…ŒìŠ¤íŠ¸ DataFrame ìƒì„±
    test_df = pd.DataFrame({
        'fname': ['test_001', 'test_002', 'test_003'],
        'dialogue': [
            "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤",
            "#Person1#: ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”\\n#Person2#: ë„¤, ì¢‹ì•„ìš”",
            "#Person1#: ì ì‹¬ ë­ ë“œì‹¤ë˜ìš”?\\n#Person2#: ê¹€ì¹˜ì°Œê°œìš”"
        ]
    })

    # DataFrame ì˜ˆì¸¡ ì‹¤í–‰
    print(f"\n  ìƒ˜í”Œ ê°œìˆ˜: {len(test_df)}")
    result_df = predictor.predict_dataframe(            # DataFrame ì˜ˆì¸¡
        test_df,
        batch_size=2,
        show_progress=True
    )

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n  ì˜ˆì¸¡ ê²°ê³¼:")
    print(result_df[['fname', 'summary']])

    # ê²€ì¦
    assert 'summary' in result_df.columns               # summary ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    assert len(result_df) == len(test_df)               # ê°œìˆ˜ ì¼ì¹˜ í™•ì¸
    assert result_df['summary'].notna().all()           # ëª¨ë‘ ê°’ì´ ìˆìŒ í™•ì¸

    print("\nâœ… DataFrame ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ì œì¶œ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸ ---------------------- #
def test_submission_creation():
    """ì œì¶œ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 5: ì œì¶œ íŒŒì¼ ìƒì„±")
    print("="*60)

    # Config ë° ëª¨ë¸ ë¡œë“œ
    config = load_config("baseline_kobart")
    model, tokenizer = load_model_and_tokenizer(config)

    # Predictor ìƒì„±
    predictor = Predictor(model, tokenizer, config)

    # í…ŒìŠ¤íŠ¸ DataFrame ìƒì„±
    test_df = pd.DataFrame({
        'fname': ['test_001', 'test_002'],
        'dialogue': [
            "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤",
            "#Person1#: ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”\\n#Person2#: ë„¤, ì¢‹ì•„ìš”"
        ]
    })

    # ì„ì‹œ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
        output_path = f.name

    # ì œì¶œ íŒŒì¼ ìƒì„±
    print(f"\n  ì¶œë ¥ ê²½ë¡œ: {output_path}")
    submission_df = predictor.create_submission(        # ì œì¶œ íŒŒì¼ ìƒì„±
        test_df,
        output_path=output_path,
        batch_size=2,
        show_progress=True
    )

    # ê²°ê³¼ í™•ì¸
    print(f"\n  ì œì¶œ DataFrame:")
    print(submission_df)

    # ê²€ì¦
    assert submission_df.columns.tolist() == ['fname', 'summary']  # ì»¬ëŸ¼ í™•ì¸
    assert len(submission_df) == len(test_df)           # ê°œìˆ˜ ì¼ì¹˜ í™•ì¸

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    assert Path(output_path).exists()                   # íŒŒì¼ ìƒì„± í™•ì¸

    # íŒŒì¼ ì½ì–´ì„œ ê²€ì¦
    saved_df = pd.read_csv(output_path)                 # ì €ì¥ëœ íŒŒì¼ ì½ê¸°
    assert len(saved_df) == len(test_df)                # ì €ì¥ëœ ê°œìˆ˜ í™•ì¸

    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    Path(output_path).unlink()                          # íŒŒì¼ ì‚­ì œ

    print("\nâœ… ì œì¶œ íŒŒì¼ ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_convenience_function():
    """í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 6: í¸ì˜ í•¨ìˆ˜")
    print("="*60)

    # Config ë° ëª¨ë¸ ë¡œë“œ
    config = load_config("baseline_kobart")
    model, tokenizer = load_model_and_tokenizer(config)

    # í¸ì˜ í•¨ìˆ˜ë¡œ Predictor ìƒì„±
    predictor = create_predictor(                       # í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©
        model=model,
        tokenizer=tokenizer,
        config=config
    )

    # ê²€ì¦
    assert isinstance(predictor, Predictor)             # íƒ€ì… í™•ì¸
    assert predictor.model is not None                  # ëª¨ë¸ ì¡´ì¬ í™•ì¸

    print(f"\n  Predictor ìƒì„± ì™„ë£Œ")
    print(f"  íƒ€ì…: {type(predictor).__name__}")

    print("\nâœ… í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ==================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    print("\n" + "="*60)
    print("Predictor í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    print("\nâš ï¸ ì°¸ê³ : ì‹¤ì œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_predictor_initialization()                 # í…ŒìŠ¤íŠ¸ 1
        test_single_prediction()                        # í…ŒìŠ¤íŠ¸ 2
        test_batch_prediction()                         # í…ŒìŠ¤íŠ¸ 3
        test_dataframe_prediction()                     # í…ŒìŠ¤íŠ¸ 4
        test_submission_creation()                      # í…ŒìŠ¤íŠ¸ 5
        test_convenience_function()                     # í…ŒìŠ¤íŠ¸ 6

        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("="*60)

    except Exception as e:
        print("\n" + "="*60)
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("="*60)
        import traceback
        traceback.print_exc()
        raise
