# ==================== Trainer í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
Trainer ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. Trainer ì´ˆê¸°í™”
2. í•™ìŠµ ì¸ì ìƒì„±
3. ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜
4. Trainer ìƒì„±
5. í†µí•© í…ŒìŠ¤íŠ¸ (ì´ˆê¸°í™”ë§Œ)
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import torch
import numpy as np

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import ModelTrainer, create_trainer


# ==================== í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ ==================== #
# ---------------------- Trainer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ---------------------- #
def test_trainer_initialization():
    """Trainer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 1: Trainer ì´ˆê¸°í™”")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")             # ë² ì´ìŠ¤ë¼ì¸ Config
    print(f"  Config ë¡œë“œ ì™„ë£Œ")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)  # ëª¨ë¸ ë¡œë“œ
    print(f"  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±
    dummy_dialogues = ["#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤"] * 10
    dummy_summaries = ["ë‘ ì‚¬ëŒì´ ì¸ì‚¬ë¥¼ ë‚˜ëˆ´ë‹¤"] * 10

    train_dataset = DialogueSummarizationDataset(      # í•™ìŠµ ë°ì´í„°ì…‹
        dialogues=dummy_dialogues,
        summaries=dummy_summaries,
        tokenizer=tokenizer,
        encoder_max_len=128,
        decoder_max_len=50,
        preprocess=False
    )
    print(f"  ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(train_dataset)}ê°œ ìƒ˜í”Œ")

    # Trainer ì´ˆê¸°í™” (WandB ë¹„í™œì„±í™”)
    trainer = ModelTrainer(                             # Trainer ìƒì„±
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=train_dataset,                     # í‰ê°€ìš©ìœ¼ë¡œ ê°™ì€ ë°ì´í„° ì‚¬ìš©
        use_wandb=False                                 # WandB ë¹„í™œì„±í™”
    )

    # ê²€ì¦
    assert trainer.model is not None                    # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    assert trainer.tokenizer is not None                # í† í¬ë‚˜ì´ì € ì¡´ì¬ í™•ì¸
    assert trainer.train_dataset is not None            # í•™ìŠµ ë°ì´í„°ì…‹ í™•ì¸
    assert trainer.rouge_calculator is not None         # ROUGE ê³„ì‚°ê¸° í™•ì¸

    print("\nâœ… Trainer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- í•™ìŠµ ì¸ì ìƒì„± í…ŒìŠ¤íŠ¸ ---------------------- #
def test_training_args():
    """í•™ìŠµ ì¸ì ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 2: í•™ìŠµ ì¸ì ìƒì„±")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")             # ë² ì´ìŠ¤ë¼ì¸ Config

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)  # ëª¨ë¸ ë¡œë“œ

    # ë”ë¯¸ ë°ì´í„°ì…‹
    dummy_dialogues = ["#Person1#: ì•ˆë…•"] * 5
    dummy_summaries = ["ì¸ì‚¬"] * 5

    train_dataset = DialogueSummarizationDataset(
        dialogues=dummy_dialogues,
        summaries=dummy_summaries,
        tokenizer=tokenizer,
        preprocess=False
    )

    # Trainer ì´ˆê¸°í™”
    trainer = ModelTrainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        use_wandb=False
    )

    # í•™ìŠµ ì¸ì í™•ì¸
    args = trainer.training_args                        # í•™ìŠµ ì¸ì
    print(f"\n  ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"  ì—í¬í¬ ìˆ˜: {args.num_train_epochs}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {args.per_device_train_batch_size}")
    print(f"  í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  í‰ê°€ ì „ëµ: {args.eval_strategy}")
    print(f"  Beam ìˆ˜: {args.generation_num_beams}")

    # ê²€ì¦
    assert args.num_train_epochs == config.training.epochs  # ì—í¬í¬ í™•ì¸
    assert args.per_device_train_batch_size == config.training.batch_size  # ë°°ì¹˜ í¬ê¸° í™•ì¸
    assert args.predict_with_generate is True           # ìƒì„± ëª¨ë“œ í™•ì¸

    print("\nâœ… í•™ìŠµ ì¸ì ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_compute_metrics():
    """ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 3: ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)

    # ë”ë¯¸ ë°ì´í„°ì…‹
    train_dataset = DialogueSummarizationDataset(
        dialogues=["ì•ˆë…•"],
        summaries=["ì¸ì‚¬"],
        tokenizer=tokenizer,
        preprocess=False
    )

    # Trainer ì´ˆê¸°í™”
    trainer = ModelTrainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        use_wandb=False
    )

    # ë”ë¯¸ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±
    # ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶œë ¥ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” í† í¬ë‚˜ì´ì§•ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
    pred_text = "ë‘ ì‚¬ëŒì´ ì¸ì‚¬ë¥¼ ë‚˜ëˆ´ë‹¤"
    label_text = "ë‘ ì‚¬ëŒì´ ì„œë¡œ ì¸ì‚¬í–ˆë‹¤"

    # í† í¬ë‚˜ì´ì§•
    pred_ids = tokenizer.encode(pred_text, add_special_tokens=False)
    label_ids = tokenizer.encode(label_text, add_special_tokens=False)

    # íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ 30ìœ¼ë¡œ)
    max_len = 30
    pred_ids = pred_ids + [tokenizer.pad_token_id] * (max_len - len(pred_ids))
    label_ids = label_ids + [-100] * (max_len - len(label_ids))  # -100ì€ ë¬´ì‹œë¨

    # numpy ë°°ì—´ë¡œ ë³€í™˜ (ë°°ì¹˜ í˜•íƒœ)
    predictions = np.array([pred_ids])                  # (1, max_len)
    labels = np.array([label_ids])                      # (1, max_len)

    # ROUGE ê³„ì‚°
    eval_preds = (predictions, labels)
    metrics = trainer.compute_metrics(eval_preds)       # ë©”íŠ¸ë¦­ ê³„ì‚°

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n  ì˜ˆì¸¡: {pred_text}")
    print(f"  ì •ë‹µ: {label_text}")
    print(f"\n  ROUGE-1: {metrics['rouge1']:.4f}")
    print(f"  ROUGE-2: {metrics['rouge2']:.4f}")
    print(f"  ROUGE-L: {metrics['rougeL']:.4f}")
    print(f"  ROUGE Sum: {metrics['rouge_sum']:.4f}")

    # ê²€ì¦
    assert 'rouge1' in metrics                          # ROUGE-1 ì¡´ì¬ í™•ì¸
    assert 'rouge_sum' in metrics                       # ROUGE Sum ì¡´ì¬ í™•ì¸
    assert 0 <= metrics['rouge1'] <= 1                  # ì ìˆ˜ ë²”ìœ„ í™•ì¸

    print("\nâœ… ROUGE ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- HuggingFace Trainer ìƒì„± í…ŒìŠ¤íŠ¸ ---------------------- #
def test_hf_trainer_creation():
    """HuggingFace Trainer ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 4: HuggingFace Trainer ìƒì„±")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)

    # ë”ë¯¸ ë°ì´í„°ì…‹
    train_dataset = DialogueSummarizationDataset(
        dialogues=["ì•ˆë…•í•˜ì„¸ìš”"] * 5,
        summaries=["ì¸ì‚¬"] * 5,
        tokenizer=tokenizer,
        preprocess=False
    )

    # Trainer ì´ˆê¸°í™”
    trainer = ModelTrainer(
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=train_dataset,
        use_wandb=False
    )

    # HuggingFace Trainer ìƒì„±
    hf_trainer = trainer._create_trainer()              # Trainer ìƒì„±

    # ê²€ì¦
    assert hf_trainer is not None                       # Trainer ì¡´ì¬ í™•ì¸
    assert hf_trainer.model is not None                 # ëª¨ë¸ í™•ì¸
    assert hf_trainer.train_dataset is not None         # í•™ìŠµ ë°ì´í„°ì…‹ í™•ì¸

    print(f"\n  Trainer íƒ€ì…: {type(hf_trainer).__name__}")
    print(f"  ëª¨ë¸: {type(hf_trainer.model).__name__}")
    print(f"  í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(hf_trainer.train_dataset)}")

    print("\nâœ… HuggingFace Trainer ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_convenience_function():
    """í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 5: í¸ì˜ í•¨ìˆ˜")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)

    # ë”ë¯¸ ë°ì´í„°ì…‹
    train_dataset = DialogueSummarizationDataset(
        dialogues=["ì•ˆë…•"],
        summaries=["ì¸ì‚¬"],
        tokenizer=tokenizer,
        preprocess=False
    )

    # í¸ì˜ í•¨ìˆ˜ë¡œ Trainer ìƒì„±
    trainer = create_trainer(                           # í¸ì˜ í•¨ìˆ˜ ì‚¬ìš©
        config=config,
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        use_wandb=False
    )

    # ê²€ì¦
    assert isinstance(trainer, ModelTrainer)            # íƒ€ì… í™•ì¸

    print(f"\n  Trainer ìƒì„± ì™„ë£Œ")
    print(f"  íƒ€ì…: {type(trainer).__name__}")

    print("\nâœ… í¸ì˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ==================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    print("\n" + "="*60)
    print("Trainer í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    print("\nâš ï¸ ì°¸ê³ : ì‹¤ì œ í•™ìŠµì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì´ˆê¸°í™” ë° ì„¤ì •ë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_trainer_initialization()                   # í…ŒìŠ¤íŠ¸ 1
        test_training_args()                            # í…ŒìŠ¤íŠ¸ 2
        test_compute_metrics()                          # í…ŒìŠ¤íŠ¸ 3
        test_hf_trainer_creation()                      # í…ŒìŠ¤íŠ¸ 4
        test_convenience_function()                     # í…ŒìŠ¤íŠ¸ 5

        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("="*60)

    except Exception as e:
        print("\n" + "="*60)
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("="*60)
        import traceback
        traceback.print_exc()
        raise
