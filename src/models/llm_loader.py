# ==================== Causal LM ëª¨ë¸ ë¡œë” ==================== #
"""
Causal LM ëª¨ë¸ ë¡œë” (Llama, Qwen, ë“±)

PRD 08: LLM íŒŒì¸íŠœë‹ ì „ëµ êµ¬í˜„
- QLoRA (4-bit quantization + LoRA)
- Chat Template ì§€ì›
- Gradient Checkpointing
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import torch

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


# ==================== LoRA Target Modules ìë™ íƒì§€ ==================== #
def _find_target_modules(model, logger=None):
    """
    ëª¨ë¸ì—ì„œ LoRAë¥¼ ì ìš©í•  ìˆ˜ ìˆëŠ” Linear ë ˆì´ì–´ë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ìŒ

    Args:
        model: ëª¨ë¸ ê°ì²´
        logger: ë¡œê±° (ì„ íƒì )

    Returns:
        list: LoRAë¥¼ ì ìš©í•  target_modules ë¦¬ìŠ¤íŠ¸
    """
    import re
    import torch.nn as nn

    # ëª¨ë“  ëª¨ë“ˆ ì´ë¦„ ìˆ˜ì§‘
    module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # ëª¨ë“ˆ ì´ë¦„ì—ì„œ ìˆ«ìì™€ êµ¬ë¶„ìë¥¼ ì œê±°í•˜ê³  ê¸°ë³¸ ì´ë¦„ë§Œ ì¶”ì¶œ
            # ì˜ˆ: "model.layers.0.self_attn.q_proj" -> "q_proj"
            parts = name.split('.')
            module_name = parts[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ (ì˜ˆ: q_proj)
            module_names.add(module_name)

    # ì¼ë°˜ì ì¸ attention/MLP ë ˆì´ì–´ ì´ë¦„ íŒ¨í„´ ì •ì˜ (ìš°ì„ ìˆœìœ„ ìˆœ)
    common_patterns = [
        # Llama/Mistral ìŠ¤íƒ€ì¼
        ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
        # GPT-Neo/GPT-J ìŠ¤íƒ€ì¼
        ['q_proj', 'k_proj', 'v_proj', 'out_proj', 'fc_in', 'fc_out'],
        # Polyglot/GPT-NeoX ìŠ¤íƒ€ì¼
        ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],
        # BLOOM ìŠ¤íƒ€ì¼
        ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'],
    ]

    # íŒ¨í„´ ë§¤ì¹­í•˜ì—¬ ê°€ì¥ ë§ì´ ì¼ì¹˜í•˜ëŠ” íŒ¨í„´ ì„ íƒ
    best_match = []
    best_match_count = 0

    for pattern in common_patterns:
        match_count = sum(1 for name in pattern if name in module_names)
        if match_count > best_match_count:
            best_match_count = match_count
            best_match = [name for name in pattern if name in module_names]

    # ë§¤ì¹­ëœ ëª¨ë“ˆì´ ì—†ìœ¼ë©´ ëª¨ë“  Linear ë ˆì´ì–´ ì‚¬ìš©
    if not best_match:
        best_match = list(module_names)
        if logger:
            logger.write(f"    âš ï¸ ì¼ë°˜ íŒ¨í„´ ë¯¸ë°œê²¬, ëª¨ë“  Linear ë ˆì´ì–´ ì‚¬ìš©: {best_match}")

    if logger:
        logger.write(f"    ğŸ” ìë™ íƒì§€ëœ target_modules: {best_match}")

    return best_match


# ==================== Causal LM ë¡œë” ==================== #
def load_causal_lm(config, logger=None):
    """
    Causal LM ëª¨ë¸ ë¡œë“œ (QLoRA í¬í•¨)

    Args:
        config: ëª¨ë¸ ì„¤ì •
        logger: ë¡œê±°

    Returns:
        model, tokenizer
    """
    if logger:
        logger.write(f"Loading Causal LM: {config.model.checkpoint}")

    # 1. Quantization ì„¤ì •
    quantization_config = None
    if hasattr(config.model, 'quantization') and config.model.quantization:
        if logger:
            logger.write("  Quantization ì„¤ì • ì ìš©...")

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=config.model.quantization.get('load_in_4bit', True),
            bnb_4bit_compute_dtype=getattr(
                torch,
                config.model.quantization.get('bnb_4bit_compute_dtype', 'float16')
            ),
            bnb_4bit_quant_type=config.model.quantization.get('bnb_4bit_quant_type', 'nf4'),
            bnb_4bit_use_double_quant=config.model.quantization.get('bnb_4bit_use_double_quant', True)
        )

    # 2. ëª¨ë¸ ë¡œë“œ
    if logger:
        logger.write("  ëª¨ë¸ ë¡œë”© ì¤‘...")

    # offload_folder ì„¤ì • (ëŒ€í˜• ëª¨ë¸ ë¡œë”© ì‹œ ë””ìŠ¤í¬ ì˜¤í”„ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”)
    from pathlib import Path
    offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
    offload_dir.mkdir(parents=True, exist_ok=True)

    model = AutoModelForCausalLM.from_pretrained(
        config.model.checkpoint,
        quantization_config=quantization_config,
        device_map="auto",
        offload_folder=str(offload_dir),
        torch_dtype=torch.float16,  # AMP GradScaler í˜¸í™˜ì„±ì„ ìœ„í•´ Float16 ê³ ì •
        trust_remote_code=True
    )

    # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
    if logger:
        logger.write("  í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")

    tokenizer = AutoTokenizer.from_pretrained(
        config.model.checkpoint,
        trust_remote_code=True
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id
        if logger:
            logger.write(f"  íŒ¨ë”© í† í° ì„¤ì •: {tokenizer.eos_token}")

    # Chat í…œí”Œë¦¿ í† í° ì¶”ê°€
    if hasattr(config.tokenizer, 'chat_template_tokens') and config.tokenizer.chat_template_tokens:
        special_tokens = {
            'additional_special_tokens': config.tokenizer.chat_template_tokens
        }
        num_added = tokenizer.add_special_tokens(special_tokens)
        if num_added > 0:
            model.resize_token_embeddings(len(tokenizer))
            if logger:
                logger.write(f"  Chat í…œí”Œë¦¿ í† í° ì¶”ê°€: {num_added}ê°œ")

    # 4. Full Fine-tuning vs LoRA ì„ íƒ
    use_full_finetuning = getattr(config, 'use_full_finetuning', False)

    if use_full_finetuning:
        # Full Fine-tuning ëª¨ë“œ
        if logger:
            logger.write("  âœ… Full Fine-tuning ëª¨ë“œ í™œì„±í™”")
            logger.write("    ëª¨ë“  íŒŒë¼ë¯¸í„° í•™ìŠµ ê°€ëŠ¥")

            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in model.parameters())

            logger.write(f"    í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} (100%)")
            logger.write(f"    ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")

        # Gradient Checkpointingì€ Full FTì—ì„œë„ ìœ ìš©
        if config.training.get('gradient_checkpointing', True):
            model.gradient_checkpointing_enable()
            model.config.use_cache = False

            if logger:
                logger.write("  âœ… Gradient Checkpointing í™œì„±í™”")

        return model, tokenizer

    # LoRA ì„¤ì •
    if hasattr(config.model, 'lora') and config.model.lora:
        if logger:
            logger.write("  LoRA ì„¤ì • ì ìš© ì¤‘...")

        # K-bit training ì¤€ë¹„ (Quantizationì´ ìˆì„ ë•Œë§Œ)
        if quantization_config is not None:
            model = prepare_model_for_kbit_training(model)
            if logger:
                logger.write("    K-bit training ì¤€ë¹„ ì™„ë£Œ")

        # LoRA Config - target_modules ìë™ íƒì§€
        # Configì— target_modulesê°€ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ íƒì§€
        target_modules = config.model.lora.get('target_modules', None)

        if target_modules is None:
            # target_modulesê°€ ì—†ìœ¼ë©´ ëª¨ë¸ì—ì„œ ìë™ìœ¼ë¡œ ì°¾ê¸°
            target_modules = _find_target_modules(model, logger)

        lora_config = LoraConfig(
            r=config.model.lora.get('r', 16),
            lora_alpha=config.model.lora.get('lora_alpha', 32),
            lora_dropout=config.model.lora.get('lora_dropout', 0.05),
            bias=config.model.lora.get('bias', 'none'),
            task_type=config.model.lora.get('task_type', 'CAUSAL_LM'),
            target_modules=target_modules
        )

        # PEFT ëª¨ë¸ ìƒì„±
        try:
            model = get_peft_model(model, lora_config)
        except ValueError as e:
            # target_modulesê°€ ëª¨ë¸ì— ì—†ëŠ” ê²½ìš° ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì¬ì‹œë„
            if "not found in the base model" in str(e):
                if logger:
                    logger.write(f"    âš ï¸ Target modules ë¶ˆì¼ì¹˜, ìë™ íƒì§€ ì¤‘...")
                target_modules = _find_target_modules(model, logger)
                lora_config.target_modules = target_modules
                model = get_peft_model(model, lora_config)
            else:
                raise

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_percentage = 100 * trainable_params / total_params

        if logger:
            logger.write(f"  âœ… LoRA ì ìš© ì™„ë£Œ")
            logger.write(f"    í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({trainable_percentage:.2f}%)")
            logger.write(f"    ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")

    # 5. Gradient Checkpointing
    if config.training.get('gradient_checkpointing', True):
        model.gradient_checkpointing_enable()
        model.config.use_cache = False  # Gradient checkpointingê³¼ í•¨ê»˜ ì‚¬ìš© ì‹œ í•„ìˆ˜

        # LoRAì™€ Gradient Checkpointing í•¨ê»˜ ì‚¬ìš© ì‹œ í•„ìš”
        if hasattr(config.model, 'lora') and config.model.lora:
            model.enable_input_require_grads()
            if logger:
                logger.write("    Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)")

        if logger:
            logger.write("  âœ… Gradient Checkpointing í™œì„±í™”")

    if logger:
        logger.write("  âœ… Causal LM ë¡œë“œ ì™„ë£Œ")

    return model, tokenizer


# ==================== LLM í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… ==================== #
def format_llm_prompt(dialogue: str, tokenizer) -> str:
    """
    LLMìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…

    Args:
        dialogue: ëŒ€í™” ì›ë¬¸
        tokenizer: í† í¬ë‚˜ì´ì €

    Returns:
        í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸
    """
    # Chat í…œí”Œë¦¿ ì‚¬ìš© (Llama 3.x ìŠ¤íƒ€ì¼)
    messages = [
        {
            "role": "system",
            "content": "You are an expert in dialogue summarization. Summarize the given dialogue concisely and accurately in Korean."
        },
        {
            "role": "user",
            "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{dialogue}\n\nìš”ì•½:"
        }
    ]

    # Chat í…œí”Œë¦¿ ì ìš©
    if hasattr(tokenizer, 'apply_chat_template'):
        try:
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # Fallback: ìˆ˜ë™ í¬ë§·íŒ… (Llama 3 ìŠ¤íƒ€ì¼)
            prompt = _manual_format_llama3(messages)
    else:
        # Fallback: ìˆ˜ë™ í¬ë§·íŒ…
        prompt = _manual_format_llama3(messages)

    return prompt


def _manual_format_llama3(messages):
    """
    Llama 3 ìŠ¤íƒ€ì¼ ìˆ˜ë™ í¬ë§·íŒ…

    Args:
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

    Returns:
        í¬ë§·íŒ…ëœ í”„ë¡¬í”„íŠ¸
    """
    prompt = "<|begin_of_text|>"

    for message in messages:
        role = message['role']
        content = message['content']

        prompt += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"

    # Assistant í„´ ì‹œì‘
    prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"

    return prompt
