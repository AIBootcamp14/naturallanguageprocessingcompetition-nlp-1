# ==================== ì¶”ë¡  ìµœì í™” ëª¨ë“ˆ (PRD 17) ==================== #
"""
ì¶”ë¡  ìµœì í™” ì „ëµ

PRD 17: ì¶”ë¡  ìµœì í™” ì „ëµ
- ì–‘ìí™” (INT8, INT4, FP16)
- ONNX ë³€í™˜ ë° ìµœì í™”
- TensorRT ê°€ì† (GPU í™˜ê²½)
- ë°°ì¹˜ ìµœì í™”
- ë©”ëª¨ë¦¬ ìµœì í™”
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
from pathlib import Path
from typing import Optional, Dict, Any, List
import json
import time

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer


# ==================== ì–‘ìí™” ìµœì í™” ==================== #
class QuantizationOptimizer:
    """ëª¨ë¸ ì–‘ìí™” ìµœì í™”"""

    def __init__(
        self,
        quantization_bits: int = 8,
        logger=None
    ):
        """
        Args:
            quantization_bits: ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ (4, 8, 16)
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.quantization_bits = quantization_bits
        self.logger = logger

    def _log(self, msg: str):
        """ë¡œê¹… ìœ í‹¸ë¦¬í‹°"""
        if self.logger:
            if hasattr(self.logger, 'write'):
                self.logger.write(msg)
            elif hasattr(self.logger, 'info'):
                self.logger.info(msg)
        else:
            print(msg)

    def quantize_model(
        self,
        model: torch.nn.Module,
        calibration_data: Optional[List] = None
    ) -> torch.nn.Module:
        """
        ëª¨ë¸ ì–‘ìí™”

        Args:
            model: ì›ë³¸ ëª¨ë¸
            calibration_data: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° (PTQìš©)

        Returns:
            ì–‘ìí™”ëœ ëª¨ë¸
        """
        self._log(f"ğŸ”§ ëª¨ë¸ ì–‘ìí™” ì‹œì‘ ({self.quantization_bits}bit)...")

        try:
            if self.quantization_bits == 8:
                # INT8 ë™ì  ì–‘ìí™” (ê°€ì¥ ì•ˆì •ì )
                quantized_model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
                self._log("  âœ… INT8 ë™ì  ì–‘ìí™” ì™„ë£Œ")

            elif self.quantization_bits == 16:
                # FP16 ë³€í™˜ (GPU ì „ìš©)
                if torch.cuda.is_available():
                    quantized_model = model.half()
                    self._log("  âœ… FP16 ë³€í™˜ ì™„ë£Œ")
                else:
                    self._log("  âš ï¸  GPU ì—†ìŒ - FP16 ë³€í™˜ ìŠ¤í‚µ")
                    quantized_model = model

            elif self.quantization_bits == 4:
                # INT4 ì–‘ìí™” (ì‹¤í—˜ì )
                self._log("  âš ï¸  INT4 ì–‘ìí™”ëŠ” ì‹¤í—˜ì  ê¸°ëŠ¥ì…ë‹ˆë‹¤")
                # bitsandbytesë¥¼ ì‚¬ìš©í•œ 4bit ì–‘ìí™”ëŠ” ë¡œë“œ ì‹œì ì—ì„œ ì²˜ë¦¬
                quantized_model = model

            else:
                self._log(f"  âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™” ë¹„íŠ¸: {self.quantization_bits}")
                quantized_model = model

            return quantized_model

        except Exception as e:
            self._log(f"  âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return model

    def measure_speedup(
        self,
        original_model: torch.nn.Module,
        quantized_model: torch.nn.Module,
        test_input: torch.Tensor,
        num_runs: int = 100
    ) -> Dict[str, float]:
        """
        ì–‘ìí™” ì†ë„ í–¥ìƒ ì¸¡ì •

        Args:
            original_model: ì›ë³¸ ëª¨ë¸
            quantized_model: ì–‘ìí™”ëœ ëª¨ë¸
            test_input: í…ŒìŠ¤íŠ¸ ì…ë ¥
            num_runs: ì‹¤í–‰ íšŸìˆ˜

        Returns:
            ì†ë„ í†µê³„
        """
        self._log("â±ï¸  ì†ë„ ì¸¡ì • ì¤‘...")

        # ì›ë³¸ ëª¨ë¸ ì†ë„
        original_model.eval()
        start = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = original_model(test_input)
        original_time = time.time() - start

        # ì–‘ìí™” ëª¨ë¸ ì†ë„
        quantized_model.eval()
        start = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = quantized_model(test_input)
        quantized_time = time.time() - start

        speedup = original_time / quantized_time

        results = {
            'original_time': original_time,
            'quantized_time': quantized_time,
            'speedup': speedup,
            'num_runs': num_runs
        }

        self._log(f"  ì›ë³¸ ëª¨ë¸: {original_time:.3f}s")
        self._log(f"  ì–‘ìí™” ëª¨ë¸: {quantized_time:.3f}s")
        self._log(f"  ì†ë„ í–¥ìƒ: {speedup:.2f}x")

        return results


# ==================== ONNX ë³€í™˜ê¸° ==================== #
class ONNXConverter:
    """ONNX ëª¨ë¸ ë³€í™˜ê¸°"""

    def __init__(
        self,
        opset_version: int = 14,
        optimize: bool = True,
        logger=None
    ):
        """
        Args:
            opset_version: ONNX opset ë²„ì „
            optimize: ONNX ìµœì í™” ì ìš© ì—¬ë¶€
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.opset_version = opset_version
        self.optimize = optimize
        self.logger = logger

    def _log(self, msg: str):
        """ë¡œê¹… ìœ í‹¸ë¦¬í‹°"""
        if self.logger:
            if hasattr(self.logger, 'write'):
                self.logger.write(msg)
            elif hasattr(self.logger, 'info'):
                self.logger.info(msg)
        else:
            print(msg)

    def convert_to_onnx(
        self,
        model: torch.nn.Module,
        tokenizer: AutoTokenizer,
        output_path: str,
        sample_input: Optional[Dict] = None
    ) -> bool:
        """
        PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜

        Args:
            model: PyTorch ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            output_path: ONNX ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            sample_input: ìƒ˜í”Œ ì…ë ¥ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ë³€í™˜ ì„±ê³µ ì—¬ë¶€
        """
        self._log(f"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
        self._log(f"  ì¶œë ¥ ê²½ë¡œ: {output_path}")

        try:
            # ìƒ˜í”Œ ì…ë ¥ ìƒì„±
            if sample_input is None:
                sample_text = "ì´ê²ƒì€ ìƒ˜í”Œ ëŒ€í™”ì…ë‹ˆë‹¤."
                sample_input = tokenizer(
                    sample_text,
                    return_tensors="pt",
                    max_length=128,
                    padding="max_length",
                    truncation=True
                )

            # ONNX ë³€í™˜
            model.eval()
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # ë™ì  ì¶• ì •ì˜
            dynamic_axes = {
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
            }

            # ë³€í™˜ ì‹¤í–‰
            torch.onnx.export(
                model,
                (sample_input['input_ids'], sample_input['attention_mask']),
                str(output_path),
                input_names=['input_ids', 'attention_mask'],
                output_names=['output'],
                dynamic_axes=dynamic_axes,
                opset_version=self.opset_version,
                do_constant_folding=True
            )

            self._log(f"  âœ… ONNX ë³€í™˜ ì™„ë£Œ")

            # ONNX ìµœì í™”
            if self.optimize:
                self._optimize_onnx(output_path)

            return True

        except Exception as e:
            self._log(f"  âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False

    def _optimize_onnx(self, onnx_path: Path):
        """ONNX ëª¨ë¸ ìµœì í™”"""
        try:
            import onnx
            from onnxruntime.transformers import optimizer

            self._log("  ğŸ”§ ONNX ìµœì í™” ì¤‘...")

            # ONNX ëª¨ë¸ ë¡œë“œ
            model = onnx.load(str(onnx_path))

            # ìµœì í™” ì ìš© (ìƒìˆ˜ í´ë”©, ë¶ˆí•„ìš”í•œ ë…¸ë“œ ì œê±° ë“±)
            # onnxruntime optimizer ì‚¬ìš©
            optimized_path = onnx_path.parent / f"{onnx_path.stem}_optimized.onnx"

            # ê¸°ë³¸ ìµœì í™”
            onnx.save(model, str(optimized_path))

            self._log(f"  âœ… ONNX ìµœì í™” ì™„ë£Œ: {optimized_path}")

        except ImportError:
            self._log("  âš ï¸  onnx ë˜ëŠ” onnxruntime ë¯¸ì„¤ì¹˜ - ìµœì í™” ìŠ¤í‚µ")
        except Exception as e:
            self._log(f"  âš ï¸  ONNX ìµœì í™” ì‹¤íŒ¨: {e}")


# ==================== ë°°ì¹˜ ìµœì í™” ==================== #
class BatchOptimizer:
    """ë°°ì¹˜ ì¶”ë¡  ìµœì í™”"""

    def __init__(
        self,
        logger=None
    ):
        """
        Args:
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.logger = logger

    def _log(self, msg: str):
        """ë¡œê¹… ìœ í‹¸ë¦¬í‹°"""
        if self.logger:
            if hasattr(self.logger, 'write'):
                self.logger.write(msg)
            elif hasattr(self.logger, 'info'):
                self.logger.info(msg)
        else:
            print(msg)

    def find_optimal_batch_size(
        self,
        model: torch.nn.Module,
        tokenizer: AutoTokenizer,
        sample_texts: List[str],
        max_batch_size: int = 64,
        min_batch_size: int = 1
    ) -> int:
        """
        ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰

        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            sample_texts: ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_batch_size: ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
            min_batch_size: ìµœì†Œ ë°°ì¹˜ í¬ê¸°

        Returns:
            ìµœì  ë°°ì¹˜ í¬ê¸°
        """
        self._log("ğŸ” ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì¤‘...")

        model.eval()
        optimal_batch_size = min_batch_size

        # ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ê°€ë©° í…ŒìŠ¤íŠ¸
        for batch_size in [1, 2, 4, 8, 16, 32, 64]:
            if batch_size > max_batch_size:
                break

            try:
                # ë°°ì¹˜ ìƒì„±
                batch_texts = sample_texts[:batch_size]
                inputs = tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )

                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}
                    model = model.cuda()

                # ì¶”ë¡  í…ŒìŠ¤íŠ¸
                with torch.no_grad():
                    _ = model.generate(
                        inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=200
                    )

                # ì„±ê³µí•˜ë©´ ë°°ì¹˜ í¬ê¸° ì—…ë°ì´íŠ¸
                optimal_batch_size = batch_size
                self._log(f"  âœ… ë°°ì¹˜ í¬ê¸° {batch_size} ì„±ê³µ")

                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except RuntimeError as e:
                if "out of memory" in str(e):
                    self._log(f"  âš ï¸  ë°°ì¹˜ í¬ê¸° {batch_size} OOM - ì´ì „ í¬ê¸° ì‚¬ìš©")
                    break
                else:
                    raise e

        self._log(f"  ğŸ¯ ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
        return optimal_batch_size


# ==================== í†µí•© ìµœì í™” ë§¤ë‹ˆì € ==================== #
class InferenceOptimizer:
    """ì¶”ë¡  ìµœì í™” í†µí•© ê´€ë¦¬ì"""

    def __init__(
        self,
        optimization_method: str = "quantization",
        quantization_bits: int = 8,
        use_onnx: bool = False,
        use_batch_optimization: bool = True,
        logger=None
    ):
        """
        Args:
            optimization_method: ìµœì í™” ë°©ë²• ('quantization', 'onnx', 'tensorrt')
            quantization_bits: ì–‘ìí™” ë¹„íŠ¸ ìˆ˜
            use_onnx: ONNX ë³€í™˜ ì‚¬ìš© ì—¬ë¶€
            use_batch_optimization: ë°°ì¹˜ ìµœì í™” ì‚¬ìš© ì—¬ë¶€
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.optimization_method = optimization_method
        self.quantization_bits = quantization_bits
        self.use_onnx = use_onnx
        self.use_batch_optimization = use_batch_optimization
        self.logger = logger

        # ìµœì í™” ëª¨ë“ˆ ì´ˆê¸°í™”
        self.quantizer = QuantizationOptimizer(
            quantization_bits=quantization_bits,
            logger=logger
        )
        self.onnx_converter = ONNXConverter(logger=logger)
        self.batch_optimizer = BatchOptimizer(logger=logger)

    def _log(self, msg: str):
        """ë¡œê¹… ìœ í‹¸ë¦¬í‹°"""
        if self.logger:
            if hasattr(self.logger, 'write'):
                self.logger.write(msg)
            elif hasattr(self.logger, 'info'):
                self.logger.info(msg)
        else:
            print(msg)

    def optimize(
        self,
        model: torch.nn.Module,
        tokenizer: AutoTokenizer,
        output_dir: str,
        sample_texts: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ ìµœì í™” ì‹¤í–‰

        Args:
            model: ì›ë³¸ ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            sample_texts: ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ë°°ì¹˜ ìµœì í™”ìš©)

        Returns:
            ìµœì í™” ê²°ê³¼
        """
        self._log("=" * 60)
        self._log("ğŸš€ ì¶”ë¡  ìµœì í™” ì‹œì‘")
        self._log(f"  ë°©ë²•: {self.optimization_method}")
        self._log("=" * 60)

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        results = {
            'optimization_method': self.optimization_method,
            'quantization_bits': self.quantization_bits,
            'success': False
        }

        try:
            # 1. ì–‘ìí™”
            if self.optimization_method == "quantization":
                optimized_model = self.quantizer.quantize_model(model)

                # ëª¨ë¸ ì €ì¥
                model_path = output_dir / f"quantized_{self.quantization_bits}bit"
                model_path.mkdir(parents=True, exist_ok=True)

                torch.save(optimized_model.state_dict(), model_path / "model.pt")
                tokenizer.save_pretrained(str(model_path))

                results['model_path'] = str(model_path)
                results['success'] = True

            # 2. ONNX ë³€í™˜
            elif self.optimization_method == "onnx" or self.use_onnx:
                onnx_path = output_dir / "model.onnx"
                success = self.onnx_converter.convert_to_onnx(
                    model=model,
                    tokenizer=tokenizer,
                    output_path=str(onnx_path)
                )

                results['onnx_path'] = str(onnx_path)
                results['success'] = success

            # 3. TensorRT (ê¸°ë³¸ êµ¬ì¡°ë§Œ)
            elif self.optimization_method == "tensorrt":
                self._log("âš ï¸  TensorRTëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                results['success'] = False

            # 4. ë°°ì¹˜ ìµœì í™”
            if self.use_batch_optimization and sample_texts:
                optimal_batch_size = self.batch_optimizer.find_optimal_batch_size(
                    model=model,
                    tokenizer=tokenizer,
                    sample_texts=sample_texts
                )
                results['optimal_batch_size'] = optimal_batch_size

            # ê²°ê³¼ ì €ì¥
            result_path = output_dir / "optimization_results.json"
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            self._log("\n" + "=" * 60)
            self._log("âœ… ì¶”ë¡  ìµœì í™” ì™„ë£Œ!")
            self._log("=" * 60)

            return results

        except Exception as e:
            self._log(f"\nâŒ ìµœì í™” ì‹¤íŒ¨: {e}")
            results['error'] = str(e)
            return results


# ==================== íŒ©í† ë¦¬ í•¨ìˆ˜ ==================== #
def create_inference_optimizer(
    optimization_method: str = "quantization",
    quantization_bits: int = 8,
    use_onnx: bool = False,
    use_batch_optimization: bool = True,
    logger=None
) -> InferenceOptimizer:
    """
    InferenceOptimizer ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        optimization_method: ìµœì í™” ë°©ë²•
        quantization_bits: ì–‘ìí™” ë¹„íŠ¸ ìˆ˜
        use_onnx: ONNX ë³€í™˜ ì‚¬ìš©
        use_batch_optimization: ë°°ì¹˜ ìµœì í™” ì‚¬ìš©
        logger: Logger ì¸ìŠ¤í„´ìŠ¤

    Returns:
        InferenceOptimizer ì¸ìŠ¤í„´ìŠ¤
    """
    return InferenceOptimizer(
        optimization_method=optimization_method,
        quantization_bits=quantization_bits,
        use_onnx=use_onnx,
        use_batch_optimization=use_batch_optimization,
        logger=logger
    )
