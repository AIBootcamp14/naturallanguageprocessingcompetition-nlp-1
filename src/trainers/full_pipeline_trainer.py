# ==================== FullPipelineTrainer ==================== #
"""
í’€ íŒŒì´í”„ë¼ì¸ Trainer

PRD 14: ë‹¤ì¤‘ ì„ íƒ ì˜µì…˜ - Full ì‹¤í–‰
í†µí•© ìš´ì˜ ê°€ëŠ¥í•œ ì£¼ìš” íŒŒì´í”„ë¼ì¸:
- ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
- Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒ)
- K-Fold êµì°¨ê²€ì¦
- TTA (Test Time Augmentation)
- Solar API í†µí•©
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import json
from pathlib import Path
from typing import List, Dict, Any

# ---------------------- ë¡œì»¬ ëª¨ë“ˆ ---------------------- #
from src.trainers.base_trainer import BaseTrainer
from src.config import load_model_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
from src.ensemble import ModelManager
from src.api import create_solar_api


# ==================== FullPipelineTrainer ==================== #
class FullPipelineTrainer(BaseTrainer):
    """í’€ íŒŒì´í”„ë¼ì¸ Trainer"""

    def train(self):
        """
        í’€ íŒŒì´í”„ë¼ì¸ í•™ìŠµ

        Returns:
            dict: í•™ìŠµ ê²°ê³¼
                - mode: 'full'
                - models: ì‚¬ìš©ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
                - ensemble_results: ì•™ìƒë¸” ê²°ê³¼
                - solar_results: Solar API ê²°ê³¼ (ì„ íƒ)
                - inference_results: ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ê²°ê³¼
                - final_metrics: ìµœì¢… í‰ê°€ ì§€í‘œ
        """
        self.log("=" * 60)
        self.log("= FULL PIPELINE ì‹¤í–‰ ì‹œì‘")
        self.log(f"=ëŒ€ìƒ ëª¨ë¸: {', '.join(self.args.models)}")
        self.log(f"=ì•™ìƒë¸” ì•™ìƒë¸” ì „ëµ: {self.args.ensemble_strategy}")
        self.log(f"= TTA ì‚¬ìš©: {self.args.use_tta}")
        self.log("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        self.log("\n[1/6] ë°ì´í„° ë¡œë”©...")
        train_df, eval_df = self.load_data()

        # 2. ëª¨ë¸ í•™ìŠµ (ë‹¤ì¤‘ ëª¨ë¸)
        self.log(f"\n[2/6] ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ({len(self.args.models)} ëª¨ë¸)...")
        model_results, model_paths = self._train_multiple_models(train_df, eval_df)

        # 3. ì•™ìƒë¸” ìƒì„±
        self.log(f"\n[3/6] ì•™ìƒë¸” ìƒì„±...")
        ensemble_results = self._create_and_evaluate_ensemble(
            model_paths=model_paths,
            eval_df=eval_df
        )

        # 4. Solar API í†µí•© (ì„ íƒ)
        solar_results = {}
        try:
            self.log(f"\n[4/6] Solar API í†µí•©...")
            solar_results = self._integrate_solar_api(eval_df)
        except Exception as e:
            self.log(f"    Solar API í†µí•© ì˜¤ë¥˜: {e}")

        # 5. TTA ì ìš© (ì„ íƒ)
        tta_results = {}
        if self.args.use_tta:
            self.log(f"\n[5/6] TTA ì ìš©...")
            tta_results = self._apply_tta(model_paths, eval_df)

        # 6. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
        self.log(f"\n[6/6] ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±...")
        inference_results = self._create_submission(model_paths)

        # ê²°ê³¼ ìˆ˜ì§‘
        results = {
            'mode': 'full',
            'models': self.args.models,
            'ensemble_strategy': self.args.ensemble_strategy,
            'use_tta': self.args.use_tta,
            'model_results': model_results,
            'ensemble_results': ensemble_results,
            'solar_results': solar_results,
            'tta_results': tta_results,
            'inference_results': inference_results
        }

        self.log("\n" + "=" * 60)
        self.log(" FULL PIPELINE ì™„ë£Œ!")

        self.log("\n=ìš”ì•½ ê°œë³„ ëª¨ë¸ ê²°ê³¼:")
        for result in model_results:
            self.log(f"\n{result['model_name']}:")
            if result['eval_metrics']:
                for key, value in result['eval_metrics'].items():
                    if 'rouge' in key.lower():
                        self.log(f"  {key}: {value:.4f}")

        self.log("\n=ìš”ì•½ ì•™ìƒë¸” ê²°ê³¼:")
        if ensemble_results:
            for key, value in ensemble_results.items():
                self.log(f"  {key}: {value:.4f}")

        if solar_results:
            self.log("\n=ìš”ì•½ Solar API ê²°ê³¼:")
            for key, value in solar_results.items():
                if isinstance(value, (int, float)):
                    self.log(f"  {key}: {value:.4f}")

        self.log("=" * 60)

        return results

    def save_results(self, results):
        """
        ê²°ê³¼ ì €ì¥

        Args:
            results: í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result_path = self.output_dir / "full_pipeline_results.json"

        # ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        saveable_results = {
            'mode': results['mode'],
            'models': results['models'],
            'ensemble_strategy': results['ensemble_strategy'],
            'use_tta': results['use_tta'],
            'model_results': results['model_results'],
            'ensemble_results': results.get('ensemble_results', {}),
            'solar_results': results.get('solar_results', {}),
            'tta_results': results.get('tta_results', {}),
            'inference_results': results.get('inference_results', {})
        }

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(saveable_results, f, indent=2, ensure_ascii=False)

        self.log(f"\n=ì €ì¥ ê²°ê³¼ ì €ì¥: {result_path}")

        # ì œì¶œ íŒŒì¼ ê²½ë¡œ ì¶œë ¥
        inference_results = results.get('inference_results', {})
        if inference_results.get('submission_path'):
            self.log(f"=ì €ì¥ ì œì¶œ íŒŒì¼: {inference_results['submission_path']}")

    def _train_multiple_models(self, train_df, eval_df):
        """
        ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ

        Args:
            train_df: í•™ìŠµ ë°ì´í„°
            eval_df: í‰ê°€ ë°ì´í„°

        Returns:
            tuple: (model_results, model_paths)
        """
        model_results = []
        model_paths = []
        failed_models = []

        for idx, model_name in enumerate(self.args.models):
            self.log(f"\n{'='*50}")
            self.log(f"ëª¨ë¸ {idx+1}/{len(self.args.models)}: {model_name}")
            self.log(f"{'='*50}")

            try:
                # Config ë¡œë“œ
                config = load_model_config(model_name)
                self._override_config(config)

                # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
                model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)

                # Dataset ìƒì„±
                model_type = config.model.get('type', 'encoder_decoder')

                train_dataset = DialogueSummarizationDataset(
                    dialogues=train_df['dialogue'].tolist(),
                    summaries=train_df['summary'].tolist(),
                    tokenizer=tokenizer,
                    encoder_max_len=config.tokenizer.encoder_max_len,
                    decoder_max_len=config.tokenizer.decoder_max_len,
                    preprocess=True,
                    model_type=model_type
                )

                eval_dataset = DialogueSummarizationDataset(
                    dialogues=eval_df['dialogue'].tolist(),
                    summaries=eval_df['summary'].tolist(),
                    tokenizer=tokenizer,
                    encoder_max_len=config.tokenizer.encoder_max_len,
                    decoder_max_len=config.tokenizer.decoder_max_len,
                    preprocess=True,
                    model_type=model_type
                )

                # Trainer ìƒì„± ë° í•™ìŠµ
                model_output_dir = self.output_dir / f"model_{idx}_{model_name.replace('-', '_')}"
                model_output_dir.mkdir(parents=True, exist_ok=True)

                config.training.output_dir = str(model_output_dir)

                trainer = create_trainer(
                    config=config,
                    model=model,
                    tokenizer=tokenizer,
                    train_dataset=train_dataset,
                    eval_dataset=eval_dataset,
                    use_wandb=getattr(self.args, 'use_wandb', True),
                    logger=self.logger
                )

                # í•™ìŠµ ìˆ˜í–‰
                train_result = trainer.train()

                # Get model path from training result (model already saved by train())
                final_model_path = train_result.get('final_model_path', str(model_output_dir / 'final_model'))
                model_paths.append(final_model_path)

                # Get evaluation metrics from training result
                eval_metrics = train_result.get('eval_metrics', {})
                model_results.append({
                    'model_name': model_name,
                    'model_path': final_model_path,
                    'eval_metrics': eval_metrics,
                    'status': 'success'
                })

                self.log(f"\nâœ… {model_name} í•™ìŠµ ì™„ë£Œ")

            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡œê¹…í•˜ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
                error_msg = f"âŒ {model_name} í•™ìŠµ ì‹¤íŒ¨: {type(e).__name__}: {str(e)}"
                self.log(f"\n{error_msg}")

                # ì˜¤ë¥˜ ìƒì„¸ ë¡œê·¸ ì €ì¥
                import traceback
                error_log_dir = self.output_dir / "errors"
                error_log_dir.mkdir(parents=True, exist_ok=True)
                error_log_path = error_log_dir / f"{model_name}_error.log"

                with open(error_log_path, 'w', encoding='utf-8') as f:
                    f.write(f"ëª¨ë¸: {model_name}\n")
                    f.write(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}\n")
                    f.write(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}\n\n")
                    f.write("ìƒì„¸ íŠ¸ë ˆì´ìŠ¤ë°±:\n")
                    f.write(traceback.format_exc())

                self.log(f"  ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥: {error_log_path}")

                # ì‹¤íŒ¨ ëª¨ë¸ ê¸°ë¡
                failed_models.append({
                    'model_name': model_name,
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'status': 'failed'
                })

                model_results.append({
                    'model_name': model_name,
                    'model_path': None,
                    'eval_metrics': {},
                    'status': 'failed',
                    'error': str(e)
                })

                # ë‹¤ìŒ ëª¨ë¸ë¡œ ê³„ì†
                continue

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        self.log(f"\n{'='*50}")
        self.log("ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ìš”ì•½")
        self.log(f"{'='*50}")
        success_count = sum(1 for r in model_results if r.get('status') == 'success')
        failed_count = len(model_results) - success_count
        self.log(f"âœ… ì„±ê³µ: {success_count}/{len(self.args.models)} ëª¨ë¸")
        self.log(f"âŒ ì‹¤íŒ¨: {failed_count}/{len(self.args.models)} ëª¨ë¸")

        if failed_models:
            self.log("\nì‹¤íŒ¨í•œ ëª¨ë¸ ëª©ë¡:")
            for failed in failed_models:
                self.log(f"  - {failed['model_name']}: {failed['error_type']}")

        return model_results, model_paths

    def _create_and_evaluate_ensemble(self, model_paths, eval_df):
        """
        ì•™ìƒë¸” ìƒì„± ë° í‰ê°€

        Args:
            model_paths: ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            eval_df: í‰ê°€ ë°ì´í„°

        Returns:
            dict: ì•™ìƒë¸” í‰ê°€ ì§€í‘œ
        """
        try:
            from transformers import (
                AutoConfig,
                AutoModelForSeq2SeqLM,
                AutoModelForCausalLM,
                AutoTokenizer
            )
            from rouge import Rouge
            import torch

            # ëª¨ë¸ ë¡œë“œ
            models = []
            tokenizers = []

            for model_path in model_paths:
                # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
                config = AutoConfig.from_pretrained(model_path)
                is_encoder_decoder = config.is_encoder_decoder if hasattr(config, 'is_encoder_decoder') else False

                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í´ë˜ìŠ¤ ì‚¬ìš©
                if is_encoder_decoder:
                    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
                else:
                    model = AutoModelForCausalLM.from_pretrained(model_path)

                tokenizer = AutoTokenizer.from_pretrained(model_path)

                if torch.cuda.is_available():
                    model = model.cuda()
                model.eval()

                models.append(model)
                tokenizers.append(tokenizer)

            # ì•™ìƒë¸” ìƒì„±
            manager = ModelManager(logger=self.logger)
            manager.models = models
            manager.tokenizers = tokenizers
            manager.model_names = self.args.models

            if self.args.ensemble_strategy in ['weighted_avg', 'rouge_weighted']:
                ensemble = manager.create_ensemble(
                    ensemble_type='weighted',
                    weights=self.args.ensemble_weights
                )
            else:
                ensemble = manager.create_ensemble(
                    ensemble_type='voting',
                    voting='hard'
                )

            # ìƒ˜í”Œ ì¶”ì¶œ
            dialogues = eval_df['dialogue'].tolist()[:100]
            references = eval_df['summary'].tolist()[:100]

            predictions = ensemble.predict(
                dialogues=dialogues,
                max_new_tokens=200,
                min_new_tokens=30,
                num_beams=4,
                batch_size=8
            )

            # ROUGE ê³„ì‚°
            rouge = Rouge()
            scores = rouge.get_scores(predictions, references, avg=True)

            return {
                'ensemble_rouge_1_f1': scores['rouge-1']['f'],
                'ensemble_rouge_2_f1': scores['rouge-2']['f'],
                'ensemble_rouge_l_f1': scores['rouge-l']['f'],
            }

        except Exception as e:
            self.log(f"    ì•™ìƒë¸” í‰ê°€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

    def _integrate_solar_api(self, eval_df):
        """
        Solar API í†µí•©

        Args:
            eval_df: í‰ê°€ ë°ì´í„°

        Returns:
            dict: Solar API í‰ê°€ ê²°ê³¼
        """
        try:
            # Solar API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            solar_client = create_solar_api(use_cache=True)

            # ì†ŒëŸ‰ ìƒ˜í”Œ ì¶”ì¶œ
            dialogues = eval_df['dialogue'].tolist()[:50]
            references = eval_df['summary'].tolist()[:50]

            self.log(f"  Solar API processing {len(dialogues)} samples ...")

            predictions = solar_client.batch_generate(
                dialogues=dialogues,
                batch_size=10,
                use_few_shot=True,
                preprocess=True
            )

            # ROUGE ê³„ì‚°
            from rouge import Rouge
            rouge = Rouge()
            scores = rouge.get_scores(predictions, references, avg=True)

            return {
                'solar_rouge_1_f1': scores['rouge-1']['f'],
                'solar_rouge_2_f1': scores['rouge-2']['f'],
                'solar_rouge_l_f1': scores['rouge-l']['f'],
                'n_samples': len(predictions)
            }

        except Exception as e:
            self.log(f"    Solar API í†µí•© ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

    def _apply_tta(self, model_paths, eval_df):
        """
        TTA (Test Time Augmentation) ì ìš©

        Args:
            model_paths: ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            eval_df: í‰ê°€ ë°ì´í„°

        Returns:
            dict: TTA ê²°ê³¼
        """
        try:
            self.log(f"  TTA ì „ëµ: {', '.join(self.args.tta_strategies)}")
            self.log(f"  ì¦ê°• íšŸìˆ˜: {self.args.tta_num_aug}")

            # TTA ê¸°ëŠ¥ êµ¬í˜„ ì˜ˆì •
            self.log("    TTA ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤.")

            return {
                'tta_applied': False,
                'strategies': self.args.tta_strategies,
                'num_aug': self.args.tta_num_aug
            }

        except Exception as e:
            self.log(f"    TTA ì ìš© ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

    def _create_submission(self, model_paths):
        """
        ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±

        Args:
            model_paths: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            dict: ì¶”ë¡  ê²°ê³¼
                - submission_path: ì œì¶œ íŒŒì¼ ê²½ë¡œ
                - num_predictions: ì˜ˆì¸¡ ê°œìˆ˜
                - best_model_used: ì‚¬ìš©ëœ ìµœì  ëª¨ë¸
        """
        # ì¶”ë¡  ë¡œê·¸ íŒŒì¼ ìƒì„±
        from src.logging.logger import Logger
        inference_log_path = self.output_dir / "inference.log"
        inference_logger = Logger(inference_log_path, print_also=False)
        inference_logger.start_redirect()

        try:
            import pandas as pd
            from transformers import (
                AutoConfig,
                AutoModelForSeq2SeqLM,
                AutoModelForCausalLM,
                AutoTokenizer
            )
            import torch

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            test_data_path = getattr(self.args, 'test_data', 'data/raw/test.csv')
            self.log(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {test_data_path}")
            inference_logger.write(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {test_data_path}")
            test_df = pd.read_csv(test_data_path)
            self.log(f"  í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_df)}")
            inference_logger.write(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(test_df)}")

            # ì„±ê³µí•œ ëª¨ë¸ ì¤‘ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš© (ê°€ì¥ ë¨¼ì € í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸)
            if not model_paths:
                self.log("    âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                inference_logger.write("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                inference_logger.stop_redirect()
                inference_logger.close()
                return {
                    'submission_path': None,
                    'num_predictions': 0,
                    'error': 'ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ'
                }

            best_model_path = model_paths[0]
            self.log(f"  ì‚¬ìš© ëª¨ë¸: {best_model_path}")
            inference_logger.write(f"ì‚¬ìš© ëª¨ë¸: {best_model_path}")

            # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
            config = AutoConfig.from_pretrained(best_model_path)
            is_encoder_decoder = config.is_encoder_decoder if hasattr(config, 'is_encoder_decoder') else False

            # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
            if is_encoder_decoder:
                self.log(f"  ëª¨ë¸ íƒ€ì…: Encoder-Decoder (Seq2Seq)")
                inference_logger.write(f"ëª¨ë¸ íƒ€ì…: Encoder-Decoder (Seq2Seq)")
                model = AutoModelForSeq2SeqLM.from_pretrained(best_model_path)
            else:
                self.log(f"  ëª¨ë¸ íƒ€ì…: Decoder-only (Causal LM)")
                inference_logger.write(f"ëª¨ë¸ íƒ€ì…: Decoder-only (Causal LM)")
                model = AutoModelForCausalLM.from_pretrained(best_model_path)

            tokenizer = AutoTokenizer.from_pretrained(best_model_path)

            if torch.cuda.is_available():
                model = model.cuda()
            model.eval()

            inference_logger.write(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            inference_logger.write(f"ë””ë°”ì´ìŠ¤: {'cuda' if torch.cuda.is_available() else 'cpu'}")
            inference_logger.write(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")

            # ë°°ì¹˜ ì¶”ë¡ 
            predictions = []
            batch_size = getattr(self.args, 'inference_batch_size', 32)
            self.log(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
            inference_logger.write(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
            inference_logger.write(f"\në°°ì¹˜ ì¶”ë¡  ì‹œì‘...")

            dialogues = test_df['dialogue'].tolist()

            for i in range(0, len(dialogues), batch_size):
                batch_dialogues = dialogues[i:i+batch_size]

                # í† í¬ë‚˜ì´ì§•
                inputs = tokenizer(
                    batch_dialogues,
                    max_length=512,
                    padding=True,
                    truncation=True,
                    return_tensors='pt'
                )

                # token_type_ids ì œê±° (Causal LMê³¼ ì¼ë¶€ Seq2Seq ëª¨ë¸ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                if 'token_type_ids' in inputs:
                    del inputs['token_type_ids']

                if torch.cuda.is_available():
                    inputs = {k: v.cuda() for k, v in inputs.items()}

                # ìƒì„±
                with torch.no_grad():
                    # Causal LMì€ max_new_tokens ì‚¬ìš©, Seq2SeqëŠ” max_length ì‚¬ìš©
                    if is_encoder_decoder:
                        # For encoder-decoder models prefer max_new_tokens to
                        # control the number of generated tokens directly and
                        # avoid accidental truncation of outputs.
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=getattr(self.args, 'max_new_tokens', 200),
                            min_new_tokens=getattr(self.args, 'min_new_tokens', 30),
                            num_beams=getattr(self.args, 'num_beams', 5),
                            early_stopping=True,
                            no_repeat_ngram_size=getattr(self.args, 'no_repeat_ngram_size', 3),
                            length_penalty=getattr(self.args, 'length_penalty', 1.0),
                            repetition_penalty=getattr(self.args, 'repetition_penalty', 1.2),
                            do_sample=False
                        )
                    else:
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=getattr(self.args, 'max_new_tokens', 200),
                            min_new_tokens=getattr(self.args, 'min_new_tokens', 30),
                            num_beams=getattr(self.args, 'num_beams', 5),
                            early_stopping=True,
                            no_repeat_ngram_size=getattr(self.args, 'no_repeat_ngram_size', 3),
                            repetition_penalty=getattr(self.args, 'repetition_penalty', 1.2)
                        )

                # ë””ì½”ë”©
                batch_predictions = tokenizer.batch_decode(
                    outputs,
                    skip_special_tokens=True
                )

                # í›„ì²˜ë¦¬: ë¶ˆì™„ì „ í† í° ì œê±° ë° ë¬¸ì¥ ì¢…ê²° ë³´ì¥
                def postprocess_summary(text):
                    """ìš”ì•½ë¬¸ í›„ì²˜ë¦¬"""
                    import re
                    text = text.strip()

                    # 1. ë¶ˆì™„ì „í•œ í”Œë ˆì´ìŠ¤í™€ë” ì œê±° (#P, #Pe, #Person ë“±)
                    text = re.sub(r'\s+#[A-Za-zê°€-í£]{0,10}$', '', text)

                    # 2. ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œê±° (1~3ì, ë‹¨ ë¬¸ì¥ë¶€í˜¸ë¡œ ëë‚˜ë©´ ì œì™¸)
                    parts = text.rsplit(' ', 1)
                    if len(parts) == 2 and len(parts[1]) <= 3 and not parts[1].endswith(('.', '!', '?', 'ã€‚', 'ï¼Ÿ', 'ï¼')):
                        text = parts[0]

                    # 3. ë¬¸ì¥ ì¢…ê²° ë³´ì¥
                    text = text.strip()
                    if text and text[-1] not in '.!?ã€‚ï¼Ÿï¼':
                        text += '.'

                    return text

                batch_predictions = [postprocess_summary(pred) for pred in batch_predictions]
                predictions.extend(batch_predictions)

                if (i // batch_size + 1) % 10 == 0:
                    self.log(f"    ì§„í–‰: {i+len(batch_predictions)}/{len(dialogues)}")
                    inference_logger.write(f"ì§„í–‰: {i+len(batch_predictions)}/{len(dialogues)}")

            # ì œì¶œ íŒŒì¼ ìƒì„± (fname ì»¬ëŸ¼ ì‚¬ìš©)
            fname_column = test_df['fname']
            submission_df = pd.DataFrame({
                'fname': fname_column,
                'summary': predictions
            })

            # íŒŒì¼ëª… ìƒì„± (ì‹¤í–‰ë˜ëŠ” í´ë”ëª…ê³¼ ë™ì¼í•˜ê²Œ)
            # output_dir ì˜ˆì‹œ: experiments/20251012/20251012_101219_test_full_pipeline_quick
            folder_name = self.output_dir.name  # ì˜ˆ: 20251012_101219_test_full_pipeline_quick

            # 1. experiments/{ë‚ ì§œ}/{ì‹¤í–‰í´ë”ëª…}/submissions/{ì‹¤í–‰í´ë”ëª…}.csv ì €ì¥
            submission_dir = self.output_dir / "submissions"
            submission_dir.mkdir(parents=True, exist_ok=True)
            submission_path_1 = submission_dir / f"{folder_name}.csv"
            submission_df.to_csv(submission_path_1, index=False, encoding='utf-8')
            self.log(f"  âœ… ì œì¶œ íŒŒì¼ ì €ì¥ (1): {submission_path_1}")
            inference_logger.write(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ (1): {submission_path_1}")

            # 2. submissions/{ë‚ ì§œ}/{ì‹¤í–‰í´ë”ëª…}.csv ì €ì¥
            from pathlib import Path
            import datetime
            date_str = datetime.datetime.now().strftime('%Y%m%d')
            global_submission_dir = Path('submissions') / date_str
            global_submission_dir.mkdir(parents=True, exist_ok=True)
            submission_path_2 = global_submission_dir / f"{folder_name}.csv"
            submission_df.to_csv(submission_path_2, index=False, encoding='utf-8')
            self.log(f"  âœ… ì œì¶œ íŒŒì¼ ì €ì¥ (2): {submission_path_2}")
            inference_logger.write(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ (2): {submission_path_2}")

            self.log(f"  ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions)}")
            inference_logger.write(f"ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions)}")
            inference_logger.write(f"\nğŸ‰ ì¶”ë¡  ì™„ë£Œ!")

            # ì¶”ë¡  ë¡œê±° ì •ë¦¬
            inference_logger.stop_redirect()
            inference_logger.close()

            return {
                'submission_path': str(submission_path_1),
                'submission_path_2': str(submission_path_2),
                'num_predictions': len(predictions),
                'best_model_used': best_model_path
            }

        except Exception as e:
            import traceback
            self.log(f"    âŒ ì¶”ë¡  ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.log(f"    ìƒì„¸: {traceback.format_exc()}")
            inference_logger.write(f"âŒ ì¶”ë¡  ì˜¤ë¥˜ ë°œìƒ: {e}")
            inference_logger.write(f"ìƒì„¸: {traceback.format_exc()}")

            # ì¶”ë¡  ë¡œê±° ì •ë¦¬
            inference_logger.stop_redirect()
            inference_logger.close()

            return {
                'submission_path': None,
                'num_predictions': 0,
                'error': str(e)
            }

    def _extract_eval_metrics(self, log_history):
        """í‰ê°€ ì§€í‘œ ì¶”ì¶œ"""
        eval_metrics = {}

        for log_entry in reversed(log_history):
            if 'eval_loss' in log_entry:
                for key, value in log_entry.items():
                    if key.startswith('eval_'):
                        eval_metrics[key] = value
                break

        return eval_metrics


# ==================== í¸ì˜ í•¨ìˆ˜ ==================== #
def create_full_pipeline_trainer(args, logger, wandb_logger=None):
    """
    FullPipelineTrainer ìƒì„± í¸ì˜ í•¨ìˆ˜

    Args:
        args: ëª…ë ¹í–‰ ì¸ì
        logger: Logger ì¸ìŠ¤í„´ìŠ¤
        wandb_logger: WandB Logger (ì„ íƒ ì‚¬í•­)

    Returns:
        FullPipelineTrainer ì¸ìŠ¤í„´ìŠ¤
    """
    return FullPipelineTrainer(args, logger, wandb_logger)
