# ==================== MultiModelEnsembleTrainer ==================== #
"""
ÃƒÂƒÃ‚Â¤ ÃƒÂ‚Ã‚Â¨x YÃƒÂƒÃ‚Â Trainer

PRD 12: ÃƒÂƒÃ‚Â¤ ÃƒÂ‚Ã‚Â¨x YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Âµ l
ÃƒÂƒÃ‚Â¬ÃƒÂƒÃ‚Â¬ ÃƒÂ‚Ã‚Â¨xD YÃƒÂ‚Ã‚ÂµXÃƒÂƒÃ‚Â  YÃƒÂƒÃ‚Â\ ÃƒÂ‚Ã‚Â°iXÃƒÂƒÃ‚Â¬ \ÃƒÂ‚Ã‚Â… ! ÃƒÂ‚Ã‚Â‰
"""

# ---------------------- \ |tÃƒÂƒÃ‚Â¬ÃƒÂ‚Ã‚Â¬ ---------------------- #
import json
from pathlib import Path
from typing import List, Dict, Any

# ---------------------- \ÃƒÂ‚Ã‚Â¸ ÃƒÂ‚Ã‚Â¨ÃƒÂƒÃ‚Âˆ ---------------------- #
from src.trainers.base_trainer import BaseTrainer
from src.config import load_model_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
from src.ensemble import ModelManager


# ==================== MultiModelEnsembleTrainer ==================== #
class MultiModelEnsembleTrainer(BaseTrainer):
    """ÃƒÂƒÃ‚Â¤ ÃƒÂ‚Ã‚Â¨x YÃƒÂƒÃ‚Â Trainer"""

    def train(self):
        """
        ÃƒÂƒÃ‚Â¤ ÃƒÂ‚Ã‚Â¨x YÃƒÂ‚Ã‚Âµ  YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â¤ÃƒÂ‚Ã‚Â‰

        Returns:
            dict: YÃƒÂ‚Ã‚Âµ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼
                - mode: 'multi_model'
                - models: ÃƒÂ‚Ã‚Â¨x ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â¤ÃƒÂ‚Ã‚Â¸
                - results:  ÃƒÂ‚Ã‚Â¨xÃƒÂƒÃ‚Â„ YÃƒÂ‚Ã‚Âµ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼
                - ensemble_strategy: YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Âµ
                - eval_metrics: YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼
        """
        self.log("=" * 60)
        self.log("=ÃƒÂ‚Ã‚Â€ MULTI MODEL ENSEMBLE ÃƒÂ‚Ã‚Â¨ÃƒÂƒÃ‚Âœ YÃƒÂ‚Ã‚Âµ ÃƒÂƒÃ‚ÂœÃƒÂ‚Ã‚Â‘")
        self.log(f"=ÃƒÂƒÃ‚Â‹ ÃƒÂ‚Ã‚Â¨x: {', '.join(self.args.models)}")
        self.log(f"=' YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Âµ: {self.args.ensemble_strategy}")
        self.log("=" * 60)

        # 1. pt0 \ÃƒÂƒÃ‚Âœ
        self.log("\n[1/4] pt0 \)...")
        train_df, eval_df = self.load_data()

        # 2.  ÃƒÂ‚Ã‚Â¨x YÃƒÂ‚Ã‚Âµ
        self.log(f"\n[2/4] ÃƒÂ‚Ã‚Â¨x YÃƒÂ‚Ã‚Âµ ({len(self.args.models)} ÃƒÂ‚Ã‚Â¨x)...")
        model_results = []
        model_paths = []

        for idx, model_name in enumerate(self.args.models):
            self.log(f"\n{'='*50}")
            self.log(f"ÃƒÂ‚Ã‚Â¨x {idx+1}/{len(self.args.models)}: {model_name}")
            self.log(f"{'='*50}")

            # Config \ÃƒÂƒÃ‚Âœ
            config = load_model_config(model_name)
            self._override_config(config)

            # ÃƒÂ‚Ã‚Â¨x  ÃƒÂ‚Ã‚Â lÃƒÂ‚Ã‚Â˜t \ÃƒÂƒÃ‚Âœ
            model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)

            # Dataset ÃƒÂƒÃ‚Â1
            model_type = config.model.get('type', 'encoder_decoder')

            train_dataset = DialogueSummarizationDataset(
                dialogues=train_df['dialogue'].tolist(),
                summaries=train_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            eval_dataset = DialogueSummarizationDataset(
                dialogues=eval_df['dialogue'].tolist(),
                summaries=eval_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            # Trainer ÃƒÂƒÃ‚Â1  YÃƒÂ‚Ã‚Âµ
            model_output_dir = self.output_dir / f"model_{idx}_{model_name.replace('-', '_')}"
            model_output_dir.mkdir(parents=True, exist_ok=True)

            # ConfigÃƒÂƒÃ‚Â output_dir $
            config.training.output_dir = str(model_output_dir)

            trainer = create_trainer(
                config=config,
                model=model,
                tokenizer=tokenizer,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                use_wandb=getattr(self.args, 'use_wandb', True),
                logger=self.logger
            )

            # YÃƒÂ‚Ã‚Âµ ÃƒÂƒÃ‚Â¤ÃƒÂ‚Ã‚Â‰
            train_result = trainer.train()

            # ÃƒÂ‚Ã‚Â¨x ÃƒÂ‚Ã‚Â¥
            final_model_path = model_output_dir / 'final_model'
            trainer.save_model(str(final_model_path))
            model_paths.append(str(final_model_path))

            # ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥
            eval_metrics = self._extract_eval_metrics(trainer.state.log_history)
            model_results.append({
                'model_name': model_name,
                'model_path': str(final_model_path),
                'eval_metrics': eval_metrics
            })

            # FIXME: Corrupted log message
            if eval_metrics:
                for key, value in eval_metrics.items():
                    if 'rouge' in key.lower():
                        self.log(f"  {key}: {value:.4f}")

        # 3. YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰
        self.log(f"\n[3/4] YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰ ...")
        ensemble_metrics = self._evaluate_ensemble(
            model_paths=model_paths,
            eval_df=eval_df,
            strategy=self.args.ensemble_strategy
        )

        # 4. ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂƒÃ‚Â‘
        results = {
            'mode': 'multi_model',
            'models': self.args.models,
            'ensemble_strategy': self.args.ensemble_strategy,
            'model_results': model_results,
            'ensemble_metrics': ensemble_metrics
        }

        self.log("\n" + "=" * 60)
        self.log(" MULTI MODEL ENSEMBLE YÃƒÂ‚Ã‚Âµ DÃƒÂƒÃ‚ÂŒ!")
        self.log("\n=ÃƒÂƒÃ‚ÂŠ ÃƒÂƒÃ‚Â„ ÃƒÂ‚Ã‚Â¨x 1ÃƒÂ‚Ã‚Â¥:")
        for result in model_results:
            self.log(f"\n{result['model_name']}:")
            if result['eval_metrics']:
                for key, value in result['eval_metrics'].items():
                    if 'rouge' in key.lower():
                        self.log(f"  {key}: {value:.4f}")

        self.log("\n=ÃƒÂƒÃ‚ÂŠ YÃƒÂƒÃ‚Â 1ÃƒÂ‚Ã‚Â¥:")
        if ensemble_metrics:
            for key, value in ensemble_metrics.items():
                self.log(f"  {key}: {value:.4f}")

        self.log("=" * 60)

        return results

    def save_results(self, results):
        """
        ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥

        Args:
            results: YÃƒÂ‚Ã‚Âµ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ TÃƒÂ‚Ã‚Â¬
        """
        result_path = self.output_dir / "multi_model_results.json"

        # ÃƒÂ‚Ã‚Â¥ ÃƒÂ‚Ã‚Â¥\ ÃƒÂƒÃ‚Âœ\ ÃƒÂƒÃ‚Â€X
        saveable_results = {
            'mode': results['mode'],
            'models': results['models'],
            'ensemble_strategy': results['ensemble_strategy'],
            'model_results': results['model_results'],
            'ensemble_metrics': results.get('ensemble_metrics', {})
        }

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(saveable_results, f, indent=2, ensure_ascii=False)

        self.log(f"\n=ÃƒÂ‚Ã‚Â¾ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥: {result_path}")

    def _override_config(self, config):
        """
        ÃƒÂ‚Ã‚Â…9ÃƒÂ‚Ã‚Â‰ xÃƒÂ‚Ã‚Â\ Config $ÃƒÂ‚Ã‚Â„|tÃƒÂƒÃ‚Âœ

        Args:
            config: Config ÃƒÂ‚Ã‚Â´
        """
        # Epochs
        if hasattr(self.args, 'epochs') and self.args.epochs is not None:
            config.training.epochs = self.args.epochs

        # Batch size
        if hasattr(self.args, 'batch_size') and self.args.batch_size is not None:
            config.training.batch_size = self.args.batch_size

        # Learning rate
        if hasattr(self.args, 'learning_rate') and self.args.learning_rate is not None:
            config.training.learning_rate = self.args.learning_rate

    def _extract_eval_metrics(self, log_history):
        """
        YÃƒÂ‚Ã‚Âµ \ÃƒÂƒÃ‚Â¸ÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰ TÃƒÂ‚Ã‚Â¸ÃƒÂ‚Ã‚Â­ ÃƒÂ‚Ã‚Â”ÃƒÂ‚Ã‚Âœ

        Args:
            log_history: TrainerX \ÃƒÂƒÃ‚Â¸ ÃƒÂ‚Ã‚ÂˆÃƒÂ‚Ã‚Â¤ÃƒÂ‚Ã‚Â ÃƒÂ‚Ã‚Â¬

        Returns:
            dict: ÃƒÂƒÃ‚Â‰ TÃƒÂ‚Ã‚Â¸ÃƒÂ‚Ã‚Â­
        """
        eval_metrics = {}

        # ÃƒÂƒÃ‚ÂˆÃƒÂƒÃ‚Â€ÃƒÂƒÃ‚Â‰ eval \ÃƒÂƒÃ‚Â¸ >0
        for log_entry in reversed(log_history):
            if 'eval_loss' in log_entry:
                for key, value in log_entry.items():
                    if key.startswith('eval_'):
                        eval_metrics[key] = value
                break

        return eval_metrics

    def _evaluate_ensemble(
        self,
        model_paths: List[str],
        eval_df,
        strategy: str
    ) -> Dict[str, float]:
        """
        YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰

        Args:
            model_paths: ÃƒÂ‚Ã‚Â¨x ÃƒÂ‚Ã‚Â½\ ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â¤ÃƒÂ‚Ã‚Â¸
            eval_df: ÃƒÂƒÃ‚Â‰ pt0ÃƒÂ‚Ã‚Â„
            strategy: YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Âµ

        Returns:
            dict: YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰ TÃƒÂ‚Ã‚Â¸ÃƒÂ‚Ã‚Â­
        """
        try:
            from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
            from rouge import Rouge
            import torch

            self.log(f"  YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Âµ: {strategy}")

            # ÃƒÂ‚Ã‚Â¨x  ÃƒÂ‚Ã‚Â lÃƒÂ‚Ã‚Â˜t \ÃƒÂƒÃ‚Âœ
            models = []
            tokenizers = []

            for model_path in model_paths:
                model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_path)

                if torch.cuda.is_available():
                    model = model.cuda()
                model.eval()

                models.append(model)
                tokenizers.append(tokenizer)

            # ModelManager\ YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â1
            manager = ModelManager(logger=self.logger)
            manager.models = models
            manager.tokenizers = tokenizers
            manager.model_names = self.args.models

            # YÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚ÂµÃƒÂƒÃ‚Â 0| ÃƒÂƒÃ‚Â1
            if strategy in ['weighted_avg', 'rouge_weighted']:
                ensemble = manager.create_ensemble(
                    ensemble_type='weighted',
                    weights=self.args.ensemble_weights
                )
            else:  # majority_vote ÃƒÂƒÃ‚Â±
                ensemble = manager.create_ensemble(
                    ensemble_type='voting',
                    voting='hard'
                )

            # !
            dialogues = eval_df['dialogue'].tolist()[:100]  # ÃƒÂƒÃ‚Â˜ ÃƒÂƒÃ‚Â‰
            references = eval_df['summary'].tolist()[:100]

            predictions = ensemble.predict(
                dialogues=dialogues,
                max_length=200,
                num_beams=4,
                batch_size=8
            )

            # ROUGE ÃƒÂƒÃ‚Â„ÃƒÂ‚Ã‚Â°
            rouge = Rouge()
            scores = rouge.get_scores(predictions, references, avg=True)

            ensemble_metrics = {
                'ensemble_rouge_1_f1': scores['rouge-1']['f'],
                'ensemble_rouge_2_f1': scores['rouge-2']['f'],
                'ensemble_rouge_l_f1': scores['rouge-l']['f'],
            }

            return ensemble_metrics

        except Exception as e:
            self.log(f"  ÃƒÂ‚Ã‚Â  YÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â‰ ÃƒÂƒÃ‚Â¤(: {e}")
            return {}


# ==================== ÃƒÂ‚Ã‚Â¸X h ==================== #
def create_multi_model_trainer(args, logger, wandb_logger=None):
    """
    MultiModelEnsembleTrainer ÃƒÂƒÃ‚Â1 ÃƒÂ‚Ã‚Â¸X h

    Args:
        args: ÃƒÂ‚Ã‚Â…9ÃƒÂ‚Ã‚Â‰ xÃƒÂ‚Ã‚Â
        logger: Logger xÃƒÂ‚Ã‚Â¤4ÃƒÂ‚Ã‚Â¤
        wandb_logger: WandB Logger ( ÃƒÂƒÃ‚Â)

    Returns:
        MultiModelEnsembleTrainer xÃƒÂ‚Ã‚Â¤4ÃƒÂ‚Ã‚Â¤
    """
    return MultiModelEnsembleTrainer(args, logger, wandb_logger)
