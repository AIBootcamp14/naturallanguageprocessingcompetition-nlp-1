# ==================== SingleModelTrainer ==================== #
"""
ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ Trainer

ê°€ì¥ ê¸°ë³¸ì ì¸ í•™ìŠµ ëª¨ë“œë¡œ, í•˜ë‚˜ì˜ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import json
from pathlib import Path

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.trainers.base_trainer import BaseTrainer
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer


# ==================== SingleModelTrainer ==================== #
class SingleModelTrainer(BaseTrainer):
    """ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ Trainer"""

    def train(self):
        """
        ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰

        Returns:
            dict: í•™ìŠµ ê²°ê³¼
                - mode: 'single'
                - model: ëª¨ë¸ ì´ë¦„
                - results: í•™ìŠµ ê²°ê³¼
                - model_path: ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
        """
        self.log("=" * 60)
        self.log("ğŸš€ SINGLE MODEL ëª¨ë“œ í•™ìŠµ ì‹œì‘")
        self.log(f"ğŸ“‹ ëª¨ë¸: {self.args.models[0]}")
        self.log("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        self.log("\n[1/5] ë°ì´í„° ë¡œë”©...")
        train_df, eval_df = self.load_data()

        # 2. Config ë¡œë“œ
        self.log("\n[2/5] Config ë¡œë”©...")
        config_path = self.get_config_path(self.args.models[0])
        config = load_config(config_path)

        # ëª…ë ¹í–‰ ì¸ìë¡œ Config ì˜¤ë²„ë¼ì´ë“œ
        self._override_config(config)

        self.log(f"  âœ… Config ë¡œë“œ ì™„ë£Œ: {config_path}")

        # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.log("\n[3/5] ëª¨ë¸ ë¡œë”©...")
        model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)
        self.log("  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # 4. Dataset ìƒì„±
        self.log("\n[4/5] Dataset ìƒì„±...")

        # ëª¨ë¸ íƒ€ì… ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’: encoder_decoder)
        model_type = config.model.get('type', 'encoder_decoder')

        train_dataset = DialogueSummarizationDataset(
            dialogues=train_df['dialogue'].tolist(),
            summaries=train_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True,
            model_type=model_type  # PRD 08: LLM ì§€ì›
        )

        eval_dataset = DialogueSummarizationDataset(
            dialogues=eval_df['dialogue'].tolist(),
            summaries=eval_df['summary'].tolist(),
            tokenizer=tokenizer,
            encoder_max_len=config.tokenizer.encoder_max_len,
            decoder_max_len=config.tokenizer.decoder_max_len,
            preprocess=True,
            model_type=model_type  # PRD 08: LLM ì§€ì›
        )

        self.log(f"  âœ… í•™ìŠµ Dataset: {len(train_dataset)}ê°œ")
        self.log(f"  âœ… ê²€ì¦ Dataset: {len(eval_dataset)}ê°œ")

        # 5. Trainer ìƒì„± ë° í•™ìŠµ
        self.log("\n[5/5] í•™ìŠµ ì‹œì‘...")
        trainer = create_trainer(
            config=config,
            model=model,
            tokenizer=tokenizer,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            use_wandb=getattr(self.args, 'use_wandb', False),
            logger=self.logger
        )

        # í•™ìŠµ ì‹¤í–‰
        train_results = trainer.train()

        # ê²°ê³¼ ìˆ˜ì§‘
        results = {
            'mode': 'single',
            'model': self.args.models[0],
            'train_results': train_results,
            'model_path': str(self.output_dir / 'final_model')
        }

        # ìµœì¢… í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€
        if hasattr(trainer, 'state') and trainer.state.log_history:
            eval_metrics = self._extract_eval_metrics(trainer.state.log_history)
            results['eval_metrics'] = eval_metrics

        self.log("\n" + "=" * 60)
        self.log("âœ… SINGLE MODEL í•™ìŠµ ì™„ë£Œ!")

        if 'eval_metrics' in results and results['eval_metrics']:
            self.log("\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
            for key, value in results['eval_metrics'].items():
                if 'rouge' in key.lower():
                    self.log(f"  {key}: {value:.4f}")

        self.log("=" * 60)

        return results

    def save_results(self, results):
        """
        ê²°ê³¼ ì €ì¥

        Args:
            results: í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result_path = self.output_dir / "single_model_results.json"

        # ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        saveable_results = {
            'mode': results['mode'],
            'model': results['model'],
            'model_path': results['model_path'],
            'eval_metrics': results.get('eval_metrics', {})
        }

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(saveable_results, f, indent=2, ensure_ascii=False)

        self.log(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_path}")

    def _override_config(self, config):
        """
        ëª…ë ¹í–‰ ì¸ìë¡œ Config ì˜¤ë²„ë¼ì´ë“œ

        Args:
            config: Config ê°ì²´
        """
        # Epochs
        if hasattr(self.args, 'epochs') and self.args.epochs is not None:
            config.training.epochs = self.args.epochs
            self.log(f"  âš™ï¸ Epochs ì˜¤ë²„ë¼ì´ë“œ: {self.args.epochs}")

        # Batch size
        if hasattr(self.args, 'batch_size') and self.args.batch_size is not None:
            config.training.batch_size = self.args.batch_size
            self.log(f"  âš™ï¸ Batch size ì˜¤ë²„ë¼ì´ë“œ: {self.args.batch_size}")

        # Learning rate
        if hasattr(self.args, 'learning_rate') and self.args.learning_rate is not None:
            config.training.learning_rate = self.args.learning_rate
            self.log(f"  âš™ï¸ Learning rate ì˜¤ë²„ë¼ì´ë“œ: {self.args.learning_rate}")

    def _extract_eval_metrics(self, log_history):
        """
        í•™ìŠµ ë¡œê·¸ì—ì„œ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ì¶œ

        Args:
            log_history: Trainerì˜ ë¡œê·¸ íˆìŠ¤í† ë¦¬

        Returns:
            dict: í‰ê°€ ë©”íŠ¸ë¦­
        """
        eval_metrics = {}

        # ë§ˆì§€ë§‰ eval ë¡œê·¸ ì°¾ê¸°
        for log_entry in reversed(log_history):
            if 'eval_loss' in log_entry:
                for key, value in log_entry.items():
                    if key.startswith('eval_'):
                        eval_metrics[key] = value
                break

        return eval_metrics
