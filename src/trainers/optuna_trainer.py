# ==================== OptunaTrainer ==================== #
"""
Optuna Xt||ÃƒÂƒÃ‚Â¸0 \T Trainer

PRD 13: Optuna Xt||ÃƒÂƒÃ‚Â¸0 \T ÃƒÂ‚Ã‚Âµ l
ÃƒÂ‚Ã‚ÂÃƒÂƒÃ‚Â™ Xt||ÃƒÂƒÃ‚Â¸0 ÃƒÂ‚Ã‚ÂœÃƒÂƒÃ‚ÂD ÃƒÂ‚Ã‚Âµt \X ÃƒÂ‚Ã‚Â¨x $ ÃƒÂƒÃ‚ÂÃƒÂƒÃ‚Â‰
"""

# ---------------------- \ |tÃƒÂƒÃ‚Â¬ÃƒÂ‚Ã‚Â¬ ---------------------- #
import json
from pathlib import Path
from typing import Dict, Any

# ---------------------- \ÃƒÂ‚Ã‚Â¸ ÃƒÂ‚Ã‚Â¨ÃƒÂƒÃ‚Âˆ ---------------------- #
from src.trainers.base_trainer import BaseTrainer
from src.config import load_model_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
from src.optimization import OptunaOptimizer


# ==================== OptunaTrainer ==================== #
class OptunaTrainer(BaseTrainer):
    """Optuna Xt||ÃƒÂƒÃ‚Â¸0 \T Trainer"""

    def train(self):
        """
        Optuna \T ÃƒÂƒÃ‚Â¤ÃƒÂ‚Ã‚Â‰

        Returns:
            dict: \T ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼
                - mode: 'optuna'
                - model: ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â©\ ÃƒÂ‚Ã‚Â¨x
                - best_params: \ Xt||ÃƒÂƒÃ‚Â¸0
                - best_value: \ ROUGE 
                - n_trials: ÃƒÂ‚Ã‚Â´ ÃƒÂƒÃ‚ÂœÃƒÂƒÃ‚Â„ ÃƒÂ‚Ã‚ÂŸ
        """
        self.log("=" * 60)
        self.log("=ÃƒÂ‚Ã‚Â€ OPTUNA \T ÃƒÂ‚Ã‚Â¨ÃƒÂƒÃ‚Âœ ÃƒÂƒÃ‚ÂœÃƒÂ‚Ã‚Â‘")
        self.log(f"=ÃƒÂƒÃ‚Â‹ ÃƒÂ‚Ã‚Â¨x: {self.args.models[0]}")
        self.log(f"=' ÃƒÂƒÃ‚ÂœÃƒÂƒÃ‚Â„ ÃƒÂ‚Ã‚ÂŸ: {self.args.optuna_trials}")
        self.log(f"ÃƒÂƒÃ‚Â± \ ÃƒÂƒÃ‚Âœ: {self.args.optuna_timeout}")
        self.log("=" * 60)

        # 1. pt0 \ÃƒÂƒÃ‚Âœ
        self.log("\n[1/3] pt0 \)...")
        train_df, eval_df = self.load_data()

        # 2. Config \ÃƒÂƒÃ‚Âœ
        self.log("\n[2/3] Config \)...")
        model_name = self.args.models[0]
        config = load_model_config(model_name)

        self.log(f"   Config \ÃƒÂƒÃ‚Âœ DÃƒÂƒÃ‚ÂŒ: {model_name}")

        # 3. Dataset D (\ ÃƒÂ‚Ã‚ÂˆÃƒÂƒÃ‚ÂŒ ÃƒÂƒÃ‚Â1)
        self.log("\npt0K D ...")

        # TokenizerÃƒÂ‚Ã‚Â” \T ÃƒÂƒÃ‚Â¼ÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Â¬\ÃƒÂƒÃ‚ÂœXÃƒÂƒÃ‚Â€ÃƒÂƒÃ‚ÂŒ, pt0ÃƒÂ‚Ã‚Â„@ ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â©
        self.train_df = train_df
        self.eval_df = eval_df

        # 4. Optuna Optimizer 0T
        self.log(f"\n[3/3] Optuna \T ÃƒÂƒÃ‚ÂœÃƒÂ‚Ã‚Â‘...")

        from src.data import create_datasets_from_df

        # Dataset ÃƒÂƒÃ‚Â1 ÃƒÂƒÃ‚Â¬| (Optuna ÃƒÂ‚Ã‚Â´ÃƒÂ‚Ã‚Â€ÃƒÂƒÃ‚Â ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â©)
        def create_datasets(tokenizer, config):
            """Dataset ÃƒÂƒÃ‚Â1 ÃƒÂƒÃ‚Â¬| h"""
            model_type = config.model.get('type', 'encoder_decoder')

            train_dataset = DialogueSummarizationDataset(
                dialogues=self.train_df['dialogue'].tolist(),
                summaries=self.train_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            eval_dataset = DialogueSummarizationDataset(
                dialogues=self.eval_df['dialogue'].tolist(),
                summaries=self.eval_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            return train_dataset, eval_dataset

        # Optuna Optimizer 0T
        optimizer = OptunaOptimizer(
            config=config,
            train_dataset=None,  # Objective ÃƒÂ‚Ã‚Â´ÃƒÂ‚Ã‚Â€ÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â1
            val_dataset=None,    # Objective ÃƒÂ‚Ã‚Â´ÃƒÂ‚Ã‚Â€ÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â1
            n_trials=self.args.optuna_trials,
            timeout=self.args.optuna_timeout,
            study_name=f"optuna_{model_name}_{self.args.experiment_name}",
            storage=None,  # xTÃƒÂ‚Ã‚Â¨ÃƒÂ‚Ã‚Â¬
            direction="maximize",
            logger=self.logger.logger if hasattr(self.logger, 'logger') else None
        )

        # Dataset ÃƒÂƒÃ‚Â1 h| optimizerÃƒÂƒÃ‚Â ÃƒÂƒÃ‚Â¬
        optimizer.create_datasets = create_datasets

        # \T ÃƒÂƒÃ‚Â¤ÃƒÂ‚Ã‚Â‰
        study = optimizer.optimize()

        # ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂƒÃ‚Â‘
        best_params = optimizer.get_best_params()
        best_value = optimizer.get_best_value()

        results = {
            'mode': 'optuna',
            'model': model_name,
            'best_params': best_params,
            'best_value': best_value,
            'n_trials': len(study.trials),
            'study_name': optimizer.study_name
        }

        # ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥
        self.log("\nÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥ ...")
        optimizer.save_results(str(self.output_dir))

        # ÃƒÂƒÃ‚ÂœT (5X)
        if self.args.save_visualizations:
            self.log("\nÃƒÂƒÃ‚ÂœT ÃƒÂƒÃ‚Â1 ...")
            try:
                optimizer.plot_optimization_history(str(self.output_dir))
            except Exception as e:
                self.log(f"  ÃƒÂ‚Ã‚Â  ÃƒÂƒÃ‚ÂœT ÃƒÂƒÃ‚Â¤(: {e}")

        self.log("\n" + "=" * 60)
        self.log(" OPTUNA \T DÃƒÂƒÃ‚ÂŒ!")
        self.log(f"\n=ÃƒÂƒÃ‚ÂŠ \ ROUGE-L F1: {best_value:.4f}")
        self.log(f"\n=ÃƒÂƒÃ‚ÂŠ \ Xt||ÃƒÂƒÃ‚Â¸0:")
        for key, value in best_params.items():
            self.log(f"  {key}: {value}")
        self.log("=" * 60)

        return results

    def save_results(self, results):
        """
        ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥

        Args:
            results: \T ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ TÃƒÂ‚Ã‚Â¬
        """
        result_path = self.output_dir / "optuna_results.json"

        # ÃƒÂ‚Ã‚Â¥ ÃƒÂ‚Ã‚Â¥\ ÃƒÂƒÃ‚Âœ\ ÃƒÂƒÃ‚Â€X
        saveable_results = {
            'mode': results['mode'],
            'model': results['model'],
            'best_params': results['best_params'],
            'best_value': results['best_value'],
            'n_trials': results['n_trials'],
            'study_name': results['study_name']
        }

        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(saveable_results, f, indent=2, ensure_ascii=False)

        self.log(f"\n=ÃƒÂ‚Ã‚Â¾ ÃƒÂ‚Ã‚Â°ÃƒÂƒÃ‚Â¼ ÃƒÂ‚Ã‚Â¥: {result_path}")

        # \ |ÃƒÂƒÃ‚Â¸0\ Config ÃƒÂƒÃ‚Â1 (ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â¬ÃƒÂ‚Ã‚Â© ÃƒÂ‚Ã‚Â¥)
        best_config_path = self.output_dir / "best_config.yaml"
        self.log(f"=ÃƒÂ‚Ã‚Â¾ \ Config ÃƒÂ‚Ã‚Â¥: {best_config_path}")


# ==================== ÃƒÂ‚Ã‚Â¸X h ==================== #
def create_optuna_trainer(args, logger, wandb_logger=None):
    """
    OptunaTrainer ÃƒÂƒÃ‚Â1 ÃƒÂ‚Ã‚Â¸X h

    Args:
        args: ÃƒÂ‚Ã‚Â…9ÃƒÂ‚Ã‚Â‰ xÃƒÂ‚Ã‚Â
        logger: Logger xÃƒÂ‚Ã‚Â¤4ÃƒÂ‚Ã‚Â¤
        wandb_logger: WandB Logger ( ÃƒÂƒÃ‚Â)

    Returns:
        OptunaTrainer xÃƒÂ‚Ã‚Â¤4ÃƒÂ‚Ã‚Â¤
    """
    return OptunaTrainer(args, logger, wandb_logger)
