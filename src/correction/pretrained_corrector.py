"""
ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •ê¸°

PRD 04, 12: ì¶”ë¡  ìµœì í™” ë° ì•™ìƒë¸” ì „ëµ êµ¬í˜„
í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ìš”ì•½ ë³´ì •
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
from typing import List, Dict, Optional

# ------------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import torch


# ==================== ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •ê¸° í´ë˜ìŠ¤ ==================== #
class PretrainedCorrector:
    """
    í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ìš”ì•½ ë³´ì •

    ì£¼ìš” ê¸°ëŠ¥:
    1. ì—¬ëŸ¬ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬
    2. ì°¸ì¡° ìš”ì•½ ìƒì„±
    3. í’ˆì§ˆ í‰ê°€ ë° ë³´ì •
    4. ì•™ìƒë¸” ì „ëµ ì ìš©

    ì‚¬ìš© ì˜ˆì‹œ:
        corrector = PretrainedCorrector(
            model_names=["gogamza/kobart-base-v2", "digit82/kobart-summarization"],
            correction_strategy="quality_based",
            quality_threshold=0.3
        )
        corrected = corrector.correct_batch(dialogues, candidate_summaries)
    """

    # ---------------------- ì´ˆê¸°í™” ë©”ì„œë“œ ---------------------- #
    def __init__(
        self,
        model_names: List[str],
        correction_strategy: str = "quality_based",
        quality_threshold: float = 0.3,
        device: Optional[torch.device] = None,
        logger=None
    ):
        """
        Args:
            model_names: í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: ["gogamza/kobart-base-v2", "digit82/kobart-summarization"]
            correction_strategy: ë³´ì • ì „ëµ
                - "threshold": ì„ê³„ê°’ ê¸°ë°˜
                - "voting": íˆ¬í‘œ ê¸°ë°˜
                - "weighted": ê°€ì¤‘ í‰ê· 
                - "quality_based": í’ˆì§ˆ ê¸°ë°˜ (ì¶”ì²œ)
            quality_threshold: í’ˆì§ˆ ì„ê³„ê°’ (0.0~1.0)
            device: ì¶”ë¡  ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ê°ì§€)
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.model_names = model_names
        self.correction_strategy = correction_strategy
        self.quality_threshold = quality_threshold
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = logger

        # -------------- ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” -------------- #
        from src.correction.model_loader import HuggingFaceModelLoader
        self.model_loader = HuggingFaceModelLoader(device=self.device, logger=logger)

        # -------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ì†Œ -------------- #
        self.models = {}                                # ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        self.tokenizers = {}                            # í† í¬ë‚˜ì´ì € ë”•ì…”ë„ˆë¦¬
        self._load_all_models()                         # ëª¨ë“  ëª¨ë¸ ë¡œë“œ

        # -------------- í’ˆì§ˆ í‰ê°€ê¸° ì´ˆê¸°í™” -------------- #
        from src.correction.quality_evaluator import QualityEvaluator
        self.evaluator = QualityEvaluator(logger=logger)

        # -------------- ì•™ìƒë¸” ì „ëµ ì´ˆê¸°í™” -------------- #
        from src.correction.ensemble_strategies import get_ensemble_strategy
        self.ensemble = get_ensemble_strategy(correction_strategy)

    # ---------------------- ëª¨ë“  ëª¨ë¸ ë¡œë“œ ë©”ì„œë“œ ---------------------- #
    def _load_all_models(self):
        """
        ëª¨ë“  í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ

        ì‹¤íŒ¨í•œ ëª¨ë¸ì€ ê±´ë„ˆëœ€ (graceful degradation)
        """
        for model_name in self.model_names:
            try:
                model, tokenizer = self.model_loader.load_model(model_name)
                self.models[model_name] = model
                self.tokenizers[model_name] = tokenizer
            except Exception as e:
                self._log(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {model_name}")
                self._log(f"   ì—ëŸ¬: {str(e)}")

    # ---------------------- ë°°ì¹˜ ë³´ì • ë©”ì„œë“œ ---------------------- #
    def correct_batch(
        self,
        dialogues: List[str],
        candidate_summaries: List[str],
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        ë°°ì¹˜ ë³´ì •

        Args:
            dialogues: ì…ë ¥ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            candidate_summaries: KoBARTê°€ ìƒì„±í•œ ì´ˆì•ˆ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            **generation_kwargs: ìƒì„± íŒŒë¼ë¯¸í„° (max_new_tokens, num_beams ë“±)

        Returns:
            ë³´ì •ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        # -------------- ë¡œë“œëœ ëª¨ë¸ í™•ì¸ -------------- #
        if not self.models:
            self._log("âš ï¸  ë¡œë“œëœ ì°¸ì¡° ëª¨ë¸ì´ ì—†ìŒ. ì›ë³¸ ìš”ì•½ ë°˜í™˜")
            return candidate_summaries

        # -------------- ë³´ì • ì‹œì‘ ë¡œê·¸ -------------- #
        self._log("=" * 60)
        self._log("ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ì‹œì‘")
        self._log(f"  - ìƒ˜í”Œ ìˆ˜: {len(dialogues)}")
        self._log(f"  - ì°¸ì¡° ëª¨ë¸ ìˆ˜: {len(self.models)}")
        self._log(f"  - ë³´ì • ì „ëµ: {self.correction_strategy}")
        self._log(f"  - í’ˆì§ˆ ì„ê³„ê°’: {self.quality_threshold}")
        self._log("=" * 60)

        # -------------- ë‹¨ê³„ 1: ì°¸ì¡° ìš”ì•½ ìƒì„± -------------- #
        # ê° í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ë¡œ ì°¸ì¡° ìš”ì•½ ìƒì„±
        reference_summaries = {}
        for model_name, model in self.models.items():
            self._log(f"\n[1/3] ì°¸ì¡° ìš”ì•½ ìƒì„± ì¤‘: {model_name}")
            tokenizer = self.tokenizers[model_name]
            summaries = self._generate_summaries(
                dialogues, model, tokenizer, batch_size, **generation_kwargs
            )
            reference_summaries[model_name] = summaries
            self._log(f"  âœ… ì™„ë£Œ: {len(summaries)}ê°œ ìš”ì•½ ìƒì„±")

        # -------------- ë‹¨ê³„ 2: í’ˆì§ˆ í‰ê°€ -------------- #
        self._log(f"\n[2/3] í’ˆì§ˆ í‰ê°€ ì¤‘...")
        quality_scores = self.evaluator.evaluate_all(
            candidate_summaries=candidate_summaries,
            reference_summaries=reference_summaries,
            dialogues=dialogues
        )
        self._log(f"  âœ… í‰ê°€ ì™„ë£Œ")

        # -------------- ë‹¨ê³„ 3: ë³´ì • ì „ëµ ì ìš© -------------- #
        self._log(f"\n[3/3] ë³´ì • ì „ëµ ì ìš© ì¤‘: {self.correction_strategy}")
        corrected_summaries = self.ensemble.select(
            candidate_summaries=candidate_summaries,
            reference_summaries=reference_summaries,
            quality_scores=quality_scores,
            threshold=self.quality_threshold
        )
        self._log(f"  âœ… ë³´ì • ì™„ë£Œ")

        # -------------- ë³´ì • í†µê³„ ì¶œë ¥ -------------- #
        num_corrected = sum([
            1 for orig, corr in zip(candidate_summaries, corrected_summaries)
            if orig != corr
        ])
        self._log(f"\nğŸ“Š ë³´ì • í†µê³„:")
        self._log(f"  - ì „ì²´: {len(dialogues)}ê°œ")
        self._log(f"  - ë³´ì •ë¨: {num_corrected}ê°œ ({num_corrected/len(dialogues)*100:.1f}%)")
        self._log(f"  - ìœ ì§€ë¨: {len(dialogues)-num_corrected}ê°œ")
        self._log("=" * 60)

        return corrected_summaries

    # ---------------------- ë‹¨ì¼ ëª¨ë¸ ìš”ì•½ ìƒì„± ë©”ì„œë“œ ---------------------- #
    def _generate_summaries(
        self,
        dialogues: List[str],
        model,
        tokenizer,
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ ë°°ì¹˜ ìš”ì•½ ìƒì„±

        Args:
            dialogues: ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            model: HuggingFace ëª¨ë¸
            tokenizer: HuggingFace í† í¬ë‚˜ì´ì €
            batch_size: ë°°ì¹˜ í¬ê¸°
            **generation_kwargs: ìƒì„± íŒŒë¼ë¯¸í„°

        Returns:
            ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        from src.inference import create_predictor

        # -------------- Predictor ìƒì„± -------------- #
        # ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©
        predictor = create_predictor(
            model=model,
            tokenizer=tokenizer,
            device=self.device,
            logger=None                                 # ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€
        )

        # -------------- ë°°ì¹˜ ì˜ˆì¸¡ -------------- #
        summaries = predictor.predict_batch(
            dialogues=dialogues,
            batch_size=batch_size,
            show_progress=False,                        # ì§„í–‰ë°” ë¹„í™œì„±í™”
            **generation_kwargs
        )

        return summaries

    # ---------------------- ë¡œê¹… í—¬í¼ ë©”ì„œë“œ ---------------------- #
    def _log(self, msg: str):
        """
        ë¡œê¹… í—¬í¼ í•¨ìˆ˜

        Args:
            msg: ë¡œê·¸ ë©”ì‹œì§€
        """
        if self.logger:
            self.logger.write(msg)
        else:
            print(msg)
