"""
ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ Î≥¥Ï†ïÍ∏∞

PRD 04, 12: Ï∂îÎ°† ÏµúÏ†ÅÌôî Î∞è ÏïôÏÉÅÎ∏î Ï†ÑÎûµ Íµ¨ÌòÑ
ÌóàÍπÖÌéòÏù¥Ïä§ ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ÏùÑ ÌôúÏö©Ìïú ÏöîÏïΩ Î≥¥Ï†ï
"""

# ------------------------- ÌëúÏ§Ä ÎùºÏù¥Î∏åÎü¨Î¶¨ ------------------------- #
from typing import List, Dict, Optional

# ------------------------- ÏÑúÎìúÌååÌã∞ ÎùºÏù¥Î∏åÎü¨Î¶¨ ------------------------- #
import torch


# ==================== ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ Î≥¥Ï†ïÍ∏∞ ÌÅ¥ÎûòÏä§ ==================== #
class PretrainedCorrector:
    """
    ÌóàÍπÖÌéòÏù¥Ïä§ ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ÏùÑ ÌôúÏö©Ìïú ÏöîÏïΩ Î≥¥Ï†ï

    Ï£ºÏöî Í∏∞Îä•:
    1. Ïó¨Îü¨ ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ Î°úÎìú Î∞è Í¥ÄÎ¶¨
    2. Ï∞∏Ï°∞ ÏöîÏïΩ ÏÉùÏÑ±
    3. ÌíàÏßà ÌèâÍ∞Ä Î∞è Î≥¥Ï†ï
    4. ÏïôÏÉÅÎ∏î Ï†ÑÎûµ Ï†ÅÏö©

    ÏÇ¨Ïö© ÏòàÏãú:
        corrector = PretrainedCorrector(
            model_names=["gogamza/kobart-base-v2", "digit82/kobart-summarization"],
            correction_strategy="quality_based",
            quality_threshold=0.3
        )
        corrected = corrector.correct_batch(dialogues, candidate_summaries)
    """

    # ---------------------- Ï¥àÍ∏∞Ìôî Î©îÏÑúÎìú ---------------------- #
    def __init__(
        self,
        model_names: List[str],
        correction_strategy: str = "quality_based",
        quality_threshold: float = 0.3,
        device: Optional[torch.device] = None,
        logger=None,
        checkpoint_dir: Optional[str] = None
    ):
        """
        Args:
            model_names: ÌóàÍπÖÌéòÏù¥Ïä§ Î™®Îç∏ Ïù¥Î¶Ñ Î¶¨Ïä§Ìä∏
                Ïòà: ["gogamza/kobart-base-v2", "digit82/kobart-summarization"]
            correction_strategy: Î≥¥Ï†ï Ï†ÑÎûµ
                - "threshold": ÏûÑÍ≥ÑÍ∞í Í∏∞Î∞ò
                - "voting": Ìà¨Ìëú Í∏∞Î∞ò
                - "weighted": Í∞ÄÏ§ë ÌèâÍ∑†
                - "quality_based": ÌíàÏßà Í∏∞Î∞ò (Ï∂îÏ≤ú)
            quality_threshold: ÌíàÏßà ÏûÑÍ≥ÑÍ∞í (0.0~1.0)
            device: Ï∂îÎ°† ÎîîÎ∞îÏù¥Ïä§ (NoneÏù¥Î©¥ ÏûêÎèô Í∞êÏßÄ)
            logger: Logger Ïù∏Ïä§ÌÑ¥Ïä§
            checkpoint_dir: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ (ÏÑ†ÌÉù)
        """
        self.model_names = model_names
        self.correction_strategy = correction_strategy
        self.quality_threshold = quality_threshold
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = logger

        # -------------- Î™®Îç∏ Î°úÎçî Ï¥àÍ∏∞Ìôî -------------- #
        from src.correction.model_loader import HuggingFaceModelLoader
        self.model_loader = HuggingFaceModelLoader(device=self.device, logger=logger)

        # -------------- Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï†ÄÏû•ÏÜå -------------- #
        self.models = {}                                # Î™®Îç∏ ÎîïÏÖîÎÑàÎ¶¨
        self.tokenizers = {}                            # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÎîïÏÖîÎÑàÎ¶¨
        self._load_all_models()                         # Î™®Îì† Î™®Îç∏ Î°úÎìú

        # -------------- ÌíàÏßà ÌèâÍ∞ÄÍ∏∞ Ï¥àÍ∏∞Ìôî -------------- #
        from src.correction.quality_evaluator import QualityEvaluator
        self.evaluator = QualityEvaluator(logger=logger)

        # -------------- ÏïôÏÉÅÎ∏î Ï†ÑÎûµ Ï¥àÍ∏∞Ìôî -------------- #
        from src.correction.ensemble_strategies import get_ensemble_strategy
        self.ensemble = get_ensemble_strategy(correction_strategy)

        # -------------- Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í¥ÄÎ¶¨Ïûê Ï¥àÍ∏∞Ìôî -------------- #
        self.checkpoint_manager = None
        if checkpoint_dir:
            from src.checkpoints.correction_checkpoint import CorrectionCheckpointManager
            self.checkpoint_manager = CorrectionCheckpointManager(checkpoint_dir)

    # ---------------------- Î™®Îì† Î™®Îç∏ Î°úÎìú Î©îÏÑúÎìú ---------------------- #
    def _load_all_models(self):
        """
        Î™®Îì† ÌóàÍπÖÌéòÏù¥Ïä§ Î™®Îç∏ Î°úÎìú

        Ïã§Ìå®Ìïú Î™®Îç∏ÏùÄ Í±¥ÎÑàÎúÄ (graceful degradation)
        """
        for model_name in self.model_names:
            try:
                model, tokenizer = self.model_loader.load_model(model_name)
                self.models[model_name] = model
                self.tokenizers[model_name] = tokenizer
            except Exception as e:
                self._log(f"‚ö†Ô∏è  Î™®Îç∏ Î°úÎìú Ïã§Ìå®, Í±¥ÎÑàÎúÄ: {model_name}")
                self._log(f"   ÏóêÎü¨: {str(e)}")

    # ---------------------- Î∞∞Ïπò Î≥¥Ï†ï Î©îÏÑúÎìú ---------------------- #
    def correct_batch(
        self,
        dialogues: List[str],
        candidate_summaries: List[str],
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        Î∞∞Ïπò Î≥¥Ï†ï

        Args:
            dialogues: ÏûÖÎ†• ÎåÄÌôî Î¶¨Ïä§Ìä∏
            candidate_summaries: KoBARTÍ∞Ä ÏÉùÏÑ±Ìïú Ï¥àÏïà ÏöîÏïΩ Î¶¨Ïä§Ìä∏
            batch_size: Î∞∞Ïπò ÌÅ¨Í∏∞
            **generation_kwargs: ÏÉùÏÑ± ÌååÎùºÎØ∏ÌÑ∞ (max_new_tokens, num_beams Îì±)

        Returns:
            Î≥¥Ï†ïÎêú ÏöîÏïΩ Î¶¨Ïä§Ìä∏
        """
        # -------------- Î°úÎìúÎêú Î™®Îç∏ ÌôïÏù∏ -------------- #
        if not self.models:
            self._log("‚ö†Ô∏è  Î°úÎìúÎêú Ï∞∏Ï°∞ Î™®Îç∏Ïù¥ ÏóÜÏùå. ÏõêÎ≥∏ ÏöîÏïΩ Î∞òÌôò")
            return candidate_summaries

        # -------------- Î≥¥Ï†ï ÏãúÏûë Î°úÍ∑∏ -------------- #
        self._log("=" * 60)
        self._log("ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ Î≥¥Ï†ï ÏãúÏûë")
        self._log(f"  - ÏÉòÌîå Ïàò: {len(dialogues)}")
        self._log(f"  - Ï∞∏Ï°∞ Î™®Îç∏ Ïàò: {len(self.models)}")
        self._log(f"  - Î≥¥Ï†ï Ï†ÑÎûµ: {self.correction_strategy}")
        self._log(f"  - ÌíàÏßà ÏûÑÍ≥ÑÍ∞í: {self.quality_threshold}")
        self._log("=" * 60)

        try:
            # -------------- Îã®Í≥Ñ 1: Ï∞∏Ï°∞ ÏöîÏïΩ ÏÉùÏÑ± -------------- #
            # Í∞Å ÌóàÍπÖÌéòÏù¥Ïä§ Î™®Îç∏Î°ú Ï∞∏Ï°∞ ÏöîÏïΩ ÏÉùÏÑ±
            reference_summaries = {}
            for model_name, model in self.models.items():
                self._log(f"\n[1/3] Ï∞∏Ï°∞ ÏöîÏïΩ ÏÉùÏÑ± Ï§ë: {model_name}")
                tokenizer = self.tokenizers[model_name]
                summaries = self._generate_summaries(
                    dialogues, model, tokenizer, batch_size, **generation_kwargs
                )
                reference_summaries[model_name] = summaries
                self._log(f"  ‚úÖ ÏôÑÎ£å: {len(summaries)}Í∞ú ÏöîÏïΩ ÏÉùÏÑ±")

            # -------------- Îã®Í≥Ñ 2: ÌíàÏßà ÌèâÍ∞Ä -------------- #
            self._log(f"\n[2/3] ÌíàÏßà ÌèâÍ∞Ä Ï§ë...")
            quality_scores = self.evaluator.evaluate_all(
                candidate_summaries=candidate_summaries,
                reference_summaries=reference_summaries,
                dialogues=dialogues
            )
            self._log(f"  ‚úÖ ÌèâÍ∞Ä ÏôÑÎ£å")

            # -------------- Îã®Í≥Ñ 3: Î≥¥Ï†ï Ï†ÑÎûµ Ï†ÅÏö© -------------- #
            self._log(f"\n[3/3] Î≥¥Ï†ï Ï†ÑÎûµ Ï†ÅÏö© Ï§ë: {self.correction_strategy}")
            corrected_summaries = self.ensemble.select(
                candidate_summaries=candidate_summaries,
                reference_summaries=reference_summaries,
                quality_scores=quality_scores,
                threshold=self.quality_threshold
            )
            self._log(f"  ‚úÖ Î≥¥Ï†ï ÏôÑÎ£å")

            # -------------- Î≥¥Ï†ï ÌÜµÍ≥Ñ Ï∂úÎ†• -------------- #
            num_corrected = sum([
                1 for orig, corr in zip(candidate_summaries, corrected_summaries)
                if orig != corr
            ])
            self._log(f"\nüìä Î≥¥Ï†ï ÌÜµÍ≥Ñ:")
            self._log(f"  - Ï†ÑÏ≤¥: {len(dialogues)}Í∞ú")
            self._log(f"  - Î≥¥Ï†ïÎê®: {num_corrected}Í∞ú ({num_corrected/len(dialogues)*100:.1f}%)")
            self._log(f"  - Ïú†ÏßÄÎê®: {len(dialogues)-num_corrected}Í∞ú")
            self._log("=" * 60)

            return corrected_summaries

        except Exception as e:
            self._log(f"\n‚ùå Î≥¥Ï†ï Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
            # ÎßàÏßÄÎßâ ÏßÑÌñâÎ•† Í∏∞Î°ù
            if self.logger and hasattr(self.logger, 'write_last_progress'):
                self.logger.write_last_progress()
            # ÏõêÎ≥∏ ÏöîÏïΩ Î∞òÌôò (Graceful degradation)
            self._log("  ‚ö†Ô∏è  ÏõêÎ≥∏ ÏöîÏïΩ Î∞òÌôò")
            return candidate_summaries

    # ---------------------- Îã®Ïùº Î™®Îç∏ ÏöîÏïΩ ÏÉùÏÑ± Î©îÏÑúÎìú ---------------------- #
    def _generate_summaries(
        self,
        dialogues: List[str],
        model,
        tokenizer,
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        Îã®Ïùº Î™®Îç∏Î°ú Î∞∞Ïπò ÏöîÏïΩ ÏÉùÏÑ±

        Args:
            dialogues: ÎåÄÌôî Î¶¨Ïä§Ìä∏
            model: HuggingFace Î™®Îç∏
            tokenizer: HuggingFace ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
            batch_size: Î∞∞Ïπò ÌÅ¨Í∏∞
            **generation_kwargs: ÏÉùÏÑ± ÌååÎùºÎØ∏ÌÑ∞

        Returns:
            ÏöîÏïΩ Î¶¨Ïä§Ìä∏
        """
        from src.inference import create_predictor

        # -------------- Predictor ÏÉùÏÑ± -------------- #
        # Í∏∞Ï°¥ ÏΩîÎìú Ïû¨ÏÇ¨Ïö©
        predictor = create_predictor(
            model=model,
            tokenizer=tokenizer,
            device=self.device,
            logger=None                                 # ÎÑàÎ¨¥ ÎßéÏùÄ Î°úÍ∑∏ Î∞©ÏßÄ
        )

        # -------------- Î∞∞Ïπò ÏòàÏ∏° -------------- #
        summaries = predictor.predict_batch(
            dialogues=dialogues,
            batch_size=batch_size,
            show_progress=False,                        # ÏßÑÌñâÎ∞î ÎπÑÌôúÏÑ±Ìôî
            **generation_kwargs
        )

        # -------------- dialogue ÌïÑÌÑ∞ÎßÅ -------------- #
        filtered_summaries = []
        filtered_count = 0
        for dialogue, summary in zip(dialogues, summaries):
            if self._is_dialogue_copy(dialogue, summary):
                # dialogueÎ•º Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨Ìïú Í≤ΩÏö∞ Îπà Î¨∏ÏûêÏó¥ Î∞òÌôò
                filtered_summaries.append("")           # Îπà ÏöîÏïΩ (ÌíàÏßà ÌèâÍ∞ÄÏóêÏÑú ÎÇÆÏùÄ Ï†êÏàò)
                filtered_count += 1
            else:
                filtered_summaries.append(summary)

        if filtered_count > 0:
            self._log(f"  ‚ö†Ô∏è  {filtered_count}Í∞ú ÏÉòÌîåÏù¥ dialogue Î≥µÏÇ¨Î°ú Í∞êÏßÄÎêòÏñ¥ ÌïÑÌÑ∞ÎßÅÎê®")

        return filtered_summaries

    # ---------------------- dialogue Î≥µÏÇ¨ Í∞êÏßÄ Î©îÏÑúÎìú ---------------------- #
    def _is_dialogue_copy(self, dialogue: str, summary: str, threshold: float = 0.9) -> bool:
        """
        ÏöîÏïΩÏù¥ dialogueÎ•º Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨Ìïú Í≤ÉÏù∏ÏßÄ Í≤ÄÏÇ¨

        Args:
            dialogue: ÏõêÎ≥∏ ÎåÄÌôî
            summary: ÏÉùÏÑ±Îêú ÏöîÏïΩ
            threshold: Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í (0.9 Ïù¥ÏÉÅÏù¥Î©¥ Î≥µÏÇ¨Î°ú Í∞ÑÏ£º)

        Returns:
            True if summary is a copy of dialogue
        """
        from difflib import SequenceMatcher

        # -------------- 0. Îπà Î¨∏ÏûêÏó¥ Ï≤¥ÌÅ¨ -------------- #
        if not summary.strip():
            return False                                # Îπà Î¨∏ÏûêÏó¥ÏùÄ Î≥µÏÇ¨ ÏïÑÎãò

        # -------------- 1. Í∏∏Ïù¥ ÎπÑÏú® Ï≤¥ÌÅ¨ -------------- #
        len_ratio = len(summary) / (len(dialogue) + 1e-6)
        if len_ratio > 0.7:                             # ÏöîÏïΩÏù¥ ÏõêÎ≥∏Ïùò 70% Ïù¥ÏÉÅÏù¥Î©¥ ÏùòÏã¨
            # 2. Î¨∏ÏûêÏó¥ Ïú†ÏÇ¨ÎèÑ Ï≤¥ÌÅ¨
            similarity = SequenceMatcher(None, dialogue, summary).ratio()
            if similarity > threshold:
                return True

        # -------------- 3. #Person1#, #Person2# ÌÉúÍ∑∏ Ï≤¥ÌÅ¨ -------------- #
        if "#Person1#" in summary or "#Person2#" in summary:
            # ÏöîÏïΩÏóê ÎåÄÌôî ÌÉúÍ∑∏Í∞Ä ÎÇ®ÏïÑÏûàÏúºÎ©¥ Î≥µÏÇ¨Î°ú Í∞ÑÏ£º
            return True

        # -------------- 4. ÎåÄÌôî ÌòïÏãù Ìå®ÌÑ¥ Ï≤¥ÌÅ¨ -------------- #
        # "Person1: ... Person2: ..." ÌòïÏãù Í∞êÏßÄ
        import re
        dialogue_pattern = r'(Person\d+[:Ôºö]|#Person\d+#[:Ôºö])'
        matches = re.findall(dialogue_pattern, summary)
        if len(matches) >= 2:                           # 2Í∞ú Ïù¥ÏÉÅÏùò ÌôîÏûê ÌÉúÍ∑∏Í∞Ä ÏûàÏúºÎ©¥ ÎåÄÌôîÎ¨∏
            return True

        return False

    # ---------------------- Î°úÍπÖ Ìó¨Ìçº Î©îÏÑúÎìú ---------------------- #
    def _log(self, msg: str):
        """
        Î°úÍπÖ Ìó¨Ìçº Ìï®Ïàò

        Args:
            msg: Î°úÍ∑∏ Î©îÏãúÏßÄ
        """
        if self.logger:
            self.logger.write(msg)
        else:
            print(msg)
