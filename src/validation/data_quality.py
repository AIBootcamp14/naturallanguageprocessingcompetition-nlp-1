# ==================== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ==================== #
"""
ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì²´í¬

PRD 11: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ëª¨ë“ˆ
- êµ¬ì¡° ê²€ì¦ (ì»¬ëŸ¼, ë°ì´í„° íƒ€ì…)
- ì˜ë¯¸ ê²€ì¦ (ì¤‘ë³µ, íŒ¨í„´)
- í†µê³„ ê²€ì¦ (ê¸¸ì´, ë¶„í¬)
"""

# ---------------------- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import json

# ---------------------- ì™¸ë¶€ íŒ¨í‚¤ì§€ ---------------------- #
import pandas as pd
import numpy as np


# ==================== DataQualityValidator í´ë˜ìŠ¤ ==================== #
class DataQualityValidator:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤"""

    def __init__(self, logger=None):
        """
        Args:
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.logger = logger
        self.validation_results = []

        self._log("DataQualityValidator ì´ˆê¸°í™”")

    def _log(self, msg: str):
        """ë¡œê·¸ ì¶œë ¥"""
        if self.logger:
            self.logger.write(msg)
        else:
            print(msg)

    def validate_dataframe(
        self,
        df: pd.DataFrame,
        required_columns: List[str] = ['dialogue', 'summary'],
        check_duplicates: bool = True,
        check_statistics: bool = True,
        check_anomalies: bool = True
    ) -> Dict[str, Any]:
        """
        ë°ì´í„°í”„ë ˆì„ ê²€ì¦ ìˆ˜í–‰

        Args:
            df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
            required_columns: í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡
            check_duplicates: ì¤‘ë³µ ê²€ì‚¬ ì—¬ë¶€
            check_statistics: í†µê³„ ê²€ì‚¬ ì—¬ë¶€
            check_anomalies: ì´ìƒì¹˜ ê²€ì‚¬ ì—¬ë¶€

        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self._log("\n" + "="*60)
        self._log("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰")
        self._log("="*60)

        results = {
            'total_rows': len(df),
            'structural_validation': {},
            'semantic_validation': {},
            'statistical_validation': {},
            'anomaly_detection': {},
            'issues': [],
            'passed': True
        }

        # 1. êµ¬ì¡° ê²€ì¦
        self._log("\n[1/4] êµ¬ì¡° ê²€ì¦...")
        structural_result = self._validate_structure(df, required_columns)
        results['structural_validation'] = structural_result

        if not structural_result['passed']:
            results['passed'] = False
            results['issues'].extend(structural_result['issues'])

        # 2. ì˜ë¯¸ ê²€ì¦
        self._log("\n[2/4] ì˜ë¯¸ ê²€ì¦...")
        if check_duplicates:
            semantic_result = self._validate_semantics(df, required_columns)
            results['semantic_validation'] = semantic_result

            if semantic_result['duplicate_count'] > 0:
                results['issues'].append(
                    f"ì¤‘ë³µ í–‰: {semantic_result['duplicate_count']}ê°œ"
                )

        # 3. í†µê³„ ê²€ì¦
        self._log("\n[3/4] í†µê³„ ê²€ì¦...")
        if check_statistics:
            statistical_result = self._validate_statistics(df, required_columns)
            results['statistical_validation'] = statistical_result

        # 4. ì´ìƒì¹˜ ê²€ì¦
        self._log("\n[4/4] ì´ìƒì¹˜ ê²€ì¦...")
        if check_anomalies:
            anomaly_result = self._detect_anomalies(df, required_columns)
            results['anomaly_detection'] = anomaly_result

            if anomaly_result['anomaly_count'] > 0:
                results['issues'].append(
                    f"ì´ìƒì¹˜ ë°œê²¬: {anomaly_result['anomaly_count']}ê°œ"
                )

        # ìµœì¢… ìš”ì•½
        self._log("\n" + "="*60)
        if results['passed'] and len(results['issues']) == 0:
            self._log("âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í†µê³¼!")
        else:
            self._log("âš ï¸ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë°œê²¬:")
            for issue in results['issues']:
                self._log(f"  - {issue}")

        self._log("="*60)

        # ê²°ê³¼ ì €ì¥
        self.validation_results.append(results)

        return results

    def _validate_structure(
        self,
        df: pd.DataFrame,
        required_columns: List[str]
    ) -> Dict[str, Any]:
        """
        êµ¬ì¡° ê²€ì¦ (ì»¬ëŸ¼, ë°ì´í„° íƒ€ì…)

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            required_columns: í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡

        Returns:
            êµ¬ì¡° ê²€ì¦ ê²°ê³¼
        """
        issues = []
        passed = True

        # 1. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            issues.append(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
            passed = False
            self._log(f"  âŒ ëˆ„ë½ëœ ì»¬ëŸ¼ ë°œê²¬: {missing_columns}")
        else:
            self._log(f"  âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬: {required_columns}")

        # 2. NULL ê°’ ì²´í¬
        null_counts = {}
        for col in required_columns:
            if col in df.columns:
                null_count = df[col].isnull().sum()
                null_counts[col] = int(null_count)

                if null_count > 0:
                    issues.append(f"{col} ì»¬ëŸ¼ì— NULL ê°’ {null_count}ê°œ")
                    passed = False
                    self._log(f"  âŒ {col}: NULL ê°’ {null_count}ê°œ ë°œê²¬")

        if not issues:
            self._log("  âœ… NULL ê°’ ì—†ìŒ")

        # 3. ë¹ˆ ë¬¸ìì—´ ì²´í¬
        empty_counts = {}
        for col in required_columns:
            if col in df.columns and df[col].dtype == 'object':
                empty_count = (df[col].str.strip() == '').sum()
                empty_counts[col] = int(empty_count)

                if empty_count > 0:
                    issues.append(f"{col} ì»¬ëŸ¼ì— ë¹ˆ ë¬¸ìì—´ {empty_count}ê°œ")
                    self._log(f"  âš ï¸ {col}: ë¹ˆ ë¬¸ìì—´ {empty_count}ê°œ ë°œê²¬")

        return {
            'passed': passed,
            'issues': issues,
            'missing_columns': missing_columns,
            'null_counts': null_counts,
            'empty_counts': empty_counts
        }

    def _validate_semantics(
        self,
        df: pd.DataFrame,
        required_columns: List[str]
    ) -> Dict[str, Any]:
        """
        ì˜ë¯¸ ê²€ì¦ (ì¤‘ë³µ, íŒ¨í„´)

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            required_columns: ê²€ì‚¬í•  ì»¬ëŸ¼

        Returns:
            ì˜ë¯¸ ê²€ì¦ ê²°ê³¼
        """
        # 1. ì „ì²´ í–‰ ì¤‘ë³µ ì²´í¬
        duplicate_rows = df.duplicated().sum()
        self._log(f"  ğŸ“Š ì „ì²´ ì¤‘ë³µ í–‰: {duplicate_rows}ê°œ")

        # 2. ì»¬ëŸ¼ë³„ ì¤‘ë³µ ì²´í¬
        column_duplicates = {}
        for col in required_columns:
            if col in df.columns:
                dup_count = df[col].duplicated().sum()
                column_duplicates[col] = int(dup_count)
                self._log(f"  ğŸ“Š {col} ì¤‘ë³µ: {dup_count}ê°œ")

        # 3. íŒ¨í„´ ê²€ì¦ (dialogue ì»¬ëŸ¼)
        pattern_issues = []
        if 'dialogue' in df.columns:
            # #Person1#, #Person2# íŒ¨í„´ ì²´í¬
            has_person_pattern = df['dialogue'].str.contains(
                r'#Person\d+#',
                regex=True,
                na=False
            )
            invalid_pattern_count = (~has_person_pattern).sum()

            if invalid_pattern_count > 0:
                pattern_issues.append(
                    f"ëŒ€í™” íŒ¨í„´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {invalid_pattern_count}ê°œ"
                )
                self._log(f"  âš ï¸ ëŒ€í™” íŒ¨í„´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {invalid_pattern_count}ê°œ")

        return {
            'duplicate_count': int(duplicate_rows),
            'column_duplicates': column_duplicates,
            'pattern_issues': pattern_issues
        }

    def _validate_statistics(
        self,
        df: pd.DataFrame,
        required_columns: List[str]
    ) -> Dict[str, Any]:
        """
        í†µê³„ ê²€ì¦ (ê¸¸ì´, ë¶„í¬)

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            required_columns: ê²€ì‚¬í•  ì»¬ëŸ¼

        Returns:
            í†µê³„ ê²€ì¦ ê²°ê³¼
        """
        statistics = {}

        for col in required_columns:
            if col in df.columns and df[col].dtype == 'object':
                # ë¬¸ìì—´ ê¸¸ì´ í†µê³„
                lengths = df[col].str.len()

                col_stats = {
                    'mean_length': float(lengths.mean()),
                    'median_length': float(lengths.median()),
                    'min_length': int(lengths.min()),
                    'max_length': int(lengths.max()),
                    'std_length': float(lengths.std())
                }

                statistics[col] = col_stats

                self._log(f"  ğŸ“Š {col} í†µê³„:")
                self._log(f"    - í‰ê· : {col_stats['mean_length']:.1f}")
                self._log(f"    - ì¤‘ì•™ê°’: {col_stats['median_length']:.1f}")
                self._log(f"    - ë²”ìœ„: [{col_stats['min_length']}, {col_stats['max_length']}]")

        return statistics

    def _detect_anomalies(
        self,
        df: pd.DataFrame,
        required_columns: List[str],
        z_threshold: float = 3.0
    ) -> Dict[str, Any]:
        """
        ì´ìƒì¹˜ ê²€ì¶œ (ê¸¸ì´ ê¸°ë°˜)

        Args:
            df: ë°ì´í„°í”„ë ˆì„
            required_columns: ê²€ì‚¬í•  ì»¬ëŸ¼
            z_threshold: Z-score ì„ê³„ê°’

        Returns:
            ì´ìƒì¹˜ ê²€ì¶œ ê²°ê³¼
        """
        anomalies = {}
        total_anomalies = 0

        for col in required_columns:
            if col in df.columns and df[col].dtype == 'object':
                # ë¬¸ìì—´ ê¸¸ì´ë¡œ ì´ìƒì¹˜ ê²€ì¶œ
                lengths = df[col].str.len()
                mean_len = lengths.mean()
                std_len = lengths.std()

                # Z-score ê³„ì‚°
                z_scores = np.abs((lengths - mean_len) / std_len)
                anomaly_mask = z_scores > z_threshold

                anomaly_count = anomaly_mask.sum()
                total_anomalies += anomaly_count

                if anomaly_count > 0:
                    anomaly_indices = df[anomaly_mask].index.tolist()
                    anomaly_lengths = lengths[anomaly_mask].tolist()

                    anomalies[col] = {
                        'count': int(anomaly_count),
                        'indices': anomaly_indices[:10],  # ìµœëŒ€ 10ê°œ
                        'example_lengths': [int(l) for l in anomaly_lengths[:5]]
                    }

                    self._log(f"  âš ï¸ {col}: ì´ìƒì¹˜ {anomaly_count}ê°œ ë°œê²¬")
                    self._log(f"     ì˜ˆì‹œ ê¸¸ì´: {anomalies[col]['example_lengths']}")

        if total_anomalies == 0:
            self._log("  âœ… ì´ìƒì¹˜ ì—†ìŒ")

        return {
            'anomaly_count': total_anomalies,
            'anomalies': anomalies,
            'z_threshold': z_threshold
        }

    def validate_file(
        self,
        file_path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        íŒŒì¼ ê²€ì¦ (CSV)

        Args:
            file_path: CSV íŒŒì¼ ê²½ë¡œ
            **kwargs: validate_dataframe ì¸ì

        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        self._log(f"\níŒŒì¼ ê²€ì¦: {file_path}")

        # CSV ë¡œë“œ
        df = pd.read_csv(file_path)

        # ê²€ì¦ ìˆ˜í–‰
        results = self.validate_dataframe(df, **kwargs)

        # íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        results['file_path'] = file_path

        return results

    def save_results(self, output_path: str):
        """
        ê²€ì¦ ê²°ê³¼ ì €ì¥

        Args:
            output_path: ì €ì¥ ê²½ë¡œ
        """
        if not self.validation_results:
            self._log("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)

        self._log(f"ê²€ì¦ ê²°ê³¼ ì €ì¥: {output_path}")

    def get_summary(self) -> Dict[str, Any]:
        """
        ê²€ì¦ ê²°ê³¼ ìš”ì•½

        Returns:
            ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.validation_results:
            return {'total_validations': 0}

        total_validations = len(self.validation_results)
        passed_count = sum(1 for r in self.validation_results if r['passed'])
        failed_count = total_validations - passed_count

        total_issues = sum(len(r['issues']) for r in self.validation_results)

        return {
            'total_validations': total_validations,
            'passed': passed_count,
            'failed': failed_count,
            'total_issues': total_issues
        }


# ==================== í¸ì˜ í•¨ìˆ˜ ==================== #
def create_validator(logger=None) -> DataQualityValidator:
    """
    Validator ìƒì„± í¸ì˜ í•¨ìˆ˜

    Args:
        logger: Logger ì¸ìŠ¤í„´ìŠ¤

    Returns:
        DataQualityValidator ì¸ìŠ¤í„´ìŠ¤
    """
    return DataQualityValidator(logger=logger)


def quick_validate(
    df: pd.DataFrame,
    required_columns: List[str] = ['dialogue', 'summary']
) -> bool:
    """
    ë¹ ë¥¸ ê²€ì¦ (êµ¬ì¡°ë§Œ)

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        required_columns: í•„ìˆ˜ ì»¬ëŸ¼

    Returns:
        ê²€ì¦ í†µê³¼ ì—¬ë¶€
    """
    validator = DataQualityValidator()
    results = validator.validate_dataframe(
        df,
        required_columns=required_columns,
        check_duplicates=False,
        check_statistics=False,
        check_anomalies=False
    )

    return results['passed']
