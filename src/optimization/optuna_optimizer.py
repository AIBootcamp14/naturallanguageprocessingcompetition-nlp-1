"""
Optuna ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ‹μ¤ν…

PRD 13: Optuna ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ „λµ
"""

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner
from typing import Dict, Any, Optional
import logging
from pathlib import Path
import json
import pandas as pd
from omegaconf import DictConfig, OmegaConf

from ..models import load_model_and_tokenizer
from ..data import DialogueSummarizationDataset
from ..training import create_trainer
from ..checkpoints.optuna_checkpoint import OptunaCheckpointManager


class OptunaOptimizer:
    """
    Optuna ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” ν΄λμ¤

    κΈ°λ¥:
    - NLP νΉν™” ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ³µκ°„ μ •μ
    - Bayesian Optimization (TPE Sampler)
    - Median Prunerλ¥Ό ν†µν• μ΅°κΈ° μΆ…λ£
    - ROUGE μ μ κΈ°λ° μµμ ν™”
    - κ²°κ³Ό μ €μ¥ λ° λ¶„μ„
    """

    def __init__(
        self,
        config: DictConfig,
        train_df: pd.DataFrame,
        eval_df: pd.DataFrame,
        n_trials: int = 50,
        timeout: Optional[int] = None,
        study_name: Optional[str] = None,
        storage: Optional[str] = None,
        direction: str = "maximize",
        logger=None,
        output_dir: Optional[str] = None
    ):
        """
        μ΄κΈ°ν™”

        Args:
            config: κΈ°λ³Έ Config (νƒμƒ‰ κ³µκ°„ μ •μμ— μ‚¬μ©)
            train_df: ν•™μµ λ°μ΄ν„°ν”„λ μ„
            eval_df: κ²€μ¦ λ°μ΄ν„°ν”„λ μ„
            n_trials: μ΄ Trial νμ
            timeout: μµλ€ μ‹¤ν–‰ μ‹κ°„ (μ΄)
            study_name: Study μ΄λ¦„
            storage: Study μ €μ¥μ† (SQLite/PostgreSQL)
            direction: μµμ ν™” λ°©ν–¥ ("maximize" or "minimize")
            logger: Logger μΈμ¤ν„΄μ¤
            output_dir: μ¶λ ¥ λ””λ ‰ν† λ¦¬ (μ²΄ν¬ν¬μΈνΈ μ €μ¥ κ²½λ΅, Noneμ΄λ©΄ config κΈ°λ³Έκ°’ μ‚¬μ©)
        """
        self.config = config
        self.train_df = train_df
        self.eval_df = eval_df
        self.n_trials = n_trials
        self.timeout = timeout
        self.study_name = study_name or f"optuna_study_{config.model.name}"
        self.storage = storage
        self.direction = direction
        self.logger = logger
        self.output_dir = output_dir

        # Optuna Study
        self.study: Optional[optuna.Study] = None

        # Best params μ €μ¥
        self.best_params: Optional[Dict[str, Any]] = None
        self.best_value: Optional[float] = None

        # β… μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬μ μ΄κΈ°ν™”
        if self.output_dir:
            checkpoint_dir = Path(self.output_dir) / "checkpoints"
        else:
            checkpoint_dir = Path("checkpoints")
        self.checkpoint_manager = OptunaCheckpointManager(
            checkpoint_dir=str(checkpoint_dir),
            study_name=self.study_name
        )

        self._log(f"OptunaOptimizer μ΄κΈ°ν™” μ™„λ£")
        self._log(f"  - Study μ΄λ¦„: {self.study_name}")
        self._log(f"  - Trial νμ: {self.n_trials}")
        self._log(f"  - λ°©ν–¥: {self.direction}")
        self._log(f"  - μ²΄ν¬ν¬μΈνΈ: {self.checkpoint_manager.get_checkpoint_path()}")

    def _log(self, msg: str):
        """λ΅κΉ… ν—¬νΌ"""
        if self.logger:
            if hasattr(self.logger, 'write'):
                self.logger.write(msg)
            else:
                self.logger.info(msg)
        else:
            print(msg)

    def create_search_space(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ³µκ°„ μƒμ„±

        Args:
            trial: Optuna Trial κ°μ²΄

        Returns:
            μƒν”λ§λ ν•μ΄νΌνλΌλ―Έν„° λ”•μ…”λ„λ¦¬
        """
        params = {}

        # ν•™μµ νλΌλ―Έν„° (ν•µμ‹¬)
        params['learning_rate'] = trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True)
        params['num_epochs'] = trial.suggest_int('num_epochs', 3, 10)
        params['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)
        params['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)

        # Scheduler
        params['scheduler_type'] = trial.suggest_categorical(
            'scheduler_type',
            ['linear', 'cosine', 'cosine_with_restarts', 'polynomial']
        )

        # Generation νλΌλ―Έν„° (KoBARTμ©)
        params['num_beams'] = trial.suggest_categorical('num_beams', [2, 4, 6, 8])
        params['length_penalty'] = trial.suggest_float('length_penalty', 0.5, 2.0)

        return params

    def objective(self, trial: optuna.Trial) -> float:
        """
        μµμ ν™” λ©μ  ν•¨μ

        Args:
            trial: Optuna Trial κ°μ²΄

        Returns:
            ROUGE μ μ (maximize)
        """
        # 1. ν•μ΄νΌνλΌλ―Έν„° μƒν”λ§
        params = self.create_search_space(trial)

        self._log(f"\n{'='*60}")
        self._log(f"Trial {trial.number} μ‹μ‘")
        self._log(f"νλΌλ―Έν„°: {params}")
        self._log(f"{'='*60}")

        try:
            # 2. Config λ³µμ‚¬ λ° μ—…λ°μ΄νΈ
            config = OmegaConf.to_container(self.config, resolve=True)
            config = OmegaConf.create(config)

            # Training νλΌλ―Έν„° μ—…λ°μ΄νΈ
            if not hasattr(config, 'training'):
                config.training = {}

            config.training.learning_rate = params['learning_rate']
            config.training.epochs = params['num_epochs']
            config.training.warmup_ratio = params['warmup_ratio']
            config.training.weight_decay = params['weight_decay']
            config.training.lr_scheduler_type = params['scheduler_type']

            # μ¶λ ¥ λ””λ ‰ν† λ¦¬ μ„¤μ • (λ…λ Ήν–‰ μΈμκ°€ μ°μ„ )
            if self.output_dir is not None:
                config.training.output_dir = self.output_dir

            # Inference νλΌλ―Έν„° μ—…λ°μ΄νΈ (KoBARTλ” inference μ„Ήμ… μ‚¬μ©)
            if hasattr(config, 'inference'):
                config.inference.num_beams = params['num_beams']
                config.inference.length_penalty = params['length_penalty']

            # WandB λΉ„ν™μ„±ν™”
            if not hasattr(config, 'logging'):
                config.logging = {}
            config.logging.use_wandb = False

            # 3. λ¨λΈ λ° ν† ν¬λ‚μ΄μ € λ΅λ“
            model, tokenizer = load_model_and_tokenizer(config, logger=self.logger)

            # 4. Dataset μƒμ„±
            model_type = config.model.get('type', 'encoder_decoder')

            train_dataset = DialogueSummarizationDataset(
                dialogues=self.train_df['dialogue'].tolist(),
                summaries=self.train_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            eval_dataset = DialogueSummarizationDataset(
                dialogues=self.eval_df['dialogue'].tolist(),
                summaries=self.eval_df['summary'].tolist(),
                tokenizer=tokenizer,
                encoder_max_len=config.tokenizer.encoder_max_len,
                decoder_max_len=config.tokenizer.decoder_max_len,
                preprocess=True,
                model_type=model_type
            )

            # 5. Trainer μƒμ„±
            trainer = create_trainer(
                config=config,
                model=model,
                tokenizer=tokenizer,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                use_wandb=False,
                logger=self.logger
            )

            # 6. ν•™μµ
            trainer.train()

            # 7. ν‰κ°€
            metrics = trainer.evaluate()

            # 8. ROUGE-L F1 μ¶”μ¶
            rouge_l_f1 = 0.0
            # λ‹¤μ–‘ν• ν‚¤ ν•μ‹ μ‹λ„ (λ€μ†λ¬Έμ κµ¬λ¶„)
            possible_keys = [
                'eval_rougeL',      # HuggingFace κΈ°λ³Έ ν•μ‹
                'eval_rouge_l',     # μ†λ¬Έμ ν•μ‹
                'eval_rouge_l_f1',  # F1 λ…μ‹ ν•μ‹
                'rougeL',           # prefix μ—†λ” ν•μ‹
                'rouge_l',
                'rouge_l_f1'
            ]

            for key in possible_keys:
                if key in metrics:
                    rouge_l_f1 = metrics[key]
                    self._log(f"  β†’ λ©”νΈλ¦­ '{key}' μ‚¬μ©: {rouge_l_f1:.4f}")
                    break

            if rouge_l_f1 == 0.0:
                # λ©”νΈλ¦­μ„ μ°Ύμ§€ λ»ν• κ²½μ° λ””λ²„κΉ… μ •λ³΄ μ¶λ ¥
                self._log(f"  β οΈ  ROUGE-L λ©”νΈλ¦­μ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤")
                self._log(f"  μ‚¬μ© κ°€λ¥ν• λ©”νΈλ¦­: {list(metrics.keys())}")

            self._log(f"Trial {trial.number} μ™„λ£")
            self._log(f"  - ROUGE-L F1: {rouge_l_f1:.4f}")

            # 9. μ¤‘κ°„ κ²°κ³Ό λ³΄κ³ 
            trial.report(rouge_l_f1, step=params['num_epochs'])

            # 10. Pruning μ²΄ν¬
            if trial.should_prune():
                self._log(f"Trial {trial.number} Pruned!")
                raise optuna.TrialPruned()

            return rouge_l_f1

        except Exception as e:
            self._log(f"Trial {trial.number} μ‹¤ν¨: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            raise optuna.TrialPruned()

    def optimize(self) -> optuna.Study:
        """
        ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” μ‹¤ν–‰ (μ²΄ν¬ν¬μΈνΈ μ§€μ›)

        Returns:
            μ™„λ£λ Optuna Study
        """
        self._log(f"\n{'='*70}")
        self._log(f"Optuna μµμ ν™” μ‹μ‘")
        self._log(f"{'='*70}")

        # 1. Sampler λ° Pruner μ„¤μ •
        sampler = TPESampler(seed=42)
        pruner = MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=3,
            interval_steps=1
        )

        # β… 2. μ²΄ν¬ν¬μΈνΈμ—μ„ Study λ³µμ› λλ” μƒλ΅ μƒμ„±
        self.study, completed_trials = self.checkpoint_manager.resume_study(
            sampler=sampler,
            pruner=pruner,
            direction=self.direction
        )

        if completed_trials > 0:
            self._log(f"π”„ μ²΄ν¬ν¬μΈνΈμ—μ„ Resume: {completed_trials}/{self.n_trials} Trial μ΄λ―Έ μ™„λ£")
            progress = self.checkpoint_manager.get_progress()
            if progress:
                self._log(f"  - ν„μ¬ μµμ κ°’: {progress['best_value']:.4f}")
                self._log(f"  - λ§μ§€λ§‰ μ €μ¥: {progress['timestamp']}")

        remaining_trials = self.n_trials - completed_trials
        if remaining_trials <= 0:
            self._log(f"β… λ¨λ“  Trial μ™„λ£λ¨. κ±΄λ„λ€.")
            self.best_params = self.study.best_params if self.study.best_trial else {}
            self.best_value = self.study.best_value if self.study.best_trial else 0.0
            return self.study

        self._log(f"  - λ‚¨μ€ Trial: {remaining_trials}κ°")

        # β… 3. Trial μ½λ°±μ— μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ¶”κ°€
        def trial_callback(study, trial):
            """Trial μ™„λ£λ§λ‹¤ μ²΄ν¬ν¬μΈνΈ μ €μ¥"""
            self.checkpoint_manager.save_checkpoint(study, trial.number)
            self._log(f"π’Ύ Trial {trial.number} μ²΄ν¬ν¬μΈνΈ μ €μ¥")

        # 4. μµμ ν™” μ‹¤ν–‰
        self.study.optimize(
            self.objective,
            n_trials=remaining_trials,
            timeout=self.timeout,
            show_progress_bar=True,
            callbacks=[trial_callback]  # β… μ²΄ν¬ν¬μΈνΈ μ½λ°± μ¶”κ°€
        )

        # 4. μµμ  νλΌλ―Έν„° μ €μ¥
        try:
            self.best_params = self.study.best_params
            self.best_value = self.study.best_value
        except ValueError as e:
            self._log(f"μ™„λ£λ trialμ΄ μ—†μµλ‹λ‹¤: {e}")
            self.best_params = {}
            self.best_value = 0.0

        self._log(f"\n{'='*70}")
        self._log(f"Optuna μµμ ν™” μ™„λ£")
        self._log(f"{'='*70}")
        self._log(f"μµμ  ROUGE-L F1: {self.best_value:.4f}")
        self._log(f"μµμ  νλΌλ―Έν„°:")
        for key, value in self.best_params.items():
            self._log(f"  - {key}: {value}")

        return self.study

    def get_best_params(self) -> Dict[str, Any]:
        """μµμ  ν•μ΄νΌνλΌλ―Έν„° λ°ν™"""
        if self.best_params is None:
            raise ValueError("optimize()λ¥Ό λ¨Όμ € μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤")
        return self.best_params

    def get_best_value(self) -> float:
        """μµμ  ROUGE μ μ λ°ν™"""
        if self.best_value is None:
            raise ValueError("optimize()λ¥Ό λ¨Όμ € μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤")
        return self.best_value

    def save_results(self, output_path: str):
        """μµμ ν™” κ²°κ³Ό μ €μ¥"""
        if self.study is None:
            raise ValueError("optimize()λ¥Ό λ¨Όμ € μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤")

        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)

        # 1. Best params μ €μ¥
        best_params_path = output_path / "best_params.json"
        with open(best_params_path, 'w', encoding='utf-8') as f:
            json.dump({
                'best_params': self.best_params,
                'best_value': self.best_value,
                'n_trials': len(self.study.trials)
            }, f, indent=2, ensure_ascii=False)

        self._log(f"μµμ  νλΌλ―Έν„° μ €μ¥: {best_params_path}")

        # 2. All trials μ €μ¥
        trials_df = self.study.trials_dataframe()
        trials_csv_path = output_path / "all_trials.csv"
        trials_df.to_csv(trials_csv_path, index=False, encoding='utf-8')

        self._log(f"μ „μ²΄ Trial μ €μ¥: {trials_csv_path}")

        # 3. Study ν†µκ³„
        stats = {
            'study_name': self.study_name,
            'n_trials': len(self.study.trials),
            'n_completed': len([t for t in self.study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
            'n_pruned': len([t for t in self.study.trials if t.state == optuna.trial.TrialState.PRUNED]),
            'n_failed': len([t for t in self.study.trials if t.state == optuna.trial.TrialState.FAIL]),
            'best_value': self.best_value
        }

        if self.best_params:
            stats['best_trial_number'] = self.study.best_trial.number

        stats_path = output_path / "study_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        self._log(f"Study ν†µκ³„ μ €μ¥: {stats_path}")
        self._log(f"  - μ™„λ£: {stats['n_completed']}")
        self._log(f"  - Pruned: {stats['n_pruned']}")
        self._log(f"  - μ‹¤ν¨: {stats['n_failed']}")

    def plot_optimization_history(self, output_path: str):
        """μµμ ν™” νμ¤ν† λ¦¬ μ‹κ°ν™”"""
        if self.study is None:
            raise ValueError("optimize()λ¥Ό λ¨Όμ € μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤")

        try:
            from optuna.visualization import (
                plot_optimization_history,
                plot_param_importances,
                plot_parallel_coordinate
            )

            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            # 1. Optimization history
            fig = plot_optimization_history(self.study)
            fig.write_html(str(output_path / "optimization_history.html"))

            # 2. Parameter importances
            fig = plot_param_importances(self.study)
            fig.write_html(str(output_path / "param_importances.html"))

            # 3. Parallel coordinate
            fig = plot_parallel_coordinate(self.study)
            fig.write_html(str(output_path / "parallel_coordinate.html"))

            self._log(f"μ‹κ°ν™” μ €μ¥ μ™„λ£: {output_path}")

        except ImportError:
            self._log("plotlyκ°€ μ„¤μΉλμ§€ μ•μ•„ μ‹κ°ν™”λ¥Ό κ±΄λ„λλ‹λ‹¤")


def create_optuna_optimizer(
    config: DictConfig,
    train_df: pd.DataFrame,
    eval_df: pd.DataFrame,
    n_trials: int = 50,
    **kwargs
) -> OptunaOptimizer:
    """
    OptunaOptimizer νΈμ μƒμ„± ν•¨μ

    Args:
        config: Config
        train_df: ν•™μµ λ°μ΄ν„°ν”„λ μ„
        eval_df: κ²€μ¦ λ°μ΄ν„°ν”„λ μ„
        n_trials: Trial νμ
        **kwargs: μ¶”κ°€ νλΌλ―Έν„°

    Returns:
        OptunaOptimizer μΈμ¤ν„΄μ¤
    """
    return OptunaOptimizer(
        config=config,
        train_df=train_df,
        eval_df=eval_df,
        n_trials=n_trials,
        **kwargs
    )
