# Causal LM (LLM) 기본 설정
# PRD 08: LLM 파인튜닝 전략

model:
  type: causal_lm
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"  # 기본 모델

# LoRA 설정
lora:
  r: 16  # LoRA rank
  alpha: 32  # LoRA alpha (일반적으로 r * 2)
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  dropout: 0.05
  use_qlora: true  # QLoRA (4-bit 양자화) 사용

# 토크나이저 설정 (PRD 08: Prompt truncation 방지)
tokenizer:
  encoder_max_len: 1024  # 512 → 1024 (필수!)
  decoder_max_len: 200   # 100 → 200 (여유)

# 학습 설정
training:
  epochs: 3
  batch_size: 8
  gradient_accumulation_steps: 8  # effective batch = 8 * 8 = 64
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.1
  max_grad_norm: 1.2  # PRD 08: 최적화
  gradient_checkpointing: true
  save_total_limit: 2
  logging_steps: 10
  num_workers: 4
  fp16: false  # QLoRA와 AMP GradScaler 호환성 문제로 비활성화
  bf16: false  # QLoRA와 AMP GradScaler 호환성 문제로 비활성화

# 추론 설정
inference:
  batch_size: 8
  generate_max_length: 100                              # 생성 최대 길이 (베이스라인 호환)
  temperature: 0.7
  do_sample: true
  top_p: 0.9
  top_k: 50
  num_beams: 1  # Sampling 사용 시 1

# LLM Dataset 설정
dataset:
  format_type: "instruction"  # "instruction" or "chat"
  use_instruction_augmentation: false  # Instruction Tuning 사용 여부

# WandB 설정
wandb:
  enabled: true
  project: "nlp-competition"
  entity: "ieyeppo"

# 실험 설정
experiment:
  name: "llm_baseline"
  tags:
    - "llm"
    - "causal_lm"
    - "qlora"
