# ==================== Encoder-Decoder 모델 공통 설정 ==================== #
# BART, T5 등 Encoder-Decoder 아키텍처 모델의 기본 설정

# ---------------------- 모델 설정 ---------------------- #
model:
  type: "encoder_decoder"                               # 모델 타입
  architecture: "bart"                                  # 모델 아키텍처


# ---------------------- 토크나이저 설정 ---------------------- #
tokenizer:
  encoder_max_len: 512                                  # 인코더 최대 길이
  decoder_max_len: 100                                  # 디코더 최대 길이

  # 특수 토큰 리스트
  special_tokens:
    # 화자 토큰
    - '#Person1#'
    - '#Person2#'
    - '#Person3#'
    - '#Person4#'
    - '#Person5#'
    - '#Person6#'
    - '#Person7#'
    # PII 마스킹 토큰
    - '#PhoneNumber#'
    - '#Address#'
    - '#DateOfBirth#'
    - '#PassportNumber#'
    - '#SSN#'
    - '#CardNumber#'
    - '#CarNumber#'
    - '#Email#'


# ---------------------- 학습 설정 ---------------------- #
training:
  # 기본 파라미터
  num_train_epochs: 20                                  # 학습 에포크 수
  learning_rate: 1.0e-05                                # 학습률
  per_device_train_batch_size: 50                       # 학습 배치 크기
  per_device_eval_batch_size: 32                        # 평가 배치 크기
  gradient_accumulation_steps: 1                        # 그래디언트 누적 스텝

  # 최적화 설정
  optim: "adamw_torch"                                  # 옵티마이저
  weight_decay: 0.01                                    # 가중치 감쇠
  max_grad_norm: 1.0                                    # 그래디언트 클리핑
  warmup_ratio: 0.1                                     # 워밍업 비율
  lr_scheduler_type: "cosine"                           # 학습률 스케줄러

  # 혼합 정밀도 학습
  fp16: true                                            # FP16 사용 여부
  bf16: false                                           # BF16 사용 여부
  gradient_checkpointing: false                         # 그래디언트 체크포인팅

  # Early Stopping
  early_stopping_patience: 3                            # Early Stopping 인내심
  early_stopping_threshold: 0.001                       # Early Stopping 임계값

  # 체크포인트 저장
  save_strategy: "epoch"                                # 저장 전략
  save_total_limit: 2                                   # 저장할 체크포인트 수
  load_best_model_at_end: true                          # 최고 모델 로드
  metric_for_best_model: "rouge_sum"                    # 최고 모델 기준 메트릭
  greater_is_better: true                               # 메트릭이 클수록 좋음

  # 평가 및 메트릭
  evaluation_strategy: "epoch"                          # 평가 전략
  predict_with_generate: true                           # 생성 기반 예측


# ---------------------- 추론 설정 ---------------------- #
inference:
  batch_size: 32                                        # 추론 배치 크기

  # 생성 전략
  num_beams: 5                                          # 빔 서치 빔 개수 (품질 향상)
  early_stopping: true                                  # 생성 조기 종료

  # ⚠️ 중요: 길이 제어 (max_length 대신 max_new_tokens 사용)
  generate_max_new_tokens: 200                          # 생성할 최대 토큰 수 (한국어 권장: 200)
  generate_min_new_tokens: 30                           # 생성할 최소 토큰 수 (문장 끊김 방지)
  generate_max_length: 512                              # 전체 최대 길이 (input+output, 여유있게 설정)

  # 반복 방지
  no_repeat_ngram_size: 3                               # 반복 방지 n-gram 크기 (3 권장)
  repetition_penalty: 1.2                               # 반복 억제 강도 (1.0=없음, >1=억제)
  length_penalty: 1.0                                   # 길이 페널티 (1.0=중립)

  # 후처리
  remove_tokens:                                        # 제거할 토큰 리스트
    - '<usr>'
    - '<s>'
    - '</s>'
    - '<pad>'
