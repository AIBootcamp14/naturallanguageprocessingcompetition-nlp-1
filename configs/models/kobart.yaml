# ==================== KoBART 모델 설정 ==================== #
# digit82/kobart-summarization 모델 전용 설정
# 대회 베이스라인 검증된 파라미터

# ---------------------- 모델 설정 ---------------------- #
model:
  name: "kobart"                                        # 모델 이름
  checkpoint: "digit82/kobart-summarization"            # HuggingFace 모델 체크포인트
  type: "encoder_decoder"                               # 모델 타입
  architecture: "bart"                                  # 모델 아키텍처


# ---------------------- 학습 설정 (베이스라인 검증값) ---------------------- #
training:
  # 베이스라인 핵심 파라미터
  per_device_train_batch_size: 50                       # 베이스라인: 큰 배치 크기가 핵심
  learning_rate: 1.0e-05                                # 베이스라인: 1e-5가 최적


# ---------------------- 추론 설정 (베이스라인 검증값) ---------------------- #
inference:
  batch_size: 32                                        # 추론 배치 크기

  # 생성 전략
  num_beams: 5                                          # 빔 서치 빔 개수 (품질 향상)
  early_stopping: true                                  # 생성 조기 종료

  # ⚠️ 중요: 길이 제어 (max_length 대신 max_new_tokens 사용)
  generate_max_new_tokens: 200                          # 생성할 최대 토큰 수 (한국어 권장: 200)
  generate_min_new_tokens: 30                           # 생성할 최소 토큰 수 (문장 끊김 방지)
  generate_max_length: 512                              # 전체 최대 길이 (input+output, 여유있게 설정)

  # 반복 방지
  no_repeat_ngram_size: 3                               # 반복 방지 n-gram 크기 (3 권장)
  repetition_penalty: 1.2                               # 반복 억제 강도 (1.0=없음, >1=억제)
  length_penalty: 1.0                                   # 길이 페널티 (1.0=중립)
