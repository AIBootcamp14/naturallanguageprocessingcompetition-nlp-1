# ==================== KoBART 모델 설정 ==================== #
# digit82/kobart-summarization 모델 전용 설정
# 대회 베이스라인 검증된 파라미터

# ---------------------- 모델 설정 ---------------------- #
model:
  name: "kobart"                                        # 모델 이름
  checkpoint: "digit82/kobart-summarization"            # HuggingFace 모델 체크포인트
  type: "encoder_decoder"                               # 모델 타입
  architecture: "bart"                                  # 모델 아키텍처


# ---------------------- 학습 설정 (Optuna 최적화 값) ---------------------- #
training:
  # Optuna 최적화 파라미터 (20251014 실험: ROUGE-L F1 0.4616)
  per_device_train_batch_size: 50                       # 베이스라인: 큰 배치 크기가 핵심
  learning_rate: 9.138518360133624e-05                  # Optuna 최적값: 기존 대비 약 1.8배 증가


# ---------------------- 추론 설정 (Optuna 최적화 값) ---------------------- #
inference:
  batch_size: 32                                        # 추론 배치 크기

  # 생성 전략 (Optuna 최적화)
  num_beams: 4                                          # Optuna 최적값: 4 (5→4, 속도↑ 품질 유지)
  early_stopping: true                                  # 생성 조기 종료

  # ⚠️ 중요: 길이 제어 (max_length 대신 max_new_tokens 사용)
  generate_max_new_tokens: 100                          # 생성할 최대 토큰 수 (최적값: 100, 적정 길이 유지)
  generate_min_new_tokens: 30                           # 생성할 최소 토큰 수 (문장 끊김 방지)
  generate_max_length: 512                              # 전체 최대 길이 (input+output, 여유있게 설정)

  # 반복 방지
  no_repeat_ngram_size: 3                               # 반복 방지 n-gram 크기 (최적값: 3)
  repetition_penalty: 1.5                               # 반복 억제 강도 (최적값: 1.5, 적절한 억제)
  length_penalty: 0.9383576982529792                    # Optuna 최적값: 0.938 (1.0→0.938, 약간 짧게 유도)

  # HuggingFace 사전학습 모델 보정 (PRD 04, 12)
  pretrained_correction:
    enabled: false                                      # 기본값: 비활성화 (명령행 --use_pretrained_correction으로 활성화)
    models:                                             # 보정에 사용할 HuggingFace 모델 리스트
      - "gogamza/kobart-base-v2"                        # KoBART 사전학습 (일반 요약)
      - "digit82/kobart-summarization"                  # KoBART 요약 특화 (대화 요약)
    strategy: "quality_based"                           # 보정 전략 (quality_based 추천, threshold, voting, weighted)
    threshold: 0.3                                      # 품질 임계값 (0.0~1.0, 낮을수록 엄격)

  # Solar API 앙상블 (PRD 09)
  solar_api:
    enabled: false                                      # 기본값: 비활성화 (명령행 --use_solar_api로 활성화)
    model: "solar-1-mini-chat"                          # Solar 모델 선택
    temperature: 0.2                                    # 생성 온도 (낮을수록 일관성)
    top_p: 0.3                                          # Top-p (낮을수록 일관성)
    batch_size: 10                                      # 배치 크기 (Rate limit 고려)
    delay: 1.0                                          # 배치 간 대기 시간 (초)
