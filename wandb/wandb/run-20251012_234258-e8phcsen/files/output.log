2025-10-12 23:42:59 | wandb: Detected [openai] in use.
2025-10-12 23:42:59 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:42:59 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:42:59 | 📋 실험명: 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:59 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:42:59 | 학습 진행 중...
2025-10-12 23:42:59 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:43:00 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:43:12 | ❌ llama-3.2-korean-3b 학습 실패: OutOfMemoryError: CUDA out of memory. Tried to allocate 15.66 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 35.47 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:12 | 오류 로그 저장: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/llama-3.2-korean-3b_error.log
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | 모델 3/6: qwen3-4b
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | 모델 타입: causal_lm
2025-10-12 23:43:12 | Loading Causal LM: Qwen/Qwen3-4B-Instruct-2507
2025-10-12 23:43:12 | 모델 로딩 중...
2025-10-12 23:43:12 | Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
2025-10-12 23:43:15 | Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.71s/it]
2025-10-12 23:43:20 | Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.21s/it]
2025-10-12 23:43:20 | Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.68s/it]
2025-10-12 23:43:21 | 토크나이저 로딩 중...
2025-10-12 23:43:21 | LoRA 설정 적용 중...
2025-10-12 23:43:21 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:43:22 | ✅ LoRA 적용 완료
2025-10-12 23:43:22 | 학습 가능 파라미터: 33,030,144 (0.81%)
2025-10-12 23:43:22 | 전체 파라미터: 4,055,498,240
2025-10-12 23:43:22 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 23:43:22 | ✅ Gradient Checkpointing 활성화
2025-10-12 23:43:22 | ✅ Causal LM 로드 완료
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | 모델 학습 시작
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | WandB 로그인 상태: ieyeppo-job
2025-10-12 23:43:22 | wandb: Finishing previous runs because reinit is set to True.
