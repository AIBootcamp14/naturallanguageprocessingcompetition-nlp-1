2025-10-12 23:42:59 | wandb: Detected [openai] in use.
2025-10-12 23:42:59 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:42:59 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:42:59 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2342-llama_3.2_3b_qlora
2025-10-12 23:42:59 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/e8phcsen
2025-10-12 23:42:59 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:42:59 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:42:59 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:42:59 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:43:00 | 0%|          | 0/63 [00:00<?, ?it/s]
2025-10-12 23:43:12 | âŒ llama-3.2-korean-3b í•™ìŠµ ì‹¤íŒ¨: OutOfMemoryError: CUDA out of memory. Tried to allocate 15.66 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 35.47 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:12 | ì˜¤ë¥˜ ë¡œê·¸ ì €ìž¥: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/llama-3.2-korean-3b_error.log
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | ëª¨ë¸ 3/6: qwen3-4b
2025-10-12 23:43:12 | ==================================================
2025-10-12 23:43:12 | ëª¨ë¸ íƒ€ìž…: causal_lm
2025-10-12 23:43:12 | Loading Causal LM: Qwen/Qwen3-4B-Instruct-2507
2025-10-12 23:43:12 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 23:43:12 | Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
2025-10-12 23:43:15 | Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.71s/it]
2025-10-12 23:43:20 | Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:07<00:04,  4.21s/it]
2025-10-12 23:43:20 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.68s/it]
2025-10-12 23:43:21 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 23:43:21 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 23:43:21 | ðŸ” ìžë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:43:22 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 23:43:22 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 33,030,144 (0.81%)
2025-10-12 23:43:22 | ì „ì²´ íŒŒë¼ë¯¸í„°: 4,055,498,240
2025-10-12 23:43:22 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 23:43:22 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 23:43:22 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | ëª¨ë¸ í•™ìŠµ ì‹œìž‘
2025-10-12 23:43:22 | ============================================================
2025-10-12 23:43:22 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-12 23:43:22 | wandb: Finishing previous runs because reinit is set to True.
