2025-10-15 04:40:25 | 📋 실험명: 1015-0440-kfold
2025-10-15 04:40:25 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/c4n143bk
2025-10-15 04:40:25 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 04:40:25 | 학습 진행 중...
2025-10-15 04:41:01 | {'loss': 2.0478, 'grad_norm': 4.035416603088379, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 04:41:35 | {'loss': 1.599, 'grad_norm': 4.239953517913818, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 04:42:10 | {'loss': 1.5374, 'grad_norm': 3.9472270011901855, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 04:42:44 | {'loss': 1.5318, 'grad_norm': 3.790060520172119, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 04:43:19 | {'loss': 1.5175, 'grad_norm': 3.4843742847442627, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 04:43:53 | {'loss': 1.4901, 'grad_norm': 3.245954990386963, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 04:50:46 | {'eval_loss': 1.4232920408248901, 'eval_rouge1': 0.387922114132403, 'eval_rouge2': 0.24568756749645132, 'eval_rougeL': 0.3809945673182335, 'eval_rouge_sum': 1.014604248947088, 'eval_runtime': 403.862, 'eval_samples_per_second': 6.168, 'eval_steps_per_second': 0.386, 'epoch': 1.0}
2025-10-15 04:51:15 | {'loss': 1.2763, 'grad_norm': 3.3582286834716797, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 04:51:50 | {'loss': 1.2201, 'grad_norm': 4.455452919006348, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 04:52:25 | {'loss': 1.2304, 'grad_norm': 3.5214297771453857, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 04:53:00 | {'loss': 1.206, 'grad_norm': 2.934473991394043, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 04:53:35 | {'loss': 1.2196, 'grad_norm': 4.211511135101318, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 04:54:10 | {'loss': 1.2396, 'grad_norm': 3.0448882579803467, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 05:00:26 | {'eval_loss': 1.375260829925537, 'eval_rouge1': 0.41445964624834153, 'eval_rouge2': 0.260801557968608, 'eval_rougeL': 0.40639291025288576, 'eval_rouge_sum': 1.0816541144698353, 'eval_runtime': 359.5581, 'eval_samples_per_second': 6.928, 'eval_steps_per_second': 0.434, 'epoch': 2.0}
2025-10-15 05:00:40 | {'loss': 1.0158, 'grad_norm': 2.9213600158691406, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 05:01:01 | {'loss': 0.8659, 'grad_norm': 2.9645509719848633, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 05:01:22 | {'loss': 0.889, 'grad_norm': 3.4673843383789062, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 05:01:43 | {'loss': 0.8687, 'grad_norm': 3.0304222106933594, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 05:02:03 | {'loss': 0.8977, 'grad_norm': 3.1289420127868652, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 05:02:24 | {'loss': 0.8912, 'grad_norm': 2.910853624343872, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 05:08:11 | {'eval_loss': 1.4098237752914429, 'eval_rouge1': 0.42206420600100036, 'eval_rouge2': 0.26855454924750455, 'eval_rougeL': 0.41477267695378967, 'eval_rouge_sum': 1.1053914322022946, 'eval_runtime': 332.1148, 'eval_samples_per_second': 7.5, 'eval_steps_per_second': 0.47, 'epoch': 3.0}
2025-10-15 05:08:22 | {'loss': 0.7972, 'grad_norm': 2.7424421310424805, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 05:08:43 | {'loss': 0.5999, 'grad_norm': 3.509894847869873, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 05:09:03 | {'loss': 0.6158, 'grad_norm': 4.322897434234619, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 05:09:25 | {'loss': 0.6265, 'grad_norm': 3.5287036895751953, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 05:09:45 | {'loss': 0.6302, 'grad_norm': 3.2348978519439697, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 05:10:06 | {'loss': 0.6495, 'grad_norm': 3.0103983879089355, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 05:16:01 | {'eval_loss': 1.4935805797576904, 'eval_rouge1': 0.4486217368374123, 'eval_rouge2': 0.29024598270744917, 'eval_rougeL': 0.4406872614543326, 'eval_rouge_sum': 1.179554980999194, 'eval_runtime': 335.0988, 'eval_samples_per_second': 7.434, 'eval_steps_per_second': 0.466, 'epoch': 4.0}
2025-10-15 05:16:06 | {'loss': 0.6344, 'grad_norm': 2.3463149070739746, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 05:16:27 | {'loss': 0.4121, 'grad_norm': 2.801440477371216, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 05:16:48 | {'loss': 0.4308, 'grad_norm': 2.9572272300720215, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 05:17:09 | {'loss': 0.4391, 'grad_norm': 3.3030176162719727, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 05:17:29 | {'loss': 0.4479, 'grad_norm': 2.5960564613342285, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 05:17:50 | {'loss': 0.4505, 'grad_norm': 2.6499838829040527, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 05:18:12 | {'loss': 0.4565, 'grad_norm': 3.1763360500335693, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 05:23:47 | {'eval_loss': 1.6073942184448242, 'eval_rouge1': 0.44293402153236777, 'eval_rouge2': 0.28199826529079086, 'eval_rougeL': 0.43473083022748754, 'eval_rouge_sum': 1.1596631170506462, 'eval_runtime': 332.367, 'eval_samples_per_second': 7.495, 'eval_steps_per_second': 0.469, 'epoch': 5.0}
2025-10-15 05:24:07 | {'loss': 0.312, 'grad_norm': 2.199693202972412, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 05:24:28 | {'loss': 0.2968, 'grad_norm': 2.6315555572509766, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 05:24:50 | {'loss': 0.3014, 'grad_norm': 2.7056884765625, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 05:25:10 | {'loss': 0.3074, 'grad_norm': 2.511983871459961, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 05:25:31 | {'loss': 0.3152, 'grad_norm': 2.757821798324585, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 05:25:53 | {'loss': 0.318, 'grad_norm': 2.8788180351257324, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 05:32:44 | {'eval_loss': 1.6705337762832642, 'eval_rouge1': 0.45055482279404624, 'eval_rouge2': 0.29055049003283456, 'eval_rougeL': 0.44064968680799216, 'eval_rouge_sum': 1.1817549996348728, 'eval_runtime': 403.7041, 'eval_samples_per_second': 6.17, 'eval_steps_per_second': 0.386, 'epoch': 6.0}
2025-10-15 05:33:09 | {'loss': 0.2446, 'grad_norm': 2.0208845138549805, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 05:33:44 | {'loss': 0.208, 'grad_norm': 2.2084391117095947, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 05:34:19 | {'loss': 0.212, 'grad_norm': 1.8786178827285767, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 05:34:55 | {'loss': 0.2119, 'grad_norm': 2.269291400909424, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 05:35:30 | {'loss': 0.2146, 'grad_norm': 2.28810715675354, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 05:36:05 | {'loss': 0.2162, 'grad_norm': 2.432166576385498, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 05:43:09 | {'eval_loss': 1.7491995096206665, 'eval_rouge1': 0.45702157295261037, 'eval_rouge2': 0.2941804542003041, 'eval_rougeL': 0.4484521270920434, 'eval_rouge_sum': 1.199654154244958, 'eval_runtime': 402.8713, 'eval_samples_per_second': 6.183, 'eval_steps_per_second': 0.387, 'epoch': 7.0}
2025-10-15 05:43:25 | {'loss': 0.1905, 'grad_norm': 1.8230026960372925, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 05:44:01 | {'loss': 0.1438, 'grad_norm': 2.1577975749969482, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 05:44:36 | {'loss': 0.1483, 'grad_norm': 1.940536618232727, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 05:45:11 | {'loss': 0.1509, 'grad_norm': 2.2261221408843994, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 05:45:46 | {'loss': 0.1515, 'grad_norm': 1.8062381744384766, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 05:46:21 | {'loss': 0.1494, 'grad_norm': 2.52072811126709, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 05:53:37 | {'eval_loss': 1.8069485425949097, 'eval_rouge1': 0.4533288041816152, 'eval_rouge2': 0.2938997906151234, 'eval_rougeL': 0.4442749902533058, 'eval_rouge_sum': 1.1915035850500444, 'eval_runtime': 406.5459, 'eval_samples_per_second': 6.127, 'eval_steps_per_second': 0.384, 'epoch': 8.0}
2025-10-15 05:53:45 | {'loss': 0.1464, 'grad_norm': 1.660768747329712, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 05:54:20 | {'loss': 0.1041, 'grad_norm': 1.4367783069610596, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 05:54:54 | {'loss': 0.1049, 'grad_norm': 1.6490882635116577, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 05:55:29 | {'loss': 0.1088, 'grad_norm': 1.7106126546859741, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 05:56:04 | {'loss': 0.1069, 'grad_norm': 1.7130498886108398, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 05:56:41 | {'loss': 0.1045, 'grad_norm': 1.7856037616729736, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 05:57:16 | {'loss': 0.1044, 'grad_norm': 1.562135934829712, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 06:04:36 | {'eval_loss': 1.8544440269470215, 'eval_rouge1': 0.461430214454143, 'eval_rouge2': 0.29523202607302135, 'eval_rougeL': 0.4510049179679937, 'eval_rouge_sum': 1.2076671584951582, 'eval_runtime': 437.2782, 'eval_samples_per_second': 5.697, 'eval_steps_per_second': 0.357, 'epoch': 9.0}
2025-10-15 06:05:11 | {'loss': 0.0823, 'grad_norm': 1.7239741086959839, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 06:05:46 | {'loss': 0.0804, 'grad_norm': 1.9269459247589111, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 06:06:21 | {'loss': 0.0789, 'grad_norm': 1.3032562732696533, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 06:06:56 | {'loss': 0.0775, 'grad_norm': 1.5978825092315674, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 06:07:32 | {'loss': 0.0806, 'grad_norm': 1.485266923904419, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 06:08:08 | {'loss': 0.0759, 'grad_norm': 1.4631805419921875, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 06:15:33 | {'eval_loss': 1.8849741220474243, 'eval_rouge1': 0.45810598155904086, 'eval_rouge2': 0.294304728938613, 'eval_rougeL': 0.4487481285868275, 'eval_rouge_sum': 1.2011588390844814, 'eval_runtime': 434.4455, 'eval_samples_per_second': 5.734, 'eval_steps_per_second': 0.359, 'epoch': 10.0}
2025-10-15 06:15:36 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 06:15:36 | {'train_runtime': 5709.8089, 'train_samples_per_second': 17.454, 'train_steps_per_second': 1.091, 'train_loss': 0.563492120288157, 'epoch': 10.0}
2025-10-15 06:15:36 | 최종 모델 저장 중...
2025-10-15 06:15:37 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_3/kfold/final_model
2025-10-15 06:15:37 | 최종 평가 중...
2025-10-15 06:22:50 | 최종 평가 결과:
2025-10-15 06:22:50 | eval_rouge1: 0.4614
2025-10-15 06:22:50 | eval_rouge2: 0.2952
2025-10-15 06:22:50 | eval_rougeL: 0.4510
2025-10-15 06:22:50 | eval_rouge_sum: 1.2077
