2025-10-12 23:43:24 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2343-qwen3_4b_qlora
2025-10-12 23:43:24 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jl8vntvn
2025-10-12 23:43:24 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:43:24 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:43:24 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:43:24 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
2025-10-12 23:43:24 | 0%|          | 0/63 [00:24<?, ?it/s]
2025-10-12 23:43:39 | âŒ qwen3-4b í•™ìŠµ ì‹¤íŒ¨: OutOfMemoryError: CUDA out of memory. Tried to allocate 18.55 GiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-10-12 23:43:39 | ì˜¤ë¥˜ ë¡œê·¸ ì €ìž¥: experiments/20251012/20251012_234209_test_full_pipeline_batch32/errors/qwen3-4b_error.log
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | ëª¨ë¸ 4/6: solar-10.7b
2025-10-12 23:43:39 | ==================================================
2025-10-12 23:43:39 | ëª¨ë¸ íƒ€ìž…: causal_lm
2025-10-12 23:43:39 | Loading Causal LM: upstage/solar-10.7b-instruct-v1.0
2025-10-12 23:43:39 | ëª¨ë¸ ë¡œë”© ì¤‘...
2025-10-12 23:43:40 | Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
2025-10-12 23:43:51 | Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:11<00:46, 11.57s/it]
2025-10-12 23:44:06 | Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:26<00:39, 13.32s/it]
2025-10-12 23:44:11 | Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:31<00:19,  9.60s/it]
2025-10-12 23:44:16 | Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:35<00:07,  7.64s/it]
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:37<00:00,  5.29s/it]
2025-10-12 23:44:17 | Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:37<00:00,  7.41s/it]
2025-10-12 23:44:18 | WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.
2025-10-12 23:44:18 | í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...
2025-10-12 23:44:19 | LoRA ì„¤ì • ì ìš© ì¤‘...
2025-10-12 23:44:19 | ðŸ” ìžë™ íƒì§€ëœ target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:44:19 | âœ… LoRA ì ìš© ì™„ë£Œ
2025-10-12 23:44:19 | í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 62,914,560 (0.58%)
2025-10-12 23:44:19 | ì „ì²´ íŒŒë¼ë¯¸í„°: 10,794,438,656
2025-10-12 23:44:19 | Input require grads í™œì„±í™” (LoRA + Gradient Checkpointing)
2025-10-12 23:44:19 | âœ… Gradient Checkpointing í™œì„±í™”
2025-10-12 23:44:19 | âœ… Causal LM ë¡œë“œ ì™„ë£Œ
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | ëª¨ë¸ í•™ìŠµ ì‹œìž‘
2025-10-12 23:44:19 | ============================================================
2025-10-12 23:44:19 | WandB ë¡œê·¸ì¸ ìƒíƒœ: ieyeppo-job
2025-10-12 23:44:19 | wandb: Finishing previous runs because reinit is set to True.
