2025-10-13 11:05:20 | 📋 실험명: 1013-1105-qwen3_4b_qlora
2025-10-13 11:05:20 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/sdpamw0e
2025-10-13 11:05:20 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 11:05:20 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 11:05:20 | 학습 진행 중...
2025-10-13 11:05:20 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
2025-10-13 11:05:20 | 100%|██████████| 750/750 [56:28<00:00,  4.52s/it]
2025-10-13 11:05:20 | 0%|          | 0/750 [00:00<?, ?it/s]
2025-10-13 11:05:41 | 1%|          | 4/750 [00:21<1:05:49,  5.29s/it]
2025-10-13 11:06:22 | {'loss': 2.5169, 'grad_norm': 6.384832382202148, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-13 11:06:22 | 1%|▏         | 10/750 [01:02<1:31:08,  7.39s/it]
2025-10-13 11:06:41 | 2%|▏         | 12/750 [01:20<1:41:42,  8.27s/it]
2025-10-13 11:07:44 | 3%|▎         | 19/750 [02:23<1:50:19,  9.06s/it]
2025-10-13 11:07:53 | {'loss': 2.6644, 'grad_norm': 4.389631748199463, 'learning_rate': 1.9e-07, 'epoch': 0.03}
2025-10-13 11:07:53 | 3%|▎         | 20/750 [02:32<1:48:57,  8.96s/it]
2025-10-13 11:08:56 | 4%|▎         | 27/750 [03:36<1:48:31,  9.01s/it]
2025-10-13 11:09:23 | {'loss': 2.6082, 'grad_norm': 4.653611660003662, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.04}
2025-10-13 11:09:23 | 4%|▍         | 30/750 [04:02<1:47:52,  8.99s/it]
2025-10-13 11:09:59 | 5%|▍         | 34/750 [04:38<1:47:02,  8.97s/it]
2025-10-13 11:10:53 | {'loss': 2.6846, 'grad_norm': 4.022918224334717, 'learning_rate': 3.9e-07, 'epoch': 0.05}
2025-10-13 11:10:53 | 5%|▌         | 40/750 [05:32<1:47:12,  9.06s/it]
2025-10-13 11:11:11 | 6%|▌         | 42/750 [05:50<1:47:07,  9.08s/it]
2025-10-13 11:12:20 | 7%|▋         | 49/750 [06:59<1:55:43,  9.91s/it]
2025-10-13 11:12:31 | {'loss': 2.5624, 'grad_norm': 4.087717533111572, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.07}
2025-10-13 11:12:31 | 7%|▋         | 50/750 [07:10<1:59:40, 10.26s/it]
2025-10-13 11:13:42 | 8%|▊         | 57/750 [08:22<2:01:29, 10.52s/it]
2025-10-13 11:14:23 | {'loss': 2.5576, 'grad_norm': 4.949262619018555, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.08}
2025-10-13 11:14:23 | 8%|▊         | 60/750 [09:03<2:24:42, 12.58s/it]
2025-10-13 11:15:16 | >> 로그 리디렉션 중료.
Traceback (most recent call last):
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/scripts/train.py", line 713, in <module>
    main()
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/scripts/train.py", line 583, in main
    results = trainer.train()
              ^^^^^^^^^^^^^^^
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/trainers/full_pipeline_trainer.py", line 59, in train
    model_results, model_paths = self._train_multiple_models(train_df, eval_df)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/trainers/full_pipeline_trainer.py", line 225, in _train_multiple_models
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py", line 262, in train
    train_result = self.trainer.train()             # 학습 실행
                   ^^^^^^^^^^^^^^^^^^^^
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/accelerate/accelerator.py", line 2730, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/ieyeppo/.pyenv/versions/nlp_py3_11_9/lib/python3.11/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
