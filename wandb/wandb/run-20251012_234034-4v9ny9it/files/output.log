2025-10-12 23:40:35 | wandb: Detected [openai] in use.
2025-10-12 23:40:35 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:40:35 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:40:35 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2340-llama_3.2_3b_qlora
2025-10-12 23:40:35 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/4v9ny9it
2025-10-12 23:40:35 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:40:35 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:40:35 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:40:35 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:40:35 | 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:41<00:00,  3.04it/s]
2025-10-12 23:40:35 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-12 23:40:47 | 1%|          | 1/125 [00:11<24:23, 11.80s/it]
2025-10-12 23:40:54 | 2%|â–         | 2/125 [00:19<19:10,  9.35s/it]
2025-10-12 23:41:11 | 3%|â–Ž         | 4/125 [00:35<17:31,  8.69s/it]
2025-10-12 23:41:19 | 4%|â–         | 5/125 [00:43<16:36,  8.31s/it]
2025-10-12 23:41:26 | 5%|â–         | 6/125 [00:51<16:02,  8.08s/it]
2025-10-12 23:41:34 | 6%|â–Œ         | 7/125 [00:59<15:40,  7.97s/it]
2025-10-12 23:41:50 | 7%|â–‹         | 9/125 [01:15<15:32,  8.04s/it]
