2025-10-12 23:34:39 | wandb: Detected [openai] in use.
2025-10-12 23:34:39 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:34:39 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:34:39 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2334-llama_3.2_3b_qlora
2025-10-12 23:34:39 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jj8hwkzh
2025-10-12 23:34:39 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:34:39 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:34:39 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:34:39 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:34:39 | 0%|          | 0/250 [00:00<?, ?it/s]
2025-10-12 23:34:46 | 1%|          | 2/250 [00:06<13:55,  3.37s/it]
2025-10-12 23:34:52 | 2%|â–         | 4/250 [00:12<12:56,  3.16s/it]
2025-10-12 23:35:03 | 3%|â–Ž         | 7/250 [00:23<13:46,  3.40s/it]
2025-10-12 23:35:09 | 4%|â–Ž         | 9/250 [00:30<13:40,  3.41s/it]
2025-10-12 23:35:13 | {'loss': 1.6234, 'grad_norm': 2.0193655490875244, 'learning_rate': 9e-08, 'epoch': 0.04}
2025-10-12 23:35:13 | 4%|â–         | 10/250 [00:33<13:26,  3.36s/it]
2025-10-12 23:35:19 | 5%|â–         | 12/250 [00:39<12:54,  3.25s/it]
2025-10-12 23:35:25 | 6%|â–Œ         | 14/250 [00:46<12:39,  3.22s/it]
2025-10-12 23:35:36 | 7%|â–‹         | 17/250 [00:56<13:17,  3.42s/it]
2025-10-12 23:35:42 | 8%|â–Š         | 19/250 [01:03<12:39,  3.29s/it]
2025-10-12 23:35:46 | {'loss': 1.5487, 'grad_norm': 2.1541213989257812, 'learning_rate': 1.9e-07, 'epoch': 0.08}
2025-10-12 23:35:46 | 8%|â–Š         | 20/250 [01:06<12:34,  3.28s/it]
2025-10-12 23:35:52 | 9%|â–‰         | 22/250 [01:12<12:16,  3.23s/it]
2025-10-12 23:35:58 | 10%|â–‰         | 24/250 [01:19<12:02,  3.20s/it]
2025-10-12 23:36:09 | 11%|â–ˆ         | 27/250 [01:29<12:48,  3.45s/it]
2025-10-12 23:36:15 | 12%|â–ˆâ–        | 29/250 [01:36<12:03,  3.27s/it]
2025-10-12 23:36:18 | {'loss': 1.5919, 'grad_norm': 2.1342108249664307, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.12}
2025-10-12 23:36:18 | 12%|â–ˆâ–        | 30/250 [01:39<11:48,  3.22s/it]
2025-10-12 23:36:24 | 13%|â–ˆâ–Ž        | 32/250 [01:45<11:12,  3.08s/it]
2025-10-12 23:36:30 | 14%|â–ˆâ–Ž        | 34/250 [01:50<10:46,  2.99s/it]
2025-10-12 23:36:40 | 15%|â–ˆâ–        | 37/250 [02:00<11:31,  3.25s/it]
2025-10-12 23:36:46 | 16%|â–ˆâ–Œ        | 39/250 [02:06<10:53,  3.10s/it]
2025-10-12 23:36:49 | {'loss': 1.6301, 'grad_norm': 2.168320655822754, 'learning_rate': 3.9e-07, 'epoch': 0.16}
2025-10-12 23:36:49 | 16%|â–ˆâ–Œ        | 40/250 [02:09<10:34,  3.02s/it]
2025-10-12 23:36:55 | 17%|â–ˆâ–‹        | 42/250 [02:15<10:15,  2.96s/it]
2025-10-12 23:37:00 | 18%|â–ˆâ–Š        | 44/250 [02:21<10:01,  2.92s/it]
2025-10-12 23:37:09 | 19%|â–ˆâ–‰        | 47/250 [02:29<09:52,  2.92s/it]
2025-10-12 23:37:16 | 20%|â–ˆâ–‰        | 49/250 [02:37<10:48,  3.23s/it]
2025-10-12 23:37:20 | {'loss': 1.624, 'grad_norm': 2.1325876712799072, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.2}
2025-10-12 23:37:20 | 20%|â–ˆâ–ˆ        | 50/250 [02:40<11:04,  3.32s/it]
2025-10-12 23:37:26 | 21%|â–ˆâ–ˆ        | 52/250 [02:47<10:56,  3.32s/it]
2025-10-12 23:37:33 | 22%|â–ˆâ–ˆâ–       | 54/250 [02:53<10:29,  3.21s/it]
2025-10-12 23:37:42 | 23%|â–ˆâ–ˆâ–Ž       | 57/250 [03:03<10:26,  3.25s/it]
2025-10-12 23:37:50 | 24%|â–ˆâ–ˆâ–Ž       | 59/250 [03:10<11:19,  3.56s/it]
2025-10-12 23:37:53 | {'loss': 1.5523, 'grad_norm': 1.8329565525054932, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.24}
2025-10-12 23:37:53 | 24%|â–ˆâ–ˆâ–       | 60/250 [03:13<10:45,  3.40s/it]
2025-10-12 23:37:59 | 25%|â–ˆâ–ˆâ–       | 62/250 [03:19<10:01,  3.20s/it]
2025-10-12 23:38:05 | 26%|â–ˆâ–ˆâ–Œ       | 64/250 [03:26<09:38,  3.11s/it]
2025-10-12 23:38:14 | 27%|â–ˆâ–ˆâ–‹       | 67/250 [03:34<09:10,  3.01s/it]
2025-10-12 23:38:22 | 28%|â–ˆâ–ˆâ–Š       | 69/250 [03:42<10:03,  3.33s/it]
2025-10-12 23:38:25 | {'loss': 1.6085, 'grad_norm': 2.066457509994507, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.28}
2025-10-12 23:38:25 | 28%|â–ˆâ–ˆâ–Š       | 70/250 [03:45<10:01,  3.34s/it]
2025-10-12 23:38:31 | 29%|â–ˆâ–ˆâ–‰       | 72/250 [03:51<09:25,  3.18s/it]
2025-10-12 23:38:37 | 30%|â–ˆâ–ˆâ–‰       | 74/250 [03:57<08:53,  3.03s/it]
2025-10-12 23:38:46 | 31%|â–ˆâ–ˆâ–ˆ       | 77/250 [04:06<08:37,  2.99s/it]
2025-10-12 23:38:53 | 32%|â–ˆâ–ˆâ–ˆâ–      | 79/250 [04:13<09:29,  3.33s/it]
2025-10-12 23:38:56 | {'loss': 1.5791, 'grad_norm': 2.310434341430664, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.32}
2025-10-12 23:38:56 | 32%|â–ˆâ–ˆâ–ˆâ–      | 80/250 [04:17<09:30,  3.36s/it]
2025-10-12 23:39:03 | 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 82/250 [04:23<09:12,  3.29s/it]
2025-10-12 23:39:08 | 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 84/250 [04:29<08:24,  3.04s/it]
2025-10-12 23:39:17 | 35%|â–ˆâ–ˆâ–ˆâ–      | 87/250 [04:37<07:50,  2.89s/it]
2025-10-12 23:39:23 | 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 89/250 [04:43<07:53,  2.94s/it]
2025-10-12 23:39:27 | {'loss': 1.5767, 'grad_norm': 1.8840872049331665, 'learning_rate': 8.900000000000001e-07, 'epoch': 0.36}
2025-10-12 23:39:27 | 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 90/250 [04:47<08:49,  3.31s/it]
