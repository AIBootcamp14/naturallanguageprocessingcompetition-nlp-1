2025-10-12 22:05:04 | wandb: Detected [openai] in use.
2025-10-12 22:05:04 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 22:05:04 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 22:05:04 | 📋 실험명: 1012-2205-llama_3.2_3b_qlora
2025-10-12 22:05:04 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/kwr8gvvo
2025-10-12 22:05:04 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:05:04 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 22:05:04 | 학습 진행 중...
2025-10-12 22:05:04 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 22:05:04 | 0%|          | 0/1558 [00:00<?, ?it/s]
2025-10-12 22:05:29 | 1%|          | 8/1558 [00:24<1:18:54,  3.05s/it]
2025-10-12 22:05:34 | {'loss': 1.6248, 'grad_norm': 2.0260937213897705, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-12 22:05:34 | 1%|          | 10/1558 [00:30<1:16:13,  2.95s/it]
2025-10-12 22:06:05 | {'loss': 1.5723, 'grad_norm': 2.1121692657470703, 'learning_rate': 1.9e-07, 'epoch': 0.01}
2025-10-12 22:06:05 | 1%|▏         | 20/1558 [01:00<1:18:26,  3.06s/it]
2025-10-12 22:06:16 | 2%|▏         | 24/1558 [01:11<1:14:37,  2.92s/it]
2025-10-12 22:06:36 | {'loss': 1.6231, 'grad_norm': 1.6558647155761719, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.02}
2025-10-12 22:06:36 | 2%|▏         | 30/1558 [01:32<1:26:44,  3.41s/it]
2025-10-12 22:07:07 | 3%|▎         | 39/1558 [02:03<1:29:12,  3.52s/it]
2025-10-12 22:07:11 | {'loss': 1.6449, 'grad_norm': 1.982552409172058, 'learning_rate': 3.9e-07, 'epoch': 0.03}
2025-10-12 22:07:11 | 3%|▎         | 40/1558 [02:06<1:28:46,  3.51s/it]
2025-10-12 22:07:45 | {'loss': 1.5791, 'grad_norm': 1.8338807821273804, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.03}
2025-10-12 22:07:45 | 3%|▎         | 50/1558 [02:40<1:25:48,  3.41s/it]
2025-10-12 22:08:02 | 4%|▎         | 55/1558 [02:57<1:22:34,  3.30s/it]
2025-10-12 22:08:20 | {'loss': 1.6087, 'grad_norm': 1.7342654466629028, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:08:20 | 4%|▍         | 60/1558 [03:15<1:29:22,  3.58s/it]
2025-10-12 22:08:53 | {'loss': 1.6157, 'grad_norm': 2.483124017715454, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:08:53 | 4%|▍         | 70/1558 [03:48<1:20:14,  3.24s/it]
2025-10-12 22:08:56 | 5%|▍         | 71/1558 [03:51<1:17:51,  3.14s/it]
2025-10-12 22:09:27 | {'loss': 1.6359, 'grad_norm': 2.1355130672454834, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.05}
2025-10-12 22:09:27 | 5%|▌         | 80/1558 [04:22<1:23:14,  3.38s/it]
2025-10-12 22:09:47 | 6%|▌         | 86/1558 [04:42<1:18:07,  3.18s/it]
2025-10-12 22:10:01 | {'loss': 1.566, 'grad_norm': 2.1057934761047363, 'learning_rate': 8.900000000000001e-07, 'epoch': 0.06}
2025-10-12 22:10:01 | 6%|▌         | 90/1558 [04:57<1:23:45,  3.42s/it]
2025-10-12 22:10:36 | {'loss': 1.6224, 'grad_norm': 2.015709400177002, 'learning_rate': 9.9e-07, 'epoch': 0.06}
2025-10-12 22:10:36 | 6%|▋         | 100/1558 [05:31<1:26:59,  3.58s/it]
2025-10-12 22:10:42 | 7%|▋         | 102/1558 [05:38<1:23:47,  3.45s/it]
2025-10-12 22:11:11 | {'loss': 1.5647, 'grad_norm': 1.6748429536819458, 'learning_rate': 1.0900000000000002e-06, 'epoch': 0.07}
2025-10-12 22:11:11 | 7%|▋         | 110/1558 [06:06<1:23:15,  3.45s/it]
2025-10-12 22:11:34 | 8%|▊         | 117/1558 [06:29<1:20:29,  3.35s/it]
2025-10-12 22:11:44 | {'loss': 1.5591, 'grad_norm': 1.9124680757522583, 'learning_rate': 1.19e-06, 'epoch': 0.08}
2025-10-12 22:11:44 | 8%|▊         | 120/1558 [06:39<1:17:31,  3.23s/it]
2025-10-12 22:12:16 | {'loss': 1.5636, 'grad_norm': 1.8828574419021606, 'learning_rate': 1.2900000000000001e-06, 'epoch': 0.08}
2025-10-12 22:12:16 | 8%|▊         | 130/1558 [07:11<1:13:57,  3.11s/it]
2025-10-12 22:12:25 | 9%|▊         | 133/1558 [07:20<1:13:58,  3.11s/it]
2025-10-12 22:12:48 | {'loss': 1.4501, 'grad_norm': 1.7867604494094849, 'learning_rate': 1.3900000000000002e-06, 'epoch': 0.09}
2025-10-12 22:12:48 | 9%|▉         | 140/1558 [07:43<1:13:38,  3.12s/it]
2025-10-12 22:13:19 | 10%|▉         | 149/1558 [08:14<1:20:18,  3.42s/it]
2025-10-12 22:13:23 | {'loss': 1.5337, 'grad_norm': 1.7558077573776245, 'learning_rate': 1.4900000000000001e-06, 'epoch': 0.1}
2025-10-12 22:13:23 | 10%|▉         | 150/1558 [08:18<1:22:56,  3.53s/it]
2025-10-12 22:13:56 | {'loss': 1.411, 'grad_norm': 1.825000524520874, 'learning_rate': 1.5900000000000002e-06, 'epoch': 0.1}
2025-10-12 22:13:56 | 10%|█         | 160/1558 [08:51<1:19:47,  3.42s/it]
2025-10-12 22:14:09 | 11%|█         | 164/1558 [09:04<1:15:29,  3.25s/it]
2025-10-12 22:14:30 | {'loss': 1.4298, 'grad_norm': 1.9290547370910645, 'learning_rate': 1.6900000000000003e-06, 'epoch': 0.11}
2025-10-12 22:14:30 | 11%|█         | 170/1558 [09:25<1:17:27,  3.35s/it]
2025-10-12 22:15:03 | 12%|█▏        | 180/1558 [09:58<1:17:04,  3.36s/it]
2025-10-12 22:15:03 | {'loss': 1.369, 'grad_norm': 1.9755674600601196, 'learning_rate': 1.79e-06, 'epoch': 0.12}
2025-10-12 22:15:03 | 12%|█▏        | 180/1558 [09:58<1:17:04,  3.36s/it]
2025-10-12 22:15:39 | {'loss': 1.3959, 'grad_norm': 2.018749713897705, 'learning_rate': 1.8900000000000001e-06, 'epoch': 0.12}
2025-10-12 22:15:39 | 12%|█▏        | 190/1558 [10:34<1:24:08,  3.69s/it]
2025-10-12 22:15:56 | 13%|█▎        | 195/1558 [10:51<1:20:03,  3.52s/it]
2025-10-12 22:16:13 | {'loss': 1.3611, 'grad_norm': 1.7554312944412231, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.13}
2025-10-12 22:16:13 | 13%|█▎        | 200/1558 [11:08<1:14:33,  3.29s/it]
2025-10-12 22:16:45 | {'loss': 1.4014, 'grad_norm': 1.555174469947815, 'learning_rate': 2.09e-06, 'epoch': 0.13}
2025-10-12 22:16:45 | 13%|█▎        | 210/1558 [11:40<1:11:15,  3.17s/it]
2025-10-12 22:16:49 | 14%|█▎        | 211/1558 [11:44<1:12:25,  3.23s/it]
2025-10-12 22:17:19 | {'loss': 1.4433, 'grad_norm': 2.096745491027832, 'learning_rate': 2.19e-06, 'epoch': 0.14}
2025-10-12 22:17:19 | 14%|█▍        | 220/1558 [12:15<1:14:44,  3.35s/it]
2025-10-12 22:17:40 | 15%|█▍        | 226/1558 [12:35<1:15:05,  3.38s/it]
2025-10-12 22:17:55 | {'loss': 1.3526, 'grad_norm': 1.9017237424850464, 'learning_rate': 2.29e-06, 'epoch': 0.15}
2025-10-12 22:17:55 | 15%|█▍        | 230/1558 [12:50<1:20:19,  3.63s/it]
2025-10-12 22:18:29 | {'loss': 1.4157, 'grad_norm': 1.548293948173523, 'learning_rate': 2.39e-06, 'epoch': 0.15}
2025-10-12 22:18:29 | 15%|█▌        | 240/1558 [13:24<1:10:05,  3.19s/it]
2025-10-12 22:18:36 | 16%|█▌        | 242/1558 [13:31<1:17:08,  3.52s/it]
2025-10-12 22:19:04 | {'loss': 1.3324, 'grad_norm': 1.7229104042053223, 'learning_rate': 2.4900000000000003e-06, 'epoch': 0.16}
2025-10-12 22:19:04 | 16%|█▌        | 250/1558 [13:59<1:17:48,  3.57s/it]
2025-10-12 22:19:32 | 17%|█▋        | 258/1558 [14:28<1:13:24,  3.39s/it]
2025-10-12 22:19:39 | {'loss': 1.2984, 'grad_norm': 1.9131561517715454, 'learning_rate': 2.59e-06, 'epoch': 0.17}
2025-10-12 22:19:39 | 17%|█▋        | 260/1558 [14:34<1:11:14,  3.29s/it]
