2025-10-13 01:51:23 | 📋 실험명: 1013-0151-kullm_v2_12.8b_qlora
2025-10-13 01:51:23 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/qb67k6z6
2025-10-13 01:51:23 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 01:51:23 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 01:51:23 | 학습 진행 중...
2025-10-13 01:51:23 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': None, 'pad_token_id': 2}.
2025-10-13 01:51:23 | 0%|          | 0/125 [00:40<?, ?it/s]
2025-10-13 01:51:36 | ❌ kullm-v2 학습 실패: RuntimeError: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0
2025-10-13 01:51:36 | 오류 로그 저장: experiments/20251013/20251013_001540_test_full_pipeline_quick/errors/kullm-v2_error.log
2025-10-13 01:51:36 | ==================================================
2025-10-13 01:51:36 | 모델 학습 결과 요약
2025-10-13 01:51:36 | ==================================================
2025-10-13 01:51:36 | ✅ 성공: 0/6 모델
2025-10-13 01:51:36 | ❌ 실패: 6/6 모델
2025-10-13 01:51:36 | 실패한 모델 목록:
2025-10-13 01:51:36 | - kobart: OverflowError
2025-10-13 01:51:36 | - llama-3.2-korean-3b: OverflowError
2025-10-13 01:51:36 | - qwen3-4b: OverflowError
2025-10-13 01:51:36 | - solar-10.7b: RuntimeError
2025-10-13 01:51:36 | - polyglot-ko-12.8b: RuntimeError
2025-10-13 01:51:36 | - kullm-v2: RuntimeError
2025-10-13 01:51:36 | [3/6] 앙상블 생성...
2025-10-13 01:51:36 | ModelManager 초기화
2025-10-13 01:51:36 | 앙상블 평가 오류 발생: 로드된 모델이 없음
2025-10-13 01:51:36 | [4/6] Solar API 통합...
2025-10-13 01:51:36 | Solar API processing 50 samples ...
2025-10-13 01:51:36 | [5/6] TTA 적용...
2025-10-13 01:51:36 | TTA 전략: paraphrase, reorder, synonym, mask
2025-10-13 01:51:36 | 증강 횟수: 1
2025-10-13 01:51:36 | TTA 기능은 아직 구현 중입니다.
2025-10-13 01:51:36 | [6/6] 추론 및 제출 파일 생성...
2025-10-13 01:51:36 | 테스트 데이터 로드: data/raw/test.csv
2025-10-13 01:51:36 | 테스트 샘플 수: 499
2025-10-13 01:51:36 | ❌ 사용 가능한 모델이 없습니다.
2025-10-13 01:51:36 | ============================================================
2025-10-13 01:51:36 | FULL PIPELINE 완료!
2025-10-13 01:51:36 | =요약 개별 모델 결과:
2025-10-13 01:51:36 | kobart:
2025-10-13 01:51:36 | llama-3.2-korean-3b:
2025-10-13 01:51:36 | qwen3-4b:
2025-10-13 01:51:36 | solar-10.7b:
2025-10-13 01:51:36 | polyglot-ko-12.8b:
2025-10-13 01:51:36 | kullm-v2:
2025-10-13 01:51:36 | =요약 앙상블 결과:
2025-10-13 01:51:36 | =요약 Solar API 결과:
2025-10-13 01:51:36 | solar_rouge_1_f1: 0.2272
2025-10-13 01:51:36 | solar_rouge_2_f1: 0.0765
2025-10-13 01:51:36 | solar_rouge_l_f1: 0.2177
2025-10-13 01:51:36 | n_samples: 50.0000
2025-10-13 01:51:36 | ============================================================
2025-10-13 01:51:36 | =저장 결과 저장: experiments/20251013/20251013_001540_test_full_pipeline_quick/full_pipeline_results.json
2025-10-13 01:51:36 | 📋 학습 로그 백업: logs/20251013/train/20251013_015136_full_kobart_ep1_aug_tta.log
2025-10-13 01:51:36 | 🔧 추론 최적화 시작 (PRD 17)...
2025-10-13 01:51:36 | ⚠️ 모델 경로를 찾을 수 없어 추론 최적화를 건너뜁니다.
2025-10-13 01:51:36 | 📈 시각화 생성 중...
2025-10-13 01:51:36 | ✅ 나눔고딕 폰트 로드 성공
2025-10-13 01:51:36 | ⚠️ 시각화 모듈 없음 (추후 구현 예정)
2025-10-13 01:51:36 | ============================================================
2025-10-13 01:51:36 | ✅ 학습 완료!
2025-10-13 01:51:36 | 📁 결과 저장: experiments/20251013/20251013_001540_test_full_pipeline_quick
2025-10-13 01:51:36 | 🔧 추론 최적화 적용됨
2025-10-13 01:51:36 | ============================================================
2025-10-13 01:51:36 | >> 로그 리디렉션 중료.
