2025-10-12 23:15:28 | wandb: Detected [openai] in use.
2025-10-12 23:15:28 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:15:28 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:15:28 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2315-llama_3.2_3b_qlora
2025-10-12 23:15:28 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/fol48gxx
2025-10-12 23:15:28 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:15:28 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:15:28 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:15:28 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:15:29 | 0%|          | 0/750 [00:00<?, ?it/s]
2025-10-12 23:15:41 | 1%|          | 4/750 [00:12<37:08,  2.99s/it]
2025-10-12 23:15:57 | {'loss': 1.6069, 'grad_norm': 2.0931859016418457, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-12 23:15:57 | 1%|â–         | 10/750 [00:28<33:25,  2.71s/it]
2025-10-12 23:16:04 | 2%|â–         | 12/750 [00:35<39:11,  3.19s/it]
2025-10-12 23:16:23 | 3%|â–Ž         | 19/750 [00:54<32:58,  2.71s/it]
2025-10-12 23:16:26 | {'loss': 1.6726, 'grad_norm': 1.9981169700622559, 'learning_rate': 1.9e-07, 'epoch': 0.03}
2025-10-12 23:16:26 | 3%|â–Ž         | 20/750 [00:57<33:40,  2.77s/it]
2025-10-12 23:16:47 | 4%|â–Ž         | 27/750 [01:18<34:33,  2.87s/it]
2025-10-12 23:16:55 | {'loss': 1.6492, 'grad_norm': 1.8897300958633423, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.04}
2025-10-12 23:16:55 | 4%|â–         | 30/750 [01:26<33:43,  2.81s/it]
2025-10-12 23:17:06 | 5%|â–         | 34/750 [01:37<33:49,  2.83s/it]
2025-10-12 23:17:24 | {'loss': 1.614, 'grad_norm': 1.8445836305618286, 'learning_rate': 3.9e-07, 'epoch': 0.05}
2025-10-12 23:17:24 | 5%|â–Œ         | 40/750 [01:55<33:51,  2.86s/it]
2025-10-12 23:17:30 | 6%|â–Œ         | 42/750 [02:01<33:10,  2.81s/it]
2025-10-12 23:17:51 | 7%|â–‹         | 49/750 [02:22<34:45,  2.98s/it]
2025-10-12 23:17:54 | {'loss': 1.6363, 'grad_norm': 1.9223698377609253, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.07}
2025-10-12 23:17:54 | 7%|â–‹         | 50/750 [02:25<34:50,  2.99s/it]
