2025-10-13 15:36:19 | wandb: Detected [openai] in use.
2025-10-13 15:36:19 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-13 15:36:19 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-13 15:36:19 | ðŸ“‹ ì‹¤í—˜ëª…: 1013-1536-llama_3.2_3b_qlora
2025-10-13 15:36:19 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/0kwvjxtj
2025-10-13 15:36:19 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 15:36:19 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 15:36:19 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-13 15:36:19 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-13 15:36:21 | 0%|          | 0/750 [00:00<?, ?it/s]
2025-10-13 15:41:12 | 1%|          | 4/750 [04:51<13:05:35, 63.18s/it]
2025-10-13 15:46:44 | {'loss': 1.5799, 'grad_norm': 2.6425631046295166, 'learning_rate': 3.6e-07, 'epoch': 0.03}
2025-10-13 15:46:44 | 1%|â–         | 10/750 [10:22<11:31:37, 56.08s/it]
2025-10-13 15:49:00 | 2%|â–         | 12/750 [12:38<12:35:25, 61.42s/it]
2025-10-13 15:55:30 | 3%|â–Ž         | 19/750 [19:09<11:20:53, 55.89s/it]
2025-10-13 15:56:29 | {'loss': 1.5259, 'grad_norm': 1.9275765419006348, 'learning_rate': 7.6e-07, 'epoch': 0.05}
2025-10-13 15:56:29 | 3%|â–Ž         | 20/750 [20:07<11:28:50, 56.62s/it]
