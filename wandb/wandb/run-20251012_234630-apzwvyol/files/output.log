2025-10-12 23:46:31 | wandb: Detected [openai] in use.
2025-10-12 23:46:31 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:46:31 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:46:31 | ðŸ“‹ ì‹¤í—˜ëª…: 1012-2346-llama_3.2_3b_qlora
2025-10-12 23:46:31 | ðŸ”— WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/apzwvyol
2025-10-12 23:46:31 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:46:31 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:46:31 | í•™ìŠµ ì§„í–‰ ì¤‘...
2025-10-12 23:46:31 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:46:31 | 0%|          | 0/250 [00:00<?, ?it/s]
2025-10-12 23:46:38 | 1%|          | 2/250 [00:07<15:08,  3.67s/it]
2025-10-12 23:46:44 | 2%|â–         | 4/250 [00:12<12:42,  3.10s/it]
2025-10-12 23:46:53 | 3%|â–Ž         | 7/250 [00:21<11:45,  2.90s/it]
2025-10-12 23:46:58 | 4%|â–Ž         | 9/250 [00:27<11:33,  2.88s/it]
2025-10-12 23:47:01 | {'loss': 1.6234, 'grad_norm': 2.0193655490875244, 'learning_rate': 9e-08, 'epoch': 0.04}
2025-10-12 23:47:01 | 4%|â–         | 10/250 [00:30<11:31,  2.88s/it]
2025-10-12 23:47:07 | 5%|â–         | 12/250 [00:35<11:22,  2.87s/it]
2025-10-12 23:47:13 | 6%|â–Œ         | 14/250 [00:42<12:01,  3.06s/it]
2025-10-12 23:47:22 | 7%|â–‹         | 17/250 [00:50<11:15,  2.90s/it]
2025-10-12 23:47:28 | 8%|â–Š         | 19/250 [00:56<10:55,  2.84s/it]
2025-10-12 23:47:31 | {'loss': 1.5487, 'grad_norm': 2.1541213989257812, 'learning_rate': 1.9e-07, 'epoch': 0.08}
2025-10-12 23:47:31 | 8%|â–Š         | 20/250 [00:59<11:03,  2.89s/it]
2025-10-12 23:47:36 | 9%|â–‰         | 22/250 [01:05<11:03,  2.91s/it]
2025-10-12 23:47:43 | 10%|â–‰         | 24/250 [01:12<11:58,  3.18s/it]
2025-10-12 23:47:52 | 11%|â–ˆ         | 27/250 [01:20<11:02,  2.97s/it]
2025-10-12 23:47:57 | 12%|â–ˆâ–        | 29/250 [01:26<10:47,  2.93s/it]
2025-10-12 23:48:00 | {'loss': 1.5919, 'grad_norm': 2.1342108249664307, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.12}
2025-10-12 23:48:00 | 12%|â–ˆâ–        | 30/250 [01:29<10:43,  2.92s/it]
2025-10-12 23:48:06 | 13%|â–ˆâ–Ž        | 32/250 [01:35<10:43,  2.95s/it]
2025-10-12 23:48:12 | 14%|â–ˆâ–Ž        | 34/250 [01:41<10:39,  2.96s/it]
2025-10-12 23:48:22 | 15%|â–ˆâ–        | 37/250 [01:51<11:10,  3.15s/it]
2025-10-12 23:48:28 | 16%|â–ˆâ–Œ        | 39/250 [01:57<10:49,  3.08s/it]
2025-10-12 23:48:31 | {'loss': 1.6301, 'grad_norm': 2.168320655822754, 'learning_rate': 3.9e-07, 'epoch': 0.16}
2025-10-12 23:48:31 | 16%|â–ˆâ–Œ        | 40/250 [02:00<10:32,  3.01s/it]
2025-10-12 23:48:37 | 17%|â–ˆâ–‹        | 42/250 [02:06<10:35,  3.05s/it]
2025-10-12 23:48:43 | 18%|â–ˆâ–Š        | 44/250 [02:12<10:10,  2.96s/it]
2025-10-12 23:48:53 | 19%|â–ˆâ–‰        | 47/250 [02:21<10:27,  3.09s/it]
2025-10-12 23:48:59 | 20%|â–ˆâ–‰        | 49/250 [02:27<10:15,  3.06s/it]
2025-10-12 23:49:02 | {'loss': 1.624, 'grad_norm': 2.1325876712799072, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.2}
2025-10-12 23:49:02 | 20%|â–ˆâ–ˆ        | 50/250 [02:30<09:56,  2.98s/it]
2025-10-12 23:49:07 | 21%|â–ˆâ–ˆ        | 52/250 [02:36<09:39,  2.93s/it]
2025-10-12 23:49:12 | 22%|â–ˆâ–ˆâ–       | 54/250 [02:41<08:58,  2.75s/it]
2025-10-12 23:49:20 | 23%|â–ˆâ–ˆâ–Ž       | 57/250 [02:49<08:28,  2.64s/it]
2025-10-12 23:49:26 | 24%|â–ˆâ–ˆâ–Ž       | 59/250 [02:55<08:54,  2.80s/it]
2025-10-12 23:49:29 | {'loss': 1.5523, 'grad_norm': 1.8329565525054932, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.24}
2025-10-12 23:49:29 | 24%|â–ˆâ–ˆâ–       | 60/250 [02:57<08:39,  2.74s/it]
2025-10-12 23:49:34 | 25%|â–ˆâ–ˆâ–       | 62/250 [03:02<08:18,  2.65s/it]
2025-10-12 23:49:39 | 26%|â–ˆâ–ˆâ–Œ       | 64/250 [03:08<08:03,  2.60s/it]
2025-10-12 23:49:47 | 27%|â–ˆâ–ˆâ–‹       | 67/250 [03:15<07:52,  2.58s/it]
2025-10-12 23:49:52 | 28%|â–ˆâ–ˆâ–Š       | 69/250 [03:21<07:57,  2.64s/it]
2025-10-12 23:49:56 | {'loss': 1.6085, 'grad_norm': 2.066457509994507, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.28}
2025-10-12 23:49:56 | 28%|â–ˆâ–ˆâ–Š       | 70/250 [03:24<08:40,  2.89s/it]
2025-10-12 23:50:01 | 29%|â–ˆâ–ˆâ–‰       | 72/250 [03:30<08:15,  2.78s/it]
2025-10-12 23:50:07 | 30%|â–ˆâ–ˆâ–‰       | 74/250 [03:35<08:05,  2.76s/it]
2025-10-12 23:50:15 | 31%|â–ˆâ–ˆâ–ˆ       | 77/250 [03:44<08:10,  2.83s/it]
