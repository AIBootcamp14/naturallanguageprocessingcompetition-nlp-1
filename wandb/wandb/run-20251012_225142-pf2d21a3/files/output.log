2025-10-12 22:51:43 | wandb: Detected [openai] in use.
2025-10-12 22:51:43 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 22:51:43 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 22:51:43 | 📋 실험명: 1012-2251-llama_3.2_3b_qlora
2025-10-12 22:51:43 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/pf2d21a3
2025-10-12 22:51:43 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 22:51:43 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 22:51:43 | 학습 진행 중...
2025-10-12 22:51:43 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 22:51:43 | 0%|          | 0/1558 [00:00<?, ?it/s]
2025-10-12 22:52:04 | 1%|          | 8/1558 [00:20<1:04:41,  2.50s/it]
2025-10-12 22:52:09 | {'loss': 1.6248, 'grad_norm': 2.0260937213897705, 'learning_rate': 9e-08, 'epoch': 0.01}
2025-10-12 22:52:09 | 1%|          | 10/1558 [00:25<1:06:06,  2.56s/it]
2025-10-12 22:52:36 | {'loss': 1.5723, 'grad_norm': 2.1121692657470703, 'learning_rate': 1.9e-07, 'epoch': 0.01}
2025-10-12 22:52:36 | 1%|▏         | 20/1558 [00:53<1:08:06,  2.66s/it]
2025-10-12 22:52:48 | 2%|▏         | 24/1558 [01:05<1:16:54,  3.01s/it]
2025-10-12 22:53:05 | {'loss': 1.6231, 'grad_norm': 1.6558647155761719, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.02}
2025-10-12 22:53:05 | 2%|▏         | 30/1558 [01:21<1:09:56,  2.75s/it]
2025-10-12 22:53:31 | 3%|▎         | 39/1558 [01:47<1:10:01,  2.77s/it]
2025-10-12 22:53:33 | {'loss': 1.6449, 'grad_norm': 1.982552409172058, 'learning_rate': 3.9e-07, 'epoch': 0.03}
2025-10-12 22:53:33 | 3%|▎         | 40/1558 [01:50<1:08:04,  2.69s/it]
2025-10-12 22:54:01 | {'loss': 1.5791, 'grad_norm': 1.8338807821273804, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.03}
2025-10-12 22:54:01 | 3%|▎         | 50/1558 [02:17<1:11:43,  2.85s/it]
2025-10-12 22:54:15 | 4%|▎         | 55/1558 [02:32<1:12:51,  2.91s/it]
2025-10-12 22:54:30 | {'loss': 1.6087, 'grad_norm': 1.7342654466629028, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:54:30 | 4%|▍         | 60/1558 [02:47<1:17:07,  3.09s/it]
2025-10-12 22:54:58 | {'loss': 1.6157, 'grad_norm': 2.483124017715454, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.04}
2025-10-12 22:54:58 | 4%|▍         | 70/1558 [03:14<1:05:15,  2.63s/it]
2025-10-12 22:55:01 | 5%|▍         | 71/1558 [03:18<1:14:05,  2.99s/it]
2025-10-12 22:55:24 | {'loss': 1.6359, 'grad_norm': 2.1355130672454834, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.05}
2025-10-12 22:55:24 | 5%|▌         | 80/1558 [03:41<1:03:05,  2.56s/it]
2025-10-12 22:55:41 | 6%|▌         | 86/1558 [03:57<1:05:17,  2.66s/it]
2025-10-12 22:55:52 | {'loss': 1.566, 'grad_norm': 2.1057934761047363, 'learning_rate': 8.900000000000001e-07, 'epoch': 0.06}
2025-10-12 22:55:52 | 6%|▌         | 90/1558 [04:08<1:04:48,  2.65s/it]
2025-10-12 22:56:22 | {'loss': 1.6224, 'grad_norm': 2.015709400177002, 'learning_rate': 9.9e-07, 'epoch': 0.06}
2025-10-12 22:56:22 | 6%|▋         | 100/1558 [04:38<1:09:43,  2.87s/it]
2025-10-12 22:56:28 | 7%|▋         | 102/1558 [04:44<1:08:59,  2.84s/it]
2025-10-12 22:56:51 | {'loss': 1.5647, 'grad_norm': 1.6748429536819458, 'learning_rate': 1.0900000000000002e-06, 'epoch': 0.07}
2025-10-12 22:56:51 | 7%|▋         | 110/1558 [05:07<1:10:15,  2.91s/it]
2025-10-12 22:57:13 | 8%|▊         | 117/1558 [05:30<1:21:07,  3.38s/it]
2025-10-12 22:57:23 | {'loss': 1.5591, 'grad_norm': 1.9124680757522583, 'learning_rate': 1.19e-06, 'epoch': 0.08}
2025-10-12 22:57:23 | 8%|▊         | 120/1558 [05:40<1:18:42,  3.28s/it]
2025-10-12 22:57:54 | {'loss': 1.5636, 'grad_norm': 1.8828574419021606, 'learning_rate': 1.2900000000000001e-06, 'epoch': 0.08}
2025-10-12 22:57:54 | 8%|▊         | 130/1558 [06:11<1:10:36,  2.97s/it]
2025-10-12 22:58:03 | 9%|▊         | 133/1558 [06:20<1:08:44,  2.89s/it]
2025-10-12 22:58:24 | {'loss': 1.4501, 'grad_norm': 1.7867604494094849, 'learning_rate': 1.3900000000000002e-06, 'epoch': 0.09}
2025-10-12 22:58:24 | 9%|▉         | 140/1558 [06:41<1:11:48,  3.04s/it]
2025-10-12 22:58:50 | 10%|▉         | 149/1558 [07:06<1:06:33,  2.83s/it]
2025-10-12 22:58:54 | {'loss': 1.5337, 'grad_norm': 1.7558077573776245, 'learning_rate': 1.4900000000000001e-06, 'epoch': 0.1}
2025-10-12 22:58:54 | 10%|▉         | 150/1558 [07:10<1:14:05,  3.16s/it]
2025-10-12 22:59:22 | {'loss': 1.411, 'grad_norm': 1.825000524520874, 'learning_rate': 1.5900000000000002e-06, 'epoch': 0.1}
2025-10-12 22:59:22 | 10%|█         | 160/1558 [07:39<1:06:02,  2.83s/it]
2025-10-12 22:59:35 | 11%|█         | 164/1558 [07:51<1:07:46,  2.92s/it]
2025-10-12 22:59:51 | {'loss': 1.4298, 'grad_norm': 1.9290547370910645, 'learning_rate': 1.6900000000000003e-06, 'epoch': 0.11}
2025-10-12 22:59:51 | 11%|█         | 170/1558 [08:08<1:04:01,  2.77s/it]
2025-10-12 23:00:21 | 12%|█▏        | 180/1558 [08:37<1:04:53,  2.83s/it]
2025-10-12 23:00:21 | {'loss': 1.369, 'grad_norm': 1.9755674600601196, 'learning_rate': 1.79e-06, 'epoch': 0.12}
2025-10-12 23:00:21 | 12%|█▏        | 180/1558 [08:37<1:04:53,  2.83s/it]
2025-10-12 23:00:50 | {'loss': 1.3959, 'grad_norm': 2.018749713897705, 'learning_rate': 1.8900000000000001e-06, 'epoch': 0.12}
2025-10-12 23:00:50 | 12%|█▏        | 190/1558 [09:07<1:05:15,  2.86s/it]
2025-10-12 23:01:06 | 13%|█▎        | 195/1558 [09:22<1:12:07,  3.17s/it]
2025-10-12 23:01:20 | {'loss': 1.3611, 'grad_norm': 1.7554312944412231, 'learning_rate': 1.9900000000000004e-06, 'epoch': 0.13}
2025-10-12 23:01:20 | 13%|█▎        | 200/1558 [09:37<1:05:42,  2.90s/it]
2025-10-12 23:01:50 | {'loss': 1.4014, 'grad_norm': 1.555174469947815, 'learning_rate': 2.09e-06, 'epoch': 0.13}
2025-10-12 23:01:50 | 13%|█▎        | 210/1558 [10:06<1:04:35,  2.87s/it]
2025-10-12 23:01:53 | 14%|█▎        | 211/1558 [10:09<1:06:08,  2.95s/it]
2025-10-12 23:02:22 | {'loss': 1.4433, 'grad_norm': 2.096745491027832, 'learning_rate': 2.19e-06, 'epoch': 0.14}
2025-10-12 23:02:22 | 14%|█▍        | 220/1558 [10:39<1:11:26,  3.20s/it]
2025-10-12 23:02:39 | 15%|█▍        | 226/1558 [10:56<1:04:12,  2.89s/it]
2025-10-12 23:02:52 | {'loss': 1.3526, 'grad_norm': 1.9017237424850464, 'learning_rate': 2.29e-06, 'epoch': 0.15}
2025-10-12 23:02:52 | 15%|█▍        | 230/1558 [11:09<1:08:09,  3.08s/it]
2025-10-12 23:03:22 | {'loss': 1.4157, 'grad_norm': 1.548293948173523, 'learning_rate': 2.39e-06, 'epoch': 0.15}
2025-10-12 23:03:22 | 15%|█▌        | 240/1558 [11:39<1:07:43,  3.08s/it]
2025-10-12 23:03:28 | 16%|█▌        | 242/1558 [11:45<1:05:53,  3.00s/it]
2025-10-12 23:03:55 | {'loss': 1.3324, 'grad_norm': 1.7229104042053223, 'learning_rate': 2.4900000000000003e-06, 'epoch': 0.16}
2025-10-12 23:03:55 | 16%|█▌        | 250/1558 [12:11<1:14:22,  3.41s/it]
