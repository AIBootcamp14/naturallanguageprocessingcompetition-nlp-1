{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b880380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ì „ì²´ ë°ì´í„°ì…‹ ë…¸ì´ì¦ˆ ë¶„ì„\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Train ë¶„ì„ (12457 samples)\n",
      "================================================================================\n",
      "1. \\n í¬í•¨: 1ê°œ (0.0%)\n",
      "   ìƒ˜í”Œ: #Person1#: ì €, ë¶ˆë§Œì´ ìˆì–´ìš”. ì—´ ë¶„ ë™ì•ˆ í…Œì´ë¸”ì—ì„œ ê¸°ë‹¤ë ¸ëŠ”ë°, ì›¨ì´í„°ê°€ ë“œë””ì–´ ì™€ì„œ ì£¼ë¬¸ì„ ë°›ì•˜ì–´ìš”. ê·¸ëŸ°ë° ë‚˜ì˜¨ ìŒì‹ì´ ì œê°€ ì£¼ë¬¸í•œ ê²Œ ì•„ë‹ˆë”ë¼ê³ ìš”.\\n#Perso...\n",
      "2. <br> íƒœê·¸ í¬í•¨: 1ê°œ (0.0%)\n",
      "   ìƒ˜í”Œ: #Person1#: ìš”ì¦˜ ì˜ ì§€ë‚´ê³  ìˆì–´ìš”?<br>#Person2#: ì œ ì½”ì¹˜ê°€ ì œ í˜ˆì••ì„ ì²´í¬í•´ ë‹¬ë¼ê³  ë¶€íƒí–ˆì–´ìš”.<br>#Person1#: ì „ì— ê³ í˜ˆì•• ìˆë‹¤ê³  ë“¤ì€ ì  ìˆë‚˜ìš”?...\n",
      "3. ì—°ì† ê³µë°±(3ê°œ+): 0ê°œ (0.0%)\n",
      "4. ì—°ì† ê°œí–‰(3ê°œ+): 0ê°œ (0.0%)\n",
      "\n",
      "5. Person íƒœê·¸ í˜•íƒœ:\n",
      "   - #Person1# í˜•íƒœ: 12457ê°œ\n",
      "   - # Person 1 # í˜•íƒœ: 0ê°œ\n",
      "   - #person1# í˜•íƒœ: 0ê°œ\n",
      "   - ì´ Person íƒœê·¸ í¬í•¨: 12457ê°œ (100.0%)\n",
      "\n",
      "6. ì•ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "   ë’¤ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "\n",
      "7. íŠ¹ìˆ˜ë¬¸ì í¬í•¨: 642ê°œ (5.2%)\n",
      "\n",
      "8. ëŒ€í™” ê¸¸ì´ í†µê³„:\n",
      "   - ìµœì†Œ: 84ì\n",
      "   - ìµœëŒ€: 2165ì\n",
      "   - í‰ê· : 406.1ì\n",
      "   - ì¤‘ì•™ê°’: 369.0ì\n",
      "\n",
      "9. ì‹¤ì œ ìƒ˜í”Œ (ì²« 3ê°œ):\n",
      "\n",
      "   [1] fname: train_0\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \\n#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \\n#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \\n#Person'...\n",
      "       summary: Mr. SmithëŠ” Dr. Hawkinsì—ê²Œ ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ëŸ¬ ì™€ì„œ, ë§¤ë…„ ê²€ì§„ í•„ìš”ì„±ì„ ì•ˆë‚´ë°›ê³  í¡ì—° ìŠµê´€ ê°œì„ ì„ ìœ„í•œ ë„ì›€ì„ ì œì•ˆë°›ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "   [2] fname: train_1\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mrs. Parker. ì˜ ì§€ë‚´ì…¨ë‚˜ìš”?\\n#Person2#: ì•ˆë…•í•˜ì„¸ìš”, Dr. Peters. ì˜ ì§€ë‚´ê³  ìˆì–´ìš”. Rickyë‘ ì €í¬ ë‘˜ ë‹¤ ë°±ì‹  ë§ìœ¼ëŸ¬ ì™”ì–´ìš”.\\n#Person1#: ì•Œê² ìŠµë‹ˆë‹¤. ë°±ì‹  ê¸°ë¡ì„ ë³´ë‹ˆ RickyëŠ” ì†Œì•„ë§ˆë¹„, íŒŒìƒ'...\n",
      "       summary: Mrs. Parkerê°€ Rickyì™€ í•¨ê»˜ ë°±ì‹  ì ‘ì¢…ì„ ìœ„í•´ ë°©ë¬¸í•˜ì˜€ê³ , Dr. PetersëŠ” Rickyì—ê²Œ ì ì ˆí•œ ë°±ì‹ ì„ ì ‘ì¢…í•˜ë„ë¡ ì•ˆë‚´í•©ë‹ˆë‹¤.\n",
      "\n",
      "   [3] fname: train_2\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì €ê¸°ìš”, ì—´ì‡  ì„¸íŠ¸ ë³¸ ì  ìˆì–´ìš”?\\n#Person2#: ì–´ë–¤ ì¢…ë¥˜ì˜ ì—´ì‡ ìš”?\\n#Person1#: ì—´ì‡  ë‹¤ì„¯ ê°œë‘ ì‘ì€ ë°œ ì¥ì‹ì´ ë‹¬ë ¤ ìˆì–´ìš”.\\n#Person2#: ì•„, ì•ˆíƒ€ê¹ë„¤ìš”! ëª» ë´¤ì–´ìš”.\\n#Person1#: ê·¸ëŸ¼, ê°™ì´ ì¢€ ì°¾ì•„ì£¼ì‹¤ ìˆ˜ ìˆì–´ìš”?'...\n",
      "       summary: #Person1#ì€ ì—´ì‡  ì„¸íŠ¸ë¥¼ ìƒì–´ë²„ë¦¬ê³  #Person2#ì—ê²Œ ì°¾ëŠ” ê²ƒì„ ë„ì™€ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤.\n",
      "\n",
      "================================================================================\n",
      "Dev ë¶„ì„ (499 samples)\n",
      "================================================================================\n",
      "1. \\n í¬í•¨: 0ê°œ (0.0%)\n",
      "2. <br> íƒœê·¸ í¬í•¨: 0ê°œ (0.0%)\n",
      "3. ì—°ì† ê³µë°±(3ê°œ+): 0ê°œ (0.0%)\n",
      "4. ì—°ì† ê°œí–‰(3ê°œ+): 0ê°œ (0.0%)\n",
      "\n",
      "5. Person íƒœê·¸ í˜•íƒœ:\n",
      "   - #Person1# í˜•íƒœ: 499ê°œ\n",
      "   - # Person 1 # í˜•íƒœ: 0ê°œ\n",
      "   - #person1# í˜•íƒœ: 0ê°œ\n",
      "   - ì´ Person íƒœê·¸ í¬í•¨: 499ê°œ (100.0%)\n",
      "\n",
      "6. ì•ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "   ë’¤ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "\n",
      "7. íŠ¹ìˆ˜ë¬¸ì í¬í•¨: 27ê°œ (5.4%)\n",
      "\n",
      "8. ëŒ€í™” ê¸¸ì´ í†µê³„:\n",
      "   - ìµœì†Œ: 114ì\n",
      "   - ìµœëŒ€: 1269ì\n",
      "   - í‰ê· : 400.1ì\n",
      "   - ì¤‘ì•™ê°’: 367.0ì\n",
      "\n",
      "9. ì‹¤ì œ ìƒ˜í”Œ (ì²« 3ê°œ):\n",
      "\n",
      "   [1] fname: dev_0\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\\n#Person2#: ìš”ì¦˜ ìˆ¨ì‰¬ê¸°ê°€ í˜ë“¤ì–´ìš”.\\n#Person1#: ìµœê·¼ì— ê°ê¸°ì— ê±¸ë ¸ë‚˜ìš”?\\n#Person2#: ì•„ë‹ˆìš”, ê°ê¸°ëŠ” ì•ˆ ê±¸ë ¸ì–´ìš”. ìˆ¨ì‰´ ë•Œ ê°€ìŠ´ì´ ë‹µë‹µí•´ìš”.\\n#Person1#: í˜¹ì‹œ ì•Œê³  ìˆëŠ” ì•Œë ˆë¥´ê¸° ìˆ'...\n",
      "       summary: #Person2#ëŠ” ìˆ¨ì‰¬ê¸° ì–´ë ¤ì›Œí•©ë‹ˆë‹¤. ì˜ì‚¬ëŠ” #Person2#ì—ê²Œ ì¦ìƒì„ í™•ì¸í•˜ê³ , ì²œì‹ ê²€ì‚¬ë¥¼ ìœ„í•´ í ì „ë¬¸ì˜ì—ê²Œ ê°€ë³¼ ê²ƒì„ ê¶Œí•©ë‹ˆë‹¤.\n",
      "\n",
      "   [2] fname: dev_1\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì•¼ Jimmy, ì˜¤ëŠ˜ ì¢€ ì´ë”° ìš´ë™í•˜ëŸ¬ ê°€ì.\\n#Person2#: ê·¸ë˜, ëª‡ ì‹œì— ê°ˆë˜?\\n#Person1#: 3ì‹œ 30ë¶„ ì–´ë•Œ?\\n#Person2#: ì¢‹ì•„. ì˜¤ëŠ˜ì€ ë‹¤ë¦¬ë‘ íŒ” ìš´ë™í•˜ëŠ” ë‚ ì´ì•¼.\\n#Person1#: ê·¼ë° ë‚˜ ì•„ê¹Œ ë†êµ¬í•´ì„œ ë‹¤ë¦¬ê°€ ì¢€ ì•„íŒŒ'...\n",
      "       summary: #Person1#ëŠ” Jimmyë¥¼ ìš´ë™í•˜ëŸ¬ ì´ˆëŒ€í•˜ê³  íŒ”ê³¼ ë³µê·¼ ìš´ë™ì„ í•˜ë„ë¡ ì„¤ë“í•©ë‹ˆë‹¤.\n",
      "\n",
      "   [3] fname: dev_2\n",
      "       dialogue (ì²« 150ì): '#Person1#: ë‚˜ ì§„ì§œ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ ì¢€ ê·¸ë§Œ ë¨¹ì–´ì•¼ê² ì–´. \\n#Person2#: ë§ì•„, ë¬´ìŠ¨ ë§ì¸ì§€ ì•Œì•„. ë‚˜ë„ ìš”ì¦˜ ê±´ê°•í•˜ê²Œ ë¨¹ìœ¼ë ¤ê³  í•˜ê±°ë“ . \\n#Person1#: ìš”ì¦˜ì€ ë­ ë¨¹ì–´? \\n#Person2#: ì£¼ë¡œ ê³¼ì¼ì´ë‘ ì±„ì†Œ, ë‹­ê³ ê¸° ë¨¹ì§€. \\n#Person'...\n",
      "       summary: #Person1#ì€ ê±´ê°•ì— ì•ˆ ì¢‹ì€ ìŒì‹ì„ ê·¸ë§Œ ë¨¹ê¸°ë¡œ ê²°ì‹¬í•˜ê³ , #Person2#ëŠ” ìì‹ ì˜ ê±´ê°•í•œ ì‹ë‹¨ì„ #Person1#ì—ê²Œ ê³µìœ í•©ë‹ˆë‹¤.\n",
      "\n",
      "================================================================================\n",
      "Test ë¶„ì„ (499 samples)\n",
      "================================================================================\n",
      "1. \\n í¬í•¨: 0ê°œ (0.0%)\n",
      "2. <br> íƒœê·¸ í¬í•¨: 0ê°œ (0.0%)\n",
      "3. ì—°ì† ê³µë°±(3ê°œ+): 0ê°œ (0.0%)\n",
      "4. ì—°ì† ê°œí–‰(3ê°œ+): 0ê°œ (0.0%)\n",
      "\n",
      "5. Person íƒœê·¸ í˜•íƒœ:\n",
      "   - #Person1# í˜•íƒœ: 499ê°œ\n",
      "   - # Person 1 # í˜•íƒœ: 0ê°œ\n",
      "   - #person1# í˜•íƒœ: 0ê°œ\n",
      "   - ì´ Person íƒœê·¸ í¬í•¨: 499ê°œ (100.0%)\n",
      "\n",
      "6. ì•ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "   ë’¤ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "\n",
      "7. íŠ¹ìˆ˜ë¬¸ì í¬í•¨: 26ê°œ (5.2%)\n",
      "\n",
      "8. ëŒ€í™” ê¸¸ì´ í†µê³„:\n",
      "   - ìµœì†Œ: 111ì\n",
      "   - ìµœëŒ€: 2275ì\n",
      "   - í‰ê· : 422.1ì\n",
      "   - ì¤‘ì•™ê°’: 386.0ì\n",
      "\n",
      "9. ì‹¤ì œ ìƒ˜í”Œ (ì²« 3ê°œ):\n",
      "\n",
      "   [1] fname: test_0\n",
      "       dialogue (ì²« 150ì): '#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. \\n#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... \\n#Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? \\n#Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. \\n#Person1#'...\n",
      "\n",
      "   [2] fname: test_1\n",
      "       dialogue (ì²« 150ì): '#Person1#: ë“œë””ì–´ ì™”ë„¤! ë­ê°€ ì´ë ‡ê²Œ ì˜¤ë˜ ê±¸ë ¸ì–´?\\n#Person2#: ì°¨ê°€ ë˜ ë§‰í˜”ì–´. Carrefour êµì°¨ë¡œ ê·¼ì²˜ì—ì„œ êµí†µì²´ì¦ì´ ì—„ì²­ ì‹¬í–ˆê±°ë“ .\\n#Person1#: ê±°ê¸´ ì¶œí‡´ê·¼ ì‹œê°„ì— í•­ìƒ í˜¼ì¡í•˜ì–ì•„. ì§‘ì— ê°ˆ ë•Œ ë‹¤ë¥¸ ê¸¸ ì¢€ ì°¾ì•„ë³´ëŠ” ê²Œ ì–´ë•Œ?\\n#Per'...\n",
      "\n",
      "   [3] fname: test_2\n",
      "       dialogue (ì²« 150ì): '#Person1#: Kate, ì—¬ê¸°ì„œ ì¼ì–´ë‚œ ì¼ì„ ë¯¿ê¸° í˜ë“¤ ê±°ì•¼.\\n#Person2#: ë¬´ìŠ¨ ì¼ì´ì•¼?\\n#Person1#: Mashaë‘ Heroê°€ ì´í˜¼í•œëŒ€.\\n#Person2#: ì„¤ë§ˆ, ë¬´ìŠ¨ ì¼ ìˆì—ˆë˜ ê±°ì•¼?\\n#Person1#: ê¸€ì„, ìì„¸íˆëŠ” ëª¨ë¥´ê² ëŠ”ë° ë‘ ë‹¬ ë™ì•ˆ ë³„ê±°'...\n",
      "\n",
      "================================================================================\n",
      "Augmented (ê³ í’ˆì§ˆ) ë¶„ì„ (3476 samples)\n",
      "================================================================================\n",
      "1. \\n í¬í•¨: 0ê°œ (0.0%)\n",
      "2. <br> íƒœê·¸ í¬í•¨: 0ê°œ (0.0%)\n",
      "3. ì—°ì† ê³µë°±(3ê°œ+): 0ê°œ (0.0%)\n",
      "4. ì—°ì† ê°œí–‰(3ê°œ+): 0ê°œ (0.0%)\n",
      "\n",
      "5. Person íƒœê·¸ í˜•íƒœ:\n",
      "   - #Person1# í˜•íƒœ: 3476ê°œ\n",
      "   - # Person 1 # í˜•íƒœ: 0ê°œ\n",
      "   - #person1# í˜•íƒœ: 0ê°œ\n",
      "   - ì´ Person íƒœê·¸ í¬í•¨: 3476ê°œ (100.0%)\n",
      "\n",
      "6. ì•ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "   ë’¤ìª½ ê³µë°± í¬í•¨: 0ê°œ (0.0%)\n",
      "\n",
      "7. íŠ¹ìˆ˜ë¬¸ì í¬í•¨: 2947ê°œ (84.8%)\n",
      "\n",
      "8. ëŒ€í™” ê¸¸ì´ í†µê³„:\n",
      "   - ìµœì†Œ: 171ì\n",
      "   - ìµœëŒ€: 2058ì\n",
      "   - í‰ê· : 559.6ì\n",
      "   - ì¤‘ì•™ê°’: 522.0ì\n",
      "\n",
      "9. ì‹¤ì œ ìƒ˜í”Œ (ì²« 3ê°œ):\n",
      "\n",
      "   [1] fname: train_9687_aug\n",
      "       dialogue (ì²« 150ì): '#Person1#: ë³´ìƒ ìš”ì²­ì„ ë°›ê³  ë†€ëìŠµë‹ˆë‹¤. ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”?  \\n#Person2#: í’ˆì§ˆì— ê´€í•œ ë¬¸ì œì…ë‹ˆë‹¤.  \\n#Person1#: ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?  \\n#Person2#: í”„ë¦¬ë¯¸ì—„ ë“±ê¸‰ ì œí’ˆì„ ìš”êµ¬í–ˆë˜ ê²ƒ, ê¸°ì–µë‚˜ì§€ ì•Šë‚˜ìš”? í’ˆì§ˆ, ì‚¬ì–‘, ê°€ê²©ì€ '...\n",
      "       summary: #Person2#ëŠ” ìƒí’ˆì˜ 3ë¶„ì˜ 1ì´ ê¸°ì¤€ì— ë¯¸ë‹¬í•˜ì—¬ #Person1#ì—ê²Œ ë³´ìƒì„ ìš”ì²­í•©ë‹ˆë‹¤.\n",
      "\n",
      "   [2] fname: train_10704_aug\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì§€ê¸ˆê¹Œì§€ ì–¼ë§ˆë¥¼ ì¼ê³  ì–´ë””ì— ì¼ëŠ”ì§€ íŒŒì•…í•´ì•¼ í•´.  \\n#Person2#: ì™œ?  \\n#Person1#: ìŒ, ë” ì ˆì•½í•´ì•¼ í•  ê²ƒ ê°™ì•„ì„œ. ëˆì„ ë” ëª¨ìœ¼ë©´ ì¼ì° ì€í‡´í•´ì„œ ì‚¶ì„ ë” ì¦ê¸¸ ìˆ˜ ìˆì–ì•„.  \\n#Person2#: ì •ë§? ì¢‹ì•„, ì˜ìˆ˜ì¦ì„ ë³´ì.  '...\n",
      "       summary: #Person1#ê³¼ #Person2#ëŠ” ì§€ì¶œì„ ê²€í† í•˜ê³  ì ˆì•½í•  ë°©ë²•ì„ ì˜ë…¼í•˜ì—¬, ì™¸ì‹ ëŒ€ì‹  ì§‘ì—ì„œ ì‹ì‚¬ë¥¼ ë” ìì£¼ í•˜ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "   [3] fname: train_2233_aug\n",
      "       dialogue (ì²« 150ì): '#Person1#: ì•„ë¹ , ì € ì‚¬ë‘í•˜ì‹œì£ ?  \\n#Person2#: ë¬¼ë¡ ì´ì§€. ì™œ? ë¬´ìŠ¨ ì¼ì´ì•¼?  \\n#Person1#: ì‹ ë¬¸ì—ì„œ ê³µì§œ íœ´ëŒ€í° í”„ë¡œëª¨ì…˜ì„ ë´¤ì–´ìš”â€¦  \\n#Person2#: ê³µì§œë¼ê³ ? ì„¸ìƒì— ê³µì§œëŠ” ì—†ì–´.  \\n#Person1#: ì•„ë‹ˆì—ìš”, íœ´ëŒ€í°ì€ ê³µì§œì¸ë°â€¦'...\n",
      "       summary: #Person1#ì€ ì•„ë¹ ì—ê²Œ ìƒˆ íœ´ëŒ€í°ì„ ì‚¬ë‹¬ë¼ê³  ë¶€íƒí•˜ë©° ì¹œêµ¬ë“¤ì´ ëª¨ë‘ íœ´ëŒ€í°ì„ ê°–ê³  ìˆë‹¤ëŠ” ì´ìœ ì™€ ì°¨ ê³ ì¥ ì‹œ ì—°ë½ì„ ìœ„í•´ í•„ìš”í•˜ë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. #Person1#ì€ ì´í›„ ìƒˆ ì°¨ë„ í•¨ê»˜ ìš”êµ¬í•˜ì§€ë§Œ, ì•„ë¹ ëŠ” ì´ ìš”êµ¬ê°€ í„°ë¬´ë‹ˆì—†ë‹¤ê³  ëŠë‚ë‹ˆë‹¤.\n",
      "\n",
      "================================================================================\n",
      "ì¢…í•© ìš”ì•½\n",
      "================================================================================\n",
      "           total  backslash_n  br_tag  multi_space  multi_newline  person_tag  \\\n",
      "Train      12457            1       1            0              0       12457   \n",
      "Dev          499            0       0            0              0         499   \n",
      "Test         499            0       0            0              0         499   \n",
      "Augmented   3476            0       0            0              0        3476   \n",
      "\n",
      "           leading_space  trailing_space  special_chars  \n",
      "Train                  0               0            642  \n",
      "Dev                    0               0             27  \n",
      "Test                   0               0             26  \n",
      "Augmented              0               0           2947  \n",
      "\n",
      "================================================================================\n",
      "ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨\n",
      "================================================================================\n",
      "\n",
      "ì „ì²´ ìƒ˜í”Œ: 13455ê°œ\n",
      "\n",
      "ë…¸ì´ì¦ˆ ë¹„ìœ¨:\n",
      "  - \\n: 1ê°œ (0.01%)\n",
      "  - <br>: 1ê°œ (0.01%)\n",
      "  - ì—°ì† ê³µë°±: 0ê°œ (0.00%)\n",
      "\n",
      "================================================================================\n",
      "ê¶Œì¥ ì‚¬í•­:\n",
      "================================================================================\n",
      "âŒ \\n ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\n",
      "âŒ <br> ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\n",
      "âŒ ì—°ì† ê³µë°± ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\n",
      "\n",
      "================================================================================\n",
      "âœ¨ ë°ì´í„°ê°€ ë§¤ìš° ê¹¨ë—í•©ë‹ˆë‹¤! ì „ì²˜ë¦¬ ë¶ˆí•„ìš”\n",
      "   â†’ ë°”ë¡œ ëª¨ë¸ í•™ìŠµ ì§„í–‰ ê°€ëŠ¥\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def analyze_noise(df, df_name):\n",
    "    \"\"\"ì‹¤ì œ ë°ì´í„°ì— ì–´ë–¤ ë…¸ì´ì¦ˆê°€ ìˆëŠ”ì§€ í™•ì¸\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{df_name} ë¶„ì„ ({len(df)} samples)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    dialogues = df['dialogue'].astype(str)\n",
    "    \n",
    "    # 1. \\\\n ì²´í¬\n",
    "    backslash_n_count = dialogues.str.contains(r'\\\\n', regex=True).sum()\n",
    "    print(f\"1. \\\\n í¬í•¨: {backslash_n_count}ê°œ ({backslash_n_count/len(df)*100:.1f}%)\")\n",
    "    if backslash_n_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'\\\\n', regex=True)].index[0]\n",
    "        print(f\"   ìƒ˜í”Œ: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 2. <br> íƒœê·¸ ì²´í¬\n",
    "    br_count = dialogues.str.contains(r'<br', regex=True, flags=re.IGNORECASE).sum()\n",
    "    print(f\"2. <br> íƒœê·¸ í¬í•¨: {br_count}ê°œ ({br_count/len(df)*100:.1f}%)\")\n",
    "    if br_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'<br', regex=True, flags=re.IGNORECASE)].index[0]\n",
    "        print(f\"   ìƒ˜í”Œ: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 3. ì—°ì† ê³µë°± ì²´í¬ (3ê°œ ì´ìƒ)\n",
    "    multi_space_count = dialogues.str.contains(r'   +', regex=True).sum()\n",
    "    print(f\"3. ì—°ì† ê³µë°±(3ê°œ+): {multi_space_count}ê°œ ({multi_space_count/len(df)*100:.1f}%)\")\n",
    "    if multi_space_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'   +', regex=True)].index[0]\n",
    "        print(f\"   ìƒ˜í”Œ: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 4. ì—°ì† ê°œí–‰ ì²´í¬ (3ê°œ ì´ìƒ)\n",
    "    multi_newline_count = dialogues.str.contains(r'\\n\\n\\n+', regex=True).sum()\n",
    "    print(f\"4. ì—°ì† ê°œí–‰(3ê°œ+): {multi_newline_count}ê°œ ({multi_newline_count/len(df)*100:.1f}%)\")\n",
    "    if multi_newline_count > 0:\n",
    "        sample_idx = dialogues[dialogues.str.contains(r'\\n\\n\\n+', regex=True)].index[0]\n",
    "        print(f\"   ìƒ˜í”Œ: {dialogues.iloc[sample_idx][:100]}...\")\n",
    "    \n",
    "    # 5. Person íƒœê·¸ í˜•íƒœ ë¶„ì„\n",
    "    print(f\"\\n5. Person íƒœê·¸ í˜•íƒœ:\")\n",
    "    \n",
    "    # ì •ê·œ í˜•íƒœ: #Person1#\n",
    "    regular_pattern = dialogues.str.contains(r'#Person\\d+#', regex=True).sum()\n",
    "    print(f\"   - #Person1# í˜•íƒœ: {regular_pattern}ê°œ\")\n",
    "    \n",
    "    # ê³µë°± ìˆëŠ” í˜•íƒœ: # Person 1 #\n",
    "    space_pattern = dialogues.str.contains(r'#\\s+Person\\s+\\d+\\s+#', regex=True).sum()\n",
    "    print(f\"   - # Person 1 # í˜•íƒœ: {space_pattern}ê°œ\")\n",
    "    \n",
    "    # ì†Œë¬¸ì í˜•íƒœ: #person1#\n",
    "    lower_pattern = dialogues.str.contains(r'#person\\d+#', regex=True).sum()\n",
    "    print(f\"   - #person1# í˜•íƒœ: {lower_pattern}ê°œ\")\n",
    "    \n",
    "    # ë¶ˆê·œì¹™ í˜•íƒœ (ì „ì²´)\n",
    "    irregular_pattern = dialogues.str.contains(r'#\\s*[Pp]erson\\s*\\d+\\s*#', regex=True).sum()\n",
    "    print(f\"   - ì´ Person íƒœê·¸ í¬í•¨: {irregular_pattern}ê°œ ({irregular_pattern/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 6. ì•ë’¤ ê³µë°± ì²´í¬\n",
    "    leading_space = dialogues.str.match(r'^\\s+').sum()\n",
    "    trailing_space = dialogues.str.match(r'.*\\s+$').sum()\n",
    "    print(f\"\\n6. ì•ìª½ ê³µë°± í¬í•¨: {leading_space}ê°œ ({leading_space/len(df)*100:.1f}%)\")\n",
    "    print(f\"   ë’¤ìª½ ê³µë°± í¬í•¨: {trailing_space}ê°œ ({trailing_space/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 7. íŠ¹ìˆ˜ë¬¸ì ì²´í¬\n",
    "    special_chars = dialogues.str.contains(r'[^\\w\\sê°€-í£#:\\n\\-.,!?()\\'\\\"@]', regex=True).sum()\n",
    "    print(f\"\\n7. íŠ¹ìˆ˜ë¬¸ì í¬í•¨: {special_chars}ê°œ ({special_chars/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # 8. ê¸¸ì´ í†µê³„\n",
    "    print(f\"\\n8. ëŒ€í™” ê¸¸ì´ í†µê³„:\")\n",
    "    lengths = dialogues.str.len()\n",
    "    print(f\"   - ìµœì†Œ: {lengths.min()}ì\")\n",
    "    print(f\"   - ìµœëŒ€: {lengths.max()}ì\")\n",
    "    print(f\"   - í‰ê· : {lengths.mean():.1f}ì\")\n",
    "    print(f\"   - ì¤‘ì•™ê°’: {lengths.median():.1f}ì\")\n",
    "    \n",
    "    # 9. ìƒ˜í”Œ ì¶œë ¥\n",
    "    print(f\"\\n9. ì‹¤ì œ ìƒ˜í”Œ (ì²« 3ê°œ):\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\n   [{i+1}] fname: {df.iloc[i]['fname']}\")\n",
    "        print(f\"       dialogue (ì²« 150ì): {repr(dialogues.iloc[i][:150])}...\")\n",
    "        if 'summary' in df.columns:\n",
    "            print(f\"       summary: {df.iloc[i]['summary']}\")\n",
    "    \n",
    "    return {\n",
    "        'total': len(df),\n",
    "        'backslash_n': backslash_n_count,\n",
    "        'br_tag': br_count,\n",
    "        'multi_space': multi_space_count,\n",
    "        'multi_newline': multi_newline_count,\n",
    "        'person_tag': irregular_pattern,\n",
    "        'leading_space': leading_space,\n",
    "        'trailing_space': trailing_space,\n",
    "        'special_chars': special_chars\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ì‹¤í–‰ ì½”ë“œ\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ì „ì²´ ë°ì´í„°ì…‹ ë…¸ì´ì¦ˆ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/train.csv')\n",
    "dev_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/dev.csv')\n",
    "test_df = pd.read_csv('/home/NLP_contest/NPL_contest/data/test.csv')\n",
    "\n",
    "# ê° ë°ì´í„°ì…‹ ë¶„ì„\n",
    "train_stats = analyze_noise(train_df, \"Train\")\n",
    "dev_stats = analyze_noise(dev_df, \"Dev\")\n",
    "test_stats = analyze_noise(test_df, \"Test\")\n",
    "\n",
    "# ì¦ê°• ë°ì´í„°ë„ í™•ì¸ (ìˆìœ¼ë©´)\n",
    "try:\n",
    "    augmented_df = pd.read_csv('/home/NLP_contest/NPL_contest/augmented_data/high_quality_augmented.csv')\n",
    "    aug_stats = analyze_noise(augmented_df, \"Augmented (ê³ í’ˆì§ˆ)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nâš ï¸ ì¦ê°• ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    aug_stats = None\n",
    "\n",
    "# ============================================================\n",
    "# ì¢…í•© ìš”ì•½\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ì¢…í•© ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    'Train': train_stats,\n",
    "    'Dev': dev_stats,\n",
    "    'Test': test_stats\n",
    "})\n",
    "\n",
    "if aug_stats:\n",
    "    summary_table['Augmented'] = aug_stats\n",
    "\n",
    "print(summary_table.T)\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ì „ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_samples = train_stats['total'] + dev_stats['total'] + test_stats['total']\n",
    "total_backslash_n = train_stats['backslash_n'] + dev_stats['backslash_n'] + test_stats['backslash_n']\n",
    "total_br = train_stats['br_tag'] + dev_stats['br_tag'] + test_stats['br_tag']\n",
    "total_multi_space = train_stats['multi_space'] + dev_stats['multi_space'] + test_stats['multi_space']\n",
    "\n",
    "print(f\"\\nì „ì²´ ìƒ˜í”Œ: {total_samples}ê°œ\")\n",
    "print(f\"\\në…¸ì´ì¦ˆ ë¹„ìœ¨:\")\n",
    "print(f\"  - \\\\n: {total_backslash_n}ê°œ ({total_backslash_n/total_samples*100:.2f}%)\")\n",
    "print(f\"  - <br>: {total_br}ê°œ ({total_br/total_samples*100:.2f}%)\")\n",
    "print(f\"  - ì—°ì† ê³µë°±: {total_multi_space}ê°œ ({total_multi_space/total_samples*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ê¶Œì¥ ì‚¬í•­:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# íŒë‹¨ ê¸°ì¤€: 1% ì´ìƒì´ë©´ ì „ì²˜ë¦¬ ê¶Œì¥\n",
    "needs_preprocessing = []\n",
    "\n",
    "if total_backslash_n / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"\\\\n â†’ \\\\n ë³€í™˜\")\n",
    "    print(\"âœ… \\\\n ì „ì²˜ë¦¬ í•„ìš” (1% ì´ìƒ)\")\n",
    "else:\n",
    "    print(\"âŒ \\\\n ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\")\n",
    "\n",
    "if total_br / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"<br> â†’ \\\\n ë³€í™˜\")\n",
    "    print(\"âœ… <br> ì „ì²˜ë¦¬ í•„ìš” (1% ì´ìƒ)\")\n",
    "else:\n",
    "    print(\"âŒ <br> ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\")\n",
    "\n",
    "if total_multi_space / total_samples > 0.01:\n",
    "    needs_preprocessing.append(\"ì—°ì† ê³µë°± ì œê±°\")\n",
    "    print(\"âœ… ì—°ì† ê³µë°± ì „ì²˜ë¦¬ í•„ìš” (1% ì´ìƒ)\")\n",
    "else:\n",
    "    print(\"âŒ ì—°ì† ê³µë°± ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (1% ë¯¸ë§Œ)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if needs_preprocessing:\n",
    "    print(\"ğŸ”§ ì „ì²˜ë¦¬ í•„ìš” í•­ëª©:\")\n",
    "    for item in needs_preprocessing:\n",
    "        print(f\"   - {item}\")\n",
    "else:\n",
    "    print(\"âœ¨ ë°ì´í„°ê°€ ë§¤ìš° ê¹¨ë—í•©ë‹ˆë‹¤! ì „ì²˜ë¦¬ ë¶ˆí•„ìš”\")\n",
    "    print(\"   â†’ ë°”ë¡œ ëª¨ë¸ í•™ìŠµ ì§„í–‰ ê°€ëŠ¥\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13105d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ë°ì´í„°ì…‹ ë…¸ì´ì¦ˆ ìš”ì•½\n",
      "================================================================================\n",
      "\n",
      "Train:\n",
      "  ì´ ìƒ˜í”Œ: 12457\n",
      "  \\n: 1ê°œ (0.01%)\n",
      "  <br>: 1ê°œ (0.01%)\n",
      "  ì—°ì† ê³µë°±: 0ê°œ (0.00%)\n",
      "  Person íƒœê·¸: 12457ê°œ (100.00%)\n",
      "\n",
      "Dev:\n",
      "  ì´ ìƒ˜í”Œ: 499\n",
      "  \\n: 0ê°œ (0.00%)\n",
      "  <br>: 0ê°œ (0.00%)\n",
      "  ì—°ì† ê³µë°±: 0ê°œ (0.00%)\n",
      "  Person íƒœê·¸: 499ê°œ (100.00%)\n",
      "\n",
      "Test:\n",
      "  ì´ ìƒ˜í”Œ: 499\n",
      "  \\n: 0ê°œ (0.00%)\n",
      "  <br>: 0ê°œ (0.00%)\n",
      "  ì—°ì† ê³µë°±: 0ê°œ (0.00%)\n",
      "  Person íƒœê·¸: 499ê°œ (100.00%)\n",
      "\n",
      "================================================================================\n",
      "ì¢…í•© í‘œ\n",
      "================================================================================\n",
      "Dataset  Total  \\n  <br>  ì—°ì†ê³µë°±  Personíƒœê·¸\n",
      "  Train  12457   1     1     0     12457\n",
      "    Dev    499   0     0     0       499\n",
      "   Test    499   0     0     0       499\n",
      "\n",
      "================================================================================\n",
      "ì „ì²´ í†µê³„\n",
      "================================================================================\n",
      "ì „ì²´ ìƒ˜í”Œ: 13455ê°œ\n",
      "\\n ì´: 1ê°œ (0.007%)\n",
      "<br> ì´: 1ê°œ (0.007%)\n",
      "\n",
      "================================================================================\n",
      "ê²°ë¡ \n",
      "================================================================================\n",
      "âœ¨ ëª¨ë“  ë°ì´í„°ì…‹ì´ ë§¤ìš° ê¹¨ë—í•©ë‹ˆë‹¤!\n",
      "   ì „ì²˜ë¦¬ ë¶ˆí•„ìš” â†’ ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train = pd.read_csv('/home/NLP_contest/NPL_contest/data/train.csv')\n",
    "dev = pd.read_csv('/home/NLP_contest/NPL_contest/data/dev.csv')\n",
    "test = pd.read_csv('/home/NLP_contest/NPL_contest/data/test.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ë°ì´í„°ì…‹ ë…¸ì´ì¦ˆ ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {\n",
    "    'Train': train,\n",
    "    'Dev': dev,\n",
    "    'Test': test\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    backslash_n = df['dialogue'].str.contains(r'\\\\n', regex=True).sum()\n",
    "    br_tag = df['dialogue'].str.contains(r'<br', regex=True, case=False).sum()\n",
    "    multi_space = df['dialogue'].str.contains(r'   +', regex=True).sum()\n",
    "    person_tag = df['dialogue'].str.contains(r'#Person\\d+#', regex=True).sum()\n",
    "    \n",
    "    summary.append({\n",
    "        'Dataset': name,\n",
    "        'Total': len(df),\n",
    "        '\\\\n': backslash_n,\n",
    "        '<br>': br_tag,\n",
    "        'ì—°ì†ê³µë°±': multi_space,\n",
    "        'Personíƒœê·¸': person_tag\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  ì´ ìƒ˜í”Œ: {len(df)}\")\n",
    "    print(f\"  \\\\n: {backslash_n}ê°œ ({backslash_n/len(df)*100:.2f}%)\")\n",
    "    print(f\"  <br>: {br_tag}ê°œ ({br_tag/len(df)*100:.2f}%)\")\n",
    "    print(f\"  ì—°ì† ê³µë°±: {multi_space}ê°œ ({multi_space/len(df)*100:.2f}%)\")\n",
    "    print(f\"  Person íƒœê·¸: {person_tag}ê°œ ({person_tag/len(df)*100:.2f}%)\")\n",
    "\n",
    "# í‘œë¡œ ì •ë¦¬\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ì¢…í•© í‘œ\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ì „ì²´ í†µê³„\n",
    "total_samples = sum([len(df) for df in datasets.values()])\n",
    "total_backslash_n = sum([df['dialogue'].str.contains(r'\\\\n', regex=True).sum() for df in datasets.values()])\n",
    "total_br = sum([df['dialogue'].str.contains(r'<br', regex=True, case=False).sum() for df in datasets.values()])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ì „ì²´ í†µê³„\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ì „ì²´ ìƒ˜í”Œ: {total_samples}ê°œ\")\n",
    "print(f\"\\\\n ì´: {total_backslash_n}ê°œ ({total_backslash_n/total_samples*100:.3f}%)\")\n",
    "print(f\"<br> ì´: {total_br}ê°œ ({total_br/total_samples*100:.3f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ê²°ë¡ \")\n",
    "print(\"=\"*80)\n",
    "if total_backslash_n / total_samples < 0.01 and total_br / total_samples < 0.01:\n",
    "    print(\"âœ¨ ëª¨ë“  ë°ì´í„°ì…‹ì´ ë§¤ìš° ê¹¨ë—í•©ë‹ˆë‹¤!\")\n",
    "    print(\"   ì „ì²˜ë¦¬ ë¶ˆí•„ìš” â†’ ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì¼ë¶€ ì „ì²˜ë¦¬ í•„ìš”\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b11ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090\n",
      "GPU Memory: 23.69 GB\n",
      "\n",
      "================================================================================\n",
      "ë°ì´í„° ë¡œë“œ ë° ê²°í•©\n",
      "================================================================================\n",
      "ì›ë³¸ Train ë°ì´í„°: 12457 samples\n",
      "ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„°: 3476 samples\n",
      "ìµœì¢… Train ë°ì´í„°: 15933 samples\n",
      "  - ì›ë³¸: 12457 samples\n",
      "  - ì¦ê°•: 3476 samples\n",
      "\n",
      "Dev: 499 samples\n",
      "Test: 499 samples\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "KoT5 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
      "================================================================================\n",
      "Tokenizer loaded: KETI-AIR/ke-t5-base\n",
      "Vocab size: 64100\n",
      "Added 7 special tokens: ['#Person1#', '#Person2#', '#Person3#', '#Person4#', '#Person5#', '#Person6#', '#Person7#']\n",
      "Total parameters: 247,463,424\n",
      "Trainable parameters: 247,463,424\n",
      "\n",
      "================================================================================\n",
      "ë°ì´í„°ì…‹ ìƒì„±\n",
      "================================================================================\n",
      "Train batches: 1992\n",
      "Dev batches: 63\n",
      "Test batches: 63\n",
      "\n",
      "================================================================================\n",
      "í•™ìŠµ ì„¤ì •\n",
      "================================================================================\n",
      "Total training steps: 11952\n",
      "Warmup steps: 1195\n",
      "Learning rate: 0.0001\n",
      "\n",
      "================================================================================\n",
      "KoT5 í•™ìŠµ ì‹œì‘ (ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° í™œìš©)\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:51<00:00,  2.80it/s, loss=4.4603, lr=9.87e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 12.3613\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 38.78\n",
      "ROUGE-2 F1: 15.92\n",
      "ROUGE-L F1: 35.26\n",
      "================================================================================\n",
      "Final Score: 89.96\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 89.96)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 2/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:52<00:00,  2.79it/s, loss=3.2594, lr=8.43e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0283\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:07<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 45.91\n",
      "ROUGE-2 F1: 21.61\n",
      "ROUGE-L F1: 40.71\n",
      "================================================================================\n",
      "Final Score: 108.23\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 108.23)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 3/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:51<00:00,  2.80it/s, loss=3.1098, lr=5.87e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.4254\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:02<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 46.37\n",
      "ROUGE-2 F1: 22.32\n",
      "ROUGE-L F1: 41.11\n",
      "================================================================================\n",
      "Final Score: 109.80\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 109.80)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 4/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:52<00:00,  2.80it/s, loss=3.2330, lr=3.02e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1495\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 47.24\n",
      "ROUGE-2 F1: 22.98\n",
      "ROUGE-L F1: 41.61\n",
      "================================================================================\n",
      "Final Score: 111.84\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 111.84)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 5/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:51<00:00,  2.80it/s, loss=3.0736, lr=8.23e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0039\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 48.11\n",
      "ROUGE-2 F1: 23.81\n",
      "ROUGE-L F1: 42.31\n",
      "================================================================================\n",
      "Final Score: 114.23\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 114.23)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "Epoch 6/6\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1992/1992 [11:52<00:00,  2.80it/s, loss=2.5434, lr=0.00e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9503\n",
      "\n",
      "Dev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\n",
      "\n",
      "ROUGE-1 F1: 48.38\n",
      "ROUGE-2 F1: 23.81\n",
      "ROUGE-L F1: 42.60\n",
      "================================================================================\n",
      "Final Score: 114.79\n",
      "================================================================================\n",
      "âœ“ New best model! (ROUGE Final: 114.79)\n",
      "Model saved to /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "\n",
      "================================================================================\n",
      "í•™ìŠµ ì™„ë£Œ!\n",
      "================================================================================\n",
      "Best ROUGE Final: 114.79\n",
      "\n",
      "================================================================================\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [01:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì œì¶œ íŒŒì¼ ì €ì¥: /home/NLP_contest/NPL_contest/outputs/submission_kot5_filtered_aug_20251013_034209.csv\n",
      "ìƒì„±ëœ ìš”ì•½ ê°œìˆ˜: 499\n",
      "\n",
      "================================================================================\n",
      "ìƒì„±ëœ ìš”ì•½ ìƒ˜í”Œ\n",
      "================================================================================\n",
      "\n",
      "[Sample 1]\n",
      "Fname: test_0\n",
      "Summary (29 words): ì•„ì´ì–¸ Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ë³´ë‚´ë‹¬ë¼ê³  ìš”ì²­í•©ë‹ˆë‹¤. Ms. Dawsonì€ #Person2# ì—ê²Œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ìš©ì€ ì œí•œë˜ì§€ë§Œ, #Person1# ì€ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ìš©ì´ ì œí•œëœë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. #Person2# ëŠ” #Person1# ì˜ ì œì•ˆì— ë™ì˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "[Sample 2]\n",
      "Fname: test_1\n",
      "Summary (17 words): ì¬ìƒì€ #Person1# ì—ê²Œ ì§€í•˜ì² ìœ¼ë¡œ ì¶œí‡´ê·¼í•  ê²ƒì„ ì œì•ˆí•˜ì§€ë§Œ, #Person1# ì€ ì°¨ë¥¼ í”¼í•˜ê³  ì‹¶ì–´í•©ë‹ˆë‹¤. #Person2# ëŠ” ëŒ€ì¤‘êµí†µìœ¼ë¡œ ì¶œê·¼í•˜ê¸°ë¡œ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "[Sample 3]\n",
      "Fname: test_2\n",
      "Summary (17 words): ì•„ì´ì–¸ì€ Kateì—ê²Œ Mashaì™€ Heroê°€ ì´í˜¼í–ˆë‹¤ê³  ë§í•©ë‹ˆë‹¤. KateëŠ” #Person1# ì—ê²Œ Mashaê°€ ì–‘ìœ¡ê¶Œì„ ê°€ì§€ê³ , ì•„ì´ë“¤ì„ ì–‘ìœ¡í•  ìˆ˜ ìˆë‹¤ê³  ë§í•©ë‹ˆë‹¤.\n",
      "\n",
      "[Sample 4]\n",
      "Fname: test_3\n",
      "Summary (12 words): ì¬ìƒì€ #Person1# ì˜ ìƒì¼ íŒŒí‹°ì— ì´ˆëŒ€í•˜ê³ , #Person1# ì€ #Person2# ì—ê²Œ ì¶¤ì¶”ê¸°ë¡œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "[Sample 5]\n",
      "Fname: test_4\n",
      "Summary (20 words): ì•„ì´ì–¸ ê³µì›ì€ #Person1# ì—ê²Œ ì˜¬ë¦¼í”½ ìŠ¤íƒ€ë””ì›€ì— ëŒ€í•´ ì´ì•¼ê¸°í•©ë‹ˆë‹¤. #Person1# ì€ #Person2# ì—ê²Œ ì¢Œì„ì´ 5000ì„ì´ë¼ê³  ì„¤ëª…í•©ë‹ˆë‹¤. #Person2# ëŠ” ë“±ì‚° ê¸ˆê¸°ë¼ê³  ì„¤ëª…í•©ë‹ˆë‹¤.\n",
      "\n",
      "================================================================================\n",
      "ìš”ì•½ ê¸¸ì´ í†µê³„\n",
      "================================================================================\n",
      "í‰ê· : 18.0 ë‹¨ì–´\n",
      "ì¤‘ê°„ê°’: 18.0 ë‹¨ì–´\n",
      "ìµœì†Œ: 6 ë‹¨ì–´\n",
      "ìµœëŒ€: 34 ë‹¨ì–´\n",
      "í‘œì¤€í¸ì°¨: 5.6 ë‹¨ì–´\n",
      "\n",
      "================================================================================\n",
      "KoT5 + ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "================================================================================\n",
      "\n",
      "ì €ì¥ëœ íŒŒì¼:\n",
      "1. ëª¨ë¸: /home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation\n",
      "2. ì œì¶œ íŒŒì¼: /home/NLP_contest/NPL_contest/outputs/submission_kot5_filtered_aug_20251013_034209.csv\n",
      "\n",
      "ê¸°ëŒ€ ì„±ëŠ¥:\n",
      "- ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„°ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
      "- ë…¸ì´ì¦ˆê°€ ì œê±°ë˜ì–´ ë” ì•ˆì •ì ì¸ í•™ìŠµ\n",
      "- ì˜ˆìƒ ì ìˆ˜: 50~55ì  (í•„í„°ë§ëœ ì¦ê°• ë°ì´í„° íš¨ê³¼)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "KoT5 í•™ìŠµ - ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° í™œìš©\n",
    "í•„í„°ë§ëœ ì¦ê°• ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ í•™ìŠµ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Config ì„¤ì •\n",
    "# ========================================\n",
    "class Config:\n",
    "    # ê²½ë¡œ\n",
    "    train_path = '/home/NLP_contest/NPL_contest/data/train.csv'  # ì›ë³¸ train ë°ì´í„°\n",
    "    augmented_path = '/home/NLP_contest/NPL_contest/augmented_data/high_quality_augmented.csv'  # í•„í„°ë§ëœ ì¦ê°• ë°ì´í„°\n",
    "    dev_path = '/home/NLP_contest/NPL_contest/data/dev.csv'  # dev ê²½ë¡œ í™•ì¸ í•„ìš”\n",
    "    test_path = '/home/NLP_contest/NPL_contest/data/test.csv'  # test ê²½ë¡œ í™•ì¸ í•„ìš”\n",
    "    output_dir = '/home/NLP_contest/NPL_contest/outputs'\n",
    "    model_save_path = '/home/NLP_contest/NPL_contest/models/kot5_with_filtered_augmentation'\n",
    "    \n",
    "    # ëª¨ë¸ ì„¤ì •\n",
    "    model_name = 'KETI-AIR/ke-t5-base'  # KoT5 base ëª¨ë¸\n",
    "    max_input_length = 512  # ì…ë ¥ ìµœëŒ€ ê¸¸ì´\n",
    "    max_target_length = 128  # ì¶œë ¥ ìµœëŒ€ ê¸¸ì´\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    batch_size = 8  # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)\n",
    "    num_epochs = 6  # í•™ìŠµ ì—í­ ìˆ˜\n",
    "    learning_rate = 1e-4  # í•™ìŠµë¥ \n",
    "    warmup_ratio = 0.1  # Warmup ë¹„ìœ¨\n",
    "    weight_decay = 0.01  # Weight decay\n",
    "    max_grad_norm = 1.0  # Gradient clipping\n",
    "    label_smoothing = 0.1  # Label smoothing\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 3  # Early stopping patience\n",
    "    \n",
    "    # ìƒì„± ì„¤ì •\n",
    "    num_beams = 4  # Beam search í¬ê¸°\n",
    "    length_penalty = 1.0  # ê¸¸ì´ í˜ë„í‹°\n",
    "    no_repeat_ngram_size = 4  # N-gram ë°˜ë³µ ë°©ì§€\n",
    "    min_length = 10  # ìµœì†Œ ìƒì„± ê¸¸ì´\n",
    "    max_gen_length = 64  # ìµœëŒ€ ìƒì„± ê¸¸ì´\n",
    "    \n",
    "    # ì¦ê°• ë°ì´í„° ì‚¬ìš© ì„¤ì •\n",
    "    use_augmentation = True  # ì¦ê°• ë°ì´í„° ì‚¬ìš© ì—¬ë¶€\n",
    "    augmentation_ratio = 1.0  # ì¦ê°• ë°ì´í„° ì‚¬ìš© ë¹„ìœ¨ (1.0 = ì „ì²´ ì‚¬ìš©)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(config.output_dir, exist_ok=True)  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(config.model_save_path, exist_ok=True)  # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ë°ì´í„° ë¡œë“œ ë° ê²°í•©\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ë°ì´í„° ë¡œë“œ ë° ê²°í•©\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì›ë³¸ train ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(config.train_path)\n",
    "print(f\"ì›ë³¸ Train ë°ì´í„°: {len(train_df)} samples\")\n",
    "\n",
    "# ì¦ê°• ë°ì´í„° ë¡œë“œ ë° ê²°í•©\n",
    "if config.use_augmentation:\n",
    "    try:\n",
    "        # í•„í„°ë§ëœ ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° ë¡œë“œ\n",
    "        augmented_df = pd.read_csv(config.augmented_path)\n",
    "        print(f\"ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„°: {len(augmented_df)} samples\")\n",
    "        \n",
    "        # ì¦ê°• ë°ì´í„° ë¹„ìœ¨ ì ìš© (í•„ìš”ì‹œ)\n",
    "        if config.augmentation_ratio < 1.0:\n",
    "            sample_size = int(len(augmented_df) * config.augmentation_ratio)\n",
    "            augmented_df = augmented_df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"ì¦ê°• ë°ì´í„° ìƒ˜í”Œë§: {len(augmented_df)} samples ({config.augmentation_ratio*100:.0f}%)\")\n",
    "        \n",
    "        # ì¦ê°• ë°ì´í„° í‘œì‹œ ì»¬ëŸ¼ ì¶”ê°€ (ë””ë²„ê¹…ìš©)\n",
    "        train_df['is_augmented'] = False\n",
    "        augmented_df['is_augmented'] = True\n",
    "        \n",
    "        # ì›ë³¸ + ì¦ê°• ë°ì´í„° ê²°í•©\n",
    "        train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "        \n",
    "        # ë°ì´í„° ì…”í”Œ\n",
    "        train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"ìµœì¢… Train ë°ì´í„°: {len(train_df)} samples\")\n",
    "        print(f\"  - ì›ë³¸: {(~train_df['is_augmented']).sum()} samples\")\n",
    "        print(f\"  - ì¦ê°•: {train_df['is_augmented'].sum()} samples\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ ì¦ê°• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.augmented_path}\")\n",
    "        print(\"ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        train_df['is_augmented'] = False\n",
    "else:\n",
    "    print(\"ì¦ê°• ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    train_df['is_augmented'] = False\n",
    "\n",
    "# Dev, Test ë°ì´í„° ë¡œë“œ\n",
    "dev_df = pd.read_csv(config.dev_path)\n",
    "test_df = pd.read_csv(config.test_path)\n",
    "\n",
    "print(f\"\\nDev: {len(dev_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "# ========================================\n",
    "class DialogueSummarizationDataset(Dataset):\n",
    "    \"\"\"ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_input_length, max_target_length, is_test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: ë°ì´í„°í”„ë ˆì„\n",
    "            tokenizer: T5 í† í¬ë‚˜ì´ì €\n",
    "            max_input_length: ì…ë ¥ ìµœëŒ€ ê¸¸ì´\n",
    "            max_target_length: ì¶œë ¥ ìµœëŒ€ ê¸¸ì´\n",
    "            is_test: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.is_test = is_test\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # ëŒ€í™” í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        dialogue = str(row['dialogue'])\n",
    "        \n",
    "        # T5ëŠ” task prefixë¥¼ ì‚¬ìš© (ì˜ˆ: \"summarize: \")\n",
    "        input_text = f\"summarize: {dialogue}\"\n",
    "        \n",
    "        # ì…ë ¥ í† í°í™”\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',  # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©\n",
    "            truncation=True,  # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¥´ê¸°\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = input_encoding['input_ids'].squeeze()  # (seq_len,)\n",
    "        attention_mask = input_encoding['attention_mask'].squeeze()  # (seq_len,)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ëª¨ë“œë©´ fnameë„ ë°˜í™˜\n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'fname': row['fname']\n",
    "            }\n",
    "        \n",
    "        # ìš”ì•½ë¬¸ í† í°í™” (í•™ìŠµìš©)\n",
    "        summary = str(row['summary'])\n",
    "        \n",
    "        target_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding['input_ids'].squeeze()  # (seq_len,)\n",
    "        \n",
    "        # íŒ¨ë”© í† í°ì€ -100ìœ¼ë¡œ ë³€ê²½ (loss ê³„ì‚°ì—ì„œ ì œì™¸)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = T5TokenizerFast.from_pretrained(config.model_name)\n",
    "print(f\"Tokenizer loaded: {config.model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Person íƒœê·¸ ì¶”ê°€\n",
    "special_tokens = [f'#Person{i}#' for i in range(1, 8)]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(f\"Added {num_added} special tokens: {special_tokens}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))  # í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ê²Œ ì„ë² ë”© ì¡°ì •\n",
    "model = model.to(device)\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ë°ì´í„°ì…‹ ìƒì„±\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = DialogueSummarizationDataset(\n",
    "    train_df, tokenizer, \n",
    "    config.max_input_length, \n",
    "    config.max_target_length\n",
    ")\n",
    "\n",
    "dev_dataset = DialogueSummarizationDataset(\n",
    "    dev_df, tokenizer,\n",
    "    config.max_input_length,\n",
    "    config.max_target_length\n",
    ")\n",
    "\n",
    "test_dataset = DialogueSummarizationDataset(\n",
    "    test_df, tokenizer,\n",
    "    config.max_input_length,\n",
    "    config.max_target_length,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "# ë°ì´í„°ë¡œë” ìƒì„±\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,  # í•™ìŠµ ë°ì´í„°ëŠ” ì…”í”Œ\n",
    "    num_workers=2  # ë³‘ë ¬ ë°ì´í„° ë¡œë”©\n",
    ")\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,  # Dev ë°ì´í„°ëŠ” ì…”í”Œ ì•ˆ í•¨\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,  # Test ë°ì´í„°ëŠ” ì…”í”Œ ì•ˆ í•¨\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Dev batches: {len(dev_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Optimizer ë° Scheduler\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"í•™ìŠµ ì„¤ì •\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Weight decayë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„° ì§€ì •\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': config.weight_decay\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                  if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Optimizer ìƒì„±\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=config.learning_rate,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Scheduler ìƒì„± (Cosine annealing with warmup)\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Early Stopping í´ë˜ìŠ¤\n",
    "# ========================================\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stoppingì„ ìœ„í•œ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=3, min_delta=0.0, mode='max'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: ê°œì„ ì´ ì—†ì–´ë„ ê¸°ë‹¤ë¦´ ì—í­ ìˆ˜\n",
    "            min_delta: ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰\n",
    "            mode: 'max' (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) or 'min' (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0  # ê°œì„  ì—†ëŠ” ì—í­ ì¹´ìš´í„°\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, score):\n",
    "        \"\"\"\n",
    "        í˜„ì¬ ì ìˆ˜ë¥¼ ì²´í¬í•˜ê³  ê°œì„  ì—¬ë¶€ ë°˜í™˜\n",
    "        \n",
    "        Args:\n",
    "            score: í˜„ì¬ ì—í­ì˜ ì ìˆ˜\n",
    "            \n",
    "        Returns:\n",
    "            bool: ê°œì„ ë˜ì—ˆìœ¼ë©´ True\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return True\n",
    "        \n",
    "        # ê°œì„  ì—¬ë¶€ ì²´í¬\n",
    "        if self.mode == 'max':\n",
    "            improved = score > self.best_score + self.min_delta\n",
    "        else:\n",
    "            improved = score < self.best_score - self.min_delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def train_epoch(model, loader, optimizer, scheduler, device, config):\n",
    "    \"\"\"\n",
    "    í•œ ì—í­ í•™ìŠµ\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµí•  ëª¨ë¸\n",
    "        loader: í•™ìŠµ ë°ì´í„°ë¡œë”\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)\n",
    "        config: ì„¤ì •\n",
    "        \n",
    "    Returns:\n",
    "        float: í‰ê·  loss\n",
    "    \"\"\"\n",
    "    model.train()  # í•™ìŠµ ëª¨ë“œ\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ROUGE í‰ê°€ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def evaluate_model_on_dev(model, dev_loader, tokenizer, device, config):\n",
    "    \"\"\"\n",
    "    Dev ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ (ROUGE ê³„ì‚°)\n",
    "    \n",
    "    Args:\n",
    "        model: í‰ê°€í•  ëª¨ë¸\n",
    "        dev_loader: Dev ë°ì´í„°ë¡œë”\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        config: ì„¤ì •\n",
    "        \n",
    "    Returns:\n",
    "        dict: ROUGE ì ìˆ˜ë“¤\n",
    "    \"\"\"\n",
    "    from rouge import Rouge\n",
    "    \n",
    "    # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (kiwipiepy)\n",
    "    try:\n",
    "        from kiwipiepy import Kiwi\n",
    "        kiwi = Kiwi()\n",
    "        use_morpheme = True\n",
    "    except:\n",
    "        use_morpheme = False\n",
    "    \n",
    "    def tokenize_for_rouge(text):\n",
    "        \"\"\"ROUGE ê³„ì‚°ì„ ìœ„í•œ í† í°í™”\"\"\"\n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        if use_morpheme:\n",
    "            # í˜•íƒœì†Œ ë¶„ì„\n",
    "            tokens = kiwi.tokenize(text)\n",
    "            morphemes = [token.form for token in tokens]\n",
    "            return ' '.join(morphemes)\n",
    "        else:\n",
    "            # ë‹¨ìˆœ ê³µë°± ë¶„ë¦¬\n",
    "            return ' '.join(text.split())\n",
    "    \n",
    "    print(\"\\nDev ë°ì´í„° í‰ê°€ (ROUGE ê³„ì‚°)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # ìš”ì•½ë¬¸ ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Generating summaries\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Beam searchë¡œ ìš”ì•½ë¬¸ ìƒì„±\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_gen_length,\n",
    "                min_length=config.min_length,\n",
    "                num_beams=config.num_beams,\n",
    "                length_penalty=config.length_penalty,\n",
    "                no_repeat_ngram_size=config.no_repeat_ngram_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
    "            for gen_ids, label_ids in zip(generated_ids, batch['labels']):\n",
    "                pred = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                label_ids = label_ids[label_ids != -100]\n",
    "                ref = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                references.append(ref)\n",
    "    \n",
    "    # ROUGE ê³„ì‚°\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    print(\"í† í°í™” ë° ROUGE ê³„ì‚° ì¤‘...\")\n",
    "    pred_tokenized = [tokenize_for_rouge(p) for p in predictions]\n",
    "    ref_tokenized = [tokenize_for_rouge(r) for r in references]\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
    "    valid_pairs = [(p, r) for p, r in zip(pred_tokenized, ref_tokenized) if p and r]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'final_score': 0.0}\n",
    "    \n",
    "    pred_valid, ref_valid = zip(*valid_pairs)\n",
    "    \n",
    "    try:\n",
    "        # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "        scores = rouge.get_scores(list(pred_valid), list(ref_valid))\n",
    "        \n",
    "        # F1 ì ìˆ˜ í‰ê· \n",
    "        rouge1 = np.mean([s['rouge-1']['f'] for s in scores]) * 100\n",
    "        rouge2 = np.mean([s['rouge-2']['f'] for s in scores]) * 100\n",
    "        rougel = np.mean([s['rouge-l']['f'] for s in scores]) * 100\n",
    "        \n",
    "        results = {\n",
    "            'rouge-1': rouge1,\n",
    "            'rouge-2': rouge2,\n",
    "            'rouge-l': rougel,\n",
    "            'final_score': rouge1 + rouge2 + rougel\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nROUGE-1 F1: {rouge1:.2f}\")\n",
    "        print(f\"ROUGE-2 F1: {rouge2:.2f}\")\n",
    "        print(f\"ROUGE-L F1: {rougel:.2f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Final Score: {results['final_score']:.2f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ROUGE ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'final_score': 0.0}\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 í•™ìŠµ ì‹œì‘ (ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° í™œìš©)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_rouge_score = 0.0  # ìµœê³  ROUGE ì ìˆ˜ ì €ì¥\n",
    "early_stopping = EarlyStopping(patience=config.patience, mode='max')\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, device, config\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Dev í‰ê°€\n",
    "    rouge_scores = evaluate_model_on_dev(\n",
    "        model, dev_loader, tokenizer, device, config\n",
    "    )\n",
    "    \n",
    "    current_score = rouge_scores['final_score']\n",
    "    \n",
    "    # Best model ì €ì¥\n",
    "    if current_score > best_rouge_score:\n",
    "        best_rouge_score = current_score\n",
    "        print(f\"âœ“ New best model! (ROUGE Final: {current_score:.2f})\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        model.save_pretrained(config.model_save_path)\n",
    "        tokenizer.save_pretrained(config.model_save_path)\n",
    "        print(f\"Model saved to {config.model_save_path}\")\n",
    "    \n",
    "    # Early stopping ì²´í¬\n",
    "    improved = early_stopping(current_score)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\nâš  Early stopping triggered at epoch {epoch + 1}\")\n",
    "        print(f\"Best ROUGE score: {best_rouge_score:.2f}\")\n",
    "        break\n",
    "    \n",
    "    if not improved:\n",
    "        print(f\"âš  No improvement for {early_stopping.counter} epoch(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best ROUGE Final: {best_rouge_score:.2f}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ========================================\n",
    "def post_process_summary(summary, max_words=25):\n",
    "    \"\"\"\n",
    "    ìƒì„±ëœ ìš”ì•½ë¬¸ í›„ì²˜ë¦¬\n",
    "    \n",
    "    Args:\n",
    "        summary: ìƒì„±ëœ ìš”ì•½ë¬¸\n",
    "        max_words: ìµœëŒ€ ë‹¨ì–´ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        str: í›„ì²˜ë¦¬ëœ ìš”ì•½ë¬¸\n",
    "    \"\"\"\n",
    "    if not summary or not summary.strip():\n",
    "        return \"#Person1#ê³¼ #Person2#ê°€ ëŒ€í™”í•©ë‹ˆë‹¤.\"\n",
    "    \n",
    "    summary = summary.strip()\n",
    "    \n",
    "    # ë§ˆì¹¨í‘œ ì¶”ê°€ (í•œêµ­ì–´ ì–´ë¯¸ë¡œ ëë‚˜ë©´)\n",
    "    if not summary.endswith('.'):\n",
    "        if summary.split()[-1][-1] in ['ë‹¤', 'ìš”', 'ìŒ', 'ì§€', 'ë‚˜']:\n",
    "            summary += '.'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ì¶”ë¡  í•¨ìˆ˜\n",
    "# ========================================\n",
    "def generate_summaries(model, loader, tokenizer, device, config):\n",
    "    \"\"\"\n",
    "    í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ìš”ì•½ë¬¸ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ ëª¨ë¸\n",
    "        loader: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        config: ì„¤ì •\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fnames, summaries)\n",
    "    \"\"\"\n",
    "    model.eval()  # í‰ê°€ ëª¨ë“œ\n",
    "    \n",
    "    fnames = []\n",
    "    summaries = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Beam searchë¡œ ìš”ì•½ë¬¸ ìƒì„±\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_gen_length,\n",
    "                min_length=config.min_length,\n",
    "                num_beams=config.num_beams,\n",
    "                length_penalty=config.length_penalty,\n",
    "                no_repeat_ngram_size=config.no_repeat_ngram_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # ë””ì½”ë”© ë° í›„ì²˜ë¦¬\n",
    "            for fname, gen_ids in zip(batch['fname'], generated_ids):\n",
    "                summary = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                summary = post_process_summary(summary, max_words=25)\n",
    "                \n",
    "                fnames.append(fname)\n",
    "                summaries.append(summary)\n",
    "    \n",
    "    return fnames, summaries\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model ë¡œë“œ\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.model_save_path)\n",
    "model = model.to(device)\n",
    "tokenizer = T5TokenizerFast.from_pretrained(config.model_save_path)\n",
    "\n",
    "# ìš”ì•½ë¬¸ ìƒì„±\n",
    "fnames, summaries = generate_summaries(\n",
    "    model, test_loader, tokenizer, device, config\n",
    ")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission_df = pd.DataFrame({\n",
    "    'fname': fnames,\n",
    "    'summary': summaries\n",
    "})\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f'submission_kot5_filtered_aug_{timestamp}.csv'\n",
    "submission_path = os.path.join(config.output_dir, submission_filename)\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nì œì¶œ íŒŒì¼ ì €ì¥: {submission_path}\")\n",
    "print(f\"ìƒì„±ëœ ìš”ì•½ ê°œìˆ˜: {len(submission_df)}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ìƒì„±ëœ ìš”ì•½ ìƒ˜í”Œ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(submission_df))):\n",
    "    summary = submission_df.iloc[i]['summary']\n",
    "    word_count = len(summary.split())\n",
    "    print(f\"\\n[Sample {i+1}]\")\n",
    "    print(f\"Fname: {submission_df.iloc[i]['fname']}\")\n",
    "    print(f\"Summary ({word_count} words): {summary}\")\n",
    "\n",
    "# ìš”ì•½ ê¸¸ì´ í†µê³„\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ìš”ì•½ ê¸¸ì´ í†µê³„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_lengths = submission_df['summary'].apply(lambda x: len(x.split()))\n",
    "print(f\"í‰ê· : {summary_lengths.mean():.1f} ë‹¨ì–´\")\n",
    "print(f\"ì¤‘ê°„ê°’: {summary_lengths.median():.1f} ë‹¨ì–´\")\n",
    "print(f\"ìµœì†Œ: {summary_lengths.min()} ë‹¨ì–´\")\n",
    "print(f\"ìµœëŒ€: {summary_lengths.max()} ë‹¨ì–´\")\n",
    "print(f\"í‘œì¤€í¸ì°¨: {summary_lengths.std():.1f} ë‹¨ì–´\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KoT5 + ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
    "print(f\"1. ëª¨ë¸: {config.model_save_path}\")\n",
    "print(f\"2. ì œì¶œ íŒŒì¼: {submission_path}\")\n",
    "print(f\"\\nê¸°ëŒ€ ì„±ëŠ¥:\")\n",
    "print(f\"- ê³ í’ˆì§ˆ ì¦ê°• ë°ì´í„°ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(f\"- ë…¸ì´ì¦ˆê°€ ì œê±°ë˜ì–´ ë” ì•ˆì •ì ì¸ í•™ìŠµ\")\n",
    "print(f\"- ì˜ˆìƒ ì ìˆ˜: 50~55ì  (í•„í„°ë§ëœ ì¦ê°• ë°ì´í„° íš¨ê³¼)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94499f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
