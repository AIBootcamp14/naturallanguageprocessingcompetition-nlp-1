{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modular Version\n",
    "\n",
    "**ëª©ì **: ëª¨ë“ˆí™”ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ baselineì„ ì¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ê¸°ëŒ€ ê²°ê³¼**: baseline.ipynbì™€ ë™ì¼í•œ ì„±ëŠ¥ (46-47ì )\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import ëª¨ë“ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# scripts ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ëª¨ë“ˆ import\n",
    "from utils import load_config, get_device, set_seed\n",
    "from data_loader import Preprocess, load_data\n",
    "from tokenizer_utils import load_tokenizer\n",
    "from model_utils import load_model_for_train, get_model_info\n",
    "from dataset import prepare_train_dataset, prepare_test_dataset\n",
    "from trainer_utils import get_trainer\n",
    "from inference_utils import run_inference\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ëª¨ë“ˆ import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Config ë¡œë“œ ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config ë¡œë“œ\n",
    "config = load_config('./config.yaml')\n",
    "\n",
    "# Device ì„¤ì •\n",
    "device = get_device()\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ì‹œë“œ ì„¤ì • (ì¬í˜„ì„±)\n",
    "set_seed(config['training']['seed'])\n",
    "print(f\"ì‹œë“œ ì„¤ì • ì™„ë£Œ: {config['training']['seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wandb ì„¤ì • (ì„ íƒì )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ\n",
    "# import wandb\n",
    "# from dotenv import load_dotenv\n",
    "# \n",
    "# load_dotenv()\n",
    "# wandb.login()\n",
    "# wandb.init(\n",
    "#     project=config['wandb']['project'],\n",
    "#     entity=config['wandb']['entity'],\n",
    "#     name=config['wandb']['name'] + \"-modular\"\n",
    "# )\n",
    "\n",
    "# Wandb ë¹„í™œì„±í™” (ê¸°ë³¸ê°’)\n",
    "config['training']['report_to'] = 'none'\n",
    "print(\"Wandb ë¹„í™œì„±í™” (report_to='none')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenizer ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer ë¡œë“œ (special tokens ì¶”ê°€)\n",
    "model_name = config['general']['model_name']\n",
    "special_tokens = config['tokenizer']['special_tokens']\n",
    "\n",
    "tokenizer = load_tokenizer(model_name, special_tokens)\n",
    "\n",
    "print(f\"âœ… Tokenizer ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"ëª¨ë¸: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Special tokens: {special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„°ì…‹ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessor ìƒì„±\n",
    "preprocessor = Preprocess(\n",
    "    bos_token=config['tokenizer']['bos_token'],\n",
    "    eos_token=config['tokenizer']['eos_token']\n",
    ")\n",
    "\n",
    "# Train/Val ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "data_path = config['general']['data_path']\n",
    "train_dataset, val_dataset = prepare_train_dataset(\n",
    "    config, preprocessor, data_path, tokenizer\n",
    ")\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "sample = train_dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = load_model_for_train(config, tokenizer, device)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "model_info = get_model_info(model)\n",
    "print(\"\\nëª¨ë¸ ì •ë³´:\")\n",
    "for key, value in model_info.items():\n",
    "    if 'parameters' in key:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trainer ì„¤ì • ë° í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer ìƒì„±\n",
    "trainer = get_trainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"í•™ìŠµ ì—í­: {config['training']['num_train_epochs']}\")\n",
    "print(f\"í•™ìŠµë¥ : {config['training']['learning_rate']}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {config['training']['per_device_train_batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹œì‘\n",
    "print(\"\\nğŸš€ í•™ìŠµ ì‹œì‘...\\n\")\n",
    "trainer.train()\n",
    "print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test ë°ì´í„° ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "test_data, test_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n",
    "\n",
    "print(f\"âœ… Test ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  ì‹¤í–‰\n",
    "print(\"\\nğŸ”® ì¶”ë¡  ì‹œì‘...\\n\")\n",
    "\n",
    "result_df = run_inference(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataloader=test_dataloader,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    save_path='./prediction/output_modular.csv'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ì¶”ë¡  ì™„ë£Œ!\")\n",
    "print(f\"ê²°ê³¼ íŒŒì¼: ./prediction/output_modular.csv\")\n",
    "print(f\"ìƒ˜í”Œ ìˆ˜: {len(result_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²˜ìŒ 5ê°œ ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 5ê°œ):\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(result_df))):\n",
    "    print(f\"\\n[{i}] {result_df.iloc[i]['fname']}\")\n",
    "    print(f\"ìš”ì•½: {result_df.iloc[i]['summary'][:100]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV ê²€ì¦\n",
    "from utils import validate_csv\n",
    "\n",
    "validation_result = validate_csv('./prediction/output_modular.csv')\n",
    "\n",
    "print(\"\\nCSV ê²€ì¦ ê²°ê³¼:\")\n",
    "print(f\"ìœ íš¨ì„±: {'âœ… í†µê³¼' if validation_result['valid'] else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print(f\"ìƒ˜í”Œ ìˆ˜: {validation_result['num_samples']}\")\n",
    "print(f\"ì»¬ëŸ¼: {validation_result['columns']}\")\n",
    "\n",
    "if validation_result['errors']:\n",
    "    print(\"\\nâš ï¸ ì˜¤ë¥˜:\")\n",
    "    for error in validation_result['errors']:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì›ë³¸ Baselineê³¼ ë¹„êµ (ì„ íƒì )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›ë³¸ baseline.ipynbì˜ ê²°ê³¼ì™€ ë¹„êµ\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    baseline_output = pd.read_csv('./prediction/output.csv')\n",
    "    modular_output = pd.read_csv('./prediction/output_modular.csv')\n",
    "    \n",
    "    # ë™ì¼í•œ ìƒ˜í”Œ ìˆ˜ì¸ì§€ í™•ì¸\n",
    "    print(f\"\\nBaseline ìƒ˜í”Œ ìˆ˜: {len(baseline_output)}\")\n",
    "    print(f\"Modular ìƒ˜í”Œ ìˆ˜: {len(modular_output)}\")\n",
    "    \n",
    "    # fname ìˆœì„œê°€ ë™ì¼í•œì§€ í™•ì¸\n",
    "    if baseline_output['fname'].equals(modular_output['fname']):\n",
    "        print(\"âœ… fname ìˆœì„œ ì¼ì¹˜\")\n",
    "    else:\n",
    "        print(\"âš ï¸ fname ìˆœì„œ ë¶ˆì¼ì¹˜\")\n",
    "    \n",
    "    # ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ ê³„ì‚°\n",
    "    identical_count = (baseline_output['summary'] == modular_output['summary']).sum()\n",
    "    print(f\"\\në™ì¼í•œ ìš”ì•½ë¬¸ ìˆ˜: {identical_count} / {len(baseline_output)}\")\n",
    "    print(f\"ì¼ì¹˜ìœ¨: {identical_count / len(baseline_output) * 100:.2f}%\")\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"âš ï¸ ì›ë³¸ baseline ê²°ê³¼ íŒŒì¼(output.csv)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"baseline.ipynbë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… ì™„ë£Œ\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„**:\n",
    "1. `./prediction/output_modular.csv`ë¥¼ ëŒ€íšŒ í”Œë«í¼ì— ì œì¶œ\n",
    "2. ì ìˆ˜ í™•ì¸ (46-47ì  ê¸°ëŒ€)\n",
    "3. ëª¨ë“ˆí™”ëœ ì½”ë“œë¥¼ í™œìš©í•˜ì—¬ ì‹¤í—˜ ì§„í–‰ (Learning rate íŠœë‹, ë°ì´í„° ì¦ê°• ë“±)\n",
    "\n",
    "**ëª¨ë“ˆ ì‚¬ìš© ë°©ë²•**:\n",
    "- ìƒˆë¡œìš´ ì‹¤í—˜ì„ ìœ„í•´ ì´ notebookì„ ë³µì‚¬í•˜ê³  configë¥¼ ìˆ˜ì •\n",
    "- ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥\n",
    "- ëª¨ë“  ì‹¤í—˜ì€ notebookìœ¼ë¡œ ì¶”ì  ê°€ëŠ¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
