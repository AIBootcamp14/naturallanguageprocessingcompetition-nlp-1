{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ êµ¬í˜„ - KoBART ê¸°ë°˜ ëŒ€í™” ìš”ì•½\n",
    "> PRD ê³„íšì— ë”°ë¥¸ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬í˜„\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 47+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "# notebooks/team/CHH -> notebooks -> team -> natural-language-processing-competition\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from rouge import Rouge\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì • ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_baseline.yaml'\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Batch Size: {config['training']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "# configì˜ ë¡œê·¸ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "log_dir = get_path(config['paths']['log_dir'])\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'baseline_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('=== Baseline Experiment Started ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì²´í¬\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    logger.write(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° ë¡œë“œ\n",
    "# config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"Data loaded successfully!\")\n",
    "logger.write(f\"Train samples: {len(train_df)}\")\n",
    "logger.write(f\"Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nSample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 16_ë°ì´í„°_í’ˆì§ˆ_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "logger.write(\"\\n=== Data Quality Validation ===\")\n",
    "\n",
    "class DataQualityValidator:\n",
    "    \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self):\n",
    "        self.quality_report = {}\n",
    "    \n",
    "    def validate_structure(self, df):\n",
    "        \"\"\"êµ¬ì¡°ì  ê²€ì¦\"\"\"\n",
    "        checks = {\n",
    "            'null_values': df.isnull().sum().sum(),\n",
    "            'duplicates': df.duplicated().sum(),\n",
    "            'empty_dialogues': (df['dialogue'].str.len() == 0).sum(),\n",
    "            'empty_summaries': (df['summary'].str.len() == 0).sum() if 'summary' in df.columns else 0\n",
    "        }\n",
    "        self.quality_report['structure'] = checks\n",
    "        return checks\n",
    "    \n",
    "    def validate_content(self, df):\n",
    "        \"\"\"ë‚´ìš© ê²€ì¦\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n",
    "        \n",
    "        checks = {\n",
    "            'avg_dialogue_length': dialogue_lengths.mean(),\n",
    "            'min_dialogue_length': dialogue_lengths.min(),\n",
    "            'max_dialogue_length': dialogue_lengths.max(),\n",
    "            'avg_summary_length': summary_lengths.mean(),\n",
    "            'summary_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0\n",
    "        }\n",
    "        self.quality_report['content'] = checks\n",
    "        return checks\n",
    "    \n",
    "    def validate_consistency(self, df):\n",
    "        \"\"\"ì¼ê´€ì„± ê²€ì¦\"\"\"\n",
    "        checks = {\n",
    "            'person_tags_consistent': all(df['dialogue'].str.contains('#Person')),\n",
    "            'encoding_issues': df['dialogue'].str.contains('\\\\?\\\\?\\\\?').sum(),\n",
    "            'special_chars': df['dialogue'].str.contains('[^\\w\\s#:.,!?ê°€-í£]').sum()\n",
    "        }\n",
    "        self.quality_report['consistency'] = checks\n",
    "        return checks\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
    "        return self.quality_report\n",
    "\n",
    "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰\n",
    "validator = DataQualityValidator()\n",
    "\n",
    "# êµ¬ì¡° ê²€ì¦\n",
    "structure_checks = validator.validate_structure(train_df)\n",
    "logger.write(f\"Structure validation:\")\n",
    "for key, value in structure_checks.items():\n",
    "    logger.write(f\"  - {key}: {value}\")\n",
    "\n",
    "# ë‚´ìš© ê²€ì¦\n",
    "content_checks = validator.validate_content(train_df)\n",
    "logger.write(f\"Content validation:\")\n",
    "logger.write(f\"  - Avg dialogue length: {content_checks['avg_dialogue_length']:.1f}\")\n",
    "logger.write(f\"  - Summary ratio: {content_checks.get('summary_ratio', 0):.2%}\")\n",
    "\n",
    "# ì¼ê´€ì„± ê²€ì¦\n",
    "consistency_checks = validator.validate_consistency(train_df)\n",
    "logger.write(f\"Consistency validation:\")\n",
    "logger.write(f\"  - Person tags consistent: {consistency_checks['person_tags_consistent']}\")\n",
    "logger.write(f\"  - Encoding issues: {consistency_checks['encoding_issues']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar API êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 09_Solar_API_ìµœì í™”.md, 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class SolarAPIOptimizer:\n",
    "    \"\"\"Solar API ìµœì í™” í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.upstage.ai/v1/solar\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def optimize_prompt(self, dialogue: str) -> str:\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ ìµœì í™” - í† í° ì ˆì•½\"\"\"\n",
    "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "        dialogue = ' '.join(dialogue.split())\n",
    "        \n",
    "        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
    "        optimized_prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”. í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”:\n",
    "{dialogue[:1000]}  # í† í° ì œí•œ\n",
    "ìš”ì•½:\"\"\"\n",
    "        \n",
    "        return optimized_prompt\n",
    "    \n",
    "    def generate_summary(self, dialogue: str, max_tokens: int = 150) -> Optional[str]:\n",
    "        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n",
    "        try:\n",
    "            prompt = self.optimize_prompt(dialogue)\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"solar-1-mini-chat\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0.3,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result['choices'][0]['message']['content']\n",
    "            else:\n",
    "                logger.write(f\"Solar API error: {response.status_code}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.write(f\"Solar API exception: {e}\")\n",
    "            return None\n",
    "\n",
    "class DualSummarizationSystem:\n",
    "    \"\"\"ëª¨ë¸ê³¼ API ë“€ì–¼ ìš”ì•½ ì‹œìŠ¤í…œ\"\"\"\n",
    "    def __init__(self, model, tokenizer, solar_api: SolarAPIOptimizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.solar_api = solar_api\n",
    "        self.device = next(model.parameters()).device\n",
    "    \n",
    "    def compare_summaries(self, dialogue: str, reference: str = None) -> Dict:\n",
    "        \"\"\"ëª¨ë¸ê³¼ API ìš”ì•½ ë¹„êµ\"\"\"\n",
    "        # ëª¨ë¸ ì˜ˆì¸¡\n",
    "        inputs = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        model_summary = self.tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # API ì˜ˆì¸¡\n",
    "        api_summary = self.solar_api.generate_summary(dialogue)\n",
    "        \n",
    "        # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "        result = {\n",
    "            'model_summary': model_summary,\n",
    "            'api_summary': api_summary\n",
    "        }\n",
    "        \n",
    "        if reference:\n",
    "            rouge = Rouge()\n",
    "            try:\n",
    "                model_scores = rouge.get_scores(model_summary, reference)[0]\n",
    "                api_scores = rouge.get_scores(api_summary, reference)[0] if api_summary else None\n",
    "                \n",
    "                result['model_rouge'] = model_scores['rouge-l']['f']\n",
    "                result['api_rouge'] = api_scores['rouge-l']['f'] if api_scores else 0\n",
    "                result['best'] = 'model' if result['model_rouge'] > result.get('api_rouge', 0) else 'api'\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Solar API ì´ˆê¸°í™” (configì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°)\n",
    "if 'solar_api' in config and 'api_key' in config['solar_api']:\n",
    "    solar_optimizer = SolarAPIOptimizer(config['solar_api']['api_key'])\n",
    "    dual_system = DualSummarizationSystem(model, tokenizer, solar_optimizer)\n",
    "    logger.write(\"Solar API dual system initialized\")\n",
    "else:\n",
    "    logger.write(\"Solar API key not found in config\")\n",
    "    solar_optimizer = None\n",
    "    dual_system = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB ì´ˆê¸°í™”\n",
    "wandb.init(\n",
    "    project=config['wandb']['project'],\n",
    "    entity=config['wandb']['entity'],\n",
    "    name=config['wandb']['name'],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\n",
    "model = BartForConditionalGeneration.from_pretrained(config['model']['name'])\n",
    "\n",
    "logger.write(f\"Model loaded: {config['model']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_dialogue(text):\n",
    "    \"\"\"ëŒ€í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    # ë…¸ì´ì¦ˆ ì œê±°\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    text = text.replace('<br>', '\\n')\n",
    "    \n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”\n",
    "    text = text.strip()\n",
    "    \n",
    "    # #Person íƒœê·¸ ìµœì í™” (ë” ëª…í™•í•˜ê²Œ)\n",
    "    import re\n",
    "    text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_summary(text):\n",
    "    \"\"\"ìš”ì•½ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©\n",
    "train_df['dialogue_preprocessed'] = train_df['dialogue'].apply(preprocess_dialogue)\n",
    "train_df['summary_preprocessed'] = train_df['summary'].apply(preprocess_summary)\n",
    "\n",
    "dev_df['dialogue_preprocessed'] = dev_df['dialogue'].apply(preprocess_dialogue)\n",
    "dev_df['summary_preprocessed'] = dev_df['summary'].apply(preprocess_summary)\n",
    "\n",
    "test_df['dialogue_preprocessed'] = test_df['dialogue'].apply(preprocess_dialogue)\n",
    "\n",
    "print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"Sample preprocessed dialogue (first 200 chars):\")\n",
    "print(train_df['dialogue_preprocessed'].iloc[0][:200])\n",
    "logger.write(\"Data preprocessing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DialogueSummaryDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n",
    "        \"\"\"\n",
    "        ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹\n",
    "        \n",
    "        Args:\n",
    "            dataframe: ë°ì´í„°í”„ë ˆì„\n",
    "            tokenizer: í† í¬ë‚˜ì´ì €\n",
    "            max_input_len: ìµœëŒ€ ì…ë ¥ ê¸¸ì´\n",
    "            max_target_len: ìµœëŒ€ íƒ€ê²Ÿ ê¸¸ì´\n",
    "            is_test: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "        dialogue = row['dialogue_preprocessed']\n",
    "        \n",
    "        # ì…ë ¥ í† í°í™”\n",
    "        inputs = self.tokenizer(\n",
    "            dialogue,\n",
    "            max_length=self.max_input_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° íƒ€ê²Ÿë„ ì²˜ë¦¬\n",
    "        if not self.is_test:\n",
    "            summary = row['summary_preprocessed']\n",
    "            \n",
    "            # íƒ€ê²Ÿ í† í°í™”\n",
    "            targets = self.tokenizer(\n",
    "                summary,\n",
    "                max_length=self.max_target_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': inputs['input_ids'].squeeze(),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'labels': targets['input_ids'].squeeze()\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': inputs['input_ids'].squeeze(),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'idx': idx\n",
    "            }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = DialogueSummaryDataset(\n",
    "    train_df, \n",
    "    tokenizer, \n",
    "    max_input_len=config['model']['max_input_length'],\n",
    "    max_target_len=config['model']['max_target_length']\n",
    ")\n",
    "\n",
    "val_dataset = DialogueSummaryDataset(\n",
    "    dev_df,\n",
    "    tokenizer,\n",
    "    max_input_len=config['model']['max_input_length'],\n",
    "    max_target_len=config['model']['max_target_length']\n",
    ")\n",
    "\n",
    "test_dataset = DialogueSummaryDataset(\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    max_input_len=config['model']['max_input_length'],\n",
    "    max_target_len=config['model']['max_target_length'],\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "logger.write(f\"Dataset created - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"Dataset shapes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader ìƒì„±\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "logger.write(f\"Using device: {device}\")\n",
    "\n",
    "# ROUGE í‰ê°€ í•¨ìˆ˜\n",
    "def compute_rouge_scores(predictions, references):\n",
    "    \"\"\"ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "    predictions = [p if p else \"empty\" for p in predictions]\n",
    "    references = [r if r else \"empty\" for r in references]\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        return {\n",
    "            'rouge-1': scores['rouge-1']['f'],\n",
    "            'rouge-2': scores['rouge-2']['f'],\n",
    "            'rouge-l': scores['rouge-l']['f']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.write(f\"Error computing ROUGE: {e}\")\n",
    "        return {'rouge-1': 0, 'rouge-2': 0, 'rouge-l': 0}\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    \"\"\"í•œ ì—í­ í•™ìŠµ\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # WandB ë¡œê¹…\n",
    "        wandb.log({\n",
    "            'train_loss': loss.item(),\n",
    "            'learning_rate': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜\n",
    "def evaluate(model, data_loader, tokenizer, device, num_samples=None):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc='Evaluating')\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            if num_samples and i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Loss ê³„ì‚°\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìƒì„± - config í‚¤ ìˆ˜ì •\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config['model']['max_target_length'],\n",
    "                num_beams=config['evaluation']['num_beams'],  # config í‚¤ ìˆ˜ì •\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=config['evaluation']['no_repeat_ngram_size']\n",
    "            )\n",
    "            \n",
    "            # ë””ì½”ë”©\n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            references.extend(refs)\n",
    "    \n",
    "    # ROUGE ì ìˆ˜ ê³„ì‚°\n",
    "    rouge_scores = compute_rouge_scores(predictions, references)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return avg_loss, rouge_scores, predictions[:5]  # ìƒ˜í”Œ ì˜ˆì¸¡ ë°˜í™˜\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "# config ê°’ë“¤ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n",
    "num_epochs = config['training'].get('num_epochs', config['training'].get('epochs', 3))\n",
    "learning_rate = config['training']['learning_rate']\n",
    "# learning_rateê°€ ë¬¸ìì—´ì¸ ê²½ìš° floatë¡œ ë³€í™˜\n",
    "if isinstance(learning_rate, str):\n",
    "    learning_rate = float(learning_rate)\n",
    "    print(f\"Learning rate converted from string to float: {learning_rate}\")\n",
    "\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,  # ì´ë¯¸ floatë¡œ ë³€í™˜ë¨\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "logger.write(f\"Optimizer and scheduler initialized\")\n",
    "logger.write(f\"Learning rate: {learning_rate}\")\n",
    "logger.write(f\"Total training steps: {num_training_steps}\")\n",
    "logger.write(f\"Warmup steps: {int(num_training_steps * config['training']['warmup_ratio'])}\")\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ ì €ì¥\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'rouge_1': [],\n",
    "    'rouge_2': [],\n",
    "    'rouge_l': []\n",
    "}\n",
    "\n",
    "# Early Stopping ì„¤ì •\n",
    "best_rouge_l = 0\n",
    "patience = config['training']['early_stopping_patience']\n",
    "patience_counter = 0\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ - configì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "model_dir = get_path(config['paths']['output_dir'])\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_path = model_dir / 'best_model.pt'\n",
    "\n",
    "logger.write(\"=\" * 50)\n",
    "logger.write(\"Starting training...\")\n",
    "logger.write(\"=\" * 50)\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(num_epochs):\n",
    "    logger.write(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    logger.write(\"-\" * 30)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    logger.write(f\"Average training loss: {train_loss:.4f}\")\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss, rouge_scores, sample_preds = evaluate(model, val_loader, tokenizer, device)\n",
    "    \n",
    "    logger.write(f\"Validation loss: {val_loss:.4f}\")\n",
    "    logger.write(f\"ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}\")\n",
    "    logger.write(f\"ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}\")\n",
    "    logger.write(f\"ROUGE-L F1: {rouge_scores['rouge-l']:.4f}\")\n",
    "    \n",
    "    # í•™ìŠµ ê¸°ë¡ ì €ì¥\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['rouge_1'].append(rouge_scores['rouge-1'])\n",
    "    training_history['rouge_2'].append(rouge_scores['rouge-2'])\n",
    "    training_history['rouge_l'].append(rouge_scores['rouge-l'])\n",
    "    \n",
    "    # WandB ë¡œê¹…\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss_epoch': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'rouge_1': rouge_scores['rouge-1'],\n",
    "        'rouge_2': rouge_scores['rouge-2'],\n",
    "        'rouge_l': rouge_scores['rouge-l']\n",
    "    })\n",
    "    \n",
    "    # ìƒ˜í”Œ ì˜ˆì¸¡ ì¶œë ¥\n",
    "    logger.write(\"\\nSample predictions:\")\n",
    "    for i, pred in enumerate(sample_preds[:2]):\n",
    "        logger.write(f\"  Sample {i+1}: {pred[:100]}...\")\n",
    "    \n",
    "    # Best model ì €ì¥\n",
    "    if rouge_scores['rouge-l'] > best_rouge_l:\n",
    "        best_rouge_l = rouge_scores['rouge-l']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'rouge_scores': rouge_scores,\n",
    "            'config': config\n",
    "        }, best_model_path)\n",
    "        \n",
    "        logger.write(f\"âœ“ New best model saved! (ROUGE-L: {best_rouge_l:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logger.write(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "logger.write(\"\\n\" + \"=\" * 50)\n",
    "logger.write(f\"Training completed!\")\n",
    "logger.write(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\n",
    "logger.write(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ëª¨ë¸ ë¡œë“œ\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "logger.write(f\"Best model loaded from epoch {checkpoint['epoch'] + 1}\")\n",
    "logger.write(f\"Best ROUGE scores: {checkpoint['rouge_scores']}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "def generate_predictions(model, data_loader, tokenizer, device):\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc='Generating predictions')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            indices = batch['idx']\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìƒì„±\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config['inference']['max_length'],\n",
    "                num_beams=config['inference']['num_beams'],\n",
    "                early_stopping=config['inference']['early_stopping'],\n",
    "                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                length_penalty=config['inference']['length_penalty'],\n",
    "                temperature=config['inference']['temperature']\n",
    "            )\n",
    "            \n",
    "            # ë””ì½”ë”©\n",
    "            predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_indices.extend(indices.tolist())\n",
    "    \n",
    "    # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "    sorted_predictions = [pred for _, pred in sorted(zip(all_indices, all_predictions))]\n",
    "    \n",
    "    return sorted_predictions\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "logger.write(\"\\nGenerating predictions for test set...\")\n",
    "test_predictions = generate_predictions(model, test_loader, tokenizer, device)\n",
    "logger.write(f\"Generated {len(test_predictions)} predictions\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nSample test predictions:\")\n",
    "for i in range(min(3, len(test_predictions))):\n",
    "    print(f\"Test {i+1}: {test_predictions[i][:150]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission_df = pd.DataFrame({\n",
    "    'fname': test_df['fname'],\n",
    "    'summary': test_predictions\n",
    "})\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ ì €ì¥ - configì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "submission_dir = get_path(config['paths']['submission_dir'])\n",
    "submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "submission_filename = f'baseline_submission_{timestamp}.csv'\n",
    "submission_path = submission_dir / submission_filename\n",
    "\n",
    "# index=Trueë¡œ ì„¤ì •í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ í¬í•¨ì‹œí‚´\n",
    "submission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=False -> index=Trueë¡œ ë³€ê²½\n",
    "logger.write(f\"\\nSubmission file saved: {submission_path}\")\n",
    "\n",
    "# ì œì¶œ íŒŒì¼ í™•ì¸\n",
    "print(f\"\\nSubmission file created: {submission_filename}\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst 3 submissions:\")\n",
    "print(submission_df.head(3))\n",
    "\n",
    "# ìµœì¢… ìš”ì•½ í†µê³„\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BASELINE EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Best ROUGE-L: {best_rouge_l:.4f}\")\n",
    "print(f\"Training epochs: {len(training_history['train_loss'])}\")\n",
    "print(f\"Final train loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Submission file: {submission_filename}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# WandB ì‹¤í—˜ ì¢…ë£Œ\n",
    "wandb.finish()\n",
    "\n",
    "logger.write(\"\\nâœ… Baseline experiment completed successfully!\")\n",
    "logger.write(f\"Log file: {log_dir / 'baseline.log'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
