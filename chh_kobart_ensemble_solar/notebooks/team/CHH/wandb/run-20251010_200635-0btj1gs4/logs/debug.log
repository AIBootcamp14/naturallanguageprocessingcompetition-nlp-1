2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_setup.py:_flush():81] Current SDK version is 0.22.2
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_setup.py:_flush():81] Configure stats pid to 303991
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_setup.py:_flush():81] Loading settings from /home/ieyeppo/.config/wandb/settings
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_setup.py:_flush():81] Loading settings from /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/wandb/settings
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_init.py:setup_run_log_directory():705] Logging user logs to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/wandb/run-20251010_200635-0btj1gs4/logs/debug.log
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_init.py:setup_run_log_directory():706] Logging internal logs to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/wandb/run-20251010_200635-0btj1gs4/logs/debug-internal.log
2025-10-10 20:06:35,478 INFO    MainThread:303991 [wandb_init.py:monkeypatch_ipython():624] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x771965370a10>
2025-10-10 20:06:35,479 INFO    MainThread:303991 [wandb_init.py:init():832] calling init triggers
2025-10-10 20:06:35,479 INFO    MainThread:303991 [wandb_init.py:init():837] wandb.init called with sweep_config: {}
config: {'augmentation': {'enabled': True, 'target_ratio': 1.5, 'techniques': {'back_translation': {'enabled': True, 'languages': ['en', 'ja'], 'translation_model': 'Helsinki-NLP/opus-mt'}, 'dialogue_reordering': {'enabled': False}, 'paraphrase': {'enabled': True, 'model': 'lcw99/t5-base-korean-paraphrase', 'num_variants': 2, 'quality_threshold': 0.7}, 'token_replacement': {'enabled': True, 'preserve_entities': True, 'replacement_ratio': 0.15}}}, 'cross_validation': {'enabled': True, 'n_splits': 5, 'shuffle': True, 'random_state': 42, 'train_all_folds': True, 'save_all_folds': True, 'ensemble_folds': True}, 'data_quality': {'enabled': True, 'checks': {'structural': {'check_nulls': True, 'check_duplicates': True, 'check_encoding': True}, 'statistical': {'outlier_detection': True, 'outlier_method': 'isolation_forest', 'outlier_threshold': 0.05}, 'semantic': {'check_information_loss': True, 'min_compression_ratio': 0.1, 'max_compression_ratio': 0.5}}, 'handle_issues': {'fix_encoding': True, 'remove_duplicates': True, 'remove_outliers': True}}, 'deployment': {'monitoring': {'enabled': True, 'health_check_interval': 60, 'metrics_port': 9090}, 'serving': {'framework': 'fastapi', 'port': 8000, 'workers': 4}, 'versioning': {'enabled': True, 'registry': 'local'}}, 'ensemble': {'base_method': 'weighted_average', 'test_time_augmentation': {'enabled': True, 'num_augmentations': 5, 'aggregation': 'mean'}, 'advanced': {'stacking': {'enabled': True, 'meta_learner': 'lgbm', 'cv_folds': 3}, 'blending': {'enabled': False, 'validation_size': 0.2}}}, 'experiment': {'name': 'full_pipeline_v1', 'version': '1.0.0', 'description': '모든 최적화 기법이 적용된 최종 파이프라인', 'timestamp': True, 'save_all_results': True, 'results_format': ['json', 'csv', 'pickle'], 'checkpointing': {'save_every_n_epochs': 1, 'save_best_only': False, 'keep_last_n': 3}}, 'gpu': {'device': 'cuda', 'cuda_device': 0, 'mixed_precision': True, 'memory_fraction': 0.95, 'use_gpu_optimization': True, 'gpu_check_path': '../../../src/utils/gpu_optimization/team_gpu_check.py', 'auto_optimization': {'enabled': True, 'find_optimal_batch_size': True, 'gradient_accumulation_auto': True}}, 'hyperparameter_optimization': {'enabled': True, 'n_trials': 100, 'metric': 'rouge_l', 'direction': 'maximize', 'sampler': 'TPESampler', 'pruner': 'MedianPruner', 'search_space': {'learning_rate': {'type': 'float', 'low': 1e-05, 'high': 0.001, 'log': True}, 'batch_size': {'type': 'categorical', 'choices': [4, 8, 16]}, 'lora_r': {'type': 'int', 'low': 4, 'high': 64, 'step': 4}, 'lora_alpha': {'type': 'int', 'low': 8, 'high': 128, 'step': 8}, 'num_beams': {'type': 'int', 'low': 2, 'high': 8}, 'temperature': {'type': 'float', 'low': 0.1, 'high': 1.0}}}, 'inference_optimization': {'batch_inference': {'enabled': True, 'optimal_batch_size': 8, 'dynamic_batching': True}, 'onnx_conversion': {'enabled': False, 'optimize': True, 'quantization': 'dynamic'}, 'tensorrt': {'enabled': False, 'precision': 'fp16'}}, 'inference': {'batch_size': 8, 'max_length': 100, 'num_beams': 4, 'no_repeat_ngram_size': 2, 'early_stopping': True, 'remove_special_tokens': True}, 'logging': {'level': 'INFO', 'format': '%(asctime)s - [%(pipeline_stage)s] - %(name)s - %(levelname)s - %(message)s', 'save_to_file': True, 'use_notebook_logger': True, 'notebook_logger_path': '../../../src/logging/notebook_logger.py', 'loggers': ['pipeline', 'training', 'evaluation', 'inference']}, 'models': {'primary_models': [{'name': 'gogamza/kobart-base-v2', 'max_input_length': 1024, 'max_target_length': 200, 'use_lora': False, 'weight': 1.0}], 'auxiliary_models': [{'name': 'digit82/kobart-summarization', 'model_type': 'seq2seq', 'weight': 0.1}, {'name': 'gogamza/kobart-summarization', 'model_type': 'seq2seq', 'weight': 0.05}]}, 'paths': {'data_dir': '../../../data/raw', 'train_file': '../../../data/raw/train.csv', 'dev_file': '../../../data/raw/dev.csv', 'test_file': '../../../data/raw/test.csv', 'output_dir': './models/full_pipeline', 'log_dir': './logs/full_pipeline', 'submission_dir': './submissions/full_pipeline', 'visualization_dir': './logs/full_pipeline/visualizations', 'cache_dir': './cache/full_pipeline', 'preprocessed_data_dir': './data/preprocessed', 'augmented_data_dir': './data/augmented'}, 'performance_targets': {'overall': 0.85, 'rouge_1': 0.45, 'rouge_2': 0.3, 'rouge_l': 0.4}, 'pipeline': {'stages': ['data_quality_check', 'data_preprocessing', 'data_augmentation', 'model_training', 'cross_validation', 'ensemble', 'hyperparameter_optimization', 'inference_optimization', 'final_prediction'], 'parallel_stages': [['model_training', 'prompt_optimization'], ['cross_validation', 'tta_preparation']]}, 'post_processing': {'grammar_correction': {'enabled': True, 'tool': 'py-hanspell'}, 'length_adjustment': {'min_length': 30, 'max_length': 150, 'target_length': 80}, 'quality_check': {'check_coherence': True, 'check_completeness': True, 'min_rouge_score': 0.3}}, 'preprocessing': {'data_split': {'validation_ratio': 0.1, 'stratify': False, 'seed': 42}, 'noise_removal': {'fix_escaped_chars': True, 'normalize_whitespace': True, 'remove_html_tags': True, 'remove_special_tokens': False}, 'token_normalization': {'person_tokens': {'standardize': True, 'format': '#Person{id}#'}, 'masking_tokens': {'preserve': True, 'tokens': ['#PhoneNumber#', '#Address#', '#SSN#', '#Email#']}}}, 'prompt_engineering': {'enabled': True, 'ab_testing': {'enabled': True, 'num_variants': 5, 'selection_metric': 'rouge_l'}, 'templates': {'zero_shot': {'enabled': True, 'template': '다음 대화를 3-5문장으로 요약하세요:\n{dialogue}\n\n요약:\n'}, 'few_shot': {'enabled': True, 'num_examples': 3, 'example_selection': 'random'}, 'chain_of_thought': {'enabled': True, 'template': '다음 대화를 단계적으로 분석하여 요약하세요.\n\n1단계: 주요 주제 파악\n2단계: 핵심 정보 추출\n3단계: 간결한 요약 작성\n\n대화: {dialogue}\n\n분석 및 요약:\n'}}}, 'reproducibility': {'seed': 42, 'deterministic': True, 'benchmark': False, 'worker_init_fn': True}, 'solar_api': {'api_key': 'up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT', 'enabled': True, 'hybrid_approach': {'use_for_validation': True, 'use_for_difficult_samples': True, 'confidence_threshold': 0.7}, 'optimization': {'batch_size': 10, 'cache_responses': True, 'token_budget': 100000}}, 'training': {'seed': 42, 'num_epochs': 30, 'batch_size': 8, 'learning_rate': '5e-5', 'use_sample': False, 'scheduler': 'linear', 'warmup_ratio': 0.0, 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'gradient_accumulation_steps': 1, 'gradient_checkpointing': False, 'fp16': True, 'early_stopping': {'metric': 'eval_rouge_sum', 'patience': 3, 'threshold': 0.001}, 'predict_with_generate': True, 'generation_max_length': 100, 'generation_num_beams': 4, 'generation_no_repeat_ngram_size': 2, 'evaluation_strategy': 'epoch', 'save_strategy': 'epoch', 'load_best_model_at_end': True, 'metric_for_best_model': 'rouge_sum', 'greater_is_better': True, 'save_total_limit': 2}, 'visualization': {'enabled': True, 'save_path': './logs/full_pipeline/visualizations', 'use_training_viz': True, 'training_viz_path': '../../../src/utils/visualizations/training_viz.py', 'plots': ['training_curves', 'model_comparison', 'confusion_matrix', 'rouge_distribution', 'sample_difficulty_heatmap', 'ensemble_weights', 'hyperparameter_importance']}, 'wandb': {'project': 'nlp-competition', 'entity': 'ieyeppo', 'name': 'full-pipeline-integrated', 'notes': '모든 기법이 통합된 최종 파이프라인', 'mode': 'online', 'tags': ['full_pipeline', 'production', 'all_techniques'], 'log_artifacts': True, 'log_datasets': True, 'log_models': True}, '_wandb': {}}
2025-10-10 20:06:35,479 INFO    MainThread:303991 [wandb_init.py:init():880] starting backend
2025-10-10 20:06:35,685 INFO    MainThread:303991 [wandb_init.py:init():883] sending inform_init request
2025-10-10 20:06:35,687 INFO    MainThread:303991 [wandb_init.py:init():891] backend started and connected
2025-10-10 20:06:35,690 INFO    MainThread:303991 [wandb_run.py:_label_probe_notebook():1330] probe notebook
2025-10-10 20:06:35,691 INFO    MainThread:303991 [wandb_init.py:init():961] updated telemetry
2025-10-10 20:06:35,695 INFO    MainThread:303991 [wandb_init.py:init():985] communicating run to backend with 90.0 second timeout
2025-10-10 20:06:36,517 INFO    MainThread:303991 [wandb_init.py:init():1036] starting run threads in backend
2025-10-10 20:06:36,611 INFO    MainThread:303991 [wandb_run.py:_console_start():2509] atexit reg
2025-10-10 20:06:36,611 INFO    MainThread:303991 [wandb_run.py:_redirect():2357] redirect: wrap_raw
2025-10-10 20:06:36,611 INFO    MainThread:303991 [wandb_run.py:_redirect():2426] Wrapping output streams.
2025-10-10 20:06:36,611 INFO    MainThread:303991 [wandb_run.py:_redirect():2449] Redirects installed.
2025-10-10 20:06:36,613 INFO    MainThread:303991 [wandb_init.py:init():1076] run started, returning control to user process
2025-10-10 20:06:36,614 INFO    MainThread:303991 [wandb_init.py:_post_run_cell_hook():594] [no run ID] resuming backend
2025-10-10 20:06:36,664 INFO    MainThread:303991 [jupyter.py:save_ipynb():404] [no run ID] not saving jupyter notebook
2025-10-10 20:06:36,664 INFO    MainThread:303991 [wandb_init.py:_pre_run_cell_hook():583] [no run ID] pausing backend
2025-10-10 20:06:36,947 INFO    MainThread:303991 [wandb_init.py:_post_run_cell_hook():594] [no run ID] resuming backend
2025-10-10 20:06:36,975 INFO    MainThread:303991 [jupyter.py:save_ipynb():404] [no run ID] not saving jupyter notebook
2025-10-10 20:06:36,975 INFO    MainThread:303991 [wandb_init.py:_pre_run_cell_hook():583] [no run ID] pausing backend
2025-10-10 20:06:37,646 INFO    MainThread:303991 [wandb_init.py:_post_run_cell_hook():594] [no run ID] resuming backend
2025-10-10 20:06:37,704 INFO    MainThread:303991 [jupyter.py:save_ipynb():404] [no run ID] not saving jupyter notebook
2025-10-10 20:06:37,704 INFO    MainThread:303991 [wandb_init.py:_pre_run_cell_hook():583] [no run ID] pausing backend
2025-10-10 21:35:00,479 INFO    MainThread:303991 [wandb_init.py:_post_run_cell_hook():594] [no run ID] resuming backend
2025-10-10 21:35:00,495 INFO    MainThread:303991 [jupyter.py:save_ipynb():404] [no run ID] not saving jupyter notebook
2025-10-10 21:35:00,495 INFO    MainThread:303991 [wandb_init.py:_pre_run_cell_hook():583] [no run ID] pausing backend
2025-10-10 21:36:46,100 INFO    MainThread:303991 [wandb_init.py:_post_run_cell_hook():594] [no run ID] resuming backend
