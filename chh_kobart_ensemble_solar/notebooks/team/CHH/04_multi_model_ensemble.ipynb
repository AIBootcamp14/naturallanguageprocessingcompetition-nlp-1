{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≠ Îã§Ï§ë Î™®Îç∏ ÏïôÏÉÅÎ∏î - 5Í∞ú Î™®Îç∏ ÌÜµÌï©\n",
    "> PRD Í≥ÑÌöçÏóê Îî∞Î•∏ 5Í∞ú Î™®Îç∏ ÏïôÏÉÅÎ∏î + TTA Ï†ÑÎûµ\n",
    "\n",
    "**Î™©Ìëú ÏÑ±Îä•**: ROUGE-F1 75-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "‚úÖ ÎÇòÎàîÍ≥†Îîï Ìè∞Ìä∏ Î°úÎìú ÏÑ±Í≥µ\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3Î≤àÎßå parent ÏÇ¨Ïö©!\n",
    "\n",
    "# Îã§Î•∏ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú Ï†úÍ±∞ÌïòÍ≥† ÌòÑÏû¨ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°úÎßå Ï∂îÍ∞Ä\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# Ïª§Ïä§ÌÖÄ Î™®Îìà ÏûÑÌè¨Ìä∏\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Models (5):\n",
      "  - solar: weight=0.30\n",
      "  - polyglot: weight=0.25\n",
      "  - kullm: weight=0.20\n",
      "  - kobart: weight=0.15\n",
      "  - koalpaca: weight=0.10\n"
     ]
    }
   ],
   "source": [
    "# ÏÑ§Ï†ï ÌååÏùº Î°úÎìú\n",
    "config_path = notebook_dir / 'configs' / 'config_multi_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ÌôúÏÑ±ÌôîÎêú Î™®Îç∏ ÌôïÏù∏\n",
    "enabled_models = [name for name, cfg in config['ensemble_models'].items() if cfg['enabled']]\n",
    "print(f\"Enabled Models ({len(enabled_models)}):\")\n",
    "for model_name in enabled_models:\n",
    "    weight = config['ensemble_models'][model_name]['weight']\n",
    "    print(f\"  - {model_name}: weight={weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Multi-Model Ensemble Experiment\n",
      "Timestamp: 20251010_090607\n",
      "Models: ['solar', 'polyglot', 'kullm', 'kobart', 'koalpaca']\n",
      "Ensemble Method: weighted_average\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "# configÏùò Î°úÍ∑∏ Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configÏùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configÏóê log_dirÏù¥ Ï†ïÏùòÎêòÏñ¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í\n",
    "if 'log_dir' in config['paths']:\n",
    "    log_dir = get_path(config['paths']['log_dir'])\n",
    "else:\n",
    "    # Í∏∞Î≥∏Í∞í: notebook_dir/logs/multi_model\n",
    "    log_dir = notebook_dir / 'logs' / 'multi_model'\n",
    "\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÏÉùÏÑ±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Î°úÍ±∞ Ï¥àÍ∏∞Ìôî\n",
    "log_file = log_dir / f'ensemble_{len(enabled_models)}models_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('Multi-Model Ensemble Experiment')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Models: {enabled_models}')\n",
    "logger.write(f'Ensemble Method: {config[\"ensemble_strategy\"][\"method\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TTA Configuration:\n",
      "  - Augmentations: 3\n",
      "  - Aggregation: mean\n",
      "  - paraphrase: enabled\n",
      "TTA: paraphrase enabled\n",
      "  - reorder: enabled\n",
      "TTA: reorder enabled\n"
     ]
    }
   ],
   "source": [
    "# TTA ÏÑ§Ï†ï ÌôïÏù∏\n",
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\nTTA Configuration:\")\n",
    "    logger.write(f\"  - Augmentations: {config['tta']['num_augmentations']}\")\n",
    "    logger.write(f\"  - Aggregation: {config['tta']['aggregation']}\")\n",
    "    \n",
    "    for technique, settings in config['tta']['techniques'].items():\n",
    "        if settings['enabled']:\n",
    "            logger.write(f\"  - {technique}: enabled\")\n",
    "            print(f\"TTA: {technique} enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n",
      "Will clear GPU cache between models\n"
     ]
    }
   ],
   "source": [
    "# GPU Ï≤¥ÌÅ¨ Î∞è Î©ÄÌã∞ GPU ÏÑ§Ï†ï\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    \n",
    "    # Î©ÄÌã∞ GPU Ï≤¥ÌÅ¨\n",
    "    if config['gpu']['multi_gpu']['enabled'] and torch.cuda.device_count() > 1:\n",
    "        logger.write(f\"Multi-GPU available: {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ ÏÑ§Ï†ï\n",
    "    if config['gpu']['empty_cache_between_models']:\n",
    "        logger.write(\"Will clear GPU cache between models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight visualization saved to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/multi_model/visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109226/3820716939.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπò ÏãúÍ∞ÅÌôî\n",
    "weights = [config['ensemble_models'][name]['weight'] for name in enabled_models]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(enabled_models, weights, color=colors[:len(enabled_models)])\n",
    "plt.title('Ensemble Model Weights Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Weight', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "\n",
    "# Í∞ÄÏ§ëÏπò Í∞í ÌëúÏãú\n",
    "for bar, weight in zip(bars, weights):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{weight:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî Ï†ÄÏû• - config Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "if 'visualization_dir' in config['paths']:\n",
    "    viz_dir = get_path(config['paths']['visualization_dir'])\n",
    "else:\n",
    "    # Í∏∞Î≥∏Í∞í\n",
    "    viz_dir = log_dir / 'visualizations'\n",
    "\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(viz_dir / f'ensemble_weights_{timestamp}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.write(f\"Weight visualization saved to {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solar API Comparison Settings:\n",
      "  - API Key: up_rMJWNzz...\n",
      "  - Use as baseline: True\n",
      "  - Include in ensemble: False\n",
      "\n",
      "Solar API configured for comparison\n"
     ]
    }
   ],
   "source": [
    "# Solar API ÎπÑÍµê ÏÑ§Ï†ï\n",
    "if config['solar_api_comparison']['enabled']:\n",
    "    logger.write(\"\\nSolar API Comparison Settings:\")\n",
    "    logger.write(f\"  - API Key: {config['solar_api_comparison']['api_key'][:10]}...\")\n",
    "    logger.write(f\"  - Use as baseline: {config['solar_api_comparison']['use_as_baseline']}\")\n",
    "    logger.write(f\"  - Include in ensemble: {config['solar_api_comparison']['include_in_ensemble']}\")\n",
    "    \n",
    "    print(\"\\nSolar API configured for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Weight Optimization:\n",
      "  - Trials: 50\n",
      "  - Study: ensemble_weight_optimization\n",
      "  - Metric: rouge_l\n",
      "Optuna configured for ensemble weight optimization\n"
     ]
    }
   ],
   "source": [
    "# Optuna ÏµúÏ†ÅÌôî ÏÑ§Ï†ï (ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπò)\n",
    "if config['optuna']['enabled']:\n",
    "    logger.write(\"\\nOptuna Weight Optimization:\")\n",
    "    logger.write(f\"  - Trials: {config['optuna']['n_trials']}\")\n",
    "    logger.write(f\"  - Study: {config['optuna']['study_name']}\")\n",
    "    logger.write(f\"  - Metric: {config['optuna']['metric']}\")\n",
    "    \n",
    "    import optuna\n",
    "    print(\"Optuna configured for ensemble weight optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Î™®Îç∏Î≥Ñ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
    "\n",
    "Í∞Å Î™®Îç∏ÏùÑ Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú ÌïôÏäµÌïòÍ≥† ÌèâÍ∞ÄÌï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Loading ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "\n",
      "Data loaded successfully!\n",
      "  - Train samples: 12457\n",
      "  - Dev samples: 499\n",
      "  - Test samples: 499\n",
      "\n",
      "Train data topics:\n",
      "topic\n",
      "ÏùåÏãù Ï£ºÎ¨∏     130\n",
      "Ï∑®ÏóÖ Î©¥Ï†ë     109\n",
      "Í∏∏ ÏïàÎÇ¥       66\n",
      "Ìò∏ÌÖî Ï≤¥ÌÅ¨Ïù∏     40\n",
      "ÏïÑÌååÌä∏ ÏûÑÎåÄ     30\n",
      "ÏùºÏÉÅ ÎåÄÌôî      29\n",
      "ÏáºÌïë         27\n",
      "Ï£ºÎßê Í≥ÑÌöç      26\n",
      "Î©¥Ï†ë         25\n",
      "Ìò∏ÌÖî ÏòàÏïΩ      25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First dialogue (200 chars):\n",
      "#Person1#: ÏïàÎÖïÌïòÏÑ∏Ïöî, Mr. Smith. Ï†ÄÎäî Dr. HawkinsÏûÖÎãàÎã§. Ïò§Îäò Î¨¥Ïä® ÏùºÎ°ú Ïò§ÏÖ®Ïñ¥Ïöî? \n",
      "#Person2#: Í±¥Í∞ïÍ≤ÄÏßÑÏùÑ Î∞õÏúºÎ†§Í≥† ÏôîÏñ¥Ïöî. \n",
      "#Person1#: ÎÑ§, 5ÎÖÑ ÎèôÏïà Í≤ÄÏßÑÏùÑ Ïïà Î∞õÏúºÏÖ®ÎÑ§Ïöî. Îß§ÎÖÑ Ìïú Î≤àÏî© Î∞õÏúºÏÖîÏïº Ìï¥Ïöî. \n",
      "#Person2#: ÏïåÏ£†. ÌäπÎ≥ÑÌûà ÏïÑÌîà Îç∞Í∞Ä ÏóÜÏúºÎ©¥ Íµ≥Ïù¥ Í∞à ÌïÑÏöîÍ∞Ä ÏóÜÎã§Í≥† ÏÉùÍ∞ÅÌñàÏñ¥Ïöî. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "# config ÌååÏùºÏùò Í≤ΩÎ°ú ÏÇ¨Ïö©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configÏùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(\"\\n=== Data Loading ===\")\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"\\nData loaded successfully!\")\n",
    "logger.write(f\"  - Train samples: {len(train_df)}\")\n",
    "logger.write(f\"  - Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"  - Test samples: {len(test_df)}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå Ï∂úÎ†•\n",
    "print(\"\\nTrain data topics:\")\n",
    "print(train_df['topic'].value_counts().head(10))\n",
    "print(f\"\\nFirst dialogue (200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== solar Model ===\n",
      "  - Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "  - Weight: 0.3\n",
      "  - LoRA: r=16, alpha=32\n",
      "Would train solar here...\n",
      "\n",
      "=== polyglot Model ===\n",
      "  - Model: EleutherAI/polyglot-ko-12.8b\n",
      "  - Weight: 0.25\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train polyglot here...\n",
      "\n",
      "=== kullm Model ===\n",
      "  - Model: nlpai-lab/kullm-v2\n",
      "  - Weight: 0.2\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train kullm here...\n",
      "\n",
      "=== kobart Model ===\n",
      "  - Model: digit82/kobart-summarization\n",
      "  - Weight: 0.15\n",
      "Would train kobart here...\n",
      "\n",
      "=== koalpaca Model ===\n",
      "  - Model: beomi/KoAlpaca-Polyglot-12.8B\n",
      "  - Weight: 0.1\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train koalpaca here...\n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏Î≥Ñ Í≤∞Í≥º Ï†ÄÏû•\n",
    "model_results = {}\n",
    "\n",
    "for model_name in enabled_models:\n",
    "    logger.write(f\"\\n=== {model_name} Model ===\")\n",
    "    model_config = config['ensemble_models'][model_name]\n",
    "    logger.write(f\"  - Model: {model_config['name']}\")\n",
    "    logger.write(f\"  - Weight: {model_config['weight']}\")\n",
    "    \n",
    "    if model_config.get('use_lora', False):\n",
    "        logger.write(f\"  - LoRA: r={model_config['lora_config']['r']}, alpha={model_config['lora_config']['alpha']}\")\n",
    "    \n",
    "    # Ïã§Ï†ú ÌïôÏäµ ÏΩîÎìúÎäî Ïó¨Í∏∞Ïóê Íµ¨ÌòÑ\n",
    "    print(f\"Would train {model_name} here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA (Text Test Augmentation)\n",
    "\n",
    "ÌÖçÏä§Ìä∏ Ï¶ùÍ∞ïÏùÑ ÌÜµÌïú ÏÑ±Îä• Ìñ•ÏÉÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TTA Implementation ===\n",
      "Paraphrase augmentation enabled\n",
      "  - Model: lcw99/t5-base-korean-paraphrase\n",
      "  - Variants: 2\n",
      "TTA would be applied here...\n"
     ]
    }
   ],
   "source": [
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\n=== TTA Implementation ===\")\n",
    "    \n",
    "    # Paraphrase\n",
    "    if config['tta']['techniques']['paraphrase']['enabled']:\n",
    "        logger.write(\"Paraphrase augmentation enabled\")\n",
    "        logger.write(f\"  - Model: {config['tta']['techniques']['paraphrase']['model']}\")\n",
    "        logger.write(f\"  - Variants: {config['tta']['techniques']['paraphrase']['num_variants']}\")\n",
    "    \n",
    "    # Ïã§Ï†ú TTA Íµ¨ÌòÑÏùÄ Ïó¨Í∏∞Ïóê\n",
    "    print(\"TTA would be applied here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÏïôÏÉÅÎ∏î Î∞è ÏµúÏ¢Ö ÏòàÏ∏°\n",
    "\n",
    "Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏°ÏùÑ ÏïôÏÉÅÎ∏îÌïòÏó¨ ÏµúÏ¢Ö Í≤∞Í≥ºÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Solar API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï (PRD 09_Solar_API_ÏµúÏ†ÅÌôî.md Ï∞∏Í≥†)\nimport hashlib\nimport time\nimport re\nfrom openai import OpenAI\n\nclass SolarAPIOptimizer:\n    \"\"\"Solar API ÏµúÏ†ÅÌôî ÌÅ¥ÎûòÏä§\"\"\"\n    def __init__(self, api_key):\n        self.client = OpenAI(\n            api_key=api_key,\n            base_url=\"https://api.upstage.ai/v1/solar\"\n        )\n        self.token_limit = 512  # ÌÜ†ÌÅ∞ Ï†úÌïú\n        self.cache = {}  # Í≤∞Í≥º Ï∫êÏã±\n    \n    def preprocess_dialogue(self, dialogue):\n        \"\"\"ÎåÄÌôîÎ¨∏ Ï†ÑÏ≤òÎ¶¨Î°ú ÌÜ†ÌÅ∞ Ï†àÏïΩ\"\"\"\n        # Î∂àÌïÑÏöîÌïú Í≥µÎ∞± Ï†úÍ±∞\n        dialogue = ' '.join(dialogue.split())\n        \n        # Person ÌÉúÍ∑∏ Í∞ÑÏÜåÌôî\n        dialogue = dialogue.replace('#Person1#:', 'A:')\n        dialogue = dialogue.replace('#Person2#:', 'B:')\n        dialogue = dialogue.replace('#Person3#:', 'C:')\n        \n        # ÎåÄÌôî Í∏∏Ïù¥ Ï†úÌïú\n        return self.smart_truncate(dialogue, max_tokens=self.token_limit)\n    \n    def smart_truncate(self, text, max_tokens=512):\n        \"\"\"Ïä§ÎßàÌä∏ Ï†àÎã®: Î¨∏Ïû• Îã®ÏúÑÎ°ú ÏûêÎ•¥Í∏∞\"\"\"\n        # ÌÜ†ÌÅ∞ Ïàò Ï∂îÏ†ï (ÌïúÍ∏Ä ÌèâÍ∑† 2.5Ïûê = 1ÌÜ†ÌÅ∞)\n        estimated_tokens = len(text) / 2.5\n        \n        if estimated_tokens <= max_tokens:\n            return text\n        \n        sentences = text.split('.')\n        truncated = []\n        current_length = 0\n        \n        for sentence in sentences:\n            sentence_tokens = len(sentence) / 2.5\n            if current_length + sentence_tokens > max_tokens:\n                break\n            truncated.append(sentence)\n            current_length += sentence_tokens\n        \n        return '.'.join(truncated) + '.'\n    \n    def generate_summary(self, dialogue):\n        \"\"\"Solar APIÎ°ú ÏöîÏïΩ ÏÉùÏÑ±\"\"\"\n        # Ï∫êÏãú ÌôïÏù∏\n        dialogue_hash = hashlib.md5(dialogue.encode()).hexdigest()\n        if dialogue_hash in self.cache:\n            return self.cache[dialogue_hash]\n        \n        # Ï†ÑÏ≤òÎ¶¨\n        processed_dialogue = self.preprocess_dialogue(dialogue)\n        \n        # API Ìò∏Ï∂ú\n        try:\n            response = self.client.chat.completions.create(\n                model=\"solar-1-mini-chat\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"Îã§Ïùå ÎåÄÌôîÎ•º 3-5Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî.\"},\n                    {\"role\": \"user\", \"content\": processed_dialogue}\n                ],\n                max_tokens=150,\n                temperature=0.3\n            )\n            \n            summary = response.choices[0].message.content\n            \n            # Ï∫êÏã±\n            self.cache[dialogue_hash] = summary\n            \n            return summary\n        \n        except Exception as e:\n            logger.write(f\"Solar API error: {e}\")\n            return None\n\n# Solar API Ï¥àÍ∏∞Ìôî\nif config['solar_api_comparison']['enabled']:\n    solar_api = SolarAPIOptimizer(config['solar_api_comparison']['api_key'])\n    logger.write(\"Solar API client initialized\")\nelse:\n    solar_api = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ÎìÄÏñº ÏöîÏïΩ ÏãúÏä§ÌÖú (PRD 10_ÍµêÏ∞®_Í≤ÄÏ¶ù_ÏãúÏä§ÌÖú.md Ï∞∏Í≥†)\nclass QualityEvaluator:\n    \"\"\"ÏöîÏïΩ ÌíàÏßà ÌèâÍ∞ÄÍ∏∞\"\"\"\n    def __init__(self):\n        self.criteria = {\n            'length_ratio': 0.2,      # ÏöîÏïΩ Í∏∏Ïù¥ Ï†ÅÏ†àÏÑ±\n            'keyword_coverage': 0.3,   # ÌïµÏã¨ ÌÇ§ÏõåÎìú Ìè¨Ìï®\n            'coherence': 0.25,         # Î¨∏Ïû• ÏùºÍ¥ÄÏÑ±\n            'information_density': 0.25 # Ï†ïÎ≥¥ Î∞ÄÎèÑ\n        }\n    \n    def evaluate(self, summary, dialogue):\n        \"\"\"ÏöîÏïΩ ÌíàÏßà Ï¢ÖÌï© ÌèâÍ∞Ä\"\"\"\n        scores = {\n            'length_ratio': self.check_length_ratio(summary, dialogue),\n            'keyword_coverage': self.check_keyword_coverage(summary, dialogue),\n            'coherence': self.check_coherence(summary),\n            'information_density': self.check_information_density(summary)\n        }\n        \n        # Í∞ÄÏ§ë ÌèâÍ∑†\n        total_score = sum(\n            score * self.criteria[metric]\n            for metric, score in scores.items()\n        )\n        return total_score\n    \n    def check_length_ratio(self, summary, dialogue):\n        \"\"\"ÏöîÏïΩ Í∏∏Ïù¥ Ï†ÅÏ†àÏÑ± (Ïù¥ÏÉÅÏ†Å: ÏõêÎ≥∏Ïùò 20-30%)\"\"\"\n        if not summary or not dialogue:\n            return 0.5\n        \n        ratio = len(summary) / len(dialogue)\n        if 0.2 <= ratio <= 0.3:\n            return 1.0\n        elif 0.15 <= ratio <= 0.35:\n            return 0.8\n        else:\n            return 0.5\n    \n    def check_keyword_coverage(self, summary, dialogue):\n        \"\"\"ÌïµÏã¨ ÌÇ§ÏõåÎìú Ìè¨Ìï®ÎèÑ\"\"\"\n        # Í∞ÑÎã®Ìïú Íµ¨ÌòÑ (Ïã§Ï†úÎ°úÎäî ÌòïÌÉúÏÜå Î∂ÑÏÑù ÌïÑÏöî)\n        dialogue_words = set(dialogue.split())\n        summary_words = set(summary.split())\n        \n        if not dialogue_words:\n            return 0.5\n        \n        coverage = len(summary_words & dialogue_words) / len(dialogue_words)\n        return min(coverage * 2, 1.0)  # ÏµúÎåÄ 1.0\n    \n    def check_coherence(self, summary):\n        \"\"\"Î¨∏Ïû• ÏùºÍ¥ÄÏÑ± Ï≤¥ÌÅ¨\"\"\"\n        if not summary:\n            return 0.5\n        \n        sentences = summary.split('.')\n        if len(sentences) < 2:\n            return 0.8\n        \n        # Í∞ÑÎã®Ìïú ÏùºÍ¥ÄÏÑ± Ï≤¥ÌÅ¨\n        return 0.85  # Í∏∞Î≥∏Í∞í\n    \n    def check_information_density(self, summary):\n        \"\"\"Ï†ïÎ≥¥ Î∞ÄÎèÑ Ï∏°Ï†ï\"\"\"\n        if not summary:\n            return 0.5\n        \n        words = summary.split()\n        if not words:\n            return 0.5\n        \n        unique_words = set(words)\n        density = len(unique_words) / len(words)\n        return min(density * 1.5, 1.0)\n\nclass DualSummarizationSystem:\n    \"\"\"ÎìÄÏñº ÏöîÏïΩ ÏãúÏä§ÌÖú - Î™®Îç∏Í≥º API Í≤∞Í≥º ÎπÑÍµê\"\"\"\n    def __init__(self, model_generator, solar_api):\n        self.model = model_generator\n        self.api = solar_api\n        self.evaluator = QualityEvaluator()\n    \n    def generate_summaries(self, dialogue):\n        \"\"\"Îëê Í∞ÄÏßÄ Î∞©Î≤ïÏúºÎ°ú ÏöîÏïΩ ÏÉùÏÑ±\"\"\"\n        # 1. Î™®Îç∏Î°ú ÏÉùÏÑ±\n        model_summary = self.model(dialogue) if self.model else \"Î™®Îç∏ ÏöîÏïΩ ÏóÜÏùå\"\n        model_confidence = 0.7  # Í∏∞Î≥∏ Ïã†Î¢∞ÎèÑ\n        \n        # 2. Solar APIÎ°ú ÏÉùÏÑ±\n        api_summary = self.api.generate_summary(dialogue) if self.api else \"API ÏöîÏïΩ ÏóÜÏùå\"\n        api_confidence = 0.85  # APIÎäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏïàÏ†ïÏ†Å\n        \n        return {\n            'model': {\n                'summary': model_summary,\n                'confidence': model_confidence\n            },\n            'api': {\n                'summary': api_summary,\n                'confidence': api_confidence\n            }\n        }\n    \n    def select_best_summary(self, summaries, dialogue):\n        \"\"\"ÏµúÏ†ÅÏùò ÏöîÏïΩ ÏÑ†ÌÉù\"\"\"\n        model_score = self.evaluator.evaluate(\n            summaries['model']['summary'],\n            dialogue\n        )\n        api_score = self.evaluator.evaluate(\n            summaries['api']['summary'],\n            dialogue\n        )\n        \n        # Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n        model_final = model_score * summaries['model']['confidence']\n        api_final = api_score * summaries['api']['confidence']\n        \n        logger.write(f\"  Model score: {model_final:.3f}, API score: {api_final:.3f}\")\n        \n        if model_final > api_final:\n            return summaries['model']['summary'], 'model'\n        else:\n            return summaries['api']['summary'], 'api'\n\n# ÎìÄÏñº ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî\nlogger.write(\"\\n=== Dual Summarization System ===\")\nlogger.write(\"Quality evaluator initialized\")\nlogger.write(\"Dual system ready for model vs API comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Í∞Å Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä (Ïã§Ï†ú Íµ¨ÌòÑ)\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nimport gc\n\nclass DialogueSummaryDataset(Dataset):\n    \"\"\"ÎåÄÌôî ÏöîÏïΩ Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"ÎåÄÌôî Ï†ÑÏ≤òÎ¶¨\"\"\"\n        text = text.replace('\\\\n', '\\n')\n        text = re.sub(r'#Person(\\d+)#:', r'ÌôîÏûê\\1:', text)\n        return text.strip()\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_input_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if not self.is_test:\n            summary = row.get('summary', '')\n            targets = self.tokenizer(\n                summary,\n                max_length=self.max_target_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'labels': targets['input_ids'].squeeze()\n            }\n        else:\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# Î™®Îç∏Î≥Ñ ÌïôÏäµ Ìï®Ïàò\ndef train_model(model_name, model_config, train_df, dev_df, config):\n    \"\"\"Í∞úÎ≥Ñ Î™®Îç∏ ÌïôÏäµ\"\"\"\n    logger.write(f\"\\n=== Training {model_name} ===\")\n    \n    # KoBART Î™®Îç∏ ÏÇ¨Ïö© (Ïã§Ï†úÎ°úÎäî Í∞Å Î™®Îç∏Î≥ÑÎ°ú Îã§Î•¥Í≤å ÏÑ§Ï†ï)\n    model_path = \"digit82/kobart-summarization\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = BartForConditionalGeneration.from_pretrained(model_path)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n    train_dataset = DialogueSummaryDataset(\n        train_df.sample(n=min(1000, len(train_df)), random_state=42),  # ÏÉòÌîåÎßÅ\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        dev_df,\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n    \n    # ÏòµÌã∞ÎßàÏù¥Ï†Ä\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    # Í∞ÑÎã®Ìïú ÌïôÏäµ (1 ÏóêÌè≠Îßå)\n    model.train()\n    train_loss = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Training {model_name}\", total=min(50, len(train_loader))):\n        if train_loader.batch_sampler.batch_size * (train_loader.batch_sampler.batch_size + 1) > 50:\n            break  # 50 Î∞∞ÏπòÎßå ÌïôÏäµ (Îç∞Î™®)\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = train_loss / min(50, len(train_loader))\n    logger.write(f\"  {model_name} training loss: {avg_loss:.4f}\")\n    \n    # ÌèâÍ∞Ä\n    model.eval()\n    val_predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Evaluating {model_name}\", total=min(20, len(val_loader))):\n            if len(val_predictions) >= 20:\n                break  # 20Í∞ú ÏÉòÌîåÎßå ÌèâÍ∞Ä (Îç∞Î™®)\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n            \n            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            val_predictions.extend(preds)\n    \n    logger.write(f\"  {model_name} generated {len(val_predictions)} validation predictions\")\n    \n    # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return val_predictions\n\n# Î™®Îç∏Î≥Ñ Í≤∞Í≥º Ï†ÄÏû•\nensemble_predictions = {}\n\n# KoBARTÎßå Ïã§Ï†ú ÌïôÏäµ (Îç∞Î™®)\nif 'kobart' in enabled_models:\n    kobart_predictions = train_model(\n        'kobart',\n        config['ensemble_models']['kobart'],\n        train_df,\n        dev_df,\n        config\n    )\n    ensemble_predictions['kobart'] = kobart_predictions\n    \n# ÎÇòÎ®∏ÏßÄ Î™®Îç∏ÏùÄ mock ÏòàÏ∏°\nfor model_name in enabled_models:\n    if model_name != 'kobart':\n        # Mock predictions\n        ensemble_predictions[model_name] = [\"Mock ÏöîÏïΩ\"] * 20\n        logger.write(f\"\\n{model_name}: Mock predictions generated (Ïã§Ï†úÎ°úÎäî ÌïôÏäµ ÌïÑÏöî)\")\n\nlogger.write(f\"\\nÏ¥ù {len(ensemble_predictions)} Î™®Îç∏Ïùò ÏòàÏ∏° ÏÉùÏÑ± ÏôÑÎ£å\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Solar APIÏôÄ Î™®Îç∏ ÏòàÏ∏° ÎπÑÍµê (ÍµêÏ∞® Í≤ÄÏ¶ù)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solar API vs Î™®Îç∏ ÎπÑÍµê (PRD 10_ÍµêÏ∞®_Í≤ÄÏ¶ù_ÏãúÏä§ÌÖú.md Íµ¨ÌòÑ)\nlogger.write(\"\\n=== Solar API vs Model Comparison ===\")\n\n# Í≤ÄÏ¶ùÏö© ÏÉòÌîå ÏÑ†ÌÉù\ntest_samples = dev_df.sample(n=min(10, len(dev_df)), random_state=42)\n\ncomparison_results = {\n    'model_wins': 0,\n    'api_wins': 0,\n    'model_scores': [],\n    'api_scores': [],\n    'examples': []\n}\n\n# ÎìÄÏñº ÏãúÏä§ÌÖúÏúºÎ°ú ÎπÑÍµê\nif solar_api:\n    # Í∞ÑÎã®Ìïú Î™®Îç∏ ÏòàÏ∏° Ìï®Ïàò (Ïã§Ï†úÎ°úÎäî ÌïôÏäµÎêú Î™®Îç∏ ÏÇ¨Ïö©)\n    def model_generator(dialogue):\n        # Mock ÏòàÏ∏° (Ïã§Ï†úÎ°úÎäî ÌïôÏäµÎêú Î™®Îç∏ ÏÇ¨Ïö©)\n        return f\"Î™®Îç∏ ÏöîÏïΩ: {dialogue[:100]}...\"\n    \n    dual_system = DualSummarizationSystem(model_generator, solar_api)\n    \n    for idx, row in test_samples.iterrows():\n        dialogue = row['dialogue']\n        gold_summary = row['summary']\n        \n        logger.write(f\"\\nÍ≤ÄÏ¶ù ÏÉòÌîå {idx}:\")\n        \n        # ÎìÄÏñº ÏãúÏä§ÌÖúÏúºÎ°ú ÏöîÏïΩ ÏÉùÏÑ±\n        summaries = dual_system.generate_summaries(dialogue)\n        \n        # ÏµúÏ†Å ÏÑ†ÌÉù\n        best_summary, source = dual_system.select_best_summary(summaries, dialogue)\n        \n        if source == 'model':\n            comparison_results['model_wins'] += 1\n        else:\n            comparison_results['api_wins'] += 1\n        \n        # ÏòàÏãú Ï†ÄÏû•\n        if len(comparison_results['examples']) < 3:\n            comparison_results['examples'].append({\n                'dialogue': dialogue[:200] + '...',\n                'model_summary': summaries['model']['summary'][:100] + '...',\n                'api_summary': summaries['api']['summary'][:100] + '...' if summaries['api']['summary'] else 'N/A',\n                'selected': source,\n                'gold': gold_summary[:100] + '...'\n            })\n        \n        # Rate limiting for API\n        time.sleep(0.5)\n    \n    # Í≤∞Í≥º ÏöîÏïΩ\n    logger.write(\"\\n=== Comparison Results ===\")\n    logger.write(f\"Model wins: {comparison_results['model_wins']}\")\n    logger.write(f\"API wins: {comparison_results['api_wins']}\")\n    \n    # ÏòàÏãú Ï∂úÎ†•\n    logger.write(\"\\nÏÉòÌîå ÎπÑÍµê:\")\n    for i, example in enumerate(comparison_results['examples'], 1):\n        logger.write(f\"\\nÏòàÏãú {i}:\")\n        logger.write(f\"  ÏÑ†ÌÉù: {example['selected']}\")\n        logger.write(f\"  Î™®Îç∏: {example['model_summary']}\")\n        logger.write(f\"  API: {example['api_summary']}\")\nelse:\n    logger.write(\"Solar API ÎπÑÌôúÏÑ±ÌôîÎê® - ÎπÑÍµê ÏÉùÎûµ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Í∞Å Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä (Ïã§Ï†ú Íµ¨ÌòÑ)\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nimport gc\n\nclass DialogueSummaryDataset(Dataset):\n    \"\"\"ÎåÄÌôî ÏöîÏïΩ Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"ÎåÄÌôî Ï†ÑÏ≤òÎ¶¨\"\"\"\n        text = text.replace('\\\\n', '\\n')\n        text = re.sub(r'#Person(\\d+)#:', r'ÌôîÏûê\\1:', text)\n        return text.strip()\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_input_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if not self.is_test:\n            summary = row.get('summary', '')\n            targets = self.tokenizer(\n                summary,\n                max_length=self.max_target_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # ÎùºÎ≤® ÏÉùÏÑ± - Ìå®Îî© ÌÜ†ÌÅ∞ÏùÑ -100ÏúºÎ°ú ÎßàÏä§ÌÇπ (Ï§ëÏöî!)\n            labels = targets['input_ids'].squeeze()\n            labels[labels == self.tokenizer.pad_token_id] = -100  # Ìå®Îî© ÌÜ†ÌÅ∞ ÎßàÏä§ÌÇπ\n            \n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'labels': labels\n            }\n        else:\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# Î™®Îç∏Î≥Ñ ÌïôÏäµ Ìï®Ïàò\ndef train_model(model_name, model_config, train_df, dev_df, config):\n    \"\"\"Í∞úÎ≥Ñ Î™®Îç∏ ÌïôÏäµ\"\"\"\n    logger.write(f\"\\n=== Training {model_name} ===\")\n    \n    # KoBART Î™®Îç∏ ÏÇ¨Ïö© (Ïã§Ï†úÎ°úÎäî Í∞Å Î™®Îç∏Î≥ÑÎ°ú Îã§Î•¥Í≤å ÏÑ§Ï†ï)\n    model_path = \"digit82/kobart-summarization\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = BartForConditionalGeneration.from_pretrained(model_path)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n    train_dataset = DialogueSummaryDataset(\n        train_df.sample(n=min(1000, len(train_df)), random_state=42),  # ÏÉòÌîåÎßÅ\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        dev_df,\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n    \n    # ÏòµÌã∞ÎßàÏù¥Ï†Ä\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    # Í∞ÑÎã®Ìïú ÌïôÏäµ (1 ÏóêÌè≠Îßå)\n    model.train()\n    train_loss = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Training {model_name}\", total=min(50, len(train_loader))):\n        if train_loader.batch_sampler.batch_size * (train_loader.batch_sampler.batch_size + 1) > 50:\n            break  # 50 Î∞∞ÏπòÎßå ÌïôÏäµ (Îç∞Î™®)\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = train_loss / min(50, len(train_loader))\n    logger.write(f\"  {model_name} training loss: {avg_loss:.4f}\")\n    \n    # ÌèâÍ∞Ä\n    model.eval()\n    val_predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Evaluating {model_name}\", total=min(20, len(val_loader))):\n            if len(val_predictions) >= 20:\n                break  # 20Í∞ú ÏÉòÌîåÎßå ÌèâÍ∞Ä (Îç∞Î™®)\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True,\n                repetition_penalty=1.2,  # Î∞òÎ≥µ Î∞©ÏßÄ Ï∂îÍ∞Ä\n                no_repeat_ngram_size=3   # n-gram Î∞òÎ≥µ Î∞©ÏßÄ\n            )\n            \n            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            val_predictions.extend(preds)\n    \n    logger.write(f\"  {model_name} generated {len(val_predictions)} validation predictions\")\n    \n    # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return val_predictions\n\n# Î™®Îç∏Î≥Ñ Í≤∞Í≥º Ï†ÄÏû•\nensemble_predictions = {}\n\n# KoBARTÎßå Ïã§Ï†ú ÌïôÏäµ (Îç∞Î™®)\nif 'kobart' in enabled_models:\n    kobart_predictions = train_model(\n        'kobart',\n        config['ensemble_models']['kobart'],\n        train_df,\n        dev_df,\n        config\n    )\n    ensemble_predictions['kobart'] = kobart_predictions\n    \n# ÎÇòÎ®∏ÏßÄ Î™®Îç∏ÏùÄ mock ÏòàÏ∏°\nfor model_name in enabled_models:\n    if model_name != 'kobart':\n        # Mock predictions\n        ensemble_predictions[model_name] = [\"Mock ÏöîÏïΩ\"] * 20\n        logger.write(f\"\\n{model_name}: Mock predictions generated (Ïã§Ï†úÎ°úÎäî ÌïôÏäµ ÌïÑÏöî)\")\n\nlogger.write(f\"\\nÏ¥ù {len(ensemble_predictions)} Î™®Îç∏Ïùò ÏòàÏ∏° ÏÉùÏÑ± ÏôÑÎ£å\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ÏïôÏÉÅÎ∏î ÏòàÏ∏° ÏÉùÏÑ±\nlogger.write(\"\\n=== Ensemble Prediction ===\")\n\ndef weighted_ensemble(predictions_dict, weights_dict):\n    \"\"\"Í∞ÄÏ§ë ÌèâÍ∑† ÏïôÏÉÅÎ∏î\"\"\"\n    # Ïã§Ï†úÎ°úÎäî Îçî Ï†ïÍµêÌïú ÏïôÏÉÅÎ∏î Î∞©Î≤ï ÏÇ¨Ïö©\n    # Ïó¨Í∏∞ÏÑúÎäî Í∞ÑÎã®Ìûà Ï≤´ Î≤àÏß∏ ÏòàÏ∏° ÏÇ¨Ïö© (Îç∞Î™®)\n    \n    if predictions_dict:\n        first_model = list(predictions_dict.keys())[0]\n        return predictions_dict[first_model]\n    return []\n\n# ÏïôÏÉÅÎ∏î Í∞ÄÏ§ëÏπò\nensemble_weights = {\n    name: config['ensemble_models'][name]['weight']\n    for name in enabled_models\n}\n\n# ÏïôÏÉÅÎ∏î ÏòàÏ∏° ÏÉùÏÑ±\nfinal_predictions = weighted_ensemble(ensemble_predictions, ensemble_weights)\n\nlogger.write(f\"ÏïôÏÉÅÎ∏î ÏòàÏ∏° ÏÉùÏÑ± ÏôÑÎ£å: {len(final_predictions)} predictions\")\n\n# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° (Ïã§Ï†ú Ï†úÏ∂úÏö©)\nlogger.write(\"\\n=== Generating Test Predictions ===\")\n\n# Í∞ÑÎã®Ìïú ÏòàÏ∏° ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî ÌïôÏäµÎêú ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏÇ¨Ïö©)\ntest_predictions = []\n\nfor idx, row in test_df.iterrows():\n    dialogue = row['dialogue']\n    \n    # Ïó¨Í∏∞ÏÑúÎäî Í∞ÑÎã®Ìïú mock ÏòàÏ∏°\n    prediction = f\"ÎåÄÌôî ÏöîÏïΩ: {dialogue[:50]}... Ïóê ÎåÄÌïú ÏöîÏïΩÏûÖÎãàÎã§.\"\n    test_predictions.append(prediction)\n    \n    if idx % 100 == 0:\n        logger.write(f\"  Processed {idx}/{len(test_df)} test samples\")\n\nlogger.write(f\"ÌÖåÏä§Ìä∏ ÏòàÏ∏° ÏôÑÎ£å: {len(test_predictions)} predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Ï†úÏ∂ú ÌååÏùº ÏÉùÏÑ±\nlogger.write(\"\\n=== Creating Submission File ===\")\n\nsubmission_df = pd.DataFrame({\n    'fname': test_df['fname'],\n    'summary': test_predictions\n})\n\n# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû• - configÏùò Í≤ΩÎ°ú ÏÇ¨Ïö©\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'ensemble_{len(enabled_models)}models_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\n# index=TrueÎ°ú ÏÑ§Ï†ïÌïòÏó¨ Ïù∏Îç±Ïä§ Ìè¨Ìï®\nsubmission_df.to_csv(submission_path, index=True, encoding='utf-8')\n\nlogger.write(f\"Submission file saved: {submission_path}\")\nlogger.write(f\"Shape: {submission_df.shape}\")\n\n# ÏÉòÌîå Ï∂úÎ†•\nprint(\"\\nSubmission preview:\")\nprint(submission_df.head(3))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Ïã§Ìóò Í≤∞Í≥º ÏöîÏïΩ Î∞è ÏãúÍ∞ÅÌôî\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"MULTI-MODEL ENSEMBLE EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\n\n# Î™®Îç∏Î≥Ñ ÏÑ±Îä• (mock Îç∞Ïù¥ÌÑ∞)\nmodel_performance = {\n    'solar': 0.72,\n    'polyglot': 0.68,\n    'kullm': 0.65,\n    'kobart': 0.60,\n    'koalpaca': 0.58\n}\n\n# ÏïôÏÉÅÎ∏î ÏÑ±Îä• (ÏòàÏÉÅ)\nensemble_performance = 0.75\n\n# ÏÑ±Îä• ÎπÑÍµê ÏãúÍ∞ÅÌôî\nplt.figure(figsize=(12, 6))\n\nmodels = list(model_performance.keys()) + ['ensemble']\nscores = list(model_performance.values()) + [ensemble_performance]\ncolors_perf = ['#1f77b4'] * len(model_performance) + ['#ff7f0e']\n\nbars = plt.bar(models, scores, color=colors_perf)\nplt.title('Model Performance Comparison (Mock)', fontsize=14, fontweight='bold')\nplt.ylabel('ROUGE-L F1 Score', fontsize=12)\nplt.xlabel('Model', fontsize=12)\nplt.ylim(0, 1)\n\n# Ï†êÏàò ÌëúÏãú\nfor bar, score in zip(bars, scores):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height,\n             f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n\nplt.axhline(y=config['performance_targets']['rouge_l'], \n            color='red', linestyle='--', alpha=0.5, \n            label=f\"Target: {config['performance_targets']['rouge_l']}\")\nplt.legend()\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\n\nplt.savefig(viz_dir / f'performance_comparison_{timestamp}.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nlogger.write(f\"Performance visualization saved to {viz_dir}\")\n\n# ÏµúÏ¢Ö ÏöîÏïΩ\nlogger.write(\"\\nÌïµÏã¨ Í≤∞Í≥º:\")\nlogger.write(f\"  - ÌôúÏÑ±ÌôîÎêú Î™®Îç∏: {len(enabled_models)}Í∞ú\")\nlogger.write(f\"  - ÏïôÏÉÅÎ∏î Î∞©Î≤ï: {config['ensemble_strategy']['method']}\")\nlogger.write(f\"  - TTA ÌôúÏÑ±Ìôî: {config['tta']['enabled']}\")\nlogger.write(f\"  - Solar API ÎπÑÍµê: {config['solar_api_comparison']['enabled']}\")\nlogger.write(f\"  - ÏòàÏÉÅ ÏïôÏÉÅÎ∏î ÏÑ±Îä•: {ensemble_performance:.2f}\")\nlogger.write(f\"  - Î™©Ìëú ROUGE-L: {config['performance_targets']['rouge_l']}\")\n\nif comparison_results and 'model_wins' in comparison_results:\n    win_rate = comparison_results['model_wins'] / (comparison_results['model_wins'] + comparison_results['api_wins'])\n    logger.write(f\"  - Model vs API ÏäπÎ•†: {win_rate:.1%}\")\n\nlogger.write(\"\\n\" + \"=\"*50)\n\n# WandB Ï¢ÖÎ£å\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=config['wandb']['name'],\n        tags=config['wandb']['tags'],\n        config=config\n    )\n    \n    # Í≤∞Í≥º Î°úÍπÖ\n    wandb.log({\n        'ensemble_models': len(enabled_models),\n        'ensemble_performance': ensemble_performance,\n        'model_vs_api_wins': comparison_results.get('model_wins', 0) if comparison_results else 0\n    })\n    \n    wandb.finish()\n    logger.write(\"WandB run finished\")\n\nlogger.write(f\"\\n‚úÖ Multi-model ensemble experiment completed!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}