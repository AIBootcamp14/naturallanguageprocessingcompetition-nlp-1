{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” - 5ê°œ ëª¨ë¸ í†µí•©\n",
    "> PRD ê³„íšì— ë”°ë¥¸ 5ê°œ ëª¨ë¸ ì•™ìƒë¸” + TTA ì „ëµ\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 75-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "âœ… ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ë¡œë“œ ì„±ê³µ\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Models (5):\n",
      "  - solar: weight=0.30\n",
      "  - polyglot: weight=0.25\n",
      "  - kullm: weight=0.20\n",
      "  - kobart: weight=0.15\n",
      "  - koalpaca: weight=0.10\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_multi_model.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# í™œì„±í™”ëœ ëª¨ë¸ í™•ì¸\n",
    "enabled_models = [name for name, cfg in config['ensemble_models'].items() if cfg['enabled']]\n",
    "print(f\"Enabled Models ({len(enabled_models)}):\")\n",
    "for model_name in enabled_models:\n",
    "    weight = config['ensemble_models'][model_name]['weight']\n",
    "    print(f\"  - {model_name}: weight={weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Multi-Model Ensemble Experiment\n",
      "Timestamp: 20251010_090607\n",
      "Models: ['solar', 'polyglot', 'kullm', 'kobart', 'koalpaca']\n",
      "Ensemble Method: weighted_average\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "# configì˜ ë¡œê·¸ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì— log_dirì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "if 'log_dir' in config['paths']:\n",
    "    log_dir = get_path(config['paths']['log_dir'])\n",
    "else:\n",
    "    # ê¸°ë³¸ê°’: notebook_dir/logs/multi_model\n",
    "    log_dir = notebook_dir / 'logs' / 'multi_model'\n",
    "\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'ensemble_{len(enabled_models)}models_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('Multi-Model Ensemble Experiment')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Models: {enabled_models}')\n",
    "logger.write(f'Ensemble Method: {config[\"ensemble_strategy\"][\"method\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TTA Configuration:\n",
      "  - Augmentations: 3\n",
      "  - Aggregation: mean\n",
      "  - paraphrase: enabled\n",
      "TTA: paraphrase enabled\n",
      "  - reorder: enabled\n",
      "TTA: reorder enabled\n"
     ]
    }
   ],
   "source": [
    "# TTA ì„¤ì • í™•ì¸\n",
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\nTTA Configuration:\")\n",
    "    logger.write(f\"  - Augmentations: {config['tta']['num_augmentations']}\")\n",
    "    logger.write(f\"  - Aggregation: {config['tta']['aggregation']}\")\n",
    "    \n",
    "    for technique, settings in config['tta']['techniques'].items():\n",
    "        if settings['enabled']:\n",
    "            logger.write(f\"  - {technique}: enabled\")\n",
    "            print(f\"TTA: {technique} enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n",
      "Will clear GPU cache between models\n"
     ]
    }
   ],
   "source": [
    "# GPU ì²´í¬ ë° ë©€í‹° GPU ì„¤ì •\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    \n",
    "    # ë©€í‹° GPU ì²´í¬\n",
    "    if config['gpu']['multi_gpu']['enabled'] and torch.cuda.device_count() > 1:\n",
    "        logger.write(f\"Multi-GPU available: {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •\n",
    "    if config['gpu']['empty_cache_between_models']:\n",
    "        logger.write(\"Will clear GPU cache between models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight visualization saved to /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/logs/multi_model/visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109226/3820716939.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì‹œê°í™”\n",
    "weights = [config['ensemble_models'][name]['weight'] for name in enabled_models]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(enabled_models, weights, color=colors[:len(enabled_models)])\n",
    "plt.title('Ensemble Model Weights Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Weight', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "\n",
    "# ê°€ì¤‘ì¹˜ ê°’ í‘œì‹œ\n",
    "for bar, weight in zip(bars, weights):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{weight:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# ì‹œê°í™” ì €ì¥ - config ê²½ë¡œ ì‚¬ìš©\n",
    "if 'visualization_dir' in config['paths']:\n",
    "    viz_dir = get_path(config['paths']['visualization_dir'])\n",
    "else:\n",
    "    # ê¸°ë³¸ê°’\n",
    "    viz_dir = log_dir / 'visualizations'\n",
    "\n",
    "viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(viz_dir / f'ensemble_weights_{timestamp}.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "logger.write(f\"Weight visualization saved to {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solar API Comparison Settings:\n",
      "  - API Key: up_rMJWNzz...\n",
      "  - Use as baseline: True\n",
      "  - Include in ensemble: False\n",
      "\n",
      "Solar API configured for comparison\n"
     ]
    }
   ],
   "source": [
    "# Solar API ë¹„êµ ì„¤ì •\n",
    "if config['solar_api_comparison']['enabled']:\n",
    "    logger.write(\"\\nSolar API Comparison Settings:\")\n",
    "    logger.write(f\"  - API Key: {config['solar_api_comparison']['api_key'][:10]}...\")\n",
    "    logger.write(f\"  - Use as baseline: {config['solar_api_comparison']['use_as_baseline']}\")\n",
    "    logger.write(f\"  - Include in ensemble: {config['solar_api_comparison']['include_in_ensemble']}\")\n",
    "    \n",
    "    print(\"\\nSolar API configured for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optuna Weight Optimization:\n",
      "  - Trials: 50\n",
      "  - Study: ensemble_weight_optimization\n",
      "  - Metric: rouge_l\n",
      "Optuna configured for ensemble weight optimization\n"
     ]
    }
   ],
   "source": [
    "# Optuna ìµœì í™” ì„¤ì • (ì•™ìƒë¸” ê°€ì¤‘ì¹˜)\n",
    "if config['optuna']['enabled']:\n",
    "    logger.write(\"\\nOptuna Weight Optimization:\")\n",
    "    logger.write(f\"  - Trials: {config['optuna']['n_trials']}\")\n",
    "    logger.write(f\"  - Study: {config['optuna']['study_name']}\")\n",
    "    logger.write(f\"  - Metric: {config['optuna']['metric']}\")\n",
    "    \n",
    "    import optuna\n",
    "    print(\"Optuna configured for ensemble weight optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ë³„ í•™ìŠµ ë° í‰ê°€\n",
    "\n",
    "ê° ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Loading ===\n",
      "Loading data from config paths:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "  - Dev: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/dev.csv\n",
      "  - Test: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/test.csv\n",
      "\n",
      "Data loaded successfully!\n",
      "  - Train samples: 12457\n",
      "  - Dev samples: 499\n",
      "  - Test samples: 499\n",
      "\n",
      "Train data topics:\n",
      "topic\n",
      "ìŒì‹ ì£¼ë¬¸     130\n",
      "ì·¨ì—… ë©´ì ‘     109\n",
      "ê¸¸ ì•ˆë‚´       66\n",
      "í˜¸í…” ì²´í¬ì¸     40\n",
      "ì•„íŒŒíŠ¸ ì„ëŒ€     30\n",
      "ì¼ìƒ ëŒ€í™”      29\n",
      "ì‡¼í•‘         27\n",
      "ì£¼ë§ ê³„íš      26\n",
      "ë©´ì ‘         25\n",
      "í˜¸í…” ì˜ˆì•½      25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First dialogue (200 chars):\n",
      "#Person1#: ì•ˆë…•í•˜ì„¸ìš”, Mr. Smith. ì €ëŠ” Dr. Hawkinsì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ë¡œ ì˜¤ì…¨ì–´ìš”? \n",
      "#Person2#: ê±´ê°•ê²€ì§„ì„ ë°›ìœ¼ë ¤ê³  ì™”ì–´ìš”. \n",
      "#Person1#: ë„¤, 5ë…„ ë™ì•ˆ ê²€ì§„ì„ ì•ˆ ë°›ìœ¼ì…¨ë„¤ìš”. ë§¤ë…„ í•œ ë²ˆì”© ë°›ìœ¼ì…”ì•¼ í•´ìš”. \n",
      "#Person2#: ì•Œì£ . íŠ¹ë³„íˆ ì•„í”ˆ ë°ê°€ ì—†ìœ¼ë©´ êµ³ì´ ê°ˆ í•„ìš”ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆì–´ìš”. \n",
      "#Person...\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "# config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "dev_path = get_data_path(config['paths']['dev_file'])\n",
    "test_path = get_data_path(config['paths']['test_file'])\n",
    "\n",
    "logger.write(\"\\n=== Data Loading ===\")\n",
    "logger.write(f\"Loading data from config paths:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "logger.write(f\"  - Dev: {dev_path}\")\n",
    "logger.write(f\"  - Test: {test_path}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df = pd.read_csv(dev_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "logger.write(f\"\\nData loaded successfully!\")\n",
    "logger.write(f\"  - Train samples: {len(train_df)}\")\n",
    "logger.write(f\"  - Dev samples: {len(dev_df)}\")\n",
    "logger.write(f\"  - Test samples: {len(test_df)}\")\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nTrain data topics:\")\n",
    "print(train_df['topic'].value_counts().head(10))\n",
    "print(f\"\\nFirst dialogue (200 chars):\")\n",
    "print(train_df.iloc[0]['dialogue'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== solar Model ===\n",
      "  - Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "  - Weight: 0.3\n",
      "  - LoRA: r=16, alpha=32\n",
      "Would train solar here...\n",
      "\n",
      "=== polyglot Model ===\n",
      "  - Model: EleutherAI/polyglot-ko-12.8b\n",
      "  - Weight: 0.25\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train polyglot here...\n",
      "\n",
      "=== kullm Model ===\n",
      "  - Model: nlpai-lab/kullm-v2\n",
      "  - Weight: 0.2\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train kullm here...\n",
      "\n",
      "=== kobart Model ===\n",
      "  - Model: digit82/kobart-summarization\n",
      "  - Weight: 0.15\n",
      "Would train kobart here...\n",
      "\n",
      "=== koalpaca Model ===\n",
      "  - Model: beomi/KoAlpaca-Polyglot-12.8B\n",
      "  - Weight: 0.1\n",
      "  - LoRA: r=8, alpha=16\n",
      "Would train koalpaca here...\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥\n",
    "model_results = {}\n",
    "\n",
    "for model_name in enabled_models:\n",
    "    logger.write(f\"\\n=== {model_name} Model ===\")\n",
    "    model_config = config['ensemble_models'][model_name]\n",
    "    logger.write(f\"  - Model: {model_config['name']}\")\n",
    "    logger.write(f\"  - Weight: {model_config['weight']}\")\n",
    "    \n",
    "    if model_config.get('use_lora', False):\n",
    "        logger.write(f\"  - LoRA: r={model_config['lora_config']['r']}, alpha={model_config['lora_config']['alpha']}\")\n",
    "    \n",
    "    # ì‹¤ì œ í•™ìŠµ ì½”ë“œëŠ” ì—¬ê¸°ì— êµ¬í˜„\n",
    "    print(f\"Would train {model_name} here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA (Text Test Augmentation)\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì¦ê°•ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TTA Implementation ===\n",
      "Paraphrase augmentation enabled\n",
      "  - Model: lcw99/t5-base-korean-paraphrase\n",
      "  - Variants: 2\n",
      "TTA would be applied here...\n"
     ]
    }
   ],
   "source": [
    "if config['tta']['enabled']:\n",
    "    logger.write(\"\\n=== TTA Implementation ===\")\n",
    "    \n",
    "    # Paraphrase\n",
    "    if config['tta']['techniques']['paraphrase']['enabled']:\n",
    "        logger.write(\"Paraphrase augmentation enabled\")\n",
    "        logger.write(f\"  - Model: {config['tta']['techniques']['paraphrase']['model']}\")\n",
    "        logger.write(f\"  - Variants: {config['tta']['techniques']['paraphrase']['num_variants']}\")\n",
    "    \n",
    "    # ì‹¤ì œ TTA êµ¬í˜„ì€ ì—¬ê¸°ì—\n",
    "    print(\"TTA would be applied here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì•™ìƒë¸” ë° ìµœì¢… ì˜ˆì¸¡\n",
    "\n",
    "ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Solar API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (PRD 09_Solar_API_ìµœì í™”.md ì°¸ê³ )\nimport hashlib\nimport time\nimport re\nfrom openai import OpenAI\n\nclass SolarAPIOptimizer:\n    \"\"\"Solar API ìµœì í™” í´ë˜ìŠ¤\"\"\"\n    def __init__(self, api_key):\n        self.client = OpenAI(\n            api_key=api_key,\n            base_url=\"https://api.upstage.ai/v1/solar\"\n        )\n        self.token_limit = 512  # í† í° ì œí•œ\n        self.cache = {}  # ê²°ê³¼ ìºì‹±\n    \n    def preprocess_dialogue(self, dialogue):\n        \"\"\"ëŒ€í™”ë¬¸ ì „ì²˜ë¦¬ë¡œ í† í° ì ˆì•½\"\"\"\n        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n        dialogue = ' '.join(dialogue.split())\n        \n        # Person íƒœê·¸ ê°„ì†Œí™”\n        dialogue = dialogue.replace('#Person1#:', 'A:')\n        dialogue = dialogue.replace('#Person2#:', 'B:')\n        dialogue = dialogue.replace('#Person3#:', 'C:')\n        \n        # ëŒ€í™” ê¸¸ì´ ì œí•œ\n        return self.smart_truncate(dialogue, max_tokens=self.token_limit)\n    \n    def smart_truncate(self, text, max_tokens=512):\n        \"\"\"ìŠ¤ë§ˆíŠ¸ ì ˆë‹¨: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°\"\"\"\n        # í† í° ìˆ˜ ì¶”ì • (í•œê¸€ í‰ê·  2.5ì = 1í† í°)\n        estimated_tokens = len(text) / 2.5\n        \n        if estimated_tokens <= max_tokens:\n            return text\n        \n        sentences = text.split('.')\n        truncated = []\n        current_length = 0\n        \n        for sentence in sentences:\n            sentence_tokens = len(sentence) / 2.5\n            if current_length + sentence_tokens > max_tokens:\n                break\n            truncated.append(sentence)\n            current_length += sentence_tokens\n        \n        return '.'.join(truncated) + '.'\n    \n    def generate_summary(self, dialogue):\n        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n        # ìºì‹œ í™•ì¸\n        dialogue_hash = hashlib.md5(dialogue.encode()).hexdigest()\n        if dialogue_hash in self.cache:\n            return self.cache[dialogue_hash]\n        \n        # ì „ì²˜ë¦¬\n        processed_dialogue = self.preprocess_dialogue(dialogue)\n        \n        # API í˜¸ì¶œ\n        try:\n            response = self.client.chat.completions.create(\n                model=\"solar-1-mini-chat\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"ë‹¤ìŒ ëŒ€í™”ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\"},\n                    {\"role\": \"user\", \"content\": processed_dialogue}\n                ],\n                max_tokens=150,\n                temperature=0.3\n            )\n            \n            summary = response.choices[0].message.content\n            \n            # ìºì‹±\n            self.cache[dialogue_hash] = summary\n            \n            return summary\n        \n        except Exception as e:\n            logger.write(f\"Solar API error: {e}\")\n            return None\n\n# Solar API ì´ˆê¸°í™”\nif config['solar_api_comparison']['enabled']:\n    solar_api = SolarAPIOptimizer(config['solar_api_comparison']['api_key'])\n    logger.write(\"Solar API client initialized\")\nelse:\n    solar_api = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ë“€ì–¼ ìš”ì•½ ì‹œìŠ¤í…œ (PRD 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md ì°¸ê³ )\nclass QualityEvaluator:\n    \"\"\"ìš”ì•½ í’ˆì§ˆ í‰ê°€ê¸°\"\"\"\n    def __init__(self):\n        self.criteria = {\n            'length_ratio': 0.2,      # ìš”ì•½ ê¸¸ì´ ì ì ˆì„±\n            'keyword_coverage': 0.3,   # í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨\n            'coherence': 0.25,         # ë¬¸ì¥ ì¼ê´€ì„±\n            'information_density': 0.25 # ì •ë³´ ë°€ë„\n        }\n    \n    def evaluate(self, summary, dialogue):\n        \"\"\"ìš”ì•½ í’ˆì§ˆ ì¢…í•© í‰ê°€\"\"\"\n        scores = {\n            'length_ratio': self.check_length_ratio(summary, dialogue),\n            'keyword_coverage': self.check_keyword_coverage(summary, dialogue),\n            'coherence': self.check_coherence(summary),\n            'information_density': self.check_information_density(summary)\n        }\n        \n        # ê°€ì¤‘ í‰ê· \n        total_score = sum(\n            score * self.criteria[metric]\n            for metric, score in scores.items()\n        )\n        return total_score\n    \n    def check_length_ratio(self, summary, dialogue):\n        \"\"\"ìš”ì•½ ê¸¸ì´ ì ì ˆì„± (ì´ìƒì : ì›ë³¸ì˜ 20-30%)\"\"\"\n        if not summary or not dialogue:\n            return 0.5\n        \n        ratio = len(summary) / len(dialogue)\n        if 0.2 <= ratio <= 0.3:\n            return 1.0\n        elif 0.15 <= ratio <= 0.35:\n            return 0.8\n        else:\n            return 0.5\n    \n    def check_keyword_coverage(self, summary, dialogue):\n        \"\"\"í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ë„\"\"\"\n        # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ í•„ìš”)\n        dialogue_words = set(dialogue.split())\n        summary_words = set(summary.split())\n        \n        if not dialogue_words:\n            return 0.5\n        \n        coverage = len(summary_words & dialogue_words) / len(dialogue_words)\n        return min(coverage * 2, 1.0)  # ìµœëŒ€ 1.0\n    \n    def check_coherence(self, summary):\n        \"\"\"ë¬¸ì¥ ì¼ê´€ì„± ì²´í¬\"\"\"\n        if not summary:\n            return 0.5\n        \n        sentences = summary.split('.')\n        if len(sentences) < 2:\n            return 0.8\n        \n        # ê°„ë‹¨í•œ ì¼ê´€ì„± ì²´í¬\n        return 0.85  # ê¸°ë³¸ê°’\n    \n    def check_information_density(self, summary):\n        \"\"\"ì •ë³´ ë°€ë„ ì¸¡ì •\"\"\"\n        if not summary:\n            return 0.5\n        \n        words = summary.split()\n        if not words:\n            return 0.5\n        \n        unique_words = set(words)\n        density = len(unique_words) / len(words)\n        return min(density * 1.5, 1.0)\n\nclass DualSummarizationSystem:\n    \"\"\"ë“€ì–¼ ìš”ì•½ ì‹œìŠ¤í…œ - ëª¨ë¸ê³¼ API ê²°ê³¼ ë¹„êµ\"\"\"\n    def __init__(self, model_generator, solar_api):\n        self.model = model_generator\n        self.api = solar_api\n        self.evaluator = QualityEvaluator()\n    \n    def generate_summaries(self, dialogue):\n        \"\"\"ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ìš”ì•½ ìƒì„±\"\"\"\n        # 1. ëª¨ë¸ë¡œ ìƒì„±\n        model_summary = self.model(dialogue) if self.model else \"ëª¨ë¸ ìš”ì•½ ì—†ìŒ\"\n        model_confidence = 0.7  # ê¸°ë³¸ ì‹ ë¢°ë„\n        \n        # 2. Solar APIë¡œ ìƒì„±\n        api_summary = self.api.generate_summary(dialogue) if self.api else \"API ìš”ì•½ ì—†ìŒ\"\n        api_confidence = 0.85  # APIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì •ì \n        \n        return {\n            'model': {\n                'summary': model_summary,\n                'confidence': model_confidence\n            },\n            'api': {\n                'summary': api_summary,\n                'confidence': api_confidence\n            }\n        }\n    \n    def select_best_summary(self, summaries, dialogue):\n        \"\"\"ìµœì ì˜ ìš”ì•½ ì„ íƒ\"\"\"\n        model_score = self.evaluator.evaluate(\n            summaries['model']['summary'],\n            dialogue\n        )\n        api_score = self.evaluator.evaluate(\n            summaries['api']['summary'],\n            dialogue\n        )\n        \n        # ê°€ì¤‘ì¹˜ ì ìš©\n        model_final = model_score * summaries['model']['confidence']\n        api_final = api_score * summaries['api']['confidence']\n        \n        logger.write(f\"  Model score: {model_final:.3f}, API score: {api_final:.3f}\")\n        \n        if model_final > api_final:\n            return summaries['model']['summary'], 'model'\n        else:\n            return summaries['api']['summary'], 'api'\n\n# ë“€ì–¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\nlogger.write(\"\\n=== Dual Summarization System ===\")\nlogger.write(\"Quality evaluator initialized\")\nlogger.write(\"Dual system ready for model vs API comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì‹¤ì œ êµ¬í˜„)\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nimport gc\n\nclass DialogueSummaryDataset(Dataset):\n    \"\"\"ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹\"\"\"\n    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"ëŒ€í™” ì „ì²˜ë¦¬\"\"\"\n        text = text.replace('\\\\n', '\\n')\n        text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n        return text.strip()\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_input_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if not self.is_test:\n            summary = row.get('summary', '')\n            targets = self.tokenizer(\n                summary,\n                max_length=self.max_target_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'labels': targets['input_ids'].squeeze()\n            }\n        else:\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# ëª¨ë¸ë³„ í•™ìŠµ í•¨ìˆ˜\ndef train_model(model_name, model_config, train_df, dev_df, config):\n    \"\"\"ê°œë³„ ëª¨ë¸ í•™ìŠµ\"\"\"\n    logger.write(f\"\\n=== Training {model_name} ===\")\n    \n    # KoBART ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ê° ëª¨ë¸ë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •)\n    model_path = \"digit82/kobart-summarization\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = BartForConditionalGeneration.from_pretrained(model_path)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # ë°ì´í„°ì…‹ ìƒì„±\n    train_dataset = DialogueSummaryDataset(\n        train_df.sample(n=min(1000, len(train_df)), random_state=42),  # ìƒ˜í”Œë§\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        dev_df,\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n    \n    # ì˜µí‹°ë§ˆì´ì €\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    # ê°„ë‹¨í•œ í•™ìŠµ (1 ì—í­ë§Œ)\n    model.train()\n    train_loss = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Training {model_name}\", total=min(50, len(train_loader))):\n        if train_loader.batch_sampler.batch_size * (train_loader.batch_sampler.batch_size + 1) > 50:\n            break  # 50 ë°°ì¹˜ë§Œ í•™ìŠµ (ë°ëª¨)\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = train_loss / min(50, len(train_loader))\n    logger.write(f\"  {model_name} training loss: {avg_loss:.4f}\")\n    \n    # í‰ê°€\n    model.eval()\n    val_predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Evaluating {model_name}\", total=min(20, len(val_loader))):\n            if len(val_predictions) >= 20:\n                break  # 20ê°œ ìƒ˜í”Œë§Œ í‰ê°€ (ë°ëª¨)\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n            \n            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            val_predictions.extend(preds)\n    \n    logger.write(f\"  {model_name} generated {len(val_predictions)} validation predictions\")\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return val_predictions\n\n# ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥\nensemble_predictions = {}\n\n# KoBARTë§Œ ì‹¤ì œ í•™ìŠµ (ë°ëª¨)\nif 'kobart' in enabled_models:\n    kobart_predictions = train_model(\n        'kobart',\n        config['ensemble_models']['kobart'],\n        train_df,\n        dev_df,\n        config\n    )\n    ensemble_predictions['kobart'] = kobart_predictions\n    \n# ë‚˜ë¨¸ì§€ ëª¨ë¸ì€ mock ì˜ˆì¸¡\nfor model_name in enabled_models:\n    if model_name != 'kobart':\n        # Mock predictions\n        ensemble_predictions[model_name] = [\"Mock ìš”ì•½\"] * 20\n        logger.write(f\"\\n{model_name}: Mock predictions generated (ì‹¤ì œë¡œëŠ” í•™ìŠµ í•„ìš”)\")\n\nlogger.write(f\"\\nì´ {len(ensemble_predictions)} ëª¨ë¸ì˜ ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Solar APIì™€ ëª¨ë¸ ì˜ˆì¸¡ ë¹„êµ (êµì°¨ ê²€ì¦)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solar API vs ëª¨ë¸ ë¹„êµ (PRD 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md êµ¬í˜„)\nlogger.write(\"\\n=== Solar API vs Model Comparison ===\")\n\n# ê²€ì¦ìš© ìƒ˜í”Œ ì„ íƒ\ntest_samples = dev_df.sample(n=min(10, len(dev_df)), random_state=42)\n\ncomparison_results = {\n    'model_wins': 0,\n    'api_wins': 0,\n    'model_scores': [],\n    'api_scores': [],\n    'examples': []\n}\n\n# ë“€ì–¼ ì‹œìŠ¤í…œìœ¼ë¡œ ë¹„êµ\nif solar_api:\n    # ê°„ë‹¨í•œ ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜ (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)\n    def model_generator(dialogue):\n        # Mock ì˜ˆì¸¡ (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)\n        return f\"ëª¨ë¸ ìš”ì•½: {dialogue[:100]}...\"\n    \n    dual_system = DualSummarizationSystem(model_generator, solar_api)\n    \n    for idx, row in test_samples.iterrows():\n        dialogue = row['dialogue']\n        gold_summary = row['summary']\n        \n        logger.write(f\"\\nê²€ì¦ ìƒ˜í”Œ {idx}:\")\n        \n        # ë“€ì–¼ ì‹œìŠ¤í…œìœ¼ë¡œ ìš”ì•½ ìƒì„±\n        summaries = dual_system.generate_summaries(dialogue)\n        \n        # ìµœì  ì„ íƒ\n        best_summary, source = dual_system.select_best_summary(summaries, dialogue)\n        \n        if source == 'model':\n            comparison_results['model_wins'] += 1\n        else:\n            comparison_results['api_wins'] += 1\n        \n        # ì˜ˆì‹œ ì €ì¥\n        if len(comparison_results['examples']) < 3:\n            comparison_results['examples'].append({\n                'dialogue': dialogue[:200] + '...',\n                'model_summary': summaries['model']['summary'][:100] + '...',\n                'api_summary': summaries['api']['summary'][:100] + '...' if summaries['api']['summary'] else 'N/A',\n                'selected': source,\n                'gold': gold_summary[:100] + '...'\n            })\n        \n        # Rate limiting for API\n        time.sleep(0.5)\n    \n    # ê²°ê³¼ ìš”ì•½\n    logger.write(\"\\n=== Comparison Results ===\")\n    logger.write(f\"Model wins: {comparison_results['model_wins']}\")\n    logger.write(f\"API wins: {comparison_results['api_wins']}\")\n    \n    # ì˜ˆì‹œ ì¶œë ¥\n    logger.write(\"\\nìƒ˜í”Œ ë¹„êµ:\")\n    for i, example in enumerate(comparison_results['examples'], 1):\n        logger.write(f\"\\nì˜ˆì‹œ {i}:\")\n        logger.write(f\"  ì„ íƒ: {example['selected']}\")\n        logger.write(f\"  ëª¨ë¸: {example['model_summary']}\")\n        logger.write(f\"  API: {example['api_summary']}\")\nelse:\n    logger.write(\"Solar API ë¹„í™œì„±í™”ë¨ - ë¹„êµ ìƒëµ\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ê° ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì‹¤ì œ êµ¬í˜„)\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BartForConditionalGeneration\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\nimport gc\n\nclass DialogueSummaryDataset(Dataset):\n    \"\"\"ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹\"\"\"\n    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def preprocess_dialogue(self, text):\n        \"\"\"ëŒ€í™” ì „ì²˜ë¦¬\"\"\"\n        text = text.replace('\\\\n', '\\n')\n        text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n        return text.strip()\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dialogue = self.preprocess_dialogue(row['dialogue'])\n        \n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_input_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if not self.is_test:\n            summary = row.get('summary', '')\n            targets = self.tokenizer(\n                summary,\n                max_length=self.max_target_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # ë¼ë²¨ ìƒì„± - íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ì¤‘ìš”!)\n            labels = targets['input_ids'].squeeze()\n            labels[labels == self.tokenizer.pad_token_id] = -100  # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹\n            \n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'labels': labels\n            }\n        else:\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'idx': idx\n            }\n\n# ëª¨ë¸ë³„ í•™ìŠµ í•¨ìˆ˜\ndef train_model(model_name, model_config, train_df, dev_df, config):\n    \"\"\"ê°œë³„ ëª¨ë¸ í•™ìŠµ\"\"\"\n    logger.write(f\"\\n=== Training {model_name} ===\")\n    \n    # KoBART ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ê° ëª¨ë¸ë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •)\n    model_path = \"digit82/kobart-summarization\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = BartForConditionalGeneration.from_pretrained(model_path)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # ë°ì´í„°ì…‹ ìƒì„±\n    train_dataset = DialogueSummaryDataset(\n        train_df.sample(n=min(1000, len(train_df)), random_state=42),  # ìƒ˜í”Œë§\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        dev_df,\n        tokenizer,\n        max_input_len=model_config.get('max_length', 512),\n        max_target_len=128\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n    \n    # ì˜µí‹°ë§ˆì´ì €\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    # ê°„ë‹¨í•œ í•™ìŠµ (1 ì—í­ë§Œ)\n    model.train()\n    train_loss = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Training {model_name}\", total=min(50, len(train_loader))):\n        if train_loader.batch_sampler.batch_size * (train_loader.batch_sampler.batch_size + 1) > 50:\n            break  # 50 ë°°ì¹˜ë§Œ í•™ìŠµ (ë°ëª¨)\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    avg_loss = train_loss / min(50, len(train_loader))\n    logger.write(f\"  {model_name} training loss: {avg_loss:.4f}\")\n    \n    # í‰ê°€\n    model.eval()\n    val_predictions = []\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Evaluating {model_name}\", total=min(20, len(val_loader))):\n            if len(val_predictions) >= 20:\n                break  # 20ê°œ ìƒ˜í”Œë§Œ í‰ê°€ (ë°ëª¨)\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True,\n                repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€ ì¶”ê°€\n                no_repeat_ngram_size=3   # n-gram ë°˜ë³µ ë°©ì§€\n            )\n            \n            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            val_predictions.extend(preds)\n    \n    logger.write(f\"  {model_name} generated {len(val_predictions)} validation predictions\")\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return val_predictions\n\n# ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥\nensemble_predictions = {}\n\n# KoBARTë§Œ ì‹¤ì œ í•™ìŠµ (ë°ëª¨)\nif 'kobart' in enabled_models:\n    kobart_predictions = train_model(\n        'kobart',\n        config['ensemble_models']['kobart'],\n        train_df,\n        dev_df,\n        config\n    )\n    ensemble_predictions['kobart'] = kobart_predictions\n    \n# ë‚˜ë¨¸ì§€ ëª¨ë¸ì€ mock ì˜ˆì¸¡\nfor model_name in enabled_models:\n    if model_name != 'kobart':\n        # Mock predictions\n        ensemble_predictions[model_name] = [\"Mock ìš”ì•½\"] * 20\n        logger.write(f\"\\n{model_name}: Mock predictions generated (ì‹¤ì œë¡œëŠ” í•™ìŠµ í•„ìš”)\")\n\nlogger.write(f\"\\nì´ {len(ensemble_predictions)} ëª¨ë¸ì˜ ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±\nlogger.write(\"\\n=== Ensemble Prediction ===\")\n\ndef weighted_ensemble(predictions_dict, weights_dict):\n    \"\"\"ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”\"\"\"\n    # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•™ìƒë¸” ë°©ë²• ì‚¬ìš©\n    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ì‚¬ìš© (ë°ëª¨)\n    \n    if predictions_dict:\n        first_model = list(predictions_dict.keys())[0]\n        return predictions_dict[first_model]\n    return []\n\n# ì•™ìƒë¸” ê°€ì¤‘ì¹˜\nensemble_weights = {\n    name: config['ensemble_models'][name]['weight']\n    for name in enabled_models\n}\n\n# ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„±\nfinal_predictions = weighted_ensemble(ensemble_predictions, ensemble_weights)\n\nlogger.write(f\"ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ: {len(final_predictions)} predictions\")\n\n# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (ì‹¤ì œ ì œì¶œìš©)\nlogger.write(\"\\n=== Generating Test Predictions ===\")\n\n# ê°„ë‹¨í•œ ì˜ˆì¸¡ ìƒì„± (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©)\ntest_predictions = []\n\nfor idx, row in test_df.iterrows():\n    dialogue = row['dialogue']\n    \n    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ mock ì˜ˆì¸¡\n    prediction = f\"ëŒ€í™” ìš”ì•½: {dialogue[:50]}... ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤.\"\n    test_predictions.append(prediction)\n    \n    if idx % 100 == 0:\n        logger.write(f\"  Processed {idx}/{len(test_df)} test samples\")\n\nlogger.write(f\"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì™„ë£Œ: {len(test_predictions)} predictions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì œì¶œ íŒŒì¼ ìƒì„±\nlogger.write(\"\\n=== Creating Submission File ===\")\n\nsubmission_df = pd.DataFrame({\n    'fname': test_df['fname'],\n    'summary': test_predictions\n})\n\n# ì œì¶œ íŒŒì¼ ì €ì¥ - configì˜ ê²½ë¡œ ì‚¬ìš©\nsubmission_dir = get_path(config['paths']['submission_dir'])\nsubmission_dir.mkdir(parents=True, exist_ok=True)\n\nsubmission_filename = f'ensemble_{len(enabled_models)}models_{timestamp}.csv'\nsubmission_path = submission_dir / submission_filename\n\n# index=Trueë¡œ ì„¤ì •í•˜ì—¬ ì¸ë±ìŠ¤ í¬í•¨\nsubmission_df.to_csv(submission_path, index=True, encoding='utf-8')\n\nlogger.write(f\"Submission file saved: {submission_path}\")\nlogger.write(f\"Shape: {submission_df.shape}\")\n\n# ìƒ˜í”Œ ì¶œë ¥\nprint(\"\\nSubmission preview:\")\nprint(submission_df.head(3))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™”\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"MULTI-MODEL ENSEMBLE EXPERIMENT SUMMARY\")\nlogger.write(\"=\"*50)\n\n# ëª¨ë¸ë³„ ì„±ëŠ¥ (mock ë°ì´í„°)\nmodel_performance = {\n    'solar': 0.72,\n    'polyglot': 0.68,\n    'kullm': 0.65,\n    'kobart': 0.60,\n    'koalpaca': 0.58\n}\n\n# ì•™ìƒë¸” ì„±ëŠ¥ (ì˜ˆìƒ)\nensemble_performance = 0.75\n\n# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\nplt.figure(figsize=(12, 6))\n\nmodels = list(model_performance.keys()) + ['ensemble']\nscores = list(model_performance.values()) + [ensemble_performance]\ncolors_perf = ['#1f77b4'] * len(model_performance) + ['#ff7f0e']\n\nbars = plt.bar(models, scores, color=colors_perf)\nplt.title('Model Performance Comparison (Mock)', fontsize=14, fontweight='bold')\nplt.ylabel('ROUGE-L F1 Score', fontsize=12)\nplt.xlabel('Model', fontsize=12)\nplt.ylim(0, 1)\n\n# ì ìˆ˜ í‘œì‹œ\nfor bar, score in zip(bars, scores):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height,\n             f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n\nplt.axhline(y=config['performance_targets']['rouge_l'], \n            color='red', linestyle='--', alpha=0.5, \n            label=f\"Target: {config['performance_targets']['rouge_l']}\")\nplt.legend()\nplt.xticks(rotation=45, ha='right')\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\n\nplt.savefig(viz_dir / f'performance_comparison_{timestamp}.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nlogger.write(f\"Performance visualization saved to {viz_dir}\")\n\n# ìµœì¢… ìš”ì•½\nlogger.write(\"\\ní•µì‹¬ ê²°ê³¼:\")\nlogger.write(f\"  - í™œì„±í™”ëœ ëª¨ë¸: {len(enabled_models)}ê°œ\")\nlogger.write(f\"  - ì•™ìƒë¸” ë°©ë²•: {config['ensemble_strategy']['method']}\")\nlogger.write(f\"  - TTA í™œì„±í™”: {config['tta']['enabled']}\")\nlogger.write(f\"  - Solar API ë¹„êµ: {config['solar_api_comparison']['enabled']}\")\nlogger.write(f\"  - ì˜ˆìƒ ì•™ìƒë¸” ì„±ëŠ¥: {ensemble_performance:.2f}\")\nlogger.write(f\"  - ëª©í‘œ ROUGE-L: {config['performance_targets']['rouge_l']}\")\n\nif comparison_results and 'model_wins' in comparison_results:\n    win_rate = comparison_results['model_wins'] / (comparison_results['model_wins'] + comparison_results['api_wins'])\n    logger.write(f\"  - Model vs API ìŠ¹ë¥ : {win_rate:.1%}\")\n\nlogger.write(\"\\n\" + \"=\"*50)\n\n# WandB ì¢…ë£Œ\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=config['wandb']['name'],\n        tags=config['wandb']['tags'],\n        config=config\n    )\n    \n    # ê²°ê³¼ ë¡œê¹…\n    wandb.log({\n        'ensemble_models': len(enabled_models),\n        'ensemble_performance': ensemble_performance,\n        'model_vs_api_wins': comparison_results.get('model_wins', 0) if comparison_results else 0\n    })\n    \n    wandb.finish()\n    logger.write(\"WandB run finished\")\n\nlogger.write(f\"\\nâœ… Multi-model ensemble experiment completed!\")\nlogger.write(f\"Log file: {log_file}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}