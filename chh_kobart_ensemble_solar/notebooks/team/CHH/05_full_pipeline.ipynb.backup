{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Full Pipeline - ëª¨ë“  ê¸°ë²• í†µí•©\n",
    "> PRD ê³„íšì— ë”°ë¥¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 85+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:52:59.499057Z",
     "iopub.status.busy": "2025-10-10T02:52:59.498955Z",
     "iopub.status.idle": "2025-10-10T02:53:01.421961Z",
     "shell.execute_reply": "2025-10-10T02:53:01.421561Z"
    }
   },
   "outputs": [],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸ - 04_multi_model_ensemble.ipynbì—ì„œ ì°¸ê³ \n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "from src.utils.visualizations.training_viz import TrainingVisualizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.423110Z",
     "iopub.status.busy": "2025-10-10T02:53:01.422930Z",
     "iopub.status.idle": "2025-10-10T02:53:01.434897Z",
     "shell.execute_reply": "2025-10-10T02:53:01.434500Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_full_pipeline.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FULL PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Pipeline Stages: {len(config['pipeline']['stages'])}\")\n",
    "for stage in config['pipeline']['stages']:\n",
    "    print(f\"  âœ“ {stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.435845Z",
     "iopub.status.busy": "2025-10-10T02:53:01.435744Z",
     "iopub.status.idle": "2025-10-10T02:53:01.439048Z",
     "shell.execute_reply": "2025-10-10T02:53:01.438646Z"
    }
   },
   "outputs": [],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "log_dir = Path(config['paths']['log_dir'])\n",
    "print(f\"Log Directory: {log_dir}\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'full_pipeline_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('FULL PIPELINE EXECUTION STARTED')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Config: {config_path}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.439995Z",
     "iopub.status.busy": "2025-10-10T02:53:01.439894Z",
     "iopub.status.idle": "2025-10-10T02:53:01.509932Z",
     "shell.execute_reply": "2025-10-10T02:53:01.509443Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU ìµœì í™” ì²´í¬\n",
    "# í•„ìš”í•œ ëª¨ë“ˆ import\n",
    "if 'check_gpu_tier' not in globals():\n",
    "    try:\n",
    "        from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import check_gpu_tier\")\n",
    "        def check_gpu_tier():\n",
    "            return \"UNKNOWN\"\n",
    "\n",
    "# configê°€ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "if 'config' not in globals():\n",
    "    print(\"Warning: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    if config['gpu']['auto_optimization']['enabled']:\n",
    "        gpu_tier = check_gpu_tier()\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "            logger.write(f\"Auto-optimization enabled\")\n",
    "            \n",
    "            if config['gpu']['auto_optimization']['find_optimal_batch_size']:\n",
    "                logger.write(\"Finding optimal batch size...\")\n",
    "                # ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì½”ë“œ\n",
    "        else:\n",
    "            print(f\"GPU Tier: {gpu_tier}\")\n",
    "            print(f\"Auto-optimization enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.511028Z",
     "iopub.status.busy": "2025-10-10T02:53:01.510918Z",
     "iopub.status.idle": "2025-10-10T02:53:01.513531Z",
     "shell.execute_reply": "2025-10-10T02:53:01.513132Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ëª©í‘œ í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE TARGETS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ROUGE-1: {config['performance_targets']['rouge_1']}\")\n",
    "print(f\"ROUGE-2: {config['performance_targets']['rouge_2']}\")\n",
    "print(f\"ROUGE-L: {config['performance_targets']['rouge_l']}\")\n",
    "print(f\"Overall Target: {config['performance_targets']['overall']}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.514537Z",
     "iopub.status.busy": "2025-10-10T02:53:01.514439Z",
     "iopub.status.idle": "2025-10-10T02:53:01.517293Z",
     "shell.execute_reply": "2025-10-10T02:53:01.516860Z"
    }
   },
   "outputs": [],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìƒíƒœ ì¶”ì \n",
    "# configê°€ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "if 'config' not in globals():\n",
    "    print(\"Error: config not loaded. Please run cell 2 first.\")\n",
    "else:\n",
    "    pipeline_status = {}\n",
    "    for stage in config['pipeline']['stages']:\n",
    "        pipeline_status[stage] = 'pending'\n",
    "\n",
    "    def update_status(stage, status):\n",
    "        pipeline_status[stage] = status\n",
    "        if 'logger' in globals():\n",
    "            logger.write(f\"[{stage}] Status: {status}\")\n",
    "        else:\n",
    "            print(f\"[{stage}] Status: {status}\")\n",
    "        \n",
    "    # ìƒíƒœ í‘œì‹œ\n",
    "    for stage, status in pipeline_status.items():\n",
    "        print(f\"{stage:30s}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.518296Z",
     "iopub.status.busy": "2025-10-10T02:53:01.518198Z",
     "iopub.status.idle": "2025-10-10T02:53:01.673499Z",
     "shell.execute_reply": "2025-10-10T02:53:01.673101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: ë°ì´í„° ë¡œë“œ - dev_dfì™€ test_df í¬í•¨!\n",
    "# Stage 1: ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ë¡œë“œ\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if 'data_quality_check' in config['pipeline']['stages']:\n",
    "    update_status('data_quality_check', 'running')\n",
    "    logger.write(\"\\n=== Data Quality Check ===\")\n",
    "    \n",
    "    # config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "    def get_data_path(path_str):\n",
    "        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # ë°ì´í„° ê²½ë¡œ\n",
    "    train_path = get_data_path(config['paths']['train_file'])\n",
    "    dev_path = get_data_path(config['paths']['dev_file']) \n",
    "    test_path = get_data_path(config['paths']['test_file'])\n",
    "    \n",
    "    logger.write(f\"Loading data from config paths:\")\n",
    "    logger.write(f\"  - Train: {train_path}\")\n",
    "    logger.write(f\"  - Dev: {dev_path}\")\n",
    "    logger.write(f\"  - Test: {test_path}\")\n",
    "    \n",
    "    # ëª¨ë“  ë°ì´í„° ë¡œë“œ - train_df, dev_df, test_df ëª¨ë‘!\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    dev_df = pd.read_csv(dev_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    logger.write(f\"âœ… Loaded {len(train_df)} training samples\")\n",
    "    logger.write(f\"âœ… Loaded {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"âœ… Loaded {len(test_df)} test samples\")\n",
    "    \n",
    "    # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦\n",
    "    if config['data_quality']['enabled']:\n",
    "        # êµ¬ì¡°ì  ê²€ì¦\n",
    "        if config['data_quality']['checks']['structural']['check_nulls']:\n",
    "            train_nulls = train_df.isnull().sum().sum()\n",
    "            dev_nulls = dev_df.isnull().sum().sum()\n",
    "            test_nulls = test_df.isnull().sum().sum()\n",
    "            logger.write(f\"Null values - Train: {train_nulls}, Dev: {dev_nulls}, Test: {test_nulls}\")\n",
    "        \n",
    "        if config['data_quality']['checks']['structural']['check_duplicates']:\n",
    "            train_dups = train_df.duplicated().sum()\n",
    "            logger.write(f\"Duplicate rows in training data: {train_dups}\")\n",
    "    \n",
    "    logger.write(\"âœ… Data loading completed successfully!\")\n",
    "    update_status('data_quality_check', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.674757Z",
     "iopub.status.busy": "2025-10-10T02:53:01.674628Z",
     "iopub.status.idle": "2025-10-10T02:53:01.724276Z",
     "shell.execute_reply": "2025-10-10T02:53:01.723835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 1.5: ìƒì„¸ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (PRD 16_ë°ì´í„°_í’ˆì§ˆ_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "# ì£¼ì˜: ì´ ì…€ì€ ì…€ 7 (ë°ì´í„° ë¡œë“œ) ì‹¤í–‰ í›„ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤!\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "\n",
    "class FullPipelineDataValidator:\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ìš© ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.validation_report = {}\n",
    "        \n",
    "    def comprehensive_validation(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"í¬ê´„ì  ë°ì´í„° ê²€ì¦\"\"\"\n",
    "        self.logger.write(\"\\n=== Comprehensive Data Quality Validation ===\")\n",
    "        \n",
    "        report = {}\n",
    "        \n",
    "        # 1. êµ¬ì¡°ì  ê²€ì¦\n",
    "        report['structural'] = self._validate_structure(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 2. í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\n",
    "        report['text_quality'] = self._validate_text_quality(train_df)\n",
    "        \n",
    "        # 3. ë¼ë²¨ ë¶„í¬ ê²€ì¦\n",
    "        report['label_distribution'] = self._validate_label_distribution(train_df)\n",
    "        \n",
    "        # 4. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦\n",
    "        report['consistency'] = self._validate_consistency(train_df, dev_df, test_df)\n",
    "        \n",
    "        # 5. ì´ìƒì¹˜ ê²€ì¶œ\n",
    "        report['outliers'] = self._detect_outliers(train_df)\n",
    "        \n",
    "        self.validation_report = report\n",
    "        return report\n",
    "    \n",
    "    def _validate_structure(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"êµ¬ì¡°ì  ê²€ì¦\"\"\"\n",
    "        return {\n",
    "            'train_shape': train_df.shape,\n",
    "            'dev_shape': dev_df.shape,\n",
    "            'test_shape': test_df.shape,\n",
    "            'train_nulls': train_df.isnull().sum().sum(),\n",
    "            'dev_nulls': dev_df.isnull().sum().sum(),\n",
    "            'test_nulls': test_df.isnull().sum().sum(),\n",
    "            'train_duplicates': train_df.duplicated().sum(),\n",
    "            'column_match': set(train_df.columns) == set(dev_df.columns)\n",
    "        }\n",
    "    \n",
    "    def _validate_text_quality(self, df) -> Dict:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        summary_lengths = df['summary'].str.len() if 'summary' in df.columns else pd.Series([0])\n",
    "        \n",
    "        # íŠ¹ìˆ˜ ë¬¸ì íŒ¨í„´\n",
    "        special_chars = df['dialogue'].str.contains('[ï¿½\\\\?\\\\x00-\\\\x1f]').sum()\n",
    "        \n",
    "        # ì¸ì½”ë”© ë¬¸ì œ\n",
    "        encoding_issues = df['dialogue'].apply(\n",
    "            lambda x: bool(re.search(r'[\\ufffd]', str(x)))\n",
    "        ).sum()\n",
    "        \n",
    "        return {\n",
    "            'avg_dialogue_length': dialogue_lengths.mean(),\n",
    "            'max_dialogue_length': dialogue_lengths.max(),\n",
    "            'min_dialogue_length': dialogue_lengths.min(),\n",
    "            'avg_summary_length': summary_lengths.mean() if 'summary' in df.columns else 0,\n",
    "            'compression_ratio': (summary_lengths / dialogue_lengths).mean() if 'summary' in df.columns else 0,\n",
    "            'special_chars_count': int(special_chars),\n",
    "            'encoding_issues': int(encoding_issues),\n",
    "            'empty_dialogues': (dialogue_lengths == 0).sum()\n",
    "        }\n",
    "    \n",
    "    def _validate_label_distribution(self, df) -> Dict:\n",
    "        \"\"\"ë¼ë²¨ ë¶„í¬ ê²€ì¦\"\"\"\n",
    "        if 'topic' not in df.columns:\n",
    "            return {}\n",
    "        \n",
    "        topic_counts = df['topic'].value_counts()\n",
    "        \n",
    "        return {\n",
    "            'unique_topics': len(topic_counts),\n",
    "            'most_common_topic': topic_counts.index[0] if len(topic_counts) > 0 else None,\n",
    "            'most_common_count': int(topic_counts.iloc[0]) if len(topic_counts) > 0 else 0,\n",
    "            'least_common_topic': topic_counts.index[-1] if len(topic_counts) > 0 else None,\n",
    "            'least_common_count': int(topic_counts.iloc[-1]) if len(topic_counts) > 0 else 0,\n",
    "            'imbalance_ratio': float(topic_counts.iloc[0] / topic_counts.iloc[-1]) if len(topic_counts) > 1 and topic_counts.iloc[-1] > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _validate_consistency(self, train_df, dev_df, test_df) -> Dict:\n",
    "        \"\"\"ë°ì´í„° ì¼ê´€ì„± ê²€ì¦\"\"\"\n",
    "        # Person íƒœê·¸ ì¼ê´€ì„±\n",
    "        train_person_tags = train_df['dialogue'].str.contains('#Person').mean()\n",
    "        dev_person_tags = dev_df['dialogue'].str.contains('#Person').mean()\n",
    "        test_person_tags = test_df['dialogue'].str.contains('#Person').mean()\n",
    "        \n",
    "        return {\n",
    "            'train_person_tag_ratio': float(train_person_tags),\n",
    "            'dev_person_tag_ratio': float(dev_person_tags),\n",
    "            'test_person_tag_ratio': float(test_person_tags),\n",
    "            'person_tag_consistent': abs(train_person_tags - dev_person_tags) < 0.1\n",
    "        }\n",
    "    \n",
    "    def _detect_outliers(self, df) -> Dict:\n",
    "        \"\"\"ì´ìƒì¹˜ ê²€ì¶œ\"\"\"\n",
    "        dialogue_lengths = df['dialogue'].str.len()\n",
    "        \n",
    "        # IQR ê¸°ë°˜ ì´ìƒì¹˜\n",
    "        Q1 = dialogue_lengths.quantile(0.25)\n",
    "        Q3 = dialogue_lengths.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = (dialogue_lengths < lower_bound) | (dialogue_lengths > upper_bound)\n",
    "        \n",
    "        return {\n",
    "            'outlier_count': int(outliers.sum()),\n",
    "            'outlier_ratio': float(outliers.mean()),\n",
    "            'lower_bound': float(lower_bound),\n",
    "            'upper_bound': float(upper_bound)\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if not self.validation_report:\n",
    "            return recommendations\n",
    "        \n",
    "        # êµ¬ì¡°ì  ë¬¸ì œ\n",
    "        if self.validation_report.get('structural', {}).get('train_nulls', 0) > 0:\n",
    "            recommendations.append(\"Remove or impute null values in training data\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¬¸ì œ\n",
    "        text_quality = self.validation_report.get('text_quality', {})\n",
    "        if text_quality.get('encoding_issues', 0) > 0:\n",
    "            recommendations.append(f\"Fix {text_quality['encoding_issues']} encoding issues\")\n",
    "        \n",
    "        if text_quality.get('special_chars_count', 0) > 0:\n",
    "            recommendations.append(\"Clean special characters from text\")\n",
    "        \n",
    "        # ë¼ë²¨ ë¶ˆê· í˜•\n",
    "        label_dist = self.validation_report.get('label_distribution', {})\n",
    "        if label_dist.get('imbalance_ratio', 0) > 10:\n",
    "            recommendations.append(\"Consider data augmentation for underrepresented topics\")\n",
    "        \n",
    "        # ì´ìƒì¹˜\n",
    "        outliers = self.validation_report.get('outliers', {})\n",
    "        if outliers.get('outlier_ratio', 0) > 0.05:\n",
    "            recommendations.append(f\"Review {outliers['outlier_count']} outlier samples\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# ì´ì œ ì‹¤ì œ ê²€ì¦ ì‹¤í–‰ - ì…€ 7ì—ì„œ ì´ë¯¸ ë¡œë“œí•œ ë°ì´í„°ë¥¼ ì‚¬ìš©\n",
    "# ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìŠ¤í‚µ\n",
    "try:\n",
    "    # train_df, dev_df, test_dfê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if 'train_df' not in locals() or 'dev_df' not in locals() or 'test_df' not in locals():\n",
    "        logger.write(\"âš ï¸ Data not loaded yet. Skipping detailed validation.\")\n",
    "        logger.write(\"   Please run cell 7 first to load the data.\")\n",
    "    else:\n",
    "        # ìƒì„¸ ê²€ì¦ ì‹¤í–‰\n",
    "        data_validator = FullPipelineDataValidator(logger)\n",
    "        validation_report = data_validator.comprehensive_validation(train_df, dev_df, test_df)\n",
    "        \n",
    "        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n",
    "        logger.write(\"\\nğŸ“Š Data Quality Report:\")\n",
    "        \n",
    "        # êµ¬ì¡°ì  ê²€ì¦ ê²°ê³¼\n",
    "        structural = validation_report.get('structural', {})\n",
    "        logger.write(f\"\\nStructural Validation:\")\n",
    "        logger.write(f\"  - Train shape: {structural.get('train_shape')}\")\n",
    "        logger.write(f\"  - Dev shape: {structural.get('dev_shape')}\")\n",
    "        logger.write(f\"  - Test shape: {structural.get('test_shape')}\")\n",
    "        logger.write(f\"  - Column match: {structural.get('column_match')}\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²°ê³¼\n",
    "        text_quality = validation_report.get('text_quality', {})\n",
    "        logger.write(f\"\\nText Quality:\")\n",
    "        logger.write(f\"  - Avg dialogue length: {text_quality.get('avg_dialogue_length', 0):.1f}\")\n",
    "        logger.write(f\"  - Compression ratio: {text_quality.get('compression_ratio', 0):.2%}\")\n",
    "        logger.write(f\"  - Encoding issues: {text_quality.get('encoding_issues', 0)}\")\n",
    "        logger.write(f\"  - Special chars: {text_quality.get('special_chars_count', 0)}\")\n",
    "        \n",
    "        # ë¼ë²¨ ë¶„í¬ ê²°ê³¼\n",
    "        label_dist = validation_report.get('label_distribution', {})\n",
    "        if label_dist:\n",
    "            logger.write(f\"\\nLabel Distribution:\")\n",
    "            logger.write(f\"  - Unique topics: {label_dist.get('unique_topics')}\")\n",
    "            logger.write(f\"  - Imbalance ratio: {label_dist.get('imbalance_ratio', 0):.2f}\")\n",
    "        \n",
    "        # ì´ìƒì¹˜ ê²€ì¶œ ê²°ê³¼\n",
    "        outliers = validation_report.get('outliers', {})\n",
    "        logger.write(f\"\\nOutlier Detection:\")\n",
    "        logger.write(f\"  - Outlier count: {outliers.get('outlier_count', 0)}\")\n",
    "        logger.write(f\"  - Outlier ratio: {outliers.get('outlier_ratio', 0):.2%}\")\n",
    "        \n",
    "        # ê¶Œì¥ì‚¬í•­\n",
    "        recommendations = data_validator.generate_recommendations()\n",
    "        if recommendations:\n",
    "            logger.write(\"\\nğŸ“‹ Recommendations:\")\n",
    "            for rec in recommendations:\n",
    "                logger.write(f\"  âœ“ {rec}\")\n",
    "        \n",
    "        # WandB ë¡œê¹…\n",
    "        if config['wandb']['mode'] != 'disabled':\n",
    "            wandb.log({\n",
    "                'data_quality/nulls': structural.get('train_nulls', 0),\n",
    "                'data_quality/duplicates': structural.get('train_duplicates', 0),\n",
    "                'data_quality/encoding_issues': text_quality.get('encoding_issues', 0),\n",
    "                'data_quality/outlier_ratio': outliers.get('outlier_ratio', 0)\n",
    "            })\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.write(f\"âš ï¸ Error during data validation: {str(e)}\")\n",
    "    logger.write(\"   Skipping detailed validation. Please check data loading in cell 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.725412Z",
     "iopub.status.busy": "2025-10-10T02:53:01.725308Z",
     "iopub.status.idle": "2025-10-10T02:53:01.728266Z",
     "shell.execute_reply": "2025-10-10T02:53:01.727730Z"
    }
   },
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ì„¤ì •\n",
    "if config['visualization']['enabled']:\n",
    "    viz = TrainingVisualizer()\n",
    "    \n",
    "    # configì˜ ì‹œê°í™” ê²½ë¡œ ì‚¬ìš©\n",
    "    viz_path = config['visualization']['save_path']\n",
    "    if not Path(viz_path).is_absolute():\n",
    "        viz_dir = notebook_dir / viz_path\n",
    "    else:\n",
    "        viz_dir = Path(viz_path)\n",
    "    \n",
    "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "    logger.write(f\"Visualizations will be saved to: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:01.729208Z",
     "iopub.status.busy": "2025-10-10T02:53:01.729114Z",
     "iopub.status.idle": "2025-10-10T02:53:03.024570Z",
     "shell.execute_reply": "2025-10-10T02:53:03.024035Z"
    }
   },
   "outputs": [],
   "source": [
    "# WandB ì´ˆê¸°í™” (ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¶”ì )\n",
    "if config['wandb']['mode'] != 'disabled':\n",
    "    wandb.init(\n",
    "        project=config['wandb']['project'],\n",
    "        entity=config['wandb']['entity'],\n",
    "        name=config['wandb']['name'],\n",
    "        tags=config['wandb']['tags'],\n",
    "        config=config\n",
    "    )\n",
    "    logger.write(\"WandB initialized for full pipeline tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì½”ë“œëŠ” config íŒŒì¼ ì„¤ì •ì— ë”°ë¼ êµ¬í˜„\n",
    "\n",
    "### ì‹¤í–‰ ë‹¨ê³„:\n",
    "1. ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
    "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¦ê°•\n",
    "3. ëª¨ë¸ í•™ìŠµ (Multi-model)\n",
    "4. K-Fold êµì°¨ ê²€ì¦\n",
    "5. Optuna ìµœì í™”\n",
    "6. ì•™ìƒë¸” + TTA\n",
    "7. ì¶”ë¡  ìµœì í™”\n",
    "8. ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.025825Z",
     "iopub.status.busy": "2025-10-10T02:53:03.025717Z",
     "iopub.status.idle": "2025-10-10T02:53:03.295081Z",
     "shell.execute_reply": "2025-10-10T02:53:03.294653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 2: ë°ì´í„° ì „ì²˜ë¦¬\n",
    "if 'data_preprocessing' in config['pipeline']['stages']:\n",
    "    update_status('data_preprocessing', 'running')\n",
    "    logger.write(\"\\n=== Data Preprocessing ===\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "    import re\n",
    "    \n",
    "    def preprocess_dialogue(text):\n",
    "        \"\"\"ëŒ€í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        # ë…¸ì´ì¦ˆ ì œê±°\n",
    "        text = text.replace('\\\\n', '\\n')\n",
    "        text = text.replace('<br>', '\\n')\n",
    "        text = text.strip()\n",
    "        \n",
    "        # #Person íƒœê·¸ ì •ê·œí™”\n",
    "        text = re.sub(r'#Person(\\d+)#:', r'í™”ì\\1:', text)\n",
    "        \n",
    "        # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_summary(text):\n",
    "        \"\"\"ìš”ì•½ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ ì ìš©\n",
    "    train_df['dialogue_preprocessed'] = train_df['dialogue'].apply(preprocess_dialogue)\n",
    "    train_df['summary_preprocessed'] = train_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    dev_df['dialogue_preprocessed'] = dev_df['dialogue'].apply(preprocess_dialogue)\n",
    "    dev_df['summary_preprocessed'] = dev_df['summary'].apply(preprocess_summary)\n",
    "    \n",
    "    test_df['dialogue_preprocessed'] = test_df['dialogue'].apply(preprocess_dialogue)\n",
    "    \n",
    "    logger.write(f\"Preprocessed {len(train_df)} training samples\")\n",
    "    logger.write(f\"Preprocessed {len(dev_df)} dev samples\")\n",
    "    logger.write(f\"Preprocessed {len(test_df)} test samples\")\n",
    "    \n",
    "    # ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "    train_dialogue_lengths = train_df['dialogue_preprocessed'].str.len()\n",
    "    train_summary_lengths = train_df['summary_preprocessed'].str.len()\n",
    "    \n",
    "    logger.write(f\"\\nText Length Statistics:\")\n",
    "    logger.write(f\"  Dialogue - Mean: {train_dialogue_lengths.mean():.1f}, Max: {train_dialogue_lengths.max()}\")\n",
    "    logger.write(f\"  Summary - Mean: {train_summary_lengths.mean():.1f}, Max: {train_summary_lengths.max()}\")\n",
    "    \n",
    "    update_status('data_preprocessing', 'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:03.567069Z",
     "iopub.status.busy": "2025-10-10T02:53:03.566966Z",
     "iopub.status.idle": "2025-10-10T02:53:03.584546Z",
     "shell.execute_reply": "2025-10-10T02:53:03.584141Z"
    }
   },
   "outputs": [],
   "source": [
    "# PRD ì „ëµ í†µí•© - Solar API, Optuna, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Optuna import - ë°˜ë“œì‹œ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna import Trial\n",
    "    from optuna.samplers import TPESampler\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"âœ… Optuna is available and will be used for hyperparameter optimization!\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    logger.write(\"âš ï¸ Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "# Solar API í†µí•© (PRD 09_Solar_API_ìµœì í™”.md, 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md)\n",
    "class PipelineSolarAPI:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ìš© Solar API í†µí•©\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, logger):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.upstage.ai/v1/solar\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        self.logger = logger\n",
    "        self.cache = {}\n",
    "        self.api_calls = 0\n",
    "        self.token_usage = 0\n",
    "        \n",
    "    def optimize_and_validate(self, model_predictions: List[str], test_dialogues: List[str], \n",
    "                            sample_size: int = 10) -> Dict:\n",
    "        \"\"\"ëª¨ë¸ ì˜ˆì¸¡ê³¼ API ì˜ˆì¸¡ ë¹„êµ ê²€ì¦\"\"\"\n",
    "        self.logger.write(\"\\n=== Solar API Cross-Validation ===\")\n",
    "        \n",
    "        comparisons = []\n",
    "        \n",
    "        # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n",
    "        sample_indices = np.random.choice(\n",
    "            len(model_predictions), \n",
    "            min(sample_size, len(model_predictions)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            dialogue = test_dialogues[idx]\n",
    "            model_pred = model_predictions[idx]\n",
    "            \n",
    "            # Solar API ì˜ˆì¸¡\n",
    "            api_pred = self.generate_summary(dialogue)\n",
    "            \n",
    "            if api_pred:\n",
    "                comparisons.append({\n",
    "                    'model': model_pred[:200],\n",
    "                    'api': api_pred[:200],\n",
    "                    'model_length': len(model_pred),\n",
    "                    'api_length': len(api_pred)\n",
    "                })\n",
    "                \n",
    "                self.api_calls += 1\n",
    "                self.token_usage += len(dialogue) // 3  # ëŒ€ëµì  í† í° ì¶”ì •\n",
    "        \n",
    "        # í†µê³„ ë¶„ì„\n",
    "        if comparisons:\n",
    "            avg_model_length = np.mean([c['model_length'] for c in comparisons])\n",
    "            avg_api_length = np.mean([c['api_length'] for c in comparisons])\n",
    "            \n",
    "            self.logger.write(f\"Comparisons completed: {len(comparisons)} samples\")\n",
    "            self.logger.write(f\"Avg model length: {avg_model_length:.1f}\")\n",
    "            self.logger.write(f\"Avg API length: {avg_api_length:.1f}\")\n",
    "            self.logger.write(f\"API calls made: {self.api_calls}\")\n",
    "            self.logger.write(f\"Estimated tokens used: {self.token_usage}\")\n",
    "            \n",
    "            return {\n",
    "                'comparisons': comparisons,\n",
    "                'avg_model_length': avg_model_length,\n",
    "                'avg_api_length': avg_api_length,\n",
    "                'api_calls': self.api_calls,\n",
    "                'token_usage': self.token_usage\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    def generate_summary(self, dialogue: str, max_tokens: int = 150) -> Optional[str]:\n",
    "        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n",
    "        # ìºì‹œ í™•ì¸\n",
    "        cache_key = hash(dialogue[:200] if len(dialogue) > 200 else dialogue)\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # í† í° ìµœì í™”\n",
    "            if len(dialogue) > 2000:\n",
    "                dialogue = dialogue[:2000] + \"...\"\n",
    "            \n",
    "            prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ 3-5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "ìš”ì•½:\"\"\"\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": \"solar-1-mini-chat\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ëŒ€í™” ìš”ì•½ AIì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": 0.3,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/chat/completions\",\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                summary = result['choices'][0]['message']['content']\n",
    "                self.cache[cache_key] = summary\n",
    "                return summary\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.write(f\"Solar API error: {e}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (PRD 13_Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°_ìµœì í™”.md)\n",
    "class PipelineOptunaOptimizer:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ìš© Optuna ìµœì í™” - ì‹¤ì œ ìµœì í™” ìˆ˜í–‰\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.best_params = None\n",
    "        self.study = None\n",
    "        \n",
    "    def optimize_hyperparameters(self, config: Dict, n_trials: int = 20, actual_training: bool = False) -> Dict:\n",
    "        \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” - ì‹¤ì œë¡œ ìˆ˜í–‰ë¨!\"\"\"\n",
    "        \n",
    "        # Optuna ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            self.logger.write(\"âš ï¸ Optuna not available - Please install: pip install optuna\")\n",
    "            self.logger.write(\"   Using default parameters instead\")\n",
    "            return config\n",
    "        \n",
    "        # hyperparameter_optimizationì´ enabledì¸ì§€ í™•ì¸\n",
    "        if not config.get('hyperparameter_optimization', {}).get('enabled', False):\n",
    "            self.logger.write(\"âš ï¸ Hyperparameter optimization is disabled in config\")\n",
    "            self.logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "            return config\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"ğŸ¯ OPTUNA HYPERPARAMETER OPTIMIZATION STARTING\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Number of trials: {n_trials}\")\n",
    "        self.logger.write(f\"Optimization metric: {config['hyperparameter_optimization'].get('metric', 'rouge_l')}\")\n",
    "        \n",
    "        def objective(trial: Trial) -> float:\n",
    "            \"\"\"ì‹¤ì œ ëª©ì  í•¨ìˆ˜ - ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\"\"\"\n",
    "            \n",
    "            # Configì˜ search_space ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì œì•ˆ\n",
    "            search_space = config['hyperparameter_optimization']['search_space']\n",
    "            \n",
    "            hp = {}\n",
    "            \n",
    "            # Learning rate\n",
    "            if 'learning_rate' in search_space:\n",
    "                lr_config = search_space['learning_rate']\n",
    "                hp['learning_rate'] = trial.suggest_float(\n",
    "                    'learning_rate', \n",
    "                    lr_config['low'], \n",
    "                    lr_config['high'], \n",
    "                    log=lr_config.get('log', True)\n",
    "                )\n",
    "            \n",
    "            # Batch size\n",
    "            if 'batch_size' in search_space:\n",
    "                bs_config = search_space['batch_size']\n",
    "                hp['batch_size'] = trial.suggest_categorical(\n",
    "                    'batch_size',\n",
    "                    bs_config['choices']\n",
    "                )\n",
    "            \n",
    "            # LoRA parameters (if using LoRA)\n",
    "            if 'lora_r' in search_space:\n",
    "                lora_r_config = search_space['lora_r']\n",
    "                hp['lora_r'] = trial.suggest_int(\n",
    "                    'lora_r',\n",
    "                    lora_r_config['low'],\n",
    "                    lora_r_config['high'],\n",
    "                    step=lora_r_config.get('step', 4)\n",
    "                )\n",
    "            \n",
    "            if 'lora_alpha' in search_space:\n",
    "                lora_alpha_config = search_space['lora_alpha']\n",
    "                hp['lora_alpha'] = trial.suggest_int(\n",
    "                    'lora_alpha',\n",
    "                    lora_alpha_config['low'],\n",
    "                    lora_alpha_config['high'],\n",
    "                    step=lora_alpha_config.get('step', 8)\n",
    "                )\n",
    "            \n",
    "            # Generation parameters\n",
    "            if 'num_beams' in search_space:\n",
    "                nb_config = search_space['num_beams']\n",
    "                hp['num_beams'] = trial.suggest_int(\n",
    "                    'num_beams',\n",
    "                    nb_config['low'],\n",
    "                    nb_config.get('high', 8)\n",
    "                )\n",
    "            \n",
    "            if 'temperature' in search_space:\n",
    "                temp_config = search_space['temperature']\n",
    "                hp['temperature'] = trial.suggest_float(\n",
    "                    'temperature',\n",
    "                    temp_config['low'],\n",
    "                    temp_config['high']\n",
    "                )\n",
    "            \n",
    "            # ì¶”ê°€ íŒŒë¼ë¯¸í„°\n",
    "            hp['warmup_ratio'] = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "            hp['weight_decay'] = trial.suggest_float('weight_decay', 0.0, 0.1)\n",
    "            hp['top_p'] = trial.suggest_float('top_p', 0.8, 1.0)\n",
    "            \n",
    "            self.logger.write(f\"\\nTrial {trial.number}: {hp}\")\n",
    "            \n",
    "            if actual_training:\n",
    "                # ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "                # ì—¬ê¸°ì— ì‹¤ì œ í•™ìŠµ ì½”ë“œë¥¼ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "                score = self._train_and_evaluate(hp, config)\n",
    "            else:\n",
    "                # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "                score = self._simulate_training(hp)\n",
    "            \n",
    "            return score\n",
    "        \n",
    "        # Optuna study ìƒì„±\n",
    "        study = optuna.create_study(\n",
    "            direction=config['hyperparameter_optimization'].get('direction', 'maximize'),\n",
    "            sampler=TPESampler(seed=42),\n",
    "            study_name='pipeline_optimization',\n",
    "            pruner=optuna.pruners.MedianPruner() if config['hyperparameter_optimization'].get('pruner') == 'MedianPruner' else None\n",
    "        )\n",
    "        \n",
    "        # ìµœì í™” ì‹¤í–‰!\n",
    "        self.logger.write(\"\\nğŸš€ Starting optimization...\")\n",
    "        study.optimize(\n",
    "            objective, \n",
    "            n_trials=n_trials,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "        self.best_params = study.best_params\n",
    "        best_value = study.best_value\n",
    "        self.study = study\n",
    "        \n",
    "        self.logger.write(\"\\n\" + \"=\"*60)\n",
    "        self.logger.write(\"âœ… OPTIMIZATION COMPLETED!\")\n",
    "        self.logger.write(\"=\"*60)\n",
    "        self.logger.write(f\"Best score: {best_value:.4f}\")\n",
    "        self.logger.write(f\"Best parameters:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            self.logger.write(f\"  - {param}: {value}\")\n",
    "        \n",
    "        # ìƒìœ„ 5ê°œ trial ì¶œë ¥\n",
    "        self.logger.write(\"\\nğŸ“Š Top 5 trials:\")\n",
    "        for i, trial in enumerate(study.best_trials[:5], 1):\n",
    "            self.logger.write(f\"{i}. Score: {trial.value:.4f}\")\n",
    "        \n",
    "        # Config ì—…ë°ì´íŠ¸\n",
    "        updated_config = self._update_config_with_best_params(config)\n",
    "        \n",
    "        # ìµœì í™” ê²°ê³¼ ì €ì¥\n",
    "        self._save_optimization_results(study, config)\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _simulate_training(self, hp: Dict) -> float:\n",
    "        \"\"\"í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\"\"\"\n",
    "        # íŒŒë¼ë¯¸í„° ì¡°í•©ì— ë”°ë¥¸ ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜\n",
    "        score = np.random.random() * 0.3 + 0.4  # 0.4~0.7 ë²”ìœ„\n",
    "        \n",
    "        # ì¢‹ì€ íŒŒë¼ë¯¸í„° ì¡°í•©ì— ë³´ë„ˆìŠ¤\n",
    "        if hp['learning_rate'] < 5e-5 and hp['batch_size'] <= 8:\n",
    "            score += 0.1\n",
    "        if hp.get('lora_r', 8) >= 16:\n",
    "            score += 0.05\n",
    "        if hp.get('num_beams', 4) >= 4:\n",
    "            score += 0.05\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _train_and_evaluate(self, hp: Dict, config: Dict) -> float:\n",
    "        \"\"\"ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (êµ¬í˜„ í•„ìš”)\"\"\"\n",
    "        # ì—¬ê¸°ì— ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ êµ¬í˜„\n",
    "        # í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´\n",
    "        return self._simulate_training(hp)\n",
    "    \n",
    "    def _update_config_with_best_params(self, config: Dict) -> Dict:\n",
    "        \"\"\"ìµœì  íŒŒë¼ë¯¸í„°ë¡œ config ì—…ë°ì´íŠ¸\"\"\"\n",
    "        updated_config = config.copy()\n",
    "        \n",
    "        if self.best_params:\n",
    "            # training ì„¹ì…˜ ì—…ë°ì´íŠ¸\n",
    "            if 'learning_rate' in self.best_params:\n",
    "                updated_config['training']['learning_rate'] = self.best_params['learning_rate']\n",
    "            if 'batch_size' in self.best_params:\n",
    "                updated_config['training']['batch_size'] = self.best_params['batch_size']\n",
    "            if 'warmup_ratio' in self.best_params:\n",
    "                updated_config['training']['warmup_ratio'] = self.best_params['warmup_ratio']\n",
    "            if 'weight_decay' in self.best_params:\n",
    "                updated_config['training']['weight_decay'] = self.best_params['weight_decay']\n",
    "            \n",
    "            # LoRA ì„¤ì • ì—…ë°ì´íŠ¸ (if applicable)\n",
    "            if 'models' in updated_config and 'primary_models' in updated_config['models']:\n",
    "                for model_config in updated_config['models']['primary_models']:\n",
    "                    if 'lora_r' in self.best_params:\n",
    "                        model_config['lora_r'] = self.best_params['lora_r']\n",
    "                    if 'lora_alpha' in self.best_params:\n",
    "                        model_config['lora_alpha'] = self.best_params['lora_alpha']\n",
    "            \n",
    "            # Generation ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "            if 'post_processing' in updated_config:\n",
    "                if 'temperature' in self.best_params:\n",
    "                    # temperature ì„¤ì • ì¶”ê°€\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['temperature'] = self.best_params['temperature']\n",
    "                if 'top_p' in self.best_params:\n",
    "                    if 'generation' not in updated_config['post_processing']:\n",
    "                        updated_config['post_processing']['generation'] = {}\n",
    "                    updated_config['post_processing']['generation']['top_p'] = self.best_params['top_p']\n",
    "            \n",
    "            self.logger.write(\"\\nâœ… Config updated with optimal hyperparameters!\")\n",
    "        \n",
    "        return updated_config\n",
    "    \n",
    "    def _save_optimization_results(self, study, config):\n",
    "        \"\"\"ìµœì í™” ê²°ê³¼ ì €ì¥\"\"\"\n",
    "        import pickle\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "        output_dir = Path(config['paths']['log_dir']) / 'optuna'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Study ê°ì²´ ì €ì¥\n",
    "        study_path = output_dir / f'optuna_study_{timestamp}.pkl'\n",
    "        with open(study_path, 'wb') as f:\n",
    "            pickle.dump(study, f)\n",
    "        \n",
    "        # ê²°ê³¼ CSV ì €ì¥\n",
    "        df = study.trials_dataframe()\n",
    "        csv_path = output_dir / f'optuna_results_{timestamp}.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„° JSON ì €ì¥\n",
    "        import json\n",
    "        json_path = output_dir / f'best_params_{timestamp}.json'\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(self.best_params, f, indent=2)\n",
    "        \n",
    "        self.logger.write(f\"\\nğŸ“ Optimization results saved:\")\n",
    "        self.logger.write(f\"  - Study: {study_path}\")\n",
    "        self.logger.write(f\"  - CSV: {csv_path}\")\n",
    "        self.logger.write(f\"  - Best params: {json_path}\")\n",
    "\n",
    "# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (PRD 05_ë¦¬ìŠ¤í¬_ê´€ë¦¬.md)\n",
    "class PipelineRiskManager:\n",
    "    \"\"\"íŒŒì´í”„ë¼ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.risks = []\n",
    "        self.mitigations_applied = []\n",
    "        \n",
    "    def monitor_pipeline_risks(self, stage: str, metrics: Dict) -> Dict:\n",
    "        \"\"\"íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "        stage_risks = []\n",
    "        \n",
    "        # ë°ì´í„° í’ˆì§ˆ ë¦¬ìŠ¤í¬\n",
    "        if stage == 'data_quality_check':\n",
    "            if metrics.get('encoding_issues', 0) > 100:\n",
    "                stage_risks.append({\n",
    "                    'type': 'data_quality',\n",
    "                    'severity': 'high',\n",
    "                    'description': f\"High encoding issues: {metrics['encoding_issues']}\",\n",
    "                    'mitigation': 'Apply text cleaning and encoding fixes'\n",
    "                })\n",
    "        \n",
    "        # í•™ìŠµ ë¦¬ìŠ¤í¬\n",
    "        elif stage == 'model_training':\n",
    "            if metrics.get('train_loss', float('inf')) > 5.0:\n",
    "                stage_risks.append({\n",
    "                    'type': 'training_instability',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"High training loss: {metrics.get('train_loss')}\",\n",
    "                    'mitigation': 'Reduce learning rate or check data'\n",
    "                })\n",
    "            \n",
    "            if metrics.get('val_loss', 0) > metrics.get('train_loss', 1) * 2:\n",
    "                stage_risks.append({\n",
    "                    'type': 'overfitting',\n",
    "                    'severity': 'high',\n",
    "                    'description': 'Significant overfitting detected',\n",
    "                    'mitigation': 'Apply regularization or early stopping'\n",
    "                })\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ë¦¬ìŠ¤í¬\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() if torch.cuda.max_memory_allocated() > 0 else 0\n",
    "            if memory_used > 0.9:\n",
    "                stage_risks.append({\n",
    "                    'type': 'memory_overflow',\n",
    "                    'severity': 'critical',\n",
    "                    'description': f\"Memory usage: {memory_used:.1%}\",\n",
    "                    'mitigation': 'Reduce batch size or model size'\n",
    "                })\n",
    "        \n",
    "        # ë¦¬ìŠ¤í¬ ê¸°ë¡ ë° ë³´ê³ \n",
    "        if stage_risks:\n",
    "            self.risks.extend(stage_risks)\n",
    "            self.logger.write(f\"\\nâš ï¸ Risks detected in {stage}:\")\n",
    "            for risk in stage_risks:\n",
    "                self.logger.write(f\"  [{risk['severity']}] {risk['type']}: {risk['description']}\")\n",
    "                self.logger.write(f\"    â†’ Mitigation: {risk['mitigation']}\")\n",
    "        \n",
    "        return {\n",
    "            'stage': stage,\n",
    "            'risks': stage_risks,\n",
    "            'risk_count': len(stage_risks)\n",
    "        }\n",
    "    \n",
    "    def apply_automatic_mitigation(self, risk_type: str, config: Dict) -> Dict:\n",
    "        \"\"\"ìë™ ë¦¬ìŠ¤í¬ ì™„í™”\"\"\"\n",
    "        mitigations = {\n",
    "            'overfitting': {\n",
    "                'action': 'increase_regularization',\n",
    "                'config_changes': {\n",
    "                    'training.weight_decay': config['training'].get('weight_decay', 0) * 2,\n",
    "                    'training.dropout': 0.3\n",
    "                }\n",
    "            },\n",
    "            'memory_overflow': {\n",
    "                'action': 'reduce_batch_size',\n",
    "                'config_changes': {\n",
    "                    'training.batch_size': max(1, config['training']['batch_size'] // 2)\n",
    "                }\n",
    "            },\n",
    "            'training_instability': {\n",
    "                'action': 'reduce_learning_rate',\n",
    "                'config_changes': {\n",
    "                    'training.learning_rate': float(config['training']['learning_rate']) * 0.1\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if risk_type in mitigations:\n",
    "            mitigation = mitigations[risk_type]\n",
    "            self.mitigations_applied.append(mitigation)\n",
    "            self.logger.write(f\"âœ“ Applied mitigation: {mitigation['action']}\")\n",
    "            \n",
    "            # Config ì—…ë°ì´íŠ¸\n",
    "            for key, value in mitigation['config_changes'].items():\n",
    "                keys = key.split('.')\n",
    "                if len(keys) == 2:\n",
    "                    config[keys[0]][keys[1]] = value\n",
    "            \n",
    "            return config\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def generate_risk_report(self) -> Dict:\n",
    "        \"\"\"ë¦¬ìŠ¤í¬ ë³´ê³ ì„œ ìƒì„±\"\"\"\n",
    "        if not self.risks:\n",
    "            return {\n",
    "                'status': 'healthy',\n",
    "                'total_risks': 0,\n",
    "                'critical_risks': 0\n",
    "            }\n",
    "        \n",
    "        critical_count = sum(1 for r in self.risks if r['severity'] == 'critical')\n",
    "        high_count = sum(1 for r in self.risks if r['severity'] == 'high')\n",
    "        \n",
    "        return {\n",
    "            'status': 'at_risk' if critical_count > 0 else 'warning' if high_count > 0 else 'healthy',\n",
    "            'total_risks': len(self.risks),\n",
    "            'critical_risks': critical_count,\n",
    "            'high_risks': high_count,\n",
    "            'mitigations_applied': len(self.mitigations_applied)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# Stage 5: Optuna ìµœì í™” ì‹¤í–‰ - ì‹¤ì œë¡œ ì‹¤í–‰ë¨!\n",
    "# =============================================================================\n",
    "if 'hyperparameter_optimization' in config['pipeline']['stages']:\n",
    "    update_status('hyperparameter_optimization', 'running')\n",
    "    \n",
    "    logger.write(\"\\n\" + \"=\"*70)\n",
    "    logger.write(\"ğŸ¯ HYPERPARAMETER OPTIMIZATION STAGE\")\n",
    "    logger.write(\"=\"*70)\n",
    "    \n",
    "    # Optuna ìµœì í™” ì‹¤í–‰\n",
    "    optuna_optimizer = PipelineOptunaOptimizer(logger)\n",
    "    \n",
    "    # Configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°\n",
    "    optimization_enabled = config.get('hyperparameter_optimization', {}).get('enabled', False)\n",
    "    n_trials = config.get('hyperparameter_optimization', {}).get('n_trials', 20)\n",
    "    \n",
    "    if optimization_enabled and OPTUNA_AVAILABLE:\n",
    "        logger.write(f\"âœ… Optimization ENABLED with {n_trials} trials\")\n",
    "        \n",
    "        # ì‹¤ì œ ìµœì í™” ì‹¤í–‰! (actual_training=FalseëŠ” ì‹œë®¬ë ˆì´ì…˜, TrueëŠ” ì‹¤ì œ í•™ìŠµ)\n",
    "        optimized_config = optuna_optimizer.optimize_hyperparameters(\n",
    "            config, \n",
    "            n_trials=n_trials,\n",
    "            actual_training=False  # Trueë¡œ ë³€ê²½í•˜ë©´ ì‹¤ì œ ëª¨ë¸ í•™ìŠµìœ¼ë¡œ ìµœì í™”\n",
    "        )\n",
    "        \n",
    "        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ config ì—…ë°ì´íŠ¸\n",
    "        if optuna_optimizer.best_params:\n",
    "            config = optimized_config\n",
    "            logger.write(\"\\nâœ… Config has been updated with optimal hyperparameters!\")\n",
    "            \n",
    "            # WandB ë¡œê¹…\n",
    "            if config['wandb']['mode'] != 'disabled':\n",
    "                wandb.log({\n",
    "                    'optuna/best_score': optuna_optimizer.study.best_value,\n",
    "                    'optuna/n_trials': n_trials,\n",
    "                    'optuna/best_params': optuna_optimizer.best_params\n",
    "                })\n",
    "    else:\n",
    "        if not optimization_enabled:\n",
    "            logger.write(\"âš ï¸ Optimization is DISABLED in config\")\n",
    "            logger.write(\"   Set hyperparameter_optimization.enabled: true to enable\")\n",
    "        if not OPTUNA_AVAILABLE:\n",
    "            logger.write(\"âš ï¸ Optuna library not available\")\n",
    "            logger.write(\"   Install with: pip install optuna\")\n",
    "    \n",
    "    update_status('hyperparameter_optimization', 'completed')\n",
    "\n",
    "# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì´ˆê¸°í™”\n",
    "risk_manager = PipelineRiskManager(logger)\n",
    "\n",
    "# Solar API ì´ˆê¸°í™” (configì—ì„œ í‚¤ í™•ì¸)\n",
    "solar_api = None\n",
    "if 'solar_api' in config and config['solar_api'].get('enabled', False):\n",
    "    if 'api_key' in config['solar_api']:\n",
    "        solar_api = PipelineSolarAPI(config['solar_api']['api_key'], logger)\n",
    "        logger.write(\"âœ… Solar API initialized for cross-validation\")\n",
    "    else:\n",
    "        logger.write(\"âš ï¸ Solar API key not found in config\")\n",
    "else:\n",
    "    logger.write(\"âš ï¸ Solar API is disabled in config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:04.942899Z",
     "iopub.status.busy": "2025-10-10T02:53:04.942804Z",
     "iopub.status.idle": "2025-10-10T02:53:05.952389Z",
     "shell.execute_reply": "2025-10-10T02:53:05.951938Z"
    }
   },
   "outputs": [],
   "source": "# Stage 4: ëª¨ë¸ í•™ìŠµ - GPU ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©!\nif 'model_training' in config['pipeline']['stages']:\n    update_status('model_training', 'running')\n    logger.write(\"\\n=== Model Training (GPU Optimized) ===\")\n    \n    from transformers import AutoTokenizer, BartForConditionalGeneration\n    from torch.utils.data import Dataset, DataLoader\n    from torch.optim import AdamW\n    from transformers import get_linear_schedule_with_warmup\n    from tqdm.auto import tqdm\n    import gc\n    \n    # Mixed Precision Training import (FP16)\n    try:\n        from torch.cuda.amp import autocast, GradScaler\n        USE_AMP = torch.cuda.is_available() and config['gpu'].get('mixed_precision', True)\n        if USE_AMP:\n            logger.write(\"âœ… Mixed Precision (FP16) Training ENABLED - 40% memory reduction\")\n    except ImportError:\n        USE_AMP = False\n        logger.write(\"âš ï¸ Mixed Precision not available\")\n    \n    # í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜\n    def get_path(path_str):\n        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n        path = Path(path_str)\n        if not path.is_absolute():\n            path = notebook_dir / path\n        return path\n    \n    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜\n    def clear_gpu_memory():\n        \"\"\"GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    # ======================================================================\n    # ğŸ”¥ CRITICAL FIX: ëª¨ë¸ ë¡œë“œ ì „ì— GPU ì™„ì „ ì •ë¦¬!\n    # ======================================================================\n    logger.write(\"\\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬ ì¤‘...\")\n    clear_gpu_memory()\n    \n    # ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì‚­ì œ\n    if 'model' in globals():\n        del model\n    if 'tokenizer' in globals():\n        del tokenizer\n    clear_gpu_memory()\n    \n    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n    class DialogueSummaryDataset(Dataset):\n        def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=128, is_test=False):\n            self.df = dataframe.reset_index(drop=True)\n            self.tokenizer = tokenizer\n            self.max_input_len = max_input_len\n            self.max_target_len = max_target_len\n            self.is_test = is_test\n            \n        def __len__(self):\n            return len(self.df)\n        \n        def __getitem__(self, idx):\n            row = self.df.iloc[idx]\n            dialogue = row.get('dialogue_preprocessed', row.get('dialogue', ''))\n            \n            inputs = self.tokenizer(\n                dialogue,\n                max_length=self.max_input_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            if not self.is_test:\n                summary = row.get('summary_preprocessed', row.get('summary', ''))\n                targets = self.tokenizer(\n                    summary,\n                    max_length=self.max_target_len,\n                    padding='max_length',\n                    truncation=True,\n                    return_tensors='pt'\n                )\n                \n                # ë¼ë²¨ ìƒì„± - íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ì¤‘ìš”!)\n                labels = targets['input_ids'].squeeze()\n                labels[labels == self.tokenizer.pad_token_id] = -100  # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹\n                \n                return {\n                    'input_ids': inputs['input_ids'].squeeze(),\n                    'attention_mask': inputs['attention_mask'].squeeze(),\n                    'labels': labels\n                }\n            else:\n                return {\n                    'input_ids': inputs['input_ids'].squeeze(),\n                    'attention_mask': inputs['attention_mask'].squeeze(),\n                    'idx': idx\n                }\n    \n    # ëª¨ë¸ ì„ íƒ\n    if 'primary_models' in config.get('models', {}):\n        model_config = config['models']['primary_models'][0]\n        model_name = model_config['name']\n    else:\n        model_name = \"gogamza/kobart-summarization\"\n        model_config = {'max_input_length': 512, 'max_target_length': 128}\n    \n    logger.write(f\"Training primary model: {model_name}\")\n    \n    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    logger.write(\"âœ… Tokenizer loaded\")\n    \n    # ëª¨ë¸ ë¡œë“œ\n    model = BartForConditionalGeneration.from_pretrained(model_name)\n    logger.write(\"âœ… Model loaded to CPU\")\n    \n    # =============================================================================\n    # GPU ìµœì í™” 1: Gradient Checkpointing (50% ë©”ëª¨ë¦¬ ê°ì†Œ)\n    # =============================================================================\n    if config['training'].get('gradient_checkpointing', True):\n        if hasattr(model, 'gradient_checkpointing_enable'):\n            model.gradient_checkpointing_enable()\n            logger.write(\"âœ… Gradient Checkpointing ENABLED - 50% memory reduction\")\n        elif hasattr(model.config, 'gradient_checkpointing'):\n            model.config.gradient_checkpointing = True\n            logger.write(\"âœ… Gradient Checkpointing ENABLED (via config)\")\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    logger.write(f\"Model moved to {device}\")\n    \n    # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…\n    if torch.cuda.is_available():\n        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        reserved_memory = torch.cuda.memory_reserved(0) / 1024**3\n        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n        logger.write(f\"GPU Memory - Total: {total_memory:.2f}GB, Reserved: {reserved_memory:.2f}GB, Allocated: {allocated_memory:.2f}GB\")\n    \n    # ë°ì´í„°ì…‹ ìƒì„± (ìƒ˜í”Œë§ ì˜µì…˜)\n    if config['training'].get('use_sample', False):\n        sample_size = config['training'].get('sample_size', 1000)\n        train_sample = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n        logger.write(f\"Using sample of {len(train_sample)} for training\")\n    else:\n        train_sample = train_df\n    \n    # =============================================================================\n    # GPU ìµœì í™” 2: Gradient Accumulation (ì‘ì€ ë°°ì¹˜ë¡œ í° ë°°ì¹˜ íš¨ê³¼)\n    # =============================================================================\n    gradient_accumulation_steps = config['training'].get('gradient_accumulation_steps', 8)\n    effective_batch_size = config['training']['batch_size'] * gradient_accumulation_steps\n    logger.write(f\"âœ… Gradient Accumulation: {gradient_accumulation_steps} steps\")\n    logger.write(f\"   Physical batch size: {config['training']['batch_size']}\")\n    logger.write(f\"   Effective batch size: {effective_batch_size}\")\n    \n    # ë°ì´í„°ë¡œë” ìƒì„±\n    train_dataset = DialogueSummaryDataset(\n        train_sample, tokenizer,\n        max_input_len=model_config.get('max_input_length', 512),\n        max_target_len=model_config.get('max_target_length', 128)\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        dev_df, tokenizer,\n        max_input_len=model_config.get('max_input_length', 512),\n        max_target_len=model_config.get('max_target_length', 128)\n    )\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        num_workers=0,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½\n        pin_memory=False  # ë©”ëª¨ë¦¬ ë¬¸ì œ ì‹œ Falseë¡œ\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=False,\n        num_workers=0,\n        pin_memory=False\n    )\n    \n    # í•™ìŠµ ì„¤ì •\n    num_epochs = config['training']['num_epochs']\n    learning_rate = float(config['training']['learning_rate']) if isinstance(config['training']['learning_rate'], str) else config['training']['learning_rate']\n    \n    # =============================================================================\n    # ğŸ”¥ CRITICAL FIX: Optimizer CPUì—ì„œ ì´ˆê¸°í™” í›„ GPUë¡œ ì´ë™\n    # =============================================================================\n    logger.write(\"\\nâš™ï¸ Optimizer ì´ˆê¸°í™” ì¤‘...\")\n    \n    # Optimizer ì´ˆê¸°í™” ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n    clear_gpu_memory()\n    \n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    num_training_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    logger.write(\"âœ… Optimizer initialized successfully\")\n    \n    # =============================================================================\n    # GPU ìµœì í™” 3: Mixed Precision Training (FP16) - GradScaler\n    # =============================================================================\n    scaler = GradScaler() if USE_AMP else None\n    \n    logger.write(f\"\\n{'='*70}\")\n    logger.write(f\"ğŸš€ TRAINING START - GPU Optimized\")\n    logger.write(f\"{'='*70}\")\n    logger.write(f\"Epochs: {num_epochs}\")\n    logger.write(f\"Gradient Accumulation: {gradient_accumulation_steps}\")\n    logger.write(f\"Mixed Precision (FP16): {USE_AMP}\")\n    logger.write(f\"Gradient Checkpointing: {config['training'].get('gradient_checkpointing', True)}\")\n    logger.write(f\"{'='*70}\\n\")\n    \n    # í•™ìŠµ ë£¨í”„ - GPU ìµœì í™” ì ìš©\n    best_loss = float('inf')\n    global_step = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        optimizer.zero_grad()\n        \n        for step, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')):\n            try:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                # Mixed Precision Forward Pass\n                if USE_AMP:\n                    with autocast():\n                        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                        loss = outputs.loss / gradient_accumulation_steps\n                else:\n                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss / gradient_accumulation_steps\n                \n                total_loss += loss.item() * gradient_accumulation_steps\n                \n                # Mixed Precision Backward Pass\n                if USE_AMP:\n                    scaler.scale(loss).backward()\n                else:\n                    loss.backward()\n                \n                # Gradient Accumulation - N stepë§ˆë‹¤ ì—…ë°ì´íŠ¸\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    if USE_AMP:\n                        scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['training'].get('max_grad_norm', 1.0))\n                    \n                    if USE_AMP:\n                        scaler.step(optimizer)\n                        scaler.update()\n                    else:\n                        optimizer.step()\n                    \n                    scheduler.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n                    \n                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n                    if global_step % 50 == 0:  # 50 stepë§ˆë‹¤\n                        clear_gpu_memory()\n                        \n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    logger.write(f\"\\nâš ï¸ OOM Error at step {step}! Clearing cache and skipping batch...\")\n                    clear_gpu_memory()\n                    optimizer.zero_grad()\n                    continue\n                else:\n                    raise e\n        \n        avg_loss = total_loss / len(train_loader)\n        logger.write(f\"  Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n        \n        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated(0) / 1024**3\n            reserved = torch.cuda.memory_reserved(0) / 1024**3\n            logger.write(f\"  GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB\")\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            output_dir = get_path(config['paths']['output_dir'])\n            output_dir.mkdir(parents=True, exist_ok=True)\n            model_path = output_dir / 'best_model_pipeline.pt'\n            \n            model_to_save = model.module if hasattr(model, 'module') else model\n            torch.save(model_to_save.state_dict(), model_path)\n            logger.write(f\"  âœ… Best model saved (loss: {best_loss:.4f})\")\n        \n        clear_gpu_memory()\n    \n    logger.write(f\"\\n{'='*70}\")\n    logger.write(f\"âœ… Training completed successfully!\")\n    logger.write(f\"Best loss: {best_loss:.4f}\")\n    logger.write(f\"Total training steps: {global_step}\")\n    logger.write(f\"{'='*70}\\n\")\n    \n    update_status('model_training', 'completed')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T02:53:05.978027Z",
     "iopub.status.busy": "2025-10-10T02:53:05.977925Z",
     "iopub.status.idle": "2025-10-10T02:53:06.016808Z",
     "shell.execute_reply": "2025-10-10T02:53:06.016366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 9: ìµœì¢… ì˜ˆì¸¡ ë° ì œì¶œ\n",
    "if 'final_prediction' in config['pipeline']['stages']:\n",
    "    update_status('final_prediction', 'running')\n",
    "    logger.write(\"\\n=== Final Prediction ===\")\n",
    "    \n",
    "    # í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜\n",
    "    def get_path(path_str):\n",
    "        \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "        path = Path(path_str)\n",
    "        if not path.is_absolute():\n",
    "            path = notebook_dir / path\n",
    "        return path\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    test_dataset = DialogueSummaryDataset(\n",
    "        test_df, tokenizer,\n",
    "        max_input_len=config['models']['primary_models'][0].get('max_input_length', 512),\n",
    "        max_target_len=config['models']['primary_models'][0].get('max_target_length', 128),\n",
    "        is_test=True\n",
    "    )\n",
    "    \n",
    "    # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì • - inference_optimizationì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    inference_batch_size = 8  # ê¸°ë³¸ê°’\n",
    "    if 'inference_optimization' in config:\n",
    "        if 'batch_inference' in config['inference_optimization']:\n",
    "            if config['inference_optimization']['batch_inference'].get('optimal_batch_size') == 'auto':\n",
    "                inference_batch_size = 8\n",
    "            else:\n",
    "                inference_batch_size = config['inference_optimization']['batch_inference'].get('optimal_batch_size', 8)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=inference_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "    # post_processing ì„¹ì…˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°\n",
    "    max_length = 150  # ê¸°ë³¸ê°’\n",
    "    if 'post_processing' in config:\n",
    "        if 'length_adjustment' in config['post_processing']:\n",
    "            max_length = config['post_processing']['length_adjustment'].get('max_length', 150)\n",
    "    \n",
    "    # hyperparameter_optimization search_spaceì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "    num_beams = 4  # ê¸°ë³¸ê°’\n",
    "    if 'hyperparameter_optimization' in config:\n",
    "        if 'search_space' in config['hyperparameter_optimization']:\n",
    "            num_beams_config = config['hyperparameter_optimization']['search_space'].get('num_beams', {})\n",
    "            if isinstance(num_beams_config, dict):\n",
    "                num_beams = num_beams_config.get('low', 4)  # low ê°’ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©\n",
    "    \n",
    "    # ì˜ˆì¸¡ ìƒì„±\n",
    "    logger.write(\"Generating predictions for test set...\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3  # ê¸°ë³¸ê°’\n",
    "            )\n",
    "            \n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    logger.write(f\"Generated {len(predictions)} predictions\")\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    # test_dfì˜ fname ì»¬ëŸ¼ ì‚¬ìš© (idê°€ ì•„ë‹˜)\n",
    "    submission_df = pd.DataFrame({\n",
    "        'fname': test_df['fname'],\n",
    "        'summary': predictions\n",
    "    })\n",
    "    \n",
    "    # ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "    submission_dir = get_path(config['paths']['submission_dir'])\n",
    "    submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    submission_path = submission_dir / f'full_pipeline_submission_{timestamp}.csv'\n",
    "    submission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=Trueë¡œ ë³€ê²½\n",
    "    \n",
    "    logger.write(f\"Submission file saved: {submission_path}\")\n",
    "    logger.write(f\"Shape: {submission_df.shape}\")\n",
    "    \n",
    "    update_status('final_prediction', 'completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}