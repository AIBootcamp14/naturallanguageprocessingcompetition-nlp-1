{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š K-Fold êµì°¨ ê²€ì¦ - 5-Fold Cross Validation\n",
    "> PRD ê³„íšì— ë”°ë¥¸ K-Fold êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì•ˆì •ì„± í‰ê°€\n",
    "\n",
    "**ëª©í‘œ ì„±ëŠ¥**: ROUGE-F1 72-75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/ieyeppo/AI_Lab/natural-language-processing-competition\n",
      "Current Dir: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent.parent.parent  # 3ë²ˆë§Œ parent ì‚¬ìš©!\n",
    "\n",
    "# ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²½ë¡œ ì œê±°í•˜ê³  í˜„ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë§Œ ì¶”ê°€\n",
    "sys.path = [p for p in sys.path if 'computer-vision-competition' not in p]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Dir: {notebook_dir}\")\n",
    "\n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "from src.logging.notebook_logger import NotebookLogger\n",
    "from src.utils.gpu_optimization.team_gpu_check import check_gpu_tier\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Splits: 5\n",
      "Model: upstage/SOLAR-10.7B-Instruct-v1.0\n",
      "Ensemble Method: weighted_average\n",
      "Save Each Fold: True\n"
     ]
    }
   ],
   "source": [
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "config_path = notebook_dir / 'configs' / 'config_kfold.yaml'\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"K-Fold Splits: {config['kfold']['n_splits']}\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Ensemble Method: {config['kfold']['ensemble_method']}\")\n",
    "print(f\"Save Each Fold: {config['kfold']['save_each_fold']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "K-Fold Cross Validation Experiment\n",
      "Timestamp: 20251010_090402\n",
      "Folds: 5\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "# configì˜ ë¡œê·¸ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì— log_dirì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "if 'log_dir' in config['paths']:\n",
    "    log_dir = get_path(config['paths']['log_dir'])\n",
    "else:\n",
    "    # ê¸°ë³¸ê°’: notebook_dir/logs/kfold\n",
    "    log_dir = notebook_dir / 'logs' / 'kfold'\n",
    "\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# ë¡œê±° ì´ˆê¸°í™”\n",
    "log_file = log_dir / f'kfold_{config[\"kfold\"][\"n_splits\"]}fold_{timestamp}.log'\n",
    "logger = NotebookLogger(\n",
    "    log_path=str(log_file),\n",
    "    print_also=True\n",
    ")\n",
    "\n",
    "logger.write('='*50)\n",
    "logger.write('K-Fold Cross Validation Experiment')\n",
    "logger.write(f'Timestamp: {timestamp}')\n",
    "logger.write(f'Folds: {config[\"kfold\"][\"n_splits\"]}')\n",
    "logger.write('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold configured:\n",
      "  - Splits: 5\n",
      "  - Shuffle: True\n",
      "  - Random State: 42\n"
     ]
    }
   ],
   "source": [
    "# K-Fold ì„¤ì •\n",
    "kfold = KFold(\n",
    "    n_splits=config['kfold']['n_splits'],\n",
    "    shuffle=config['kfold']['shuffle'],\n",
    "    random_state=config['kfold']['random_state']\n",
    ")\n",
    "\n",
    "logger.write(f\"KFold configured:\")\n",
    "logger.write(f\"  - Splits: {config['kfold']['n_splits']}\")\n",
    "logger.write(f\"  - Shuffle: {config['kfold']['shuffle']}\")\n",
    "logger.write(f\"  - Random State: {config['kfold']['random_state']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from config path:\n",
      "  - Train: /home/ieyeppo/AI_Lab/natural-language-processing-competition/notebooks/team/CHH/../../../data/raw/train.csv\n",
      "\n",
      "Data loaded: 12457 samples\n",
      "Fold 1: Train=9965, Val=2492\n",
      "Fold 1: Train=9965, Val=2492\n",
      "Fold 2: Train=9965, Val=2492\n",
      "Fold 2: Train=9965, Val=2492\n",
      "Fold 3: Train=9966, Val=2491\n",
      "Fold 3: Train=9966, Val=2491\n",
      "Fold 4: Train=9966, Val=2491\n",
      "Fold 4: Train=9966, Val=2491\n",
      "Fold 5: Train=9966, Val=2491\n",
      "Fold 5: Train=9966, Val=2491\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "# config íŒŒì¼ì˜ ê²½ë¡œ ì‚¬ìš©\n",
    "def get_data_path(path_str):\n",
    "    \"\"\"configì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    path = Path(path_str)\n",
    "    if not path.is_absolute():\n",
    "        path = notebook_dir / path\n",
    "    return path\n",
    "\n",
    "# configì—ì„œ ë°ì´í„° ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "train_path = get_data_path(config['paths']['train_file'])\n",
    "\n",
    "logger.write(f\"Loading data from config path:\")\n",
    "logger.write(f\"  - Train: {train_path}\")\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "logger.write(f\"\\nData loaded: {len(train_df)} samples\")\n",
    "\n",
    "# Foldë³„ ë°ì´í„° ë¶„í•  í™•ì¸\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df), 1):\n",
    "    logger.write(f\"Fold {fold_idx}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    print(f\"Fold {fold_idx}: Train={len(train_idx)}, Val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 16_ë°ì´í„°_í’ˆì§ˆ_ê²€ì¦_ì‹œìŠ¤í…œ.md)\nimport numpy as np\nfrom typing import Dict, List\n\nclass KFoldDataValidator:\n    \"\"\"K-Foldìš© ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"\n    \n    def __init__(self):\n        self.validation_results = []\n        \n    def validate_fold_distribution(self, train_df: pd.DataFrame, kfold) -> Dict:\n        \"\"\"Fold ë¶„í¬ ê²€ì¦\"\"\"\n        results = {}\n        \n        # ì£¼ì œ ë¶„í¬ í™•ì¸\n        if 'topic' in train_df.columns:\n            topic_counts = train_df['topic'].value_counts()\n            \n            # ê° foldì˜ ì£¼ì œ ë¶„í¬ í™•ì¸\n            fold_topic_distributions = []\n            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n                fold_train = train_df.iloc[train_idx]\n                fold_val = train_df.iloc[val_idx]\n                \n                train_topics = fold_train['topic'].value_counts(normalize=True)\n                val_topics = fold_val['topic'].value_counts(normalize=True)\n                \n                # ë¶„í¬ ì°¨ì´ ê³„ì‚° (KL divergence ê·¼ì‚¬)\n                topic_diff = 0\n                for topic in topic_counts.index:\n                    train_prop = train_topics.get(topic, 0)\n                    val_prop = val_topics.get(topic, 0)\n                    if train_prop > 0 and val_prop > 0:\n                        topic_diff += abs(train_prop - val_prop)\n                \n                fold_topic_distributions.append({\n                    'fold': fold_idx + 1,\n                    'distribution_diff': topic_diff,\n                    'train_unique_topics': len(train_topics),\n                    'val_unique_topics': len(val_topics)\n                })\n            \n            results['topic_distributions'] = fold_topic_distributions\n        \n        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ í™•ì¸\n        dialogue_lengths = train_df['dialogue'].str.len()\n        \n        fold_length_stats = []\n        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n            train_lengths = dialogue_lengths.iloc[train_idx]\n            val_lengths = dialogue_lengths.iloc[val_idx]\n            \n            fold_length_stats.append({\n                'fold': fold_idx + 1,\n                'train_mean_length': train_lengths.mean(),\n                'val_mean_length': val_lengths.mean(),\n                'length_diff': abs(train_lengths.mean() - val_lengths.mean())\n            })\n        \n        results['length_distributions'] = fold_length_stats\n        \n        # ë°ì´í„° ëˆ„ì¶œ ê²€ì‚¬\n        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n            train_set = set(train_idx)\n            val_set = set(val_idx)\n            \n            # ì¸ë±ìŠ¤ ì¤‘ë³µ ê²€ì‚¬\n            overlap = train_set.intersection(val_set)\n            if overlap:\n                logger.write(f\"âš ï¸ WARNING: Data leakage detected in fold {fold_idx+1}: {len(overlap)} overlapping indices\")\n                results['data_leakage'] = True\n            else:\n                results['data_leakage'] = False\n        \n        self.validation_results = results\n        return results\n    \n    def recommend_stratification(self) -> List[str]:\n        \"\"\"ì¸µí™” ì¶”ì²œ\"\"\"\n        recommendations = []\n        \n        if 'topic_distributions' in self.validation_results:\n            max_diff = max([f['distribution_diff'] for f in self.validation_results['topic_distributions']])\n            if max_diff > 0.2:\n                recommendations.append(\"Consider stratified K-Fold based on topic distribution\")\n        \n        if 'length_distributions' in self.validation_results:\n            max_length_diff = max([f['length_diff'] for f in self.validation_results['length_distributions']])\n            if max_length_diff > 500:\n                recommendations.append(\"Consider stratification based on text length\")\n        \n        if self.validation_results.get('data_leakage', False):\n            recommendations.append(\"CRITICAL: Fix data leakage issue immediately\")\n        \n        return recommendations\n\n# K-Fold ë°ì´í„° ê²€ì¦ ì‹¤í–‰\nkfold_validator = KFoldDataValidator()\nvalidation_results = kfold_validator.validate_fold_distribution(train_df, kfold)\n\nlogger.write(\"\\n=== K-Fold Data Validation ===\")\nif 'topic_distributions' in validation_results:\n    logger.write(\"\\nTopic Distribution Analysis:\")\n    for fold_stat in validation_results['topic_distributions'][:2]:  # ì²˜ìŒ 2ê°œ foldë§Œ ì¶œë ¥\n        logger.write(f\"  Fold {fold_stat['fold']}: Distribution diff={fold_stat['distribution_diff']:.3f}\")\n\nif 'length_distributions' in validation_results:\n    logger.write(\"\\nText Length Distribution:\")\n    for fold_stat in validation_results['length_distributions'][:2]:  # ì²˜ìŒ 2ê°œ foldë§Œ ì¶œë ¥\n        logger.write(f\"  Fold {fold_stat['fold']}: Mean length diff={fold_stat['length_diff']:.1f}\")\n\nrecommendations = kfold_validator.recommend_stratification()\nif recommendations:\n    logger.write(\"\\nğŸ“‹ Stratification Recommendations:\")\n    for rec in recommendations:\n        logger.write(f\"  â€¢ {rec}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solar API êµì°¨ ê²€ì¦ ì‹œìŠ¤í…œ (PRD 09_Solar_API_ìµœì í™”.md, 10_êµì°¨_ê²€ì¦_ì‹œìŠ¤í…œ.md)\nimport requests\nimport json\nfrom typing import Optional, Dict\n\nclass KFoldSolarValidator:\n    \"\"\"K-Foldìš© Solar API ê²€ì¦\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://api.upstage.ai/v1/solar\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        self.fold_comparisons = []\n        \n    def validate_fold_predictions(self, fold_idx: int, model_predictions: List[str], \n                                 test_dialogues: List[str], sample_size: int = 5) -> Dict:\n        \"\"\"Fold ì˜ˆì¸¡ ê²€ì¦\"\"\"\n        comparisons = []\n        \n        # ëœë¤ ìƒ˜í”Œ ì„ íƒ\n        sample_indices = np.random.choice(len(model_predictions), \n                                        min(sample_size, len(model_predictions)), \n                                        replace=False)\n        \n        for idx in sample_indices:\n            dialogue = test_dialogues[idx]\n            model_pred = model_predictions[idx]\n            \n            # Solar API ì˜ˆì¸¡\n            api_pred = self.generate_with_solar(dialogue)\n            \n            if api_pred:\n                # ê¸¸ì´ ë¹„êµ\n                model_len = len(model_pred)\n                api_len = len(api_pred)\n                \n                comparisons.append({\n                    'model_length': model_len,\n                    'api_length': api_len,\n                    'model_summary': model_pred[:100],\n                    'api_summary': api_pred[:100]\n                })\n        \n        result = {\n            'fold': fold_idx,\n            'comparisons': comparisons,\n            'avg_model_length': np.mean([c['model_length'] for c in comparisons]),\n            'avg_api_length': np.mean([c['api_length'] for c in comparisons])\n        }\n        \n        self.fold_comparisons.append(result)\n        return result\n    \n    def generate_with_solar(self, dialogue: str, max_tokens: int = 150) -> Optional[str]:\n        \"\"\"Solar APIë¡œ ìš”ì•½ ìƒì„±\"\"\"\n        try:\n            # í† í° ì ˆì•½ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì œí•œ\n            if len(dialogue) > 2000:\n                dialogue = dialogue[:2000] + \"...\"\n            \n            prompt = f\"\"\"ë‹¤ìŒ ëŒ€í™”ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:\n\n{dialogue}\n\nìš”ì•½:\"\"\"\n            \n            payload = {\n                \"model\": \"solar-1-mini-chat\",\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ëŒ€í™” ìš”ì•½ AIì…ë‹ˆë‹¤.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                \"max_tokens\": max_tokens,\n                \"temperature\": 0.3,\n                \"top_p\": 0.9\n            }\n            \n            response = requests.post(\n                f\"{self.base_url}/chat/completions\",\n                headers=self.headers,\n                json=payload,\n                timeout=30\n            )\n            \n            if response.status_code == 200:\n                result = response.json()\n                return result['choices'][0]['message']['content']\n            else:\n                logger.write(f\"Solar API error: {response.status_code}\")\n                return None\n                \n        except Exception as e:\n            logger.write(f\"Solar API exception: {e}\")\n            return None\n    \n    def get_fold_consensus(self) -> Dict:\n        \"\"\"Fold ê°„ ì¼ê´€ì„± ë¶„ì„\"\"\"\n        if not self.fold_comparisons:\n            return {}\n        \n        all_model_lengths = []\n        all_api_lengths = []\n        \n        for fold_comp in self.fold_comparisons:\n            all_model_lengths.append(fold_comp['avg_model_length'])\n            all_api_lengths.append(fold_comp['avg_api_length'])\n        \n        return {\n            'model_length_consistency': np.std(all_model_lengths),\n            'api_length_consistency': np.std(all_api_lengths),\n            'avg_model_vs_api_ratio': np.mean(all_model_lengths) / np.mean(all_api_lengths) if np.mean(all_api_lengths) > 0 else 0\n        }\n\n# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ (PRD 05_ë¦¬ìŠ¤í¬_ê´€ë¦¬.md)\nclass KFoldRiskManager:\n    \"\"\"K-Fold í•™ìŠµ ë¦¬ìŠ¤í¬ ê´€ë¦¬\"\"\"\n    \n    def __init__(self):\n        self.fold_risks = []\n        self.critical_risks = []\n        \n    def assess_fold_risk(self, fold_idx: int, fold_result: Dict) -> Dict:\n        \"\"\"Foldë³„ ë¦¬ìŠ¤í¬ í‰ê°€\"\"\"\n        risks = []\n        \n        # ì„±ëŠ¥ í¸ì°¨ ë¦¬ìŠ¤í¬\n        if 'best_rouge_l' in fold_result:\n            rouge_score = fold_result['best_rouge_l']\n            \n            if rouge_score < 0.3:\n                risks.append({\n                    'type': 'poor_performance',\n                    'severity': 'high',\n                    'fold': fold_idx,\n                    'metric': f'ROUGE-L: {rouge_score:.4f}',\n                    'mitigation': 'Check data quality, increase epochs, or adjust hyperparameters'\n                })\n            \n            # ì´ì „ foldì™€ ë¹„êµ\n            if self.fold_risks:\n                prev_scores = [f.get('rouge_score', 0) for f in self.fold_risks]\n                avg_prev = np.mean(prev_scores)\n                if avg_prev > 0 and abs(rouge_score - avg_prev) > 0.1:\n                    risks.append({\n                        'type': 'inconsistent_performance',\n                        'severity': 'medium',\n                        'fold': fold_idx,\n                        'metric': f'Deviation from avg: {abs(rouge_score - avg_prev):.4f}',\n                        'mitigation': 'Review fold data distribution and training process'\n                    })\n        \n        # í•™ìŠµ ì•ˆì •ì„± ë¦¬ìŠ¤í¬\n        if 'train_loss' in fold_result and 'val_loss' in fold_result:\n            train_loss = fold_result['train_loss']\n            val_loss = fold_result['val_loss']\n            \n            if val_loss > train_loss * 2:\n                risks.append({\n                    'type': 'severe_overfitting',\n                    'severity': 'critical',\n                    'fold': fold_idx,\n                    'metric': f'Val/Train ratio: {val_loss/train_loss:.2f}',\n                    'mitigation': 'Apply regularization, reduce model complexity, or use early stopping'\n                })\n        \n        # ë©”ëª¨ë¦¬ ë¦¬ìŠ¤í¬\n        if torch.cuda.is_available():\n            memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n            if memory_used > 0.85:\n                risks.append({\n                    'type': 'memory_pressure',\n                    'severity': 'medium',\n                    'fold': fold_idx,\n                    'metric': f'Memory usage: {memory_used:.1%}',\n                    'mitigation': 'Reduce batch size or enable gradient accumulation'\n                })\n        \n        # ë¦¬ìŠ¤í¬ ê¸°ë¡\n        fold_risk_summary = {\n            'fold': fold_idx,\n            'risks': risks,\n            'risk_count': len(risks),\n            'rouge_score': fold_result.get('best_rouge_l', 0)\n        }\n        \n        self.fold_risks.append(fold_risk_summary)\n        \n        # ì‹¬ê°í•œ ë¦¬ìŠ¤í¬ ì¶”ì \n        for risk in risks:\n            if risk['severity'] == 'critical':\n                self.critical_risks.append(risk)\n                logger.write(f\"âš ï¸ CRITICAL RISK in Fold {fold_idx}: {risk['type']} - {risk['metric']}\")\n        \n        return fold_risk_summary\n    \n    def get_overall_risk_assessment(self) -> Dict:\n        \"\"\"ì „ì²´ ë¦¬ìŠ¤í¬ í‰ê°€\"\"\"\n        if not self.fold_risks:\n            return {'status': 'no_data'}\n        \n        total_risks = sum([f['risk_count'] for f in self.fold_risks])\n        avg_risk_per_fold = total_risks / len(self.fold_risks)\n        \n        # Fold ê°„ ì„±ëŠ¥ í¸ì°¨\n        rouge_scores = [f['rouge_score'] for f in self.fold_risks if f['rouge_score'] > 0]\n        performance_variance = np.var(rouge_scores) if rouge_scores else 0\n        \n        assessment = {\n            'total_risks': total_risks,\n            'critical_risks': len(self.critical_risks),\n            'avg_risks_per_fold': avg_risk_per_fold,\n            'performance_variance': performance_variance,\n            'risk_level': 'low' if avg_risk_per_fold < 1 else 'medium' if avg_risk_per_fold < 2 else 'high'\n        }\n        \n        return assessment\n    \n    def suggest_improvements(self) -> List[str]:\n        \"\"\"ê°œì„  ì œì•ˆ\"\"\"\n        suggestions = []\n        \n        assessment = self.get_overall_risk_assessment()\n        \n        if assessment.get('performance_variance', 0) > 0.01:\n            suggestions.append(\"High variance between folds - consider stratified sampling\")\n        \n        if assessment.get('critical_risks', 0) > 0:\n            suggestions.append(\"Critical risks detected - immediate attention required\")\n        \n        if assessment.get('risk_level') == 'high':\n            suggestions.append(\"Overall risk level is high - review training configuration\")\n        \n        # ë¦¬ìŠ¤í¬ íƒ€ì…ë³„ ì œì•ˆ\n        risk_types = {}\n        for fold_risk in self.fold_risks:\n            for risk in fold_risk['risks']:\n                risk_type = risk['type']\n                if risk_type not in risk_types:\n                    risk_types[risk_type] = 0\n                risk_types[risk_type] += 1\n        \n        if 'severe_overfitting' in risk_types:\n            suggestions.append(\"Overfitting detected in multiple folds - increase regularization\")\n        \n        if 'memory_pressure' in risk_types:\n            suggestions.append(\"Memory issues detected - optimize batch size or use gradient accumulation\")\n        \n        return suggestions\n\n# Solar API ê²€ì¦ ì´ˆê¸°í™” (configì—ì„œ í‚¤ í™•ì¸)\nsolar_validator = None\nif 'solar_api' in config and 'api_key' in config['solar_api']:\n    solar_validator = KFoldSolarValidator(config['solar_api']['api_key'])\n    logger.write(\"Solar API validator initialized for K-Fold cross-validation\")\nelse:\n    logger.write(\"Solar API key not found - skipping API validation\")\n\n# ë¦¬ìŠ¤í¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”\nrisk_manager = KFoldRiskManager()\nlogger.write(\"K-Fold risk management system initialized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Tier: LOW\n",
      "Will clear GPU cache between folds\n"
     ]
    }
   ],
   "source": [
    "# GPU ì²´í¬\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tier = check_gpu_tier()\n",
    "    logger.write(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.write(f\"GPU Tier: {gpu_tier}\")\n",
    "    \n",
    "    if config['gpu']['empty_cache_between_folds']:\n",
    "        logger.write(\"Will clear GPU cache between folds\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, BartForConditionalGeneration\n\nclass DialogueSummaryDataset(Dataset):\n    \"\"\"ëŒ€í™” ìš”ì•½ ë°ì´í„°ì…‹\"\"\"\n    def __init__(self, dataframe, tokenizer, max_input_len=512, max_target_len=150, is_test=False):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n        self.is_test = is_test\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        dialogue = row['dialogue']\n        \n        # ì…ë ¥ í† í°í™”\n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_input_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if not self.is_test:\n            # í•™ìŠµ/ê²€ì¦ ëª¨ë“œ\n            summary = row['summary']\n            \n            # íƒ€ê²Ÿ í† í°í™”\n            targets = self.tokenizer(\n                summary,\n                max_length=self.max_target_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n            # ë¼ë²¨ ìƒì„± - íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ì¤‘ìš”!)\n            labels = targets['input_ids'].squeeze()\n            labels[labels == self.tokenizer.pad_token_id] = -100  # íŒ¨ë”© í† í° ë§ˆìŠ¤í‚¹\n            \n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'labels': labels\n            }\n        else:\n            # í…ŒìŠ¤íŠ¸ ëª¨ë“œ\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze()\n            }\n\n# ROUGE ìŠ¤ì½”ì–´ ê³„ì‚° í•¨ìˆ˜\nfrom rouge import Rouge\n\ndef compute_rouge_scores(predictions, references):\n    \"\"\"ROUGE ì ìˆ˜ ê³„ì‚°\"\"\"\n    rouge = Rouge()\n    \n    # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n    predictions = [p if p else \"ìš”ì•½ ì—†ìŒ\" for p in predictions]\n    references = [r if r else \"ìš”ì•½ ì—†ìŒ\" for r in references]\n    \n    try:\n        scores = rouge.get_scores(predictions, references, avg=True)\n        return {\n            'rouge-1': scores['rouge-1']['f'],\n            'rouge-2': scores['rouge-2']['f'],\n            'rouge-l': scores['rouge-l']['f']\n        }\n    except Exception as e:\n        logger.write(f\"ROUGE calculation error: {e}\")\n        return {\n            'rouge-1': 0.0,\n            'rouge-2': 0.0,\n            'rouge-l': 0.0\n        }\n\nlogger.write(\"Dataset class and ROUGE scorer defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# K-Fold í•™ìŠµ í•¨ìˆ˜ (PRD ì „ëµ í†µí•©)\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nimport gc\n\ndef train_fold_with_validation(model, train_loader, val_loader, tokenizer, fold_idx, config, \n                              solar_validator=None, risk_manager=None):\n    \"\"\"í–¥ìƒëœ Fold í•™ìŠµ (Solar API ê²€ì¦ ë° ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§ í¬í•¨)\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # learning_rate ë³€í™˜\n    learning_rate = config['training']['learning_rate']\n    if isinstance(learning_rate, str):\n        learning_rate = float(learning_rate)\n    \n    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n    optimizer = AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=config['training']['weight_decay']\n    )\n    \n    num_epochs = config['training']['num_epochs']\n    num_training_steps = num_epochs * len(train_loader)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    best_rouge_l = 0\n    best_epoch = 0\n    fold_history = {'train_loss': [], 'val_loss': [], 'rouge_l': [], 'risks': []}\n    \n    for epoch in range(num_epochs):\n        # í•™ìŠµ\n        model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Fold {fold_idx} Epoch {epoch+1}')\n        for batch in progress_bar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            progress_bar.set_postfix({'loss': loss.item()})\n            \n            # ë©”ëª¨ë¦¬ ê´€ë¦¬\n            if batch.get('__index', 0) % 50 == 0:\n                torch.cuda.empty_cache()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        fold_history['train_loss'].append(avg_train_loss)\n        \n        # ê²€ì¦\n        model.eval()\n        val_loss = 0\n        predictions = []\n        references = []\n        val_dialogues = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validating'):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                \n                # ì˜ˆì¸¡ ìƒì„±\n                generated_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=config['model']['max_target_length'],\n                    num_beams=config['evaluation']['num_beams'],\n                    early_stopping=True,\n                    no_repeat_ngram_size=config['evaluation'].get('no_repeat_ngram_size', 2),\n                    temperature=config['evaluation'].get('temperature', 1.0)\n                )\n                \n                preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n                refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n                \n                predictions.extend(preds)\n                references.extend(refs)\n                \n                # ëŒ€í™” ì›ë¬¸ ì €ì¥ (Solar API ë¹„êµìš©)\n                dialogues = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n                val_dialogues.extend(dialogues)\n        \n        avg_val_loss = val_loss / len(val_loader)\n        fold_history['val_loss'].append(avg_val_loss)\n        \n        # ROUGE ê³„ì‚°\n        rouge_scores = compute_rouge_scores(predictions[:100], references[:100])  # ìƒ˜í”Œë§Œ í‰ê°€\n        fold_history['rouge_l'].append(rouge_scores['rouge-l'])\n        \n        logger.write(f\"\\nFold {fold_idx} Epoch {epoch+1}:\")\n        logger.write(f\"  Train Loss: {avg_train_loss:.4f}\")\n        logger.write(f\"  Val Loss: {avg_val_loss:.4f}\")\n        logger.write(f\"  ROUGE-L: {rouge_scores['rouge-l']:.4f}\")\n        \n        # Solar API ê²€ì¦ (2 ì—í­ë§ˆë‹¤)\n        if solar_validator and epoch % 2 == 0 and len(predictions) > 0:\n            logger.write(f\"  ğŸ”„ Solar API validation...\")\n            solar_result = solar_validator.validate_fold_predictions(\n                fold_idx=fold_idx,\n                model_predictions=predictions[:10],\n                test_dialogues=val_dialogues[:10],\n                sample_size=3\n            )\n            \n            if solar_result and 'comparisons' in solar_result:\n                logger.write(f\"    Model avg length: {solar_result['avg_model_length']:.1f}\")\n                logger.write(f\"    API avg length: {solar_result['avg_api_length']:.1f}\")\n        \n        # ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§\n        if risk_manager:\n            fold_result = {\n                'best_rouge_l': rouge_scores['rouge-l'],\n                'train_loss': avg_train_loss,\n                'val_loss': avg_val_loss\n            }\n            \n            risk_assessment = risk_manager.assess_fold_risk(fold_idx, fold_result)\n            fold_history['risks'].append(risk_assessment)\n            \n            if risk_assessment['risk_count'] > 0:\n                logger.write(f\"  âš ï¸ {risk_assessment['risk_count']} risks detected\")\n                \n                # ìë™ ì™„í™” ì ìš©\n                for risk in risk_assessment['risks']:\n                    if risk['severity'] == 'critical':\n                        logger.write(f\"    Critical: {risk['type']}\")\n                        \n                        # í•™ìŠµë¥  ì¡°ì •\n                        if risk['type'] == 'severe_overfitting':\n                            for param_group in optimizer.param_groups:\n                                param_group['lr'] *= 0.5\n                            logger.write(f\"    â†’ Learning rate reduced\")\n        \n        # Best model ì¶”ì \n        if rouge_scores['rouge-l'] > best_rouge_l:\n            best_rouge_l = rouge_scores['rouge-l']\n            best_epoch = epoch\n            \n            # ëª¨ë¸ ì €ì¥ (ì„¤ì •ëœ ê²½ìš°)\n            if config['kfold']['save_each_fold']:\n                output_dir = get_path(config['paths']['output_dir'])\n                output_dir.mkdir(parents=True, exist_ok=True)\n                model_path = output_dir / f'fold_{fold_idx}_best_model.pt'\n                \n                torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'rouge_scores': rouge_scores,\n                    'epoch': epoch,\n                    'fold_history': fold_history\n                }, model_path)\n                \n                logger.write(f\"  âœ“ Saved best model for fold {fold_idx}\")\n        \n        # Early stopping\n        if config['training'].get('early_stopping_patience'):\n            if epoch - best_epoch >= config['training']['early_stopping_patience']:\n                logger.write(f\"  Early stopping triggered at epoch {epoch+1}\")\n                break\n    \n    return best_rouge_l, fold_history\n\nlogger.write(\"Enhanced train fold function defined with PRD strategies\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# WandB ì´ˆê¸°í™”\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=config['wandb']['name'],\n        tags=config['wandb']['tags'],\n        config=config\n    )\n    logger.write(\"WandB initialized for K-Fold experiment\")\n\n# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)\nlogger.write(f\"\\nInitializing model: {config['model']['name']}\")\ntokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\nlogger.write(\"Tokenizer loaded\")\n\n# K-Fold ë©”ì¸ í•™ìŠµ ë£¨í”„ (PRD ì „ëµ í†µí•©)\nall_fold_results = []\nall_fold_histories = []\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Starting K-Fold Cross Validation with PRD Strategies\")\nlogger.write(\"=\"*50)\n\nfor fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df), 1):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"FOLD {fold_idx}/{config['kfold']['n_splits']}\")\n    logger.write(f\"{'='*30}\")\n    \n    # GPU ìºì‹œ ì •ë¦¬\n    if config['gpu']['empty_cache_between_folds'] and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n        logger.write(\"GPU cache cleared\")\n    \n    # Fold ë°ì´í„° ë¶„í• \n    fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n    fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n    \n    logger.write(f\"Fold {fold_idx} data split: Train={len(fold_train_df)}, Val={len(fold_val_df)}\")\n    \n    # ë°ì´í„°ì…‹ ìƒì„±\n    train_dataset = DialogueSummaryDataset(\n        fold_train_df, \n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length']\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        fold_val_df,\n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length']\n    )\n    \n    # DataLoader ìƒì„±\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # ëª¨ë¸ ì´ˆê¸°í™” (ê° Foldë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸)\n    model = BartForConditionalGeneration.from_pretrained(config['model']['name'])\n    logger.write(f\"Model initialized for fold {fold_idx}\")\n    \n    # Fold í•™ìŠµ (í–¥ìƒëœ ë²„ì „ ì‚¬ìš©)\n    best_rouge_l, fold_history = train_fold_with_validation(\n        model, train_loader, val_loader, tokenizer, fold_idx, config,\n        solar_validator=solar_validator,  # Solar API ê²€ì¦\n        risk_manager=risk_manager  # ë¦¬ìŠ¤í¬ ê´€ë¦¬\n    )\n    \n    # ê²°ê³¼ ì €ì¥\n    fold_result = {\n        'fold': fold_idx,\n        'best_rouge_l': best_rouge_l,\n        'train_size': len(fold_train_df),\n        'val_size': len(fold_val_df),\n        'final_train_loss': fold_history['train_loss'][-1] if fold_history['train_loss'] else 0,\n        'final_val_loss': fold_history['val_loss'][-1] if fold_history['val_loss'] else 0,\n        'total_risks': sum([r['risk_count'] for r in fold_history.get('risks', [])])\n    }\n    all_fold_results.append(fold_result)\n    all_fold_histories.append(fold_history)\n    \n    logger.write(f\"\\nFold {fold_idx} completed:\")\n    logger.write(f\"  Best ROUGE-L: {best_rouge_l:.4f}\")\n    logger.write(f\"  Total risks encountered: {fold_result['total_risks']}\")\n    \n    # WandB ë¡œê¹…\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            f'fold_{fold_idx}_rouge_l': best_rouge_l,\n            f'fold_{fold_idx}_train_loss': fold_result['final_train_loss'],\n            f'fold_{fold_idx}_val_loss': fold_result['final_val_loss'],\n            f'fold_{fold_idx}_risks': fold_result['total_risks']\n        })\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    del model, train_dataset, val_dataset, train_loader, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# Solar API ì¼ê´€ì„± ë¶„ì„\nif solar_validator:\n    consensus = solar_validator.get_fold_consensus()\n    if consensus:\n        logger.write(\"\\n=== Solar API Consensus Analysis ===\")\n        logger.write(f\"Model length consistency (std): {consensus.get('model_length_consistency', 0):.2f}\")\n        logger.write(f\"API length consistency (std): {consensus.get('api_length_consistency', 0):.2f}\")\n        logger.write(f\"Model vs API ratio: {consensus.get('avg_model_vs_api_ratio', 1):.2f}\")\n\n# ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìµœì¢… í‰ê°€\nif risk_manager:\n    overall_risk = risk_manager.get_overall_risk_assessment()\n    logger.write(\"\\n=== Overall Risk Assessment ===\")\n    logger.write(f\"Total risks: {overall_risk.get('total_risks', 0)}\")\n    logger.write(f\"Critical risks: {overall_risk.get('critical_risks', 0)}\")\n    logger.write(f\"Risk level: {overall_risk.get('risk_level', 'unknown')}\")\n    \n    improvements = risk_manager.suggest_improvements()\n    if improvements:\n        logger.write(\"\\nğŸ“‹ Improvement Suggestions:\")\n        for suggestion in improvements:\n            logger.write(f\"  â€¢ {suggestion}\")\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"K-Fold Cross Validation with PRD Strategies Completed\")\nlogger.write(\"=\"*50)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# K-Fold í•™ìŠµ í•¨ìˆ˜\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nimport gc\n\ndef train_fold(model, train_loader, val_loader, tokenizer, fold_idx, config):\n    \"\"\"ë‹¨ì¼ Fold í•™ìŠµ\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # learning_rate ë³€í™˜\n    learning_rate = config['training']['learning_rate']\n    if isinstance(learning_rate, str):\n        learning_rate = float(learning_rate)\n    \n    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n    optimizer = AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=config['training']['weight_decay']\n    )\n    \n    num_epochs = config['training']['num_epochs']\n    num_training_steps = num_epochs * len(train_loader)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(num_training_steps * config['training']['warmup_ratio']),\n        num_training_steps=num_training_steps\n    )\n    \n    best_rouge_l = 0\n    best_epoch = 0\n    fold_history = {'train_loss': [], 'val_loss': [], 'rouge_l': []}\n    \n    for epoch in range(num_epochs):\n        # í•™ìŠµ\n        model.train()\n        total_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Fold {fold_idx} Epoch {epoch+1}')\n        for batch in progress_bar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            progress_bar.set_postfix({'loss': loss.item()})\n        \n        avg_train_loss = total_loss / len(train_loader)\n        fold_history['train_loss'].append(avg_train_loss)\n        \n        # ê²€ì¦\n        model.eval()\n        val_loss = 0\n        predictions = []\n        references = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validating'):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                \n                # ì˜ˆì¸¡ ìƒì„±\n                generated_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=config['model']['max_target_length'],\n                    num_beams=config['evaluation']['num_beams'],\n                    early_stopping=True\n                )\n                \n                preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n                refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n                \n                predictions.extend(preds)\n                references.extend(refs)\n        \n        avg_val_loss = val_loss / len(val_loader)\n        fold_history['val_loss'].append(avg_val_loss)\n        \n        # ROUGE ê³„ì‚°\n        rouge_scores = compute_rouge_scores(predictions[:100], references[:100])  # ìƒ˜í”Œë§Œ í‰ê°€\n        fold_history['rouge_l'].append(rouge_scores['rouge-l'])\n        \n        logger.write(f\"Fold {fold_idx} Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, ROUGE-L={rouge_scores['rouge-l']:.4f}\")\n        \n        # Best model ì¶”ì \n        if rouge_scores['rouge-l'] > best_rouge_l:\n            best_rouge_l = rouge_scores['rouge-l']\n            best_epoch = epoch\n            \n            # ëª¨ë¸ ì €ì¥ (ì„¤ì •ëœ ê²½ìš°)\n            if config['kfold']['save_each_fold']:\n                output_dir = get_path(config['paths']['output_dir'])\n                output_dir.mkdir(parents=True, exist_ok=True)\n                model_path = output_dir / f'fold_{fold_idx}_best_model.pt'\n                torch.save(model.state_dict(), model_path)\n                logger.write(f\"  Saved best model for fold {fold_idx}\")\n    \n    return best_rouge_l, fold_history\n\nlogger.write(\"Train fold function defined\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# WandB ì´ˆê¸°í™”\nif config['wandb']['mode'] != 'disabled':\n    wandb.init(\n        project=config['wandb']['project'],\n        entity=config['wandb']['entity'],\n        name=config['wandb']['name'],\n        tags=config['wandb']['tags'],\n        config=config\n    )\n    logger.write(\"WandB initialized for K-Fold experiment\")\n\n# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)\nlogger.write(f\"\\nInitializing model: {config['model']['name']}\")\ntokenizer = AutoTokenizer.from_pretrained(config['model']['name'])\nlogger.write(\"Tokenizer loaded\")\n\n# K-Fold ë©”ì¸ í•™ìŠµ ë£¨í”„\nall_fold_results = []\nall_fold_histories = []\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"Starting K-Fold Cross Validation\")\nlogger.write(\"=\"*50)\n\nfor fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_df), 1):\n    logger.write(f\"\\n{'='*30}\")\n    logger.write(f\"FOLD {fold_idx}/{config['kfold']['n_splits']}\")\n    logger.write(f\"{'='*30}\")\n    \n    # GPU ìºì‹œ ì •ë¦¬\n    if config['gpu']['empty_cache_between_folds'] and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n        logger.write(\"GPU cache cleared\")\n    \n    # Fold ë°ì´í„° ë¶„í• \n    fold_train_df = train_df.iloc[train_idx].reset_index(drop=True)\n    fold_val_df = train_df.iloc[val_idx].reset_index(drop=True)\n    \n    logger.write(f\"Fold {fold_idx} data split: Train={len(fold_train_df)}, Val={len(fold_val_df)}\")\n    \n    # ë°ì´í„°ì…‹ ìƒì„±\n    train_dataset = DialogueSummaryDataset(\n        fold_train_df, \n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length']\n    )\n    \n    val_dataset = DialogueSummaryDataset(\n        fold_val_df,\n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length']\n    )\n    \n    # DataLoader ìƒì„±\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['training']['batch_size'],\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # ëª¨ë¸ ì´ˆê¸°í™” (ê° Foldë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸)\n    model = BartForConditionalGeneration.from_pretrained(config['model']['name'])\n    logger.write(f\"Model initialized for fold {fold_idx}\")\n    \n    # Fold í•™ìŠµ\n    best_rouge_l, fold_history = train_fold(\n        model, train_loader, val_loader, tokenizer, fold_idx, config\n    )\n    \n    # ê²°ê³¼ ì €ì¥\n    fold_result = {\n        'fold': fold_idx,\n        'best_rouge_l': best_rouge_l,\n        'train_size': len(fold_train_df),\n        'val_size': len(fold_val_df)\n    }\n    all_fold_results.append(fold_result)\n    all_fold_histories.append(fold_history)\n    \n    logger.write(f\"\\nFold {fold_idx} completed - Best ROUGE-L: {best_rouge_l:.4f}\")\n    \n    # WandB ë¡œê¹…\n    if config['wandb']['mode'] != 'disabled':\n        wandb.log({\n            f'fold_{fold_idx}_rouge_l': best_rouge_l,\n            f'fold_{fold_idx}_train_loss': fold_history['train_loss'][-1],\n            f'fold_{fold_idx}_val_loss': fold_history['val_loss'][-1]\n        })\n    \n    # ë©”ëª¨ë¦¬ ì •ë¦¬\n    del model, train_dataset, val_dataset, train_loader, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"K-Fold Cross Validation Completed\")\nlogger.write(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "source": "## K-Fold ê²°ê³¼ ë¶„ì„ ë° ì•™ìƒë¸”",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# K-Fold ê²°ê³¼ ë¶„ì„\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"K-FOLD RESULTS ANALYSIS\")\nlogger.write(\"=\"*50)\n\n# ê²°ê³¼ DataFrame ìƒì„±\nresults_df = pd.DataFrame(all_fold_results)\nlogger.write(\"\\nFold Results:\")\nlogger.write(results_df.to_string())\n\n# í†µê³„ ê³„ì‚°\nmean_rouge = results_df['best_rouge_l'].mean()\nstd_rouge = results_df['best_rouge_l'].std()\nmin_rouge = results_df['best_rouge_l'].min()\nmax_rouge = results_df['best_rouge_l'].max()\n\nlogger.write(f\"\\n{'='*30}\")\nlogger.write(\"ROUGE-L Statistics:\")\nlogger.write(f\"  Mean: {mean_rouge:.4f}\")\nlogger.write(f\"  Std:  {std_rouge:.4f}\")\nlogger.write(f\"  Min:  {min_rouge:.4f}\")\nlogger.write(f\"  Max:  {max_rouge:.4f}\")\nlogger.write(f\"  95% CI: [{mean_rouge - 1.96*std_rouge:.4f}, {mean_rouge + 1.96*std_rouge:.4f}]\")\nlogger.write(f\"{'='*30}\")\n\n# WandB ë¡œê¹…\nif config['wandb']['mode'] != 'disabled':\n    wandb.log({\n        'kfold_mean_rouge_l': mean_rouge,\n        'kfold_std_rouge_l': std_rouge,\n        'kfold_min_rouge_l': min_rouge,\n        'kfold_max_rouge_l': max_rouge\n    })",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì‹œê°í™”\nfrom src.utils.visualizations.training_viz import TrainingVisualizer\n\nviz = TrainingVisualizer()\n\n# ì‹œê°í™” ì €ì¥ ê²½ë¡œ\nviz_dir = get_path(config.get('paths', {}).get('visualization_dir', 'visualizations'))\nviz_dir.mkdir(parents=True, exist_ok=True)\n\n# K-Fold ê²°ê³¼ ì‹œê°í™”\nif len(all_fold_results) > 0:\n    # Foldë³„ ROUGE-L ì ìˆ˜ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n    fold_rouge_scores = [{'rouge-l': r['best_rouge_l']} for r in all_fold_results]\n    \n    # K-Fold ê²°ê³¼ í”Œë¡¯\n    viz.plot_kfold_results(\n        fold_rouge_scores,\n        save_path=viz_dir / f'kfold_{config[\"kfold\"][\"n_splits\"]}fold_results.png'\n    )\n    \n    logger.write(f\"Visualization saved to {viz_dir}\")\n\n# ê²°ê³¼ ì €ì¥ (JSON)\nimport json\n\nresults_summary = {\n    'config': config,\n    'fold_results': all_fold_results,\n    'statistics': {\n        'mean_rouge_l': float(mean_rouge),\n        'std_rouge_l': float(std_rouge),\n        'min_rouge_l': float(min_rouge),\n        'max_rouge_l': float(max_rouge)\n    },\n    'timestamp': timestamp\n}\n\nresults_path = log_dir / f'kfold_results_{timestamp}.json'\nwith open(results_path, 'w', encoding='utf-8') as f:\n    json.dump(results_summary, f, indent=4, ensure_ascii=False)\n\nlogger.write(f\"\\nResults saved to {results_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± (ì„¤ì •ëœ ê²½ìš°)\nif config['kfold'].get('generate_ensemble_predictions', False):\n    logger.write(\"\\n\" + \"=\"*50)\n    logger.write(\"GENERATING ENSEMBLE PREDICTIONS\")\n    logger.write(\"=\"*50)\n    \n    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n    test_path = get_data_path(config['paths']['test_file'])\n    test_df = pd.read_csv(test_path)\n    logger.write(f\"Test data loaded: {len(test_df)} samples\")\n    \n    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\n    test_dataset = DialogueSummaryDataset(\n        test_df,\n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length'],\n        is_test=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['inference']['batch_size'],\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # ê° Fold ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±\n    all_fold_predictions = []\n    \n    output_dir = get_path(config['paths']['output_dir'])\n    \n    for fold_idx in range(1, config['kfold']['n_splits'] + 1):\n        model_path = output_dir / f'fold_{fold_idx}_best_model.pt'\n        \n        if model_path.exists():\n            logger.write(f\"\\nLoading fold {fold_idx} model...\")\n            \n            # ëª¨ë¸ ë¡œë“œ\n            model = BartForConditionalGeneration.from_pretrained(config['model']['name'])\n            model.load_state_dict(torch.load(model_path))\n            model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n            model.eval()\n            \n            # ì˜ˆì¸¡ ìƒì„±\n            fold_predictions = []\n            \n            with torch.no_grad():\n                for batch in tqdm(test_loader, desc=f'Fold {fold_idx} predictions'):\n                    input_ids = batch['input_ids'].to(model.device)\n                    attention_mask = batch['attention_mask'].to(model.device)\n                    \n                    generated_ids = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        max_length=config['inference']['max_length'],\n                        num_beams=config['inference']['num_beams'],\n                        early_stopping=config['inference']['early_stopping'],\n                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size']\n                    )\n                    \n                    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n                    fold_predictions.extend(preds)\n            \n            all_fold_predictions.append(fold_predictions)\n            logger.write(f\"  Generated {len(fold_predictions)} predictions\")\n            \n            # ë©”ëª¨ë¦¬ ì •ë¦¬\n            del model\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    # ì•™ìƒë¸” (ê°„ë‹¨í•œ íˆ¬í‘œ ë°©ì‹)\n    if len(all_fold_predictions) > 0:\n        logger.write(f\"\\nEnsembling {len(all_fold_predictions)} fold predictions...\")\n        \n        ensemble_predictions = []\n        for i in range(len(test_df)):\n            # ê° foldì˜ ì˜ˆì¸¡ ìˆ˜ì§‘\n            fold_preds = [fold_pred[i] for fold_pred in all_fold_predictions]\n            \n            # ê°€ì¥ ë¹ˆë²ˆí•œ ì˜ˆì¸¡ ì„ íƒ (ê°„ë‹¨í•œ íˆ¬í‘œ)\n            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•™ìƒë¸” ë°©ë²• ì‚¬ìš© ê°€ëŠ¥\n            from collections import Counter\n            most_common = Counter(fold_preds).most_common(1)[0][0]\n            ensemble_predictions.append(most_common)\n        \n        # ì œì¶œ íŒŒì¼ ìƒì„±\n        submission_df = pd.DataFrame({\n            'fname': test_df['fname'],\n            'summary': ensemble_predictions\n        })\n        \n        submission_dir = get_path(config['paths']['submission_dir'])\n        submission_dir.mkdir(parents=True, exist_ok=True)\n        \n        submission_path = submission_dir / f'kfold_ensemble_submission_{timestamp}.csv'\n        # index=Trueë¡œ ì„¤ì •í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ í¬í•¨ì‹œí‚´\n        submission_df.to_csv(submission_path, index=True, encoding='utf-8')  # index=False -> index=Trueë¡œ ë³€ê²½\n        \n        logger.write(f\"\\nEnsemble submission saved to {submission_path}\")\n        logger.write(f\"Shape: {submission_df.shape}\")\n    else:\n        logger.write(\"\\nNo fold models found for ensemble prediction\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ì•™ìƒë¸” ì˜ˆì¸¡ ìƒì„± (ì„¤ì •ëœ ê²½ìš°)\nif config['kfold'].get('generate_ensemble_predictions', False):\n    logger.write(\"\\n\" + \"=\"*50)\n    logger.write(\"GENERATING ENSEMBLE PREDICTIONS\")\n    logger.write(\"=\"*50)\n    \n    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n    test_path = get_data_path(config['paths']['test_file'])\n    test_df = pd.read_csv(test_path)\n    logger.write(f\"Test data loaded: {len(test_df)} samples\")\n    \n    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹\n    test_dataset = DialogueSummaryDataset(\n        test_df,\n        tokenizer,\n        max_input_len=config['model']['max_input_length'],\n        max_target_len=config['model']['max_target_length'],\n        is_test=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['inference']['batch_size'],\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    # ê° Fold ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±\n    all_fold_predictions = []\n    \n    output_dir = get_path(config['paths']['output_dir'])\n    \n    for fold_idx in range(1, config['kfold']['n_splits'] + 1):\n        model_path = output_dir / f'fold_{fold_idx}_best_model.pt'\n        \n        if model_path.exists():\n            logger.write(f\"\\nLoading fold {fold_idx} model...\")\n            \n            # ëª¨ë¸ ë¡œë“œ\n            model = BartForConditionalGeneration.from_pretrained(config['model']['name'])\n            model.load_state_dict(torch.load(model_path))\n            model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n            model.eval()\n            \n            # ì˜ˆì¸¡ ìƒì„±\n            fold_predictions = []\n            \n            with torch.no_grad():\n                for batch in tqdm(test_loader, desc=f'Fold {fold_idx} predictions'):\n                    input_ids = batch['input_ids'].to(model.device)\n                    attention_mask = batch['attention_mask'].to(model.device)\n                    \n                    generated_ids = model.generate(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        max_length=config['inference']['max_length'],\n                        num_beams=config['inference']['num_beams'],\n                        early_stopping=config['inference']['early_stopping'],\n                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size']\n                    )\n                    \n                    preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n                    fold_predictions.extend(preds)\n            \n            all_fold_predictions.append(fold_predictions)\n            logger.write(f\"  Generated {len(fold_predictions)} predictions\")\n            \n            # ë©”ëª¨ë¦¬ ì •ë¦¬\n            del model\n            torch.cuda.empty_cache()\n            gc.collect()\n    \n    # ì•™ìƒë¸” (ê°„ë‹¨í•œ íˆ¬í‘œ ë°©ì‹)\n    if len(all_fold_predictions) > 0:\n        logger.write(f\"\\nEnsembling {len(all_fold_predictions)} fold predictions...\")\n        \n        ensemble_predictions = []\n        for i in range(len(test_df)):\n            # ê° foldì˜ ì˜ˆì¸¡ ìˆ˜ì§‘\n            fold_preds = [fold_pred[i] for fold_pred in all_fold_predictions]\n            \n            # ê°€ì¥ ë¹ˆë²ˆí•œ ì˜ˆì¸¡ ì„ íƒ (ê°„ë‹¨í•œ íˆ¬í‘œ)\n            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•™ìƒë¸” ë°©ë²• ì‚¬ìš© ê°€ëŠ¥\n            from collections import Counter\n            most_common = Counter(fold_preds).most_common(1)[0][0]\n            ensemble_predictions.append(most_common)\n        \n        # ì œì¶œ íŒŒì¼ ìƒì„±\n        submission_df = pd.DataFrame({\n            'id': test_df['id'],\n            'summary': ensemble_predictions\n        })\n        \n        submission_dir = get_path(config['paths']['submission_dir'])\n        submission_dir.mkdir(parents=True, exist_ok=True)\n        \n        submission_path = submission_dir / f'kfold_ensemble_submission_{timestamp}.csv'\n        submission_df.to_csv(submission_path, index=False, encoding='utf-8')\n        \n        logger.write(f\"\\nEnsemble submission saved to {submission_path}\")\n        logger.write(f\"Shape: {submission_df.shape}\")\n    else:\n        logger.write(\"\\nNo fold models found for ensemble prediction\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ìµœì¢… ìš”ì•½\nlogger.write(\"\\n\" + \"=\"*50)\nlogger.write(\"K-FOLD CROSS VALIDATION SUMMARY\")\nlogger.write(\"=\"*50)\nlogger.write(f\"Model: {config['model']['name']}\")\nlogger.write(f\"Folds: {config['kfold']['n_splits']}\")\nlogger.write(f\"Mean ROUGE-L: {mean_rouge:.4f} (Â±{std_rouge:.4f})\")\nlogger.write(f\"Best Fold: {results_df.loc[results_df['best_rouge_l'].idxmax(), 'fold']}\")\nlogger.write(f\"Best Score: {max_rouge:.4f}\")\nlogger.write(f\"Worst Fold: {results_df.loc[results_df['best_rouge_l'].idxmin(), 'fold']}\")\nlogger.write(f\"Worst Score: {min_rouge:.4f}\")\n\nif config['kfold']['save_each_fold']:\n    logger.write(f\"\\nFold models saved to: {output_dir}\")\n\nlogger.write(f\"\\nLog file: {log_file}\")\nlogger.write(\"=\"*50)\n\n# WandB ì¢…ë£Œ\nif config['wandb']['mode'] != 'disabled':\n    wandb.summary['kfold_final_mean_rouge'] = mean_rouge\n    wandb.summary['kfold_final_std_rouge'] = std_rouge\n    wandb.finish()\n\nlogger.write(\"\\nâœ… K-Fold Cross Validation completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}