2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_setup.py:_flush():81] Current SDK version is 0.22.2
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_setup.py:_flush():81] Configure stats pid to 38961
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_setup.py:_flush():81] Loading settings from /home/ieyeppo/.config/wandb/settings
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_setup.py:_flush():81] Loading settings from /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/settings
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_init.py:setup_run_log_directory():705] Logging user logs to /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_214600-vrzzcda9/logs/debug.log
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_init.py:setup_run_log_directory():706] Logging internal logs to /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_214600-vrzzcda9/logs/debug-internal.log
2025-10-12 21:46:00,989 INFO    MainThread:38961 [wandb_init.py:init():832] calling init triggers
2025-10-12 21:46:00,990 INFO    MainThread:38961 [wandb_init.py:init():837] wandb.init called with sweep_config: {}
config: {'experiment': {'name': 'llama_3.2_3b_qlora', 'seed': 42, 'deterministic': True, 'use_wandb': False, 'wandb_project': 'nlp-competition', 'wandb_entity': None, 'wandb_tags': [], 'tags': ['llama', '3b', 'qlora', 'korean']}, 'paths': {'train_data': 'data/raw/train.csv', 'dev_data': 'data/raw/dev.csv', 'test_data': 'data/raw/test.csv', 'output_dir': 'outputs', 'model_save_dir': 'models', 'submission_dir': 'submissions'}, 'training': {'output_dir': 'experiments/20251012/20251012_214256_test_full_pipeline_quick/model_1_llama_3.2_korean_3b', 'epochs': 1, 'batch_size': 8, 'learning_rate': 5e-06, 'weight_decay': 0.1, 'warmup_steps': 500, 'save_total_limit': 2, 'logging_steps': 10, 'num_workers': 4, 'early_stopping_patience': 3, 'device': 'cuda', 'gradient_accumulation_steps': 8, 'lr_scheduler_type': 'cosine', 'warmup_ratio': 0.1, 'max_grad_norm': 1.2, 'gradient_checkpointing': True, 'fp16': False, 'bf16': False}, 'logging': {'log_level': 'INFO', 'log_dir': 'logs', 'save_steps': 100, 'logging_steps': 10}, 'strategies': {'data_augmentation': False, 'cross_validation': False, 'ensemble': False, 'optuna': False}, 'evaluation': {'metric': 'rouge', 'rouge_types': ['rouge1', 'rouge2', 'rougeL'], 'use_stemmer': False, 'tokenizer': 'mecab'}, 'debug': {'use_subset': False, 'subset_size': 100, 'print_sample_predictions': True, 'num_samples_to_print': 5}, 'model': {'type': 'causal_lm', 'checkpoint': 'Bllossom/llama-3.2-Korean-Bllossom-3B', 'lora': {'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}, 'size': '3B', 'dtype': 'fp16', 'chat_template': 'llama'}, 'tokenizer': {'encoder_max_len': 1024, 'decoder_max_len': 200}, 'inference': {'batch_size': 8, 'generate_max_length': 100, 'temperature': 0.7, 'do_sample': True, 'top_p': 0.9, 'top_k': 50, 'num_beams': 1}, 'dataset': {'format_type': 'chat', 'use_instruction_augmentation': False}, 'wandb': {'enabled': True, 'project': 'nlp-competition', 'entity': 'ieyeppo'}, '_base_': '../base/causal_lm.yaml', 'lora': {'r': 16, 'alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], 'dropout': 0.05, 'use_qlora': True}, '_wandb': {}}
2025-10-12 21:46:00,990 INFO    MainThread:38961 [wandb_init.py:init():880] starting backend
2025-10-12 21:46:01,196 INFO    MainThread:38961 [wandb_init.py:init():883] sending inform_init request
2025-10-12 21:46:01,201 INFO    MainThread:38961 [wandb_init.py:init():891] backend started and connected
2025-10-12 21:46:01,203 INFO    MainThread:38961 [wandb_init.py:init():961] updated telemetry
2025-10-12 21:46:01,206 INFO    MainThread:38961 [wandb_init.py:init():985] communicating run to backend with 90.0 second timeout
