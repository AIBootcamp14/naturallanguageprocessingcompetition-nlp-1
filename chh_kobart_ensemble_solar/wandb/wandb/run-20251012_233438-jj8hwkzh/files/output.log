2025-10-12 23:34:39 | wandb: Detected [openai] in use.
2025-10-12 23:34:39 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:34:39 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:34:39 | 📋 실험명: 1012-2334-llama_3.2_3b_qlora
2025-10-12 23:34:39 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jj8hwkzh
2025-10-12 23:34:39 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:34:39 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:34:39 | 학습 진행 중...
2025-10-12 23:34:39 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:34:39 | 0%|          | 0/250 [00:00<?, ?it/s]
2025-10-12 23:34:46 | 1%|          | 2/250 [00:06<13:55,  3.37s/it]
2025-10-12 23:34:52 | 2%|▏         | 4/250 [00:12<12:56,  3.16s/it]
2025-10-12 23:35:03 | 3%|▎         | 7/250 [00:23<13:46,  3.40s/it]
2025-10-12 23:35:09 | 4%|▎         | 9/250 [00:30<13:40,  3.41s/it]
2025-10-12 23:35:13 | {'loss': 1.6234, 'grad_norm': 2.0193655490875244, 'learning_rate': 9e-08, 'epoch': 0.04}
2025-10-12 23:35:13 | 4%|▍         | 10/250 [00:33<13:26,  3.36s/it]
2025-10-12 23:35:19 | 5%|▍         | 12/250 [00:39<12:54,  3.25s/it]
2025-10-12 23:35:25 | 6%|▌         | 14/250 [00:46<12:39,  3.22s/it]
2025-10-12 23:35:36 | 7%|▋         | 17/250 [00:56<13:17,  3.42s/it]
2025-10-12 23:35:42 | 8%|▊         | 19/250 [01:03<12:39,  3.29s/it]
2025-10-12 23:35:46 | {'loss': 1.5487, 'grad_norm': 2.1541213989257812, 'learning_rate': 1.9e-07, 'epoch': 0.08}
2025-10-12 23:35:46 | 8%|▊         | 20/250 [01:06<12:34,  3.28s/it]
2025-10-12 23:35:52 | 9%|▉         | 22/250 [01:12<12:16,  3.23s/it]
2025-10-12 23:35:58 | 10%|▉         | 24/250 [01:19<12:02,  3.20s/it]
2025-10-12 23:36:09 | 11%|█         | 27/250 [01:29<12:48,  3.45s/it]
2025-10-12 23:36:15 | 12%|█▏        | 29/250 [01:36<12:03,  3.27s/it]
2025-10-12 23:36:18 | {'loss': 1.5919, 'grad_norm': 2.1342108249664307, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.12}
2025-10-12 23:36:18 | 12%|█▏        | 30/250 [01:39<11:48,  3.22s/it]
2025-10-12 23:36:24 | 13%|█▎        | 32/250 [01:45<11:12,  3.08s/it]
2025-10-12 23:36:30 | 14%|█▎        | 34/250 [01:50<10:46,  2.99s/it]
2025-10-12 23:36:40 | 15%|█▍        | 37/250 [02:00<11:31,  3.25s/it]
2025-10-12 23:36:46 | 16%|█▌        | 39/250 [02:06<10:53,  3.10s/it]
2025-10-12 23:36:49 | {'loss': 1.6301, 'grad_norm': 2.168320655822754, 'learning_rate': 3.9e-07, 'epoch': 0.16}
2025-10-12 23:36:49 | 16%|█▌        | 40/250 [02:09<10:34,  3.02s/it]
2025-10-12 23:36:55 | 17%|█▋        | 42/250 [02:15<10:15,  2.96s/it]
2025-10-12 23:37:00 | 18%|█▊        | 44/250 [02:21<10:01,  2.92s/it]
2025-10-12 23:37:09 | 19%|█▉        | 47/250 [02:29<09:52,  2.92s/it]
2025-10-12 23:37:16 | 20%|█▉        | 49/250 [02:37<10:48,  3.23s/it]
2025-10-12 23:37:20 | {'loss': 1.624, 'grad_norm': 2.1325876712799072, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.2}
2025-10-12 23:37:20 | 20%|██        | 50/250 [02:40<11:04,  3.32s/it]
2025-10-12 23:37:26 | 21%|██        | 52/250 [02:47<10:56,  3.32s/it]
2025-10-12 23:37:33 | 22%|██▏       | 54/250 [02:53<10:29,  3.21s/it]
2025-10-12 23:37:42 | 23%|██▎       | 57/250 [03:03<10:26,  3.25s/it]
2025-10-12 23:37:50 | 24%|██▎       | 59/250 [03:10<11:19,  3.56s/it]
2025-10-12 23:37:53 | {'loss': 1.5523, 'grad_norm': 1.8329565525054932, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.24}
2025-10-12 23:37:53 | 24%|██▍       | 60/250 [03:13<10:45,  3.40s/it]
2025-10-12 23:37:59 | 25%|██▍       | 62/250 [03:19<10:01,  3.20s/it]
2025-10-12 23:38:05 | 26%|██▌       | 64/250 [03:26<09:38,  3.11s/it]
2025-10-12 23:38:14 | 27%|██▋       | 67/250 [03:34<09:10,  3.01s/it]
2025-10-12 23:38:22 | 28%|██▊       | 69/250 [03:42<10:03,  3.33s/it]
2025-10-12 23:38:25 | {'loss': 1.6085, 'grad_norm': 2.066457509994507, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.28}
2025-10-12 23:38:25 | 28%|██▊       | 70/250 [03:45<10:01,  3.34s/it]
2025-10-12 23:38:31 | 29%|██▉       | 72/250 [03:51<09:25,  3.18s/it]
2025-10-12 23:38:37 | 30%|██▉       | 74/250 [03:57<08:53,  3.03s/it]
2025-10-12 23:38:46 | 31%|███       | 77/250 [04:06<08:37,  2.99s/it]
2025-10-12 23:38:53 | 32%|███▏      | 79/250 [04:13<09:29,  3.33s/it]
2025-10-12 23:38:56 | {'loss': 1.5791, 'grad_norm': 2.310434341430664, 'learning_rate': 7.900000000000001e-07, 'epoch': 0.32}
2025-10-12 23:38:56 | 32%|███▏      | 80/250 [04:17<09:30,  3.36s/it]
2025-10-12 23:39:03 | 33%|███▎      | 82/250 [04:23<09:12,  3.29s/it]
2025-10-12 23:39:08 | 34%|███▎      | 84/250 [04:29<08:24,  3.04s/it]
2025-10-12 23:39:17 | 35%|███▍      | 87/250 [04:37<07:50,  2.89s/it]
2025-10-12 23:39:23 | 36%|███▌      | 89/250 [04:43<07:53,  2.94s/it]
2025-10-12 23:39:27 | {'loss': 1.5767, 'grad_norm': 1.8840872049331665, 'learning_rate': 8.900000000000001e-07, 'epoch': 0.36}
2025-10-12 23:39:27 | 36%|███▌      | 90/250 [04:47<08:49,  3.31s/it]
