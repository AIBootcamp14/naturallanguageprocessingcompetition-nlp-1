_wandb:
    value:
        cli_version: 0.22.2
        e:
            s9u7mggi9e7tm2231u8ij3dgl3n6dqtv:
                args:
                    - --mode
                    - kfold
                    - --models
                    - kobart
                    - --epochs
                    - "10"
                    - --batch_size
                    - "16"
                    - --gradient_accumulation_steps
                    - "10"
                    - --learning_rate
                    - "9.14e-5"
                    - --warmup_ratio
                    - "0.00136"
                    - --weight_decay
                    - "0.0995"
                    - --scheduler_type
                    - cosine
                    - --max_grad_norm
                    - "1.0"
                    - --label_smoothing
                    - "0.1"
                    - --use_augmentation
                    - --augmentation_ratio
                    - "0.5"
                    - --augmentation_methods
                    - back_translation
                    - paraphrase
                    - --k_folds
                    - "5"
                    - --experiment_name
                    - kobart_balanced_kfold
                    - --seed
                    - "42"
                    - --resume
                codePath: scripts/train.py
                codePathLocal: scripts/train.py
                cpu_count: 10
                cpu_count_logical: 20
                cudaVersion: "12.9"
                disk:
                    /:
                        total: "1081101176832"
                        used: "67000647680"
                email: ieyeppo.job@gmail.com
                executable: /home/ieyeppo/.pyenv/versions/nlp3_11_9/bin/python
                git:
                    commit: 21356f862e0d268091fcb65b569d913cbc51282d
                    remote: git@github.com:iejob/natural-language-processing-competition.git
                gpu: NVIDIA GeForce RTX 3080 Ti Laptop GPU
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      memoryTotal: "17179869184"
                      name: NVIDIA GeForce RTX 3080 Ti Laptop GPU
                      uuid: GPU-5d8a4483-7097-1ed5-e08d-d7cca05fc40e
                host: ZONYEKONGZEE
                memory:
                    total: "16606470144"
                os: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39
                program: /home/ieyeppo/AI_Lab/natural-language-processing-competition/scripts/train.py
                python: CPython 3.11.9
                root: /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb
                startedAt: "2025-10-14T16:29:28.985883Z"
                writerId: s9u7mggi9e7tm2231u8ij3dgl3n6dqtv
        m: []
        python_version: 3.11.9
        t:
            "1":
                - 1
            "2":
                - 1
                - 5
                - 11
                - 35
                - 49
                - 53
                - 71
                - 95
                - 98
            "3":
                - 2
                - 13
                - 15
                - 16
            "4": 3.11.9
            "5": 0.22.2
            "10":
                - 20
            "12": 0.22.2
            "13": linux-x86_64
augmentation_methods:
    value:
        - back_translation
        - paraphrase
augmentation_ratio:
    value: 0.5
batch_size:
    value: 16
config:
    value: configs/train_config.yaml
correction_models:
    value:
        - gogamza/kobart-base-v2
        - digit82/kobart-summarization
correction_strategy:
    value: quality_based
correction_threshold:
    value: 0.3
debug:
    value: false
dev_data:
    value: data/raw/dev.csv
ensemble_strategy:
    value: weighted_avg
ensemble_weights:
    value: null
epochs:
    value: 10
experiment_name:
    value: kobart_balanced_kfold
fold_seed:
    value: 42
gradient_accumulation_steps:
    value: 10
ignore_checkpoint:
    value: false
k_folds:
    value: 5
label_smoothing:
    value: 0.1
learning_rate:
    value: 9.14e-05
length_penalty:
    value: null
lora_rank:
    value: 16
max_grad_norm:
    value: 1
max_new_tokens:
    value: null
max_train_samples:
    value: null
min_new_tokens:
    value: null
mode:
    value: kfold
models:
    value:
        - kobart
no_repeat_ngram_size:
    value: null
no_wandb:
    value: false
num_beams:
    value: null
optimization_method:
    value: quantization
optimize_inference:
    value: false
optuna_pruner:
    value: median
optuna_sampler:
    value: tpe
optuna_timeout:
    value: 7200
optuna_trials:
    value: 100
output_dir:
    value: experiments/20251015/20251015_012927_kobart_balanced_kfold
prompt_strategy:
    value: zero_shot_simple
quality_threshold:
    value: 0.7
quantization_bits:
    value: 8
repetition_penalty:
    value: null
resume:
    value: true
resume_from:
    value: null
save_visualizations:
    value: false
scheduler_type:
    value: cosine
seed:
    value: 42
solar_api_key:
    value: null
solar_model:
    value: solar-1-mini-chat
temperature:
    value: null
top_k:
    value: null
top_p:
    value: null
train_data:
    value: data/raw/train.csv
tta_num_aug:
    value: 1
tta_strategies:
    value:
        - paraphrase
use_augmentation:
    value: true
use_batch_optimization:
    value: false
use_full_finetuning:
    value: false
use_onnx:
    value: false
use_pretrained_correction:
    value: false
use_solar_api:
    value: false
use_tta:
    value: false
use_wandb:
    value: true
validate_data_quality:
    value: false
wandb_project:
    value: dialogue-summarization
warmup_ratio:
    value: 0.00136
weight_decay:
    value: 0.0995
