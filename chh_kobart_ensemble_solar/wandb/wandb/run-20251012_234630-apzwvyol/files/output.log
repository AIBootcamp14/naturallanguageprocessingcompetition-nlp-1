2025-10-12 23:46:31 | wandb: Detected [openai] in use.
2025-10-12 23:46:31 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:46:31 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:46:31 | 📋 실험명: 1012-2346-llama_3.2_3b_qlora
2025-10-12 23:46:31 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/apzwvyol
2025-10-12 23:46:31 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:46:31 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:46:31 | 학습 진행 중...
2025-10-12 23:46:31 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:46:31 | 0%|          | 0/250 [00:00<?, ?it/s]
2025-10-12 23:46:38 | 1%|          | 2/250 [00:07<15:08,  3.67s/it]
2025-10-12 23:46:44 | 2%|▏         | 4/250 [00:12<12:42,  3.10s/it]
2025-10-12 23:46:53 | 3%|▎         | 7/250 [00:21<11:45,  2.90s/it]
2025-10-12 23:46:58 | 4%|▎         | 9/250 [00:27<11:33,  2.88s/it]
2025-10-12 23:47:01 | {'loss': 1.6234, 'grad_norm': 2.0193655490875244, 'learning_rate': 9e-08, 'epoch': 0.04}
2025-10-12 23:47:01 | 4%|▍         | 10/250 [00:30<11:31,  2.88s/it]
2025-10-12 23:47:07 | 5%|▍         | 12/250 [00:35<11:22,  2.87s/it]
2025-10-12 23:47:13 | 6%|▌         | 14/250 [00:42<12:01,  3.06s/it]
2025-10-12 23:47:22 | 7%|▋         | 17/250 [00:50<11:15,  2.90s/it]
2025-10-12 23:47:28 | 8%|▊         | 19/250 [00:56<10:55,  2.84s/it]
2025-10-12 23:47:31 | {'loss': 1.5487, 'grad_norm': 2.1541213989257812, 'learning_rate': 1.9e-07, 'epoch': 0.08}
2025-10-12 23:47:31 | 8%|▊         | 20/250 [00:59<11:03,  2.89s/it]
2025-10-12 23:47:36 | 9%|▉         | 22/250 [01:05<11:03,  2.91s/it]
2025-10-12 23:47:43 | 10%|▉         | 24/250 [01:12<11:58,  3.18s/it]
2025-10-12 23:47:52 | 11%|█         | 27/250 [01:20<11:02,  2.97s/it]
2025-10-12 23:47:57 | 12%|█▏        | 29/250 [01:26<10:47,  2.93s/it]
2025-10-12 23:48:00 | {'loss': 1.5919, 'grad_norm': 2.1342108249664307, 'learning_rate': 2.9000000000000003e-07, 'epoch': 0.12}
2025-10-12 23:48:00 | 12%|█▏        | 30/250 [01:29<10:43,  2.92s/it]
2025-10-12 23:48:06 | 13%|█▎        | 32/250 [01:35<10:43,  2.95s/it]
2025-10-12 23:48:12 | 14%|█▎        | 34/250 [01:41<10:39,  2.96s/it]
2025-10-12 23:48:22 | 15%|█▍        | 37/250 [01:51<11:10,  3.15s/it]
2025-10-12 23:48:28 | 16%|█▌        | 39/250 [01:57<10:49,  3.08s/it]
2025-10-12 23:48:31 | {'loss': 1.6301, 'grad_norm': 2.168320655822754, 'learning_rate': 3.9e-07, 'epoch': 0.16}
2025-10-12 23:48:31 | 16%|█▌        | 40/250 [02:00<10:32,  3.01s/it]
2025-10-12 23:48:37 | 17%|█▋        | 42/250 [02:06<10:35,  3.05s/it]
2025-10-12 23:48:43 | 18%|█▊        | 44/250 [02:12<10:10,  2.96s/it]
2025-10-12 23:48:53 | 19%|█▉        | 47/250 [02:21<10:27,  3.09s/it]
2025-10-12 23:48:59 | 20%|█▉        | 49/250 [02:27<10:15,  3.06s/it]
2025-10-12 23:49:02 | {'loss': 1.624, 'grad_norm': 2.1325876712799072, 'learning_rate': 4.900000000000001e-07, 'epoch': 0.2}
2025-10-12 23:49:02 | 20%|██        | 50/250 [02:30<09:56,  2.98s/it]
2025-10-12 23:49:07 | 21%|██        | 52/250 [02:36<09:39,  2.93s/it]
2025-10-12 23:49:12 | 22%|██▏       | 54/250 [02:41<08:58,  2.75s/it]
2025-10-12 23:49:20 | 23%|██▎       | 57/250 [02:49<08:28,  2.64s/it]
2025-10-12 23:49:26 | 24%|██▎       | 59/250 [02:55<08:54,  2.80s/it]
2025-10-12 23:49:29 | {'loss': 1.5523, 'grad_norm': 1.8329565525054932, 'learning_rate': 5.900000000000001e-07, 'epoch': 0.24}
2025-10-12 23:49:29 | 24%|██▍       | 60/250 [02:57<08:39,  2.74s/it]
2025-10-12 23:49:34 | 25%|██▍       | 62/250 [03:02<08:18,  2.65s/it]
2025-10-12 23:49:39 | 26%|██▌       | 64/250 [03:08<08:03,  2.60s/it]
2025-10-12 23:49:47 | 27%|██▋       | 67/250 [03:15<07:52,  2.58s/it]
2025-10-12 23:49:52 | 28%|██▊       | 69/250 [03:21<07:57,  2.64s/it]
2025-10-12 23:49:56 | {'loss': 1.6085, 'grad_norm': 2.066457509994507, 'learning_rate': 6.900000000000001e-07, 'epoch': 0.28}
2025-10-12 23:49:56 | 28%|██▊       | 70/250 [03:24<08:40,  2.89s/it]
2025-10-12 23:50:01 | 29%|██▉       | 72/250 [03:30<08:15,  2.78s/it]
2025-10-12 23:50:07 | 30%|██▉       | 74/250 [03:35<08:05,  2.76s/it]
2025-10-12 23:50:15 | 31%|███       | 77/250 [03:44<08:10,  2.83s/it]
