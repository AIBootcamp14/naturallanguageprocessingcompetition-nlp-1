2025-10-15 08:17:34 | 📋 실험명: 1015-0817-kfold
2025-10-15 08:17:34 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/wfwrczgj
2025-10-15 08:17:34 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-15 08:17:34 | 학습 진행 중...
2025-10-15 08:18:09 | {'loss': 2.0263, 'grad_norm': 4.585365295410156, 'learning_rate': 1.80972e-05, 'epoch': 0.16}
2025-10-15 08:18:44 | {'loss': 1.5969, 'grad_norm': 3.945899724960327, 'learning_rate': 3.63772e-05, 'epoch': 0.32}
2025-10-15 08:19:18 | {'loss': 1.5625, 'grad_norm': 3.566885471343994, 'learning_rate': 5.4657199999999996e-05, 'epoch': 0.48}
2025-10-15 08:19:53 | {'loss': 1.5191, 'grad_norm': 3.5406200885772705, 'learning_rate': 7.29372e-05, 'epoch': 0.64}
2025-10-15 08:20:28 | {'loss': 1.4951, 'grad_norm': 5.17777156829834, 'learning_rate': 9.12172e-05, 'epoch': 0.8}
2025-10-15 08:21:02 | {'loss': 1.4711, 'grad_norm': 3.418663501739502, 'learning_rate': 8.982083769633509e-05, 'epoch': 0.96}
2025-10-15 08:27:52 | {'eval_loss': 1.461945652961731, 'eval_rouge1': 0.3802352974167428, 'eval_rouge2': 0.23150527722084896, 'eval_rougeL': 0.3665082725373369, 'eval_rouge_sum': 0.9782488471749287, 'eval_runtime': 401.4722, 'eval_samples_per_second': 6.205, 'eval_steps_per_second': 0.389, 'epoch': 1.0}
2025-10-15 08:28:22 | {'loss': 1.2767, 'grad_norm': 3.114906072616577, 'learning_rate': 8.82257242582897e-05, 'epoch': 1.12}
2025-10-15 08:28:56 | {'loss': 1.2027, 'grad_norm': 9.599915504455566, 'learning_rate': 8.663061082024434e-05, 'epoch': 1.28}
2025-10-15 08:29:31 | {'loss': 1.2227, 'grad_norm': 3.5160365104675293, 'learning_rate': 8.503549738219895e-05, 'epoch': 1.44}
2025-10-15 08:30:06 | {'loss': 1.233, 'grad_norm': 3.0145466327667236, 'learning_rate': 8.344038394415358e-05, 'epoch': 1.61}
2025-10-15 08:30:40 | {'loss': 1.2113, 'grad_norm': 2.954576015472412, 'learning_rate': 8.18452705061082e-05, 'epoch': 1.77}
2025-10-15 08:31:15 | {'loss': 1.2178, 'grad_norm': 3.00075626373291, 'learning_rate': 8.025015706806283e-05, 'epoch': 1.93}
2025-10-15 08:38:18 | {'eval_loss': 1.3617980480194092, 'eval_rouge1': 0.41366312933762833, 'eval_rouge2': 0.2595388922820762, 'eval_rougeL': 0.40564432351311197, 'eval_rouge_sum': 1.0788463451328165, 'eval_runtime': 406.9416, 'eval_samples_per_second': 6.121, 'eval_steps_per_second': 0.383, 'epoch': 2.0}
2025-10-15 08:38:41 | {'loss': 1.014, 'grad_norm': 2.852675199508667, 'learning_rate': 7.865504363001744e-05, 'epoch': 2.09}
2025-10-15 08:39:15 | {'loss': 0.8649, 'grad_norm': 2.8584976196289062, 'learning_rate': 7.705993019197208e-05, 'epoch': 2.25}
2025-10-15 08:39:50 | {'loss': 0.8653, 'grad_norm': 3.4780771732330322, 'learning_rate': 7.546481675392669e-05, 'epoch': 2.41}
2025-10-15 08:40:25 | {'loss': 0.8813, 'grad_norm': 4.451434135437012, 'learning_rate': 7.386970331588133e-05, 'epoch': 2.57}
2025-10-15 08:41:00 | {'loss': 0.8924, 'grad_norm': 4.225039482116699, 'learning_rate': 7.227458987783596e-05, 'epoch': 2.73}
2025-10-15 08:41:34 | {'loss': 0.8926, 'grad_norm': 3.2117528915405273, 'learning_rate': 7.067947643979058e-05, 'epoch': 2.89}
2025-10-15 08:48:48 | {'eval_loss': 1.400778889656067, 'eval_rouge1': 0.444172764291807, 'eval_rouge2': 0.28327201304474336, 'eval_rougeL': 0.4351974766858461, 'eval_rouge_sum': 1.1626422540223964, 'eval_runtime': 409.7698, 'eval_samples_per_second': 6.079, 'eval_steps_per_second': 0.381, 'epoch': 3.0}
2025-10-15 08:49:02 | {'loss': 0.822, 'grad_norm': 2.997591257095337, 'learning_rate': 6.90843630017452e-05, 'epoch': 3.05}
2025-10-15 08:49:36 | {'loss': 0.5986, 'grad_norm': 2.8086087703704834, 'learning_rate': 6.748924956369983e-05, 'epoch': 3.21}
2025-10-15 08:50:11 | {'loss': 0.613, 'grad_norm': 2.742763042449951, 'learning_rate': 6.589413612565445e-05, 'epoch': 3.37}
2025-10-15 08:50:46 | {'loss': 0.6333, 'grad_norm': 2.8307390213012695, 'learning_rate': 6.429902268760908e-05, 'epoch': 3.53}
2025-10-15 08:51:21 | {'loss': 0.6351, 'grad_norm': 2.9665637016296387, 'learning_rate': 6.27039092495637e-05, 'epoch': 3.69}
2025-10-15 08:51:55 | {'loss': 0.6378, 'grad_norm': 3.029090404510498, 'learning_rate': 6.110879581151833e-05, 'epoch': 3.85}
2025-10-15 08:59:21 | {'eval_loss': 1.4894907474517822, 'eval_rouge1': 0.43638973720701324, 'eval_rouge2': 0.2747001547572166, 'eval_rougeL': 0.426314184947476, 'eval_rouge_sum': 1.137404076911706, 'eval_runtime': 413.815, 'eval_samples_per_second': 6.02, 'eval_steps_per_second': 0.377, 'epoch': 4.0}
2025-10-15 08:59:26 | {'loss': 0.6361, 'grad_norm': 3.076024293899536, 'learning_rate': 5.9513682373472944e-05, 'epoch': 4.01}
2025-10-15 09:00:01 | {'loss': 0.4228, 'grad_norm': 2.944962739944458, 'learning_rate': 5.7918568935427575e-05, 'epoch': 4.17}
2025-10-15 09:00:37 | {'loss': 0.4255, 'grad_norm': 2.8129031658172607, 'learning_rate': 5.63234554973822e-05, 'epoch': 4.33}
2025-10-15 09:01:12 | {'loss': 0.4437, 'grad_norm': 2.460153102874756, 'learning_rate': 5.4728342059336824e-05, 'epoch': 4.49}
2025-10-15 09:01:47 | {'loss': 0.4427, 'grad_norm': 2.9103126525878906, 'learning_rate': 5.313322862129145e-05, 'epoch': 4.65}
2025-10-15 09:02:22 | {'loss': 0.4488, 'grad_norm': 3.138399600982666, 'learning_rate': 5.153811518324607e-05, 'epoch': 4.82}
2025-10-15 09:02:56 | {'loss': 0.4589, 'grad_norm': 3.1528427600860596, 'learning_rate': 4.99430017452007e-05, 'epoch': 4.98}
2025-10-15 09:09:42 | {'eval_loss': 1.6024926900863647, 'eval_rouge1': 0.44834003601720895, 'eval_rouge2': 0.28535035362973676, 'eval_rougeL': 0.43748306837903067, 'eval_rouge_sum': 1.1711734580259765, 'eval_runtime': 400.8262, 'eval_samples_per_second': 6.215, 'eval_steps_per_second': 0.389, 'epoch': 5.0}
2025-10-15 09:10:15 | {'loss': 0.3149, 'grad_norm': 2.7062528133392334, 'learning_rate': 4.834788830715533e-05, 'epoch': 5.14}
2025-10-15 09:10:50 | {'loss': 0.3003, 'grad_norm': 2.493779182434082, 'learning_rate': 4.6752774869109946e-05, 'epoch': 5.3}
2025-10-15 09:11:24 | {'loss': 0.2992, 'grad_norm': 2.2161717414855957, 'learning_rate': 4.515766143106457e-05, 'epoch': 5.46}
2025-10-15 09:11:59 | {'loss': 0.3096, 'grad_norm': 2.093888521194458, 'learning_rate': 4.3562547993019195e-05, 'epoch': 5.62}
2025-10-15 09:12:34 | {'loss': 0.3166, 'grad_norm': 2.8410837650299072, 'learning_rate': 4.196743455497382e-05, 'epoch': 5.78}
2025-10-15 09:13:08 | {'loss': 0.315, 'grad_norm': 2.665344715118408, 'learning_rate': 4.0372321116928443e-05, 'epoch': 5.94}
2025-10-15 09:20:13 | {'eval_loss': 1.660935401916504, 'eval_rouge1': 0.44128966623783117, 'eval_rouge2': 0.2799396234003353, 'eval_rougeL': 0.4301104477876049, 'eval_rouge_sum': 1.1513397374257712, 'eval_runtime': 411.2356, 'eval_samples_per_second': 6.057, 'eval_steps_per_second': 0.379, 'epoch': 6.0}
2025-10-15 09:20:37 | {'loss': 0.2494, 'grad_norm': 2.1023616790771484, 'learning_rate': 3.877720767888307e-05, 'epoch': 6.1}
2025-10-15 09:21:12 | {'loss': 0.2072, 'grad_norm': 2.4705212116241455, 'learning_rate': 3.718209424083769e-05, 'epoch': 6.26}
2025-10-15 09:21:48 | {'loss': 0.2125, 'grad_norm': 2.225376605987549, 'learning_rate': 3.558698080279232e-05, 'epoch': 6.42}
2025-10-15 09:22:22 | {'loss': 0.2142, 'grad_norm': 2.724674701690674, 'learning_rate': 3.399186736474694e-05, 'epoch': 6.58}
2025-10-15 09:22:57 | {'loss': 0.2162, 'grad_norm': 3.3995749950408936, 'learning_rate': 3.2396753926701566e-05, 'epoch': 6.74}
2025-10-15 09:23:32 | {'loss': 0.2172, 'grad_norm': 2.1786365509033203, 'learning_rate': 3.080164048865619e-05, 'epoch': 6.9}
2025-10-15 09:30:38 | {'eval_loss': 1.7340869903564453, 'eval_rouge1': 0.447717806167723, 'eval_rouge2': 0.2826964494950548, 'eval_rougeL': 0.43628483993552913, 'eval_rouge_sum': 1.166699095598307, 'eval_runtime': 405.2914, 'eval_samples_per_second': 6.146, 'eval_steps_per_second': 0.385, 'epoch': 7.0}
2025-10-15 09:30:54 | {'loss': 0.1898, 'grad_norm': 1.9973840713500977, 'learning_rate': 2.9206527050610818e-05, 'epoch': 7.06}
2025-10-15 09:31:29 | {'loss': 0.1441, 'grad_norm': 2.1207711696624756, 'learning_rate': 2.7611413612565442e-05, 'epoch': 7.22}
2025-10-15 09:32:04 | {'loss': 0.1482, 'grad_norm': 2.006422758102417, 'learning_rate': 2.6016300174520067e-05, 'epoch': 7.38}
2025-10-15 09:32:38 | {'loss': 0.1475, 'grad_norm': 2.0903074741363525, 'learning_rate': 2.442118673647469e-05, 'epoch': 7.54}
2025-10-15 09:33:13 | {'loss': 0.1506, 'grad_norm': 1.9477741718292236, 'learning_rate': 2.282607329842932e-05, 'epoch': 7.7}
2025-10-15 09:33:48 | {'loss': 0.1501, 'grad_norm': 2.1785311698913574, 'learning_rate': 2.1230959860383943e-05, 'epoch': 7.87}
2025-10-15 09:41:05 | {'eval_loss': 1.8073689937591553, 'eval_rouge1': 0.45383190623399816, 'eval_rouge2': 0.28704865280262043, 'eval_rougeL': 0.4416267408768111, 'eval_rouge_sum': 1.1825072999134296, 'eval_runtime': 407.5316, 'eval_samples_per_second': 6.112, 'eval_steps_per_second': 0.383, 'epoch': 8.0}
2025-10-15 09:41:13 | {'loss': 0.1407, 'grad_norm': 1.6800915002822876, 'learning_rate': 1.963584642233857e-05, 'epoch': 8.03}
2025-10-15 09:41:48 | {'loss': 0.106, 'grad_norm': 1.4382919073104858, 'learning_rate': 1.8040732984293196e-05, 'epoch': 8.19}
2025-10-15 09:42:23 | {'loss': 0.106, 'grad_norm': 1.586861491203308, 'learning_rate': 1.644561954624782e-05, 'epoch': 8.35}
2025-10-15 09:42:58 | {'loss': 0.1074, 'grad_norm': 1.9099141359329224, 'learning_rate': 1.4850506108202444e-05, 'epoch': 8.51}
2025-10-15 09:43:32 | {'loss': 0.105, 'grad_norm': 1.8284330368041992, 'learning_rate': 1.3255392670157069e-05, 'epoch': 8.67}
2025-10-15 09:44:07 | {'loss': 0.1043, 'grad_norm': 1.6146959066390991, 'learning_rate': 1.1660279232111693e-05, 'epoch': 8.83}
2025-10-15 09:44:42 | {'loss': 0.1048, 'grad_norm': 1.8312900066375732, 'learning_rate': 1.0065165794066318e-05, 'epoch': 8.99}
2025-10-15 09:51:32 | {'eval_loss': 1.8449456691741943, 'eval_rouge1': 0.4632358847820775, 'eval_rouge2': 0.2954698985130164, 'eval_rougeL': 0.45103180921559993, 'eval_rouge_sum': 1.209737592510694, 'eval_runtime': 407.9498, 'eval_samples_per_second': 6.106, 'eval_steps_per_second': 0.382, 'epoch': 9.0}
2025-10-15 09:52:07 | {'loss': 0.0804, 'grad_norm': 1.5581990480422974, 'learning_rate': 8.470052356020942e-06, 'epoch': 9.15}
2025-10-15 09:52:42 | {'loss': 0.0805, 'grad_norm': 2.0906929969787598, 'learning_rate': 6.8749389179755664e-06, 'epoch': 9.31}
2025-10-15 09:53:17 | {'loss': 0.0803, 'grad_norm': 1.7105121612548828, 'learning_rate': 5.279825479930192e-06, 'epoch': 9.47}
2025-10-15 09:53:52 | {'loss': 0.0783, 'grad_norm': 1.3272407054901123, 'learning_rate': 3.684712041884817e-06, 'epoch': 9.63}
2025-10-15 09:54:21 | {'loss': 0.0791, 'grad_norm': 1.532141089439392, 'learning_rate': 2.0895986038394414e-06, 'epoch': 9.79}
2025-10-15 09:54:42 | {'loss': 0.0762, 'grad_norm': 1.4244526624679565, 'learning_rate': 4.944851657940663e-07, 'epoch': 9.95}
2025-10-15 10:02:03 | {'eval_loss': 1.8738415241241455, 'eval_rouge1': 0.45870145522481587, 'eval_rouge2': 0.29342990467761537, 'eval_rougeL': 0.44636547813740673, 'eval_rouge_sum': 1.198496838039838, 'eval_runtime': 434.3487, 'eval_samples_per_second': 5.735, 'eval_steps_per_second': 0.359, 'epoch': 10.0}
2025-10-15 10:02:07 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-15 10:02:07 | {'train_runtime': 6272.6737, 'train_samples_per_second': 15.888, 'train_steps_per_second': 0.993, 'train_loss': 0.5623991288113173, 'epoch': 10.0}
2025-10-15 10:02:07 | 최종 모델 저장 중...
2025-10-15 10:02:08 | → 모델 저장 위치: experiments/20251015/20251015_013734_kobart_balanced_kfold/fold_5/kfold/final_model
2025-10-15 10:02:08 | 최종 평가 중...
2025-10-15 10:08:13 | 최종 평가 결과:
2025-10-15 10:08:13 | eval_rouge1: 0.4632
2025-10-15 10:08:13 | eval_rouge2: 0.2955
2025-10-15 10:08:13 | eval_rougeL: 0.4510
2025-10-15 10:08:13 | eval_rouge_sum: 1.2097
