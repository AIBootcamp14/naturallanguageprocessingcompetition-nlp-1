2025-10-13 01:50:11 | 📋 실험명: 1013-0150-solar_10.7b_qlora
2025-10-13 01:50:11 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/jeto238r
2025-10-13 01:50:11 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-13 01:50:11 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-13 01:50:11 | 학습 진행 중...
2025-10-13 01:50:11 | 100%|██████████| 125/125 [1:06:37<00:00, 31.98s/it]
2025-10-13 01:50:11 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-13 01:50:34 | ❌ solar-10.7b 학습 실패: RuntimeError: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0
2025-10-13 01:50:34 | 오류 로그 저장: experiments/20251013/20251013_001540_test_full_pipeline_quick/errors/solar-10.7b_error.log
2025-10-13 01:50:34 | ==================================================
2025-10-13 01:50:34 | 모델 5/6: polyglot-ko-12.8b
2025-10-13 01:50:34 | ==================================================
2025-10-13 01:50:34 | 모델 타입: causal_lm
2025-10-13 01:50:34 | Loading Causal LM: EleutherAI/polyglot-ko-12.8b
2025-10-13 01:50:34 | 모델 로딩 중...
2025-10-13 01:50:35 | Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]
2025-10-13 01:50:36 | Loading checkpoint shards:   4%|▎         | 1/28 [00:01<00:30,  1.13s/it]
2025-10-13 01:50:36 | Loading checkpoint shards:   7%|▋         | 2/28 [00:01<00:18,  1.42it/s]
2025-10-13 01:50:37 | Loading checkpoint shards:  11%|█         | 3/28 [00:02<00:15,  1.59it/s]
2025-10-13 01:50:37 | Loading checkpoint shards:  14%|█▍        | 4/28 [00:02<00:13,  1.73it/s]
2025-10-13 01:50:38 | Loading checkpoint shards:  18%|█▊        | 5/28 [00:03<00:12,  1.90it/s]
2025-10-13 01:50:38 | Loading checkpoint shards:  21%|██▏       | 6/28 [00:03<00:11,  1.95it/s]
2025-10-13 01:50:39 | Loading checkpoint shards:  25%|██▌       | 7/28 [00:03<00:10,  1.99it/s]
2025-10-13 01:50:39 | Loading checkpoint shards:  29%|██▊       | 8/28 [00:04<00:10,  1.99it/s]
2025-10-13 01:50:39 | Loading checkpoint shards: 100%|██████████| 28/28 [00:04<00:00,  6.14it/s]
2025-10-13 01:50:40 | WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.
2025-10-13 01:50:40 | 토크나이저 로딩 중...
2025-10-13 01:50:40 | LoRA 설정 적용 중...
2025-10-13 01:50:40 | 🔍 자동 탐지된 target_modules: ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h']
2025-10-13 01:50:40 | ✅ LoRA 적용 완료
2025-10-13 01:50:40 | 학습 가능 파라미터: 52,428,800 (0.40%)
2025-10-13 01:50:40 | 전체 파라미터: 12,946,032,640
2025-10-13 01:50:40 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-13 01:50:40 | ✅ Gradient Checkpointing 활성화
2025-10-13 01:50:40 | ✅ Causal LM 로드 완료
2025-10-13 01:50:40 | ============================================================
2025-10-13 01:50:40 | 모델 학습 시작
2025-10-13 01:50:40 | ============================================================
2025-10-13 01:50:41 | WandB 로그인 상태: ieyeppo-job
2025-10-13 01:50:41 | wandb: Finishing previous runs because reinit is set to True.
