# ==================== Model Loader í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
ëª¨ë¸ ë¡œë” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. í† í¬ë‚˜ì´ì € ë¡œë”©
2. íŠ¹ìˆ˜ í† í° ì¶”ê°€
3. ëª¨ë¸ ë¡œë”©
4. ë””ë°”ì´ìŠ¤ ë°°ì¹˜
5. ì „ì²´ ë¡œë”© íŒŒì´í”„ë¼ì¸
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import torch

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.config import load_config
from src.models import ModelLoader, load_model_and_tokenizer


# ==================== í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ ==================== #
# ---------------------- í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸ ---------------------- #
def test_load_tokenizer():
    """í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 1: í† í¬ë‚˜ì´ì € ë¡œë”©")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")                 # ë² ì´ìŠ¤ë¼ì¸ Config

    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = ModelLoader(config)                            # ë¡œë” ì´ˆê¸°í™”

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = loader.load_tokenizer()                     # í† í¬ë‚˜ì´ì € ë¡œë“œ

    # ê²€ì¦
    assert tokenizer is not None                            # í† í¬ë‚˜ì´ì € ì¡´ì¬ í™•ì¸
    assert len(tokenizer) > 0                               # ì–´íœ˜ í¬ê¸° í™•ì¸

    print(f"\n  ì–´íœ˜ í¬ê¸°: {len(tokenizer):,}")
    print(f"  pad_token: {tokenizer.pad_token}")
    print(f"  eos_token: {tokenizer.eos_token}")
    print(f"  bos_token: {tokenizer.bos_token}")

    print("\nâœ… í† í¬ë‚˜ì´ì € ë¡œë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- íŠ¹ìˆ˜ í† í° ì¶”ê°€ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_special_tokens():
    """íŠ¹ìˆ˜ í† í° ì¶”ê°€ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 2: íŠ¹ìˆ˜ í† í° ì¶”ê°€")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")                 # ë² ì´ìŠ¤ë¼ì¸ Config

    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = ModelLoader(config)                            # ë¡œë” ì´ˆê¸°í™”

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = loader.load_tokenizer()                     # í† í¬ë‚˜ì´ì € ë¡œë“œ

    # íŠ¹ìˆ˜ í† í° í™•ì¸
    if hasattr(config.model, 'special_tokens') and config.model.special_tokens:
        special_tokens = list(config.model.special_tokens)  # íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸
        print(f"\n  ì„¤ì •ëœ íŠ¹ìˆ˜ í† í°: {len(special_tokens)}ê°œ")

        # ìƒ˜í”Œ í† í° ì¶œë ¥
        for token in special_tokens[:5]:
            token_id = tokenizer.convert_tokens_to_ids(token)  # í† í° ID í™•ì¸
            print(f"    {token}: {token_id}")

        # íŠ¹ìˆ˜ í† í°ì´ ì–´íœ˜ì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert all(token in tokenizer.get_vocab() for token in special_tokens)

    print("\nâœ… íŠ¹ìˆ˜ í† í° ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ---------------------- #
def test_load_model():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 3: ëª¨ë¸ ë¡œë”©")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")                 # ë² ì´ìŠ¤ë¼ì¸ Config

    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = ModelLoader(config)                            # ë¡œë” ì´ˆê¸°í™”

    # í† í¬ë‚˜ì´ì € ë¨¼ì € ë¡œë“œ
    tokenizer = loader.load_tokenizer()                     # í† í¬ë‚˜ì´ì € ë¡œë“œ

    # ëª¨ë¸ ë¡œë“œ
    model = loader.load_model(tokenizer)                    # ëª¨ë¸ ë¡œë“œ

    # ê²€ì¦
    assert model is not None                                # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    assert next(model.parameters()).device.type in ['cuda', 'cpu']  # ë””ë°”ì´ìŠ¤ í™•ì¸

    print("\nâœ… ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ë””ë°”ì´ìŠ¤ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_device_placement():
    """ë””ë°”ì´ìŠ¤ ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 4: ë””ë°”ì´ìŠ¤ ë°°ì¹˜")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")                 # ë² ì´ìŠ¤ë¼ì¸ Config

    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = ModelLoader(config)                            # ë¡œë” ì´ˆê¸°í™”

    # ë””ë°”ì´ìŠ¤ í™•ì¸
    print(f"\n  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {loader.device}")
    print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"  CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  GPU ê°œìˆ˜: {torch.cuda.device_count()}")

    print("\nâœ… ë””ë°”ì´ìŠ¤ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ---------------------- ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ---------------------- #
def test_full_pipeline():
    """ì „ì²´ ë¡œë”© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ 5: ì „ì²´ íŒŒì´í”„ë¼ì¸")
    print("="*60)

    # Config ë¡œë“œ
    config = load_config("baseline_kobart")                 # ë² ì´ìŠ¤ë¼ì¸ Config

    # í¸ì˜ í•¨ìˆ˜ë¡œ í•œ ë²ˆì— ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(config)     # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ

    # ê²€ì¦
    assert model is not None                                # ëª¨ë¸ ì¡´ì¬ í™•ì¸
    assert tokenizer is not None                            # í† í¬ë‚˜ì´ì € ì¡´ì¬ í™•ì¸

    # -------------- ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ -------------- #
    print("\nê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:")
    test_text = "#Person1#: ì•ˆë…•í•˜ì„¸ìš”\\n#Person2#: ë°˜ê°‘ìŠµë‹ˆë‹¤"

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        test_text,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (token_type_ids ì œê±°)
    inputs = {
        k: v.to(model.device)
        for k, v in inputs.items()
        if k in ['input_ids', 'attention_mask']             # BARTëŠ” ì´ ë‘ ê°œë§Œ ì‚¬ìš©
    }

    # ì¶”ë¡  (í‰ê°€ ëª¨ë“œ)
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=50,
            num_beams=2
        )

    # ë””ì½”ë”©
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"  ì…ë ¥: {test_text[:30]}...")
    print(f"  ìƒì„±: {generated_text[:50]}...")

    print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")


# ==================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    print("\n" + "="*60)
    print("Model Loader í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_load_tokenizer()                               # í…ŒìŠ¤íŠ¸ 1
        test_special_tokens()                               # í…ŒìŠ¤íŠ¸ 2
        test_load_model()                                   # í…ŒìŠ¤íŠ¸ 3
        test_device_placement()                             # í…ŒìŠ¤íŠ¸ 4
        test_full_pipeline()                                # í…ŒìŠ¤íŠ¸ 5

        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("="*60)

    except Exception as e:
        print("\n" + "="*60)
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("="*60)
        raise
