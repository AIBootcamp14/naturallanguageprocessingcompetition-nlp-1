"""
ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •ê¸°

PRD 04, 12: ì¶”ë¡  ìµœì í™” ë° ì•™ìƒë¸” ì „ëµ êµ¬í˜„
í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ìš”ì•½ ë³´ì •
"""

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
from typing import List, Dict, Optional

# ------------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import torch


# ==================== ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì •ê¸° í´ë˜ìŠ¤ ==================== #
class PretrainedCorrector:
    """
    í—ˆê¹…í˜ì´ìŠ¤ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ í™œìš©í•œ ìš”ì•½ ë³´ì •

    ì£¼ìš” ê¸°ëŠ¥:
    1. ì—¬ëŸ¬ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬
    2. ì°¸ì¡° ìš”ì•½ ìƒì„±
    3. í’ˆì§ˆ í‰ê°€ ë° ë³´ì •
    4. ì•™ìƒë¸” ì „ëµ ì ìš©

    ì‚¬ìš© ì˜ˆì‹œ:
        corrector = PretrainedCorrector(
            model_names=["gogamza/kobart-base-v2", "digit82/kobart-summarization"],
            correction_strategy="quality_based",
            quality_threshold=0.3
        )
        corrected = corrector.correct_batch(dialogues, candidate_summaries)
    """

    # ---------------------- ì´ˆê¸°í™” ë©”ì„œë“œ ---------------------- #
    def __init__(
        self,
        model_names: List[str],
        correction_strategy: str = "quality_based",
        quality_threshold: float = 0.3,
        device: Optional[torch.device] = None,
        logger=None,
        checkpoint_dir: Optional[str] = None
    ):
        """
        Args:
            model_names: í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: ["gogamza/kobart-base-v2", "digit82/kobart-summarization"]
            correction_strategy: ë³´ì • ì „ëµ
                - "threshold": ì„ê³„ê°’ ê¸°ë°˜
                - "voting": íˆ¬í‘œ ê¸°ë°˜
                - "weighted": ê°€ì¤‘ í‰ê· 
                - "quality_based": í’ˆì§ˆ ê¸°ë°˜ (ì¶”ì²œ)
            quality_threshold: í’ˆì§ˆ ì„ê³„ê°’ (0.0~1.0)
            device: ì¶”ë¡  ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ê°ì§€)
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
            checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ (ì„ íƒ)
        """
        self.model_names = model_names
        self.correction_strategy = correction_strategy
        self.quality_threshold = quality_threshold
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger = logger

        # -------------- ëª¨ë¸ ë¡œë” ì´ˆê¸°í™” -------------- #
        from src.correction.model_loader import HuggingFaceModelLoader
        self.model_loader = HuggingFaceModelLoader(device=self.device, logger=logger)

        # -------------- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ì†Œ -------------- #
        self.models = {}                                # ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        self.tokenizers = {}                            # í† í¬ë‚˜ì´ì € ë”•ì…”ë„ˆë¦¬
        self._load_all_models()                         # ëª¨ë“  ëª¨ë¸ ë¡œë“œ

        # -------------- í’ˆì§ˆ í‰ê°€ê¸° ì´ˆê¸°í™” -------------- #
        from src.correction.quality_evaluator import QualityEvaluator
        self.evaluator = QualityEvaluator(logger=logger)

        # -------------- ì•™ìƒë¸” ì „ëµ ì´ˆê¸°í™” -------------- #
        from src.correction.ensemble_strategies import get_ensemble_strategy
        self.ensemble = get_ensemble_strategy(correction_strategy)

        # -------------- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™” -------------- #
        self.checkpoint_manager = None
        if checkpoint_dir:
            from src.checkpoints.correction_checkpoint import CorrectionCheckpointManager
            self.checkpoint_manager = CorrectionCheckpointManager(checkpoint_dir)

    # ---------------------- ëª¨ë“  ëª¨ë¸ ë¡œë“œ ë©”ì„œë“œ ---------------------- #
    def _load_all_models(self):
        """
        ëª¨ë“  í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ

        ì‹¤íŒ¨í•œ ëª¨ë¸ì€ ê±´ë„ˆëœ€ (graceful degradation)
        """
        for model_name in self.model_names:
            try:
                model, tokenizer = self.model_loader.load_model(model_name)
                self.models[model_name] = model
                self.tokenizers[model_name] = tokenizer
            except Exception as e:
                self._log(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {model_name}")
                self._log(f"   ì—ëŸ¬: {str(e)}")

    # ---------------------- ë°°ì¹˜ ë³´ì • ë©”ì„œë“œ ---------------------- #
    def correct_batch(
        self,
        dialogues: List[str],
        candidate_summaries: List[str],
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        ë°°ì¹˜ ë³´ì •

        Args:
            dialogues: ì…ë ¥ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            candidate_summaries: KoBARTê°€ ìƒì„±í•œ ì´ˆì•ˆ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            **generation_kwargs: ìƒì„± íŒŒë¼ë¯¸í„° (max_new_tokens, num_beams ë“±)

        Returns:
            ë³´ì •ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        # -------------- ë¡œë“œëœ ëª¨ë¸ í™•ì¸ -------------- #
        if not self.models:
            self._log("âš ï¸  ë¡œë“œëœ ì°¸ì¡° ëª¨ë¸ì´ ì—†ìŒ. ì›ë³¸ ìš”ì•½ ë°˜í™˜")
            return candidate_summaries

        # -------------- ë³´ì • ì‹œì‘ ë¡œê·¸ -------------- #
        self._log("=" * 60)
        self._log("ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ì‹œì‘")
        self._log(f"  - ìƒ˜í”Œ ìˆ˜: {len(dialogues)}")
        self._log(f"  - ì°¸ì¡° ëª¨ë¸ ìˆ˜: {len(self.models)}")
        self._log(f"  - ë³´ì • ì „ëµ: {self.correction_strategy}")
        self._log(f"  - í’ˆì§ˆ ì„ê³„ê°’: {self.quality_threshold}")
        self._log("=" * 60)

        try:
            # -------------- ë‹¨ê³„ 1: ì°¸ì¡° ìš”ì•½ ìƒì„± -------------- #
            # ê° í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ë¡œ ì°¸ì¡° ìš”ì•½ ìƒì„±
            reference_summaries = {}
            for model_name, model in self.models.items():
                self._log(f"\n[1/3] ì°¸ì¡° ìš”ì•½ ìƒì„± ì¤‘: {model_name}")
                tokenizer = self.tokenizers[model_name]
                summaries = self._generate_summaries(
                    dialogues, model, tokenizer, batch_size, **generation_kwargs
                )
                reference_summaries[model_name] = summaries
                self._log(f"  âœ… ì™„ë£Œ: {len(summaries)}ê°œ ìš”ì•½ ìƒì„±")

            # -------------- ë‹¨ê³„ 2: í’ˆì§ˆ í‰ê°€ -------------- #
            self._log(f"\n[2/3] í’ˆì§ˆ í‰ê°€ ì¤‘...")
            quality_scores = self.evaluator.evaluate_all(
                candidate_summaries=candidate_summaries,
                reference_summaries=reference_summaries,
                dialogues=dialogues
            )
            self._log(f"  âœ… í‰ê°€ ì™„ë£Œ")

            # -------------- ë‹¨ê³„ 3: ë³´ì • ì „ëµ ì ìš© -------------- #
            self._log(f"\n[3/3] ë³´ì • ì „ëµ ì ìš© ì¤‘: {self.correction_strategy}")
            corrected_summaries = self.ensemble.select(
                candidate_summaries=candidate_summaries,
                reference_summaries=reference_summaries,
                quality_scores=quality_scores,
                threshold=self.quality_threshold
            )
            self._log(f"  âœ… ë³´ì • ì™„ë£Œ")

            # -------------- ë³´ì • í†µê³„ ì¶œë ¥ -------------- #
            num_corrected = sum([
                1 for orig, corr in zip(candidate_summaries, corrected_summaries)
                if orig != corr
            ])
            self._log(f"\nğŸ“Š ë³´ì • í†µê³„:")
            self._log(f"  - ì „ì²´: {len(dialogues)}ê°œ")
            self._log(f"  - ë³´ì •ë¨: {num_corrected}ê°œ ({num_corrected/len(dialogues)*100:.1f}%)")
            self._log(f"  - ìœ ì§€ë¨: {len(dialogues)-num_corrected}ê°œ")
            self._log("=" * 60)

            return corrected_summaries

        except Exception as e:
            self._log(f"\nâŒ ë³´ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ë§ˆì§€ë§‰ ì§„í–‰ë¥  ê¸°ë¡
            if self.logger and hasattr(self.logger, 'write_last_progress'):
                self.logger.write_last_progress()
            # ì›ë³¸ ìš”ì•½ ë°˜í™˜ (Graceful degradation)
            self._log("  âš ï¸  ì›ë³¸ ìš”ì•½ ë°˜í™˜")
            return candidate_summaries

    # ---------------------- ë‹¨ì¼ ëª¨ë¸ ìš”ì•½ ìƒì„± ë©”ì„œë“œ ---------------------- #
    def _generate_summaries(
        self,
        dialogues: List[str],
        model,
        tokenizer,
        batch_size: int = 16,
        **generation_kwargs
    ) -> List[str]:
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ ë°°ì¹˜ ìš”ì•½ ìƒì„±

        Args:
            dialogues: ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            model: HuggingFace ëª¨ë¸
            tokenizer: HuggingFace í† í¬ë‚˜ì´ì €
            batch_size: ë°°ì¹˜ í¬ê¸°
            **generation_kwargs: ìƒì„± íŒŒë¼ë¯¸í„°

        Returns:
            ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        from src.inference import create_predictor

        # -------------- Predictor ìƒì„± -------------- #
        # ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©
        predictor = create_predictor(
            model=model,
            tokenizer=tokenizer,
            device=self.device,
            logger=None                                 # ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€
        )

        # -------------- ë°°ì¹˜ ì˜ˆì¸¡ -------------- #
        summaries = predictor.predict_batch(
            dialogues=dialogues,
            batch_size=batch_size,
            show_progress=False,                        # ì§„í–‰ë°” ë¹„í™œì„±í™”
            **generation_kwargs
        )

        # -------------- dialogue í•„í„°ë§ -------------- #
        filtered_summaries = []
        filtered_count = 0
        for dialogue, summary in zip(dialogues, summaries):
            if self._is_dialogue_copy(dialogue, summary):
                # dialogueë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
                filtered_summaries.append("")           # ë¹ˆ ìš”ì•½ (í’ˆì§ˆ í‰ê°€ì—ì„œ ë‚®ì€ ì ìˆ˜)
                filtered_count += 1
            else:
                filtered_summaries.append(summary)

        if filtered_count > 0:
            self._log(f"  âš ï¸  {filtered_count}ê°œ ìƒ˜í”Œì´ dialogue ë³µì‚¬ë¡œ ê°ì§€ë˜ì–´ í•„í„°ë§ë¨")

        return filtered_summaries

    # ---------------------- dialogue ë³µì‚¬ ê°ì§€ ë©”ì„œë“œ ---------------------- #
    def _is_dialogue_copy(self, dialogue: str, summary: str, threshold: float = 0.9) -> bool:
        """
        ìš”ì•½ì´ dialogueë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ê²ƒì¸ì§€ ê²€ì‚¬

        Args:
            dialogue: ì›ë³¸ ëŒ€í™”
            summary: ìƒì„±ëœ ìš”ì•½
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.9 ì´ìƒì´ë©´ ë³µì‚¬ë¡œ ê°„ì£¼)

        Returns:
            True if summary is a copy of dialogue
        """
        from difflib import SequenceMatcher

        # -------------- 0. ë¹ˆ ë¬¸ìì—´ ì²´í¬ -------------- #
        if not summary.strip():
            return False                                # ë¹ˆ ë¬¸ìì—´ì€ ë³µì‚¬ ì•„ë‹˜

        # -------------- 1. ê¸¸ì´ ë¹„ìœ¨ ì²´í¬ -------------- #
        len_ratio = len(summary) / (len(dialogue) + 1e-6)
        if len_ratio > 0.7:                             # ìš”ì•½ì´ ì›ë³¸ì˜ 70% ì´ìƒì´ë©´ ì˜ì‹¬
            # 2. ë¬¸ìì—´ ìœ ì‚¬ë„ ì²´í¬
            similarity = SequenceMatcher(None, dialogue, summary).ratio()
            if similarity > threshold:
                return True

        # -------------- 3. #Person1#, #Person2# íƒœê·¸ ì²´í¬ -------------- #
        if "#Person1#" in summary or "#Person2#" in summary:
            # ìš”ì•½ì— ëŒ€í™” íƒœê·¸ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ë³µì‚¬ë¡œ ê°„ì£¼
            return True

        # -------------- 4. ëŒ€í™” í˜•ì‹ íŒ¨í„´ ì²´í¬ -------------- #
        # "Person1: ... Person2: ..." í˜•ì‹ ê°ì§€
        import re
        dialogue_pattern = r'(Person\d+[:ï¼š]|#Person\d+#[:ï¼š])'
        matches = re.findall(dialogue_pattern, summary)
        if len(matches) >= 2:                           # 2ê°œ ì´ìƒì˜ í™”ì íƒœê·¸ê°€ ìˆìœ¼ë©´ ëŒ€í™”ë¬¸
            return True

        return False

    # ---------------------- ë¡œê¹… í—¬í¼ ë©”ì„œë“œ ---------------------- #
    def _log(self, msg: str):
        """
        ë¡œê¹… í—¬í¼ í•¨ìˆ˜

        Args:
            msg: ë¡œê·¸ ë©”ì‹œì§€
        """
        if self.logger:
            self.logger.write(msg)
        else:
            print(msg)
