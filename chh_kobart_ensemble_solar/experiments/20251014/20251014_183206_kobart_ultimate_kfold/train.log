2025-10-14 18:32:06 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-14 18:32:06 | WandB 로그인 상태: ieyeppo-job
2025-10-14 18:32:07 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-14 18:32:07 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-14 18:32:07 | wandb: setting up run vsrvwfcc
2025-10-14 18:32:07 | wandb: Tracking run with wandb version 0.22.2
2025-10-14 18:32:07 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251014_183207-vsrvwfcc
wandb: Run `wandb offline` to turn off syncing.
2025-10-14 18:32:07 | wandb: Syncing run 1014-1832-kobart_ultimate_kfold
2025-10-14 18:32:07 | wandb: ⭐️ View project at https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization
2025-10-14 18:32:07 | wandb: 🚀 View run at https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/vsrvwfcc
2025-10-14 18:32:08 | 📋 실험명: 1014-1832-kobart_ultimate_kfold
2025-10-14 18:32:08 | 🔗 WandB URL: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/vsrvwfcc
2025-10-14 18:32:08 | ✅ WandB 초기화 완료
2025-10-14 18:32:08 | 프로젝트: dialogue-summarization
2025-10-14 18:32:08 | 실험명: kobart_ultimate_kfold
2025-10-14 18:32:11 | 📊 KFOLD 모드 실행 중...
2025-10-14 18:32:11 | ============================================================
2025-10-14 18:32:11 | 🔄 K-FOLD 교차검증 모드 학습 시작
2025-10-14 18:32:11 | 📋 K-Folds: 5
2025-10-14 18:32:11 | 📋 모델: kobart
2025-10-14 18:32:11 | 📋 Fold Seed: 42
2025-10-14 18:32:11 | ============================================================
2025-10-14 18:32:11 | [1/3] 전체 데이터 로딩...
2025-10-14 18:32:11 | ✅ 학습 데이터: 12457개
2025-10-14 18:32:11 | ✅ 검증 데이터: 499개
2025-10-14 18:32:11 | ✅ 전체 데이터: 12457개
2025-10-14 18:32:11 | [2/3] Config 로딩...
2025-10-14 18:32:11 | ✅ Config 로드 완료: kobart
2025-10-14 18:32:11 | [3/3] K-Fold 교차검증 실행...
2025-10-14 18:32:11 | ========================================
2025-10-14 18:32:11 | 📌 Fold 1/5 학습 시작
2025-10-14 18:32:11 | ========================================
2025-10-14 18:32:11 | 학습: 9965개
2025-10-14 18:32:11 | 검증: 2492개
2025-10-14 18:32:11 | 모델 타입: encoder_decoder
2025-10-14 18:32:11 | ============================================================
2025-10-14 18:32:11 | 모델 및 토크나이저 로딩 시작
2025-10-14 18:32:11 | ============================================================
2025-10-14 18:32:11 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 18:32:11 | 모델 로딩: digit82/kobart-summarization
2025-10-14 18:32:12 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 18:32:13 | → 디바이스: cuda
2025-10-14 18:32:13 | → 전체 파라미터: 123,859,968
2025-10-14 18:32:13 | → 학습 가능 파라미터: 123,859,968
2025-10-14 18:32:13 | ============================================================
2025-10-14 18:32:13 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 18:32:13 | ============================================================
2025-10-14 18:32:14 | ============================================================
2025-10-14 18:32:14 | 모델 학습 시작
2025-10-14 18:32:14 | ============================================================
2025-10-14 18:32:14 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 18:32:14 | 학습 진행 중...
2025-10-14 18:32:20 | {'loss': 2.0792, 'grad_norm': 4.350188255310059, 'learning_rate': 1.498464e-05, 'epoch': 0.16}
2025-10-14 18:32:27 | {'loss': 1.6245, 'grad_norm': 3.9649720191955566, 'learning_rate': 3.012064e-05, 'epoch': 0.32}
2025-10-14 18:32:33 | {'loss': 1.5566, 'grad_norm': 3.6407880783081055, 'learning_rate': 4.525663999999999e-05, 'epoch': 0.48}
2025-10-14 18:32:39 | {'loss': 1.5259, 'grad_norm': 3.231820583343506, 'learning_rate': 6.039264e-05, 'epoch': 0.64}
2025-10-14 18:32:45 | {'loss': 1.5052, 'grad_norm': 4.472958087921143, 'learning_rate': 7.552863999999999e-05, 'epoch': 0.8}
2025-10-14 18:32:50 | {'loss': 1.455, 'grad_norm': 3.3984909057617188, 'learning_rate': 7.437243979057592e-05, 'epoch': 0.96}
2025-10-14 18:34:57 | {'eval_loss': 1.4035348892211914, 'eval_rouge1': 0.38441297240470096, 'eval_rouge2': 0.23844187191566898, 'eval_rougeL': 0.3778971080743134, 'eval_rouge_sum': 1.0007519523946833, 'eval_runtime': 125.4886, 'eval_samples_per_second': 19.858, 'eval_steps_per_second': 1.243, 'epoch': 1.0}
2025-10-14 18:35:03 | {'loss': 1.2867, 'grad_norm': 3.65316104888916, 'learning_rate': 7.305167190226876e-05, 'epoch': 1.12}
2025-10-14 18:35:09 | {'loss': 1.2331, 'grad_norm': 3.2833919525146484, 'learning_rate': 7.17309040139616e-05, 'epoch': 1.28}
2025-10-14 18:35:15 | {'loss': 1.2185, 'grad_norm': 3.0745151042938232, 'learning_rate': 7.041013612565445e-05, 'epoch': 1.44}
2025-10-14 18:35:21 | {'loss': 1.2489, 'grad_norm': 4.522928714752197, 'learning_rate': 6.908936823734729e-05, 'epoch': 1.61}
2025-10-14 18:35:27 | {'loss': 1.2109, 'grad_norm': 3.243159294128418, 'learning_rate': 6.776860034904013e-05, 'epoch': 1.77}
2025-10-14 18:35:33 | {'loss': 1.2269, 'grad_norm': 3.67496657371521, 'learning_rate': 6.644783246073298e-05, 'epoch': 1.93}
2025-10-14 18:37:40 | {'eval_loss': 1.3476669788360596, 'eval_rouge1': 0.4101963474878054, 'eval_rouge2': 0.2603465147747414, 'eval_rougeL': 0.4034523831086717, 'eval_rouge_sum': 1.0739952453712185, 'eval_runtime': 124.7112, 'eval_samples_per_second': 19.982, 'eval_steps_per_second': 1.251, 'epoch': 2.0}
2025-10-14 18:37:44 | {'loss': 1.0284, 'grad_norm': 2.5922701358795166, 'learning_rate': 6.512706457242582e-05, 'epoch': 2.09}
2025-10-14 18:37:50 | {'loss': 0.8813, 'grad_norm': 2.8886783123016357, 'learning_rate': 6.380629668411867e-05, 'epoch': 2.25}
2025-10-14 18:37:56 | {'loss': 0.8997, 'grad_norm': 3.207404136657715, 'learning_rate': 6.248552879581151e-05, 'epoch': 2.41}
2025-10-14 18:38:03 | {'loss': 0.9097, 'grad_norm': 3.6875829696655273, 'learning_rate': 6.116476090750436e-05, 'epoch': 2.57}
2025-10-14 18:38:09 | {'loss': 0.9138, 'grad_norm': 3.7465217113494873, 'learning_rate': 5.98439930191972e-05, 'epoch': 2.73}
2025-10-14 18:38:14 | {'loss': 0.9156, 'grad_norm': 3.163588523864746, 'learning_rate': 5.852322513089005e-05, 'epoch': 2.89}
2025-10-14 18:40:26 | {'eval_loss': 1.3815046548843384, 'eval_rouge1': 0.4270602451226267, 'eval_rouge2': 0.27383412843913746, 'eval_rougeL': 0.4180326176480184, 'eval_rouge_sum': 1.1189269912097826, 'eval_runtime': 127.1051, 'eval_samples_per_second': 19.606, 'eval_steps_per_second': 1.227, 'epoch': 3.0}
2025-10-14 18:40:29 | {'loss': 0.8474, 'grad_norm': 3.3767240047454834, 'learning_rate': 5.720245724258289e-05, 'epoch': 3.05}
2025-10-14 18:40:35 | {'loss': 0.6486, 'grad_norm': 3.261800527572632, 'learning_rate': 5.588168935427574e-05, 'epoch': 3.21}
2025-10-14 18:40:40 | {'loss': 0.6557, 'grad_norm': 3.802757740020752, 'learning_rate': 5.456092146596858e-05, 'epoch': 3.37}
2025-10-14 18:40:47 | {'loss': 0.6617, 'grad_norm': 2.815209150314331, 'learning_rate': 5.324015357766143e-05, 'epoch': 3.53}
2025-10-14 18:40:53 | {'loss': 0.6735, 'grad_norm': 3.310244560241699, 'learning_rate': 5.191938568935427e-05, 'epoch': 3.69}
2025-10-14 18:40:59 | {'loss': 0.6801, 'grad_norm': 3.1397340297698975, 'learning_rate': 5.0598617801047115e-05, 'epoch': 3.85}
2025-10-14 18:43:08 | {'eval_loss': 1.4671918153762817, 'eval_rouge1': 0.4479277011977492, 'eval_rouge2': 0.2863959824971783, 'eval_rougeL': 0.43733239398490836, 'eval_rouge_sum': 1.171656077679836, 'eval_runtime': 123.2934, 'eval_samples_per_second': 20.212, 'eval_steps_per_second': 1.265, 'epoch': 4.0}
2025-10-14 18:43:09 | {'loss': 0.666, 'grad_norm': 2.720121145248413, 'learning_rate': 4.927784991273996e-05, 'epoch': 4.01}
2025-10-14 18:43:15 | {'loss': 0.4594, 'grad_norm': 3.2461514472961426, 'learning_rate': 4.795708202443281e-05, 'epoch': 4.17}
2025-10-14 18:43:21 | {'loss': 0.4773, 'grad_norm': 3.053020477294922, 'learning_rate': 4.6636314136125646e-05, 'epoch': 4.33}
2025-10-14 18:43:26 | {'loss': 0.4794, 'grad_norm': 3.3808929920196533, 'learning_rate': 4.53155462478185e-05, 'epoch': 4.49}
2025-10-14 18:43:33 | {'loss': 0.4938, 'grad_norm': 2.931063413619995, 'learning_rate': 4.399477835951134e-05, 'epoch': 4.65}
2025-10-14 18:43:39 | {'loss': 0.4967, 'grad_norm': 2.8307433128356934, 'learning_rate': 4.267401047120419e-05, 'epoch': 4.82}
2025-10-14 18:43:44 | {'loss': 0.5091, 'grad_norm': 4.161962032318115, 'learning_rate': 4.135324258289703e-05, 'epoch': 4.98}
2025-10-14 18:45:55 | {'eval_loss': 1.5301175117492676, 'eval_rouge1': 0.43978035996379244, 'eval_rouge2': 0.28013793012387034, 'eval_rougeL': 0.42997399085678384, 'eval_rouge_sum': 1.1498922809444467, 'eval_runtime': 130.032, 'eval_samples_per_second': 19.165, 'eval_steps_per_second': 1.2, 'epoch': 5.0}
2025-10-14 18:46:02 | {'loss': 0.362, 'grad_norm': 2.587031841278076, 'learning_rate': 4.0032474694589875e-05, 'epoch': 5.14}
2025-10-14 18:46:07 | {'loss': 0.3478, 'grad_norm': 2.6016807556152344, 'learning_rate': 3.8711706806282714e-05, 'epoch': 5.3}
2025-10-14 18:46:13 | {'loss': 0.355, 'grad_norm': 2.7945384979248047, 'learning_rate': 3.739093891797556e-05, 'epoch': 5.46}
2025-10-14 18:46:20 | {'loss': 0.3517, 'grad_norm': 3.2083404064178467, 'learning_rate': 3.6070171029668406e-05, 'epoch': 5.62}
2025-10-14 18:46:26 | {'loss': 0.3568, 'grad_norm': 2.8974175453186035, 'learning_rate': 3.474940314136125e-05, 'epoch': 5.78}
2025-10-14 18:46:32 | {'loss': 0.3616, 'grad_norm': 2.897446870803833, 'learning_rate': 3.34286352530541e-05, 'epoch': 5.94}
2025-10-14 18:48:43 | {'eval_loss': 1.6174769401550293, 'eval_rouge1': 0.45916227871506987, 'eval_rouge2': 0.29375500779066627, 'eval_rougeL': 0.4479906388046416, 'eval_rouge_sum': 1.2009079253103776, 'eval_runtime': 129.1527, 'eval_samples_per_second': 19.295, 'eval_steps_per_second': 1.208, 'epoch': 6.0}
2025-10-14 18:48:48 | {'loss': 0.2898, 'grad_norm': 2.5371663570404053, 'learning_rate': 3.2107867364746944e-05, 'epoch': 6.1}
2025-10-14 18:48:54 | {'loss': 0.2505, 'grad_norm': 2.8237478733062744, 'learning_rate': 3.078709947643979e-05, 'epoch': 6.26}
2025-10-14 18:49:00 | {'loss': 0.2624, 'grad_norm': 2.309027671813965, 'learning_rate': 2.9466331588132632e-05, 'epoch': 6.42}
2025-10-14 18:49:06 | {'loss': 0.2615, 'grad_norm': 2.3456733226776123, 'learning_rate': 2.8145563699825478e-05, 'epoch': 6.58}
2025-10-14 18:49:12 | {'loss': 0.2591, 'grad_norm': 2.377690553665161, 'learning_rate': 2.682479581151832e-05, 'epoch': 6.74}
2025-10-14 18:49:18 | {'loss': 0.2579, 'grad_norm': 2.692850351333618, 'learning_rate': 2.5504027923211166e-05, 'epoch': 6.9}
2025-10-14 18:51:33 | {'eval_loss': 1.693981647491455, 'eval_rouge1': 0.46268968243671704, 'eval_rouge2': 0.29323916091519653, 'eval_rougeL': 0.45106964343407374, 'eval_rouge_sum': 1.2069984867859873, 'eval_runtime': 131.8602, 'eval_samples_per_second': 18.899, 'eval_steps_per_second': 1.183, 'epoch': 7.0}
2025-10-14 18:51:37 | {'loss': 0.2318, 'grad_norm': 1.872711420059204, 'learning_rate': 2.4183260034904012e-05, 'epoch': 7.06}
2025-10-14 18:51:43 | {'loss': 0.1844, 'grad_norm': 2.366612434387207, 'learning_rate': 2.2862492146596854e-05, 'epoch': 7.22}
2025-10-14 18:51:50 | {'loss': 0.1896, 'grad_norm': 2.4417405128479004, 'learning_rate': 2.15417242582897e-05, 'epoch': 7.38}
2025-10-14 18:51:56 | {'loss': 0.1901, 'grad_norm': 8.997404098510742, 'learning_rate': 2.0220956369982546e-05, 'epoch': 7.54}
2025-10-14 18:52:02 | {'loss': 0.1892, 'grad_norm': 2.1021599769592285, 'learning_rate': 1.8900188481675392e-05, 'epoch': 7.7}
2025-10-14 18:52:07 | {'loss': 0.1951, 'grad_norm': 1.9933325052261353, 'learning_rate': 1.7579420593368238e-05, 'epoch': 7.87}
2025-10-14 18:54:23 | {'eval_loss': 1.737402081489563, 'eval_rouge1': 0.45740937241353574, 'eval_rouge2': 0.28895053640889645, 'eval_rougeL': 0.44553389688953726, 'eval_rouge_sum': 1.1918938057119695, 'eval_runtime': 131.047, 'eval_samples_per_second': 19.016, 'eval_steps_per_second': 1.19, 'epoch': 8.0}
2025-10-14 18:54:26 | {'loss': 0.185, 'grad_norm': 1.6118154525756836, 'learning_rate': 1.625865270506108e-05, 'epoch': 8.03}
2025-10-14 18:54:31 | {'loss': 0.1411, 'grad_norm': 1.9048258066177368, 'learning_rate': 1.4937884816753926e-05, 'epoch': 8.19}
2025-10-14 18:54:38 | {'loss': 0.1427, 'grad_norm': 2.3966057300567627, 'learning_rate': 1.361711692844677e-05, 'epoch': 8.35}
2025-10-14 18:54:44 | {'loss': 0.1451, 'grad_norm': 2.2232072353363037, 'learning_rate': 1.2296349040139616e-05, 'epoch': 8.51}
2025-10-14 18:54:50 | {'loss': 0.1441, 'grad_norm': 2.009233236312866, 'learning_rate': 1.097558115183246e-05, 'epoch': 8.67}
2025-10-14 18:54:55 | {'loss': 0.1467, 'grad_norm': 2.268505334854126, 'learning_rate': 9.654813263525306e-06, 'epoch': 8.83}
2025-10-14 18:55:01 | {'loss': 0.1448, 'grad_norm': 2.0195302963256836, 'learning_rate': 8.334045375218148e-06, 'epoch': 8.99}
2025-10-14 18:57:11 | {'eval_loss': 1.775104284286499, 'eval_rouge1': 0.46749945680444904, 'eval_rouge2': 0.29894996228220266, 'eval_rougeL': 0.4565775883487474, 'eval_rouge_sum': 1.223027007435399, 'eval_runtime': 129.2383, 'eval_samples_per_second': 19.282, 'eval_steps_per_second': 1.207, 'epoch': 9.0}
2025-10-14 18:57:18 | {'loss': 0.1159, 'grad_norm': 1.8334227800369263, 'learning_rate': 7.013277486910994e-06, 'epoch': 9.15}
2025-10-14 18:57:25 | {'loss': 0.1182, 'grad_norm': 2.135625123977661, 'learning_rate': 5.692509598603838e-06, 'epoch': 9.31}
2025-10-14 18:57:30 | {'loss': 0.1138, 'grad_norm': 2.338747978210449, 'learning_rate': 4.371741710296684e-06, 'epoch': 9.47}
2025-10-14 18:57:36 | {'loss': 0.1162, 'grad_norm': 1.5658403635025024, 'learning_rate': 3.0509738219895288e-06, 'epoch': 9.63}
2025-10-14 18:57:42 | {'loss': 0.1138, 'grad_norm': 1.728804588317871, 'learning_rate': 1.7302059336823733e-06, 'epoch': 9.79}
2025-10-14 18:57:48 | {'loss': 0.1143, 'grad_norm': 1.8559144735336304, 'learning_rate': 4.094380453752181e-07, 'epoch': 9.95}
2025-10-14 19:00:03 | {'eval_loss': 1.7988409996032715, 'eval_rouge1': 0.4612382015495211, 'eval_rouge2': 0.2944732559726578, 'eval_rougeL': 0.45151516965685556, 'eval_rouge_sum': 1.2072266271790344, 'eval_runtime': 133.0523, 'eval_samples_per_second': 18.729, 'eval_steps_per_second': 1.172, 'epoch': 10.0}
2025-10-14 19:00:05 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 19:00:05 | {'train_runtime': 1670.8411, 'train_samples_per_second': 59.641, 'train_steps_per_second': 3.729, 'train_loss': 0.5961520090914462, 'epoch': 10.0}
2025-10-14 19:00:05 | 최종 모델 저장 중...
2025-10-14 19:00:06 | → 모델 저장 위치: experiments/20251014/20251014_183206_kobart_ultimate_kfold/fold_1/kfold/final_model
2025-10-14 19:00:06 | 최종 평가 중...
2025-10-14 19:02:14 | 최종 평가 결과:
2025-10-14 19:02:14 | eval_rouge1: 0.4675
2025-10-14 19:02:14 | eval_rouge2: 0.2989
2025-10-14 19:02:14 | eval_rougeL: 0.4566
2025-10-14 19:02:14 | eval_rouge_sum: 1.2230
2025-10-14 19:02:14 | ============================================================
2025-10-14 19:02:14 | ✅ 학습 완료!
2025-10-14 19:02:14 | ============================================================
2025-10-14 19:02:14 | ========================================
2025-10-14 19:02:14 | 📌 Fold 2/5 학습 시작
2025-10-14 19:02:14 | ========================================
2025-10-14 19:02:14 | 학습: 9965개
2025-10-14 19:02:14 | 검증: 2492개
2025-10-14 19:02:14 | 모델 타입: encoder_decoder
2025-10-14 19:02:14 | ============================================================
2025-10-14 19:02:14 | 모델 및 토크나이저 로딩 시작
2025-10-14 19:02:14 | ============================================================
2025-10-14 19:02:14 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 19:02:14 | 모델 로딩: digit82/kobart-summarization
2025-10-14 19:02:14 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 19:02:15 | → 디바이스: cuda
2025-10-14 19:02:15 | → 전체 파라미터: 123,859,968
2025-10-14 19:02:15 | → 학습 가능 파라미터: 123,859,968
2025-10-14 19:02:15 | ============================================================
2025-10-14 19:02:15 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 19:02:15 | ============================================================
2025-10-14 19:02:16 | ============================================================
2025-10-14 19:02:16 | 모델 학습 시작
2025-10-14 19:02:16 | ============================================================
2025-10-14 19:02:16 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 19:02:16 | 학습 진행 중...
2025-10-14 19:02:24 | {'loss': 2.0788, 'grad_norm': 4.7134199142456055, 'learning_rate': 1.498464e-05, 'epoch': 0.16}
2025-10-14 19:02:30 | {'loss': 1.6265, 'grad_norm': 4.299804210662842, 'learning_rate': 3.012064e-05, 'epoch': 0.32}
2025-10-14 19:02:35 | {'loss': 1.5247, 'grad_norm': 3.733032464981079, 'learning_rate': 4.525663999999999e-05, 'epoch': 0.48}
2025-10-14 19:02:41 | {'loss': 1.5213, 'grad_norm': 3.5071680545806885, 'learning_rate': 6.039264e-05, 'epoch': 0.64}
2025-10-14 19:02:47 | {'loss': 1.5002, 'grad_norm': 3.078484535217285, 'learning_rate': 7.552863999999999e-05, 'epoch': 0.8}
2025-10-14 19:02:54 | {'loss': 1.4781, 'grad_norm': 4.002840518951416, 'learning_rate': 7.437243979057592e-05, 'epoch': 0.96}
2025-10-14 19:05:10 | {'eval_loss': 1.4222439527511597, 'eval_rouge1': 0.39491805850269, 'eval_rouge2': 0.2542075116190235, 'eval_rougeL': 0.38777388503420823, 'eval_rouge_sum': 1.0368994551559219, 'eval_runtime': 135.1958, 'eval_samples_per_second': 18.433, 'eval_steps_per_second': 1.154, 'epoch': 1.0}
2025-10-14 19:05:16 | {'loss': 1.2746, 'grad_norm': 3.8462963104248047, 'learning_rate': 7.305167190226876e-05, 'epoch': 1.12}
2025-10-14 19:05:22 | {'loss': 1.2159, 'grad_norm': 3.6319632530212402, 'learning_rate': 7.17309040139616e-05, 'epoch': 1.28}
2025-10-14 19:05:27 | {'loss': 1.225, 'grad_norm': 3.1199121475219727, 'learning_rate': 7.041013612565445e-05, 'epoch': 1.44}
2025-10-14 19:05:33 | {'loss': 1.2278, 'grad_norm': 2.9416556358337402, 'learning_rate': 6.908936823734729e-05, 'epoch': 1.61}
2025-10-14 19:05:40 | {'loss': 1.2472, 'grad_norm': 3.198589324951172, 'learning_rate': 6.776860034904013e-05, 'epoch': 1.77}
2025-10-14 19:05:46 | {'loss': 1.2183, 'grad_norm': 3.3354876041412354, 'learning_rate': 6.644783246073298e-05, 'epoch': 1.93}
2025-10-14 19:07:57 | {'eval_loss': 1.3617842197418213, 'eval_rouge1': 0.4070170398469484, 'eval_rouge2': 0.25709111691684733, 'eval_rougeL': 0.39940316522974084, 'eval_rouge_sum': 1.0635113219935366, 'eval_runtime': 129.0923, 'eval_samples_per_second': 19.304, 'eval_steps_per_second': 1.208, 'epoch': 2.0}
2025-10-14 19:08:02 | {'loss': 1.0311, 'grad_norm': 3.211784839630127, 'learning_rate': 6.512706457242582e-05, 'epoch': 2.09}
2025-10-14 19:08:08 | {'loss': 0.8729, 'grad_norm': 3.1675920486450195, 'learning_rate': 6.380629668411867e-05, 'epoch': 2.25}
2025-10-14 19:08:14 | {'loss': 0.8933, 'grad_norm': 2.9413864612579346, 'learning_rate': 6.248552879581151e-05, 'epoch': 2.41}
2025-10-14 19:08:20 | {'loss': 0.9008, 'grad_norm': 3.3175320625305176, 'learning_rate': 6.116476090750436e-05, 'epoch': 2.57}
2025-10-14 19:08:26 | {'loss': 0.9283, 'grad_norm': 3.1874876022338867, 'learning_rate': 5.98439930191972e-05, 'epoch': 2.73}
2025-10-14 19:08:33 | {'loss': 0.9156, 'grad_norm': 3.1477913856506348, 'learning_rate': 5.852322513089005e-05, 'epoch': 2.89}
2025-10-14 19:10:55 | {'eval_loss': 1.3849223852157593, 'eval_rouge1': 0.4433248982593536, 'eval_rouge2': 0.2794243807793972, 'eval_rougeL': 0.43257455118241117, 'eval_rouge_sum': 1.1553238302211621, 'eval_runtime': 137.9092, 'eval_samples_per_second': 18.07, 'eval_steps_per_second': 1.131, 'epoch': 3.0}
2025-10-14 19:10:59 | {'loss': 0.8409, 'grad_norm': 2.8763034343719482, 'learning_rate': 5.720245724258289e-05, 'epoch': 3.05}
2025-10-14 19:11:05 | {'loss': 0.6381, 'grad_norm': 3.1142771244049072, 'learning_rate': 5.588168935427574e-05, 'epoch': 3.21}
2025-10-14 19:11:10 | {'loss': 0.6562, 'grad_norm': 2.7473955154418945, 'learning_rate': 5.456092146596858e-05, 'epoch': 3.37}
2025-10-14 19:11:17 | {'loss': 0.6597, 'grad_norm': 3.256941795349121, 'learning_rate': 5.324015357766143e-05, 'epoch': 3.53}
2025-10-14 19:11:23 | {'loss': 0.6796, 'grad_norm': 2.836772918701172, 'learning_rate': 5.191938568935427e-05, 'epoch': 3.69}
2025-10-14 19:11:29 | {'loss': 0.6736, 'grad_norm': 2.9901156425476074, 'learning_rate': 5.0598617801047115e-05, 'epoch': 3.85}
2025-10-14 19:13:42 | {'eval_loss': 1.482421636581421, 'eval_rouge1': 0.45738659619992855, 'eval_rouge2': 0.28849742884130597, 'eval_rougeL': 0.44547692282624707, 'eval_rouge_sum': 1.1913609478674816, 'eval_runtime': 127.8185, 'eval_samples_per_second': 19.496, 'eval_steps_per_second': 1.22, 'epoch': 4.0}
2025-10-14 19:13:44 | {'loss': 0.6682, 'grad_norm': 2.0796003341674805, 'learning_rate': 4.927784991273996e-05, 'epoch': 4.01}
2025-10-14 19:13:50 | {'loss': 0.4613, 'grad_norm': 2.629284620285034, 'learning_rate': 4.795708202443281e-05, 'epoch': 4.17}
2025-10-14 19:13:55 | {'loss': 0.475, 'grad_norm': 3.013458490371704, 'learning_rate': 4.6636314136125646e-05, 'epoch': 4.33}
2025-10-14 19:14:01 | {'loss': 0.4868, 'grad_norm': 3.0381932258605957, 'learning_rate': 4.53155462478185e-05, 'epoch': 4.49}
2025-10-14 19:14:08 | {'loss': 0.4892, 'grad_norm': 2.969614267349243, 'learning_rate': 4.399477835951134e-05, 'epoch': 4.65}
2025-10-14 19:14:14 | {'loss': 0.4963, 'grad_norm': 3.525269031524658, 'learning_rate': 4.267401047120419e-05, 'epoch': 4.82}
2025-10-14 19:14:20 | {'loss': 0.4949, 'grad_norm': 2.874645709991455, 'learning_rate': 4.135324258289703e-05, 'epoch': 4.98}
2025-10-14 19:16:33 | {'eval_loss': 1.553964614868164, 'eval_rouge1': 0.45554643096620717, 'eval_rouge2': 0.2941457014624436, 'eval_rougeL': 0.44387719765243605, 'eval_rouge_sum': 1.1935693300810868, 'eval_runtime': 132.2379, 'eval_samples_per_second': 18.845, 'eval_steps_per_second': 1.18, 'epoch': 5.0}
2025-10-14 19:16:39 | {'loss': 0.3602, 'grad_norm': 2.7772746086120605, 'learning_rate': 4.0032474694589875e-05, 'epoch': 5.14}
2025-10-14 19:16:45 | {'loss': 0.3476, 'grad_norm': 2.8933327198028564, 'learning_rate': 3.8711706806282714e-05, 'epoch': 5.3}
2025-10-14 19:16:53 | {'loss': 0.3494, 'grad_norm': 2.758112907409668, 'learning_rate': 3.739093891797556e-05, 'epoch': 5.46}
2025-10-14 19:16:59 | {'loss': 0.358, 'grad_norm': 2.6433939933776855, 'learning_rate': 3.6070171029668406e-05, 'epoch': 5.62}
2025-10-14 19:17:04 | {'loss': 0.3556, 'grad_norm': 3.8468987941741943, 'learning_rate': 3.474940314136125e-05, 'epoch': 5.78}
2025-10-14 19:17:10 | {'loss': 0.3703, 'grad_norm': 2.969109535217285, 'learning_rate': 3.34286352530541e-05, 'epoch': 5.94}
2025-10-14 19:19:20 | {'eval_loss': 1.6068288087844849, 'eval_rouge1': 0.45227523554487187, 'eval_rouge2': 0.28692200319641975, 'eval_rougeL': 0.4411419525352629, 'eval_rouge_sum': 1.1803391912765546, 'eval_runtime': 127.7571, 'eval_samples_per_second': 19.506, 'eval_steps_per_second': 1.221, 'epoch': 6.0}
2025-10-14 19:19:25 | {'loss': 0.2921, 'grad_norm': 2.3841075897216797, 'learning_rate': 3.2107867364746944e-05, 'epoch': 6.1}
2025-10-14 19:19:31 | {'loss': 0.2463, 'grad_norm': 2.3365540504455566, 'learning_rate': 3.078709947643979e-05, 'epoch': 6.26}
2025-10-14 19:19:38 | {'loss': 0.2553, 'grad_norm': 2.2422235012054443, 'learning_rate': 2.9466331588132632e-05, 'epoch': 6.42}
2025-10-14 19:19:43 | {'loss': 0.2563, 'grad_norm': 2.4070005416870117, 'learning_rate': 2.8145563699825478e-05, 'epoch': 6.58}
2025-10-14 19:19:49 | {'loss': 0.2646, 'grad_norm': 2.775299310684204, 'learning_rate': 2.682479581151832e-05, 'epoch': 6.74}
2025-10-14 19:19:54 | {'loss': 0.2657, 'grad_norm': 2.223527431488037, 'learning_rate': 2.5504027923211166e-05, 'epoch': 6.9}
2025-10-14 19:22:13 | {'eval_loss': 1.6771456003189087, 'eval_rouge1': 0.44516039176326144, 'eval_rouge2': 0.28478291918191273, 'eval_rougeL': 0.43446991898891957, 'eval_rouge_sum': 1.1644132299340937, 'eval_runtime': 134.8245, 'eval_samples_per_second': 18.483, 'eval_steps_per_second': 1.157, 'epoch': 7.0}
2025-10-14 19:22:17 | {'loss': 0.2371, 'grad_norm': 1.6593823432922363, 'learning_rate': 2.4183260034904012e-05, 'epoch': 7.06}
2025-10-14 19:22:22 | {'loss': 0.1846, 'grad_norm': 1.8930472135543823, 'learning_rate': 2.2862492146596854e-05, 'epoch': 7.22}
2025-10-14 19:22:29 | {'loss': 0.1881, 'grad_norm': 2.4548606872558594, 'learning_rate': 2.15417242582897e-05, 'epoch': 7.38}
2025-10-14 19:22:35 | {'loss': 0.192, 'grad_norm': 2.6559650897979736, 'learning_rate': 2.0220956369982546e-05, 'epoch': 7.54}
2025-10-14 19:22:41 | {'loss': 0.1957, 'grad_norm': 2.8652384281158447, 'learning_rate': 1.8900188481675392e-05, 'epoch': 7.7}
2025-10-14 19:22:47 | {'loss': 0.1912, 'grad_norm': 2.3191773891448975, 'learning_rate': 1.7579420593368238e-05, 'epoch': 7.87}
2025-10-14 19:25:05 | {'eval_loss': 1.7368847131729126, 'eval_rouge1': 0.44950054574612885, 'eval_rouge2': 0.28451792801980114, 'eval_rougeL': 0.43870757504612523, 'eval_rouge_sum': 1.1727260488120552, 'eval_runtime': 132.6104, 'eval_samples_per_second': 18.792, 'eval_steps_per_second': 1.176, 'epoch': 8.0}
2025-10-14 19:25:06 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 19:25:06 | {'train_runtime': 1370.3882, 'train_samples_per_second': 72.717, 'train_steps_per_second': 4.546, 'train_loss': 0.7117712015516302, 'epoch': 8.0}
2025-10-14 19:25:06 | 최종 모델 저장 중...
2025-10-14 19:25:07 | → 모델 저장 위치: experiments/20251014/20251014_183206_kobart_ultimate_kfold/fold_2/kfold/final_model
2025-10-14 19:25:07 | 최종 평가 중...
2025-10-14 19:27:19 | 최종 평가 결과:
2025-10-14 19:27:19 | eval_rouge1: 0.4555
2025-10-14 19:27:19 | eval_rouge2: 0.2941
2025-10-14 19:27:19 | eval_rougeL: 0.4439
2025-10-14 19:27:19 | eval_rouge_sum: 1.1936
2025-10-14 19:27:19 | ============================================================
2025-10-14 19:27:19 | ✅ 학습 완료!
2025-10-14 19:27:19 | ============================================================
2025-10-14 19:27:19 | ========================================
2025-10-14 19:27:19 | 📌 Fold 3/5 학습 시작
2025-10-14 19:27:19 | ========================================
2025-10-14 19:27:19 | 학습: 9966개
2025-10-14 19:27:19 | 검증: 2491개
2025-10-14 19:27:19 | 모델 타입: encoder_decoder
2025-10-14 19:27:19 | ============================================================
2025-10-14 19:27:19 | 모델 및 토크나이저 로딩 시작
2025-10-14 19:27:19 | ============================================================
2025-10-14 19:27:19 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 19:27:20 | 모델 로딩: digit82/kobart-summarization
2025-10-14 19:27:20 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 19:27:22 | → 디바이스: cuda
2025-10-14 19:27:22 | → 전체 파라미터: 123,859,968
2025-10-14 19:27:22 | → 학습 가능 파라미터: 123,859,968
2025-10-14 19:27:22 | ============================================================
2025-10-14 19:27:22 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 19:27:22 | ============================================================
2025-10-14 19:27:22 | ============================================================
2025-10-14 19:27:22 | 모델 학습 시작
2025-10-14 19:27:22 | ============================================================
2025-10-14 19:27:22 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 19:27:22 | 학습 진행 중...
2025-10-14 19:27:30 | {'loss': 2.069, 'grad_norm': 3.93947434425354, 'learning_rate': 1.498464e-05, 'epoch': 0.16}
2025-10-14 19:27:35 | {'loss': 1.6106, 'grad_norm': 4.53277063369751, 'learning_rate': 3.012064e-05, 'epoch': 0.32}
2025-10-14 19:27:41 | {'loss': 1.538, 'grad_norm': 4.588138103485107, 'learning_rate': 4.525663999999999e-05, 'epoch': 0.48}
2025-10-14 19:27:47 | {'loss': 1.5305, 'grad_norm': 3.871079921722412, 'learning_rate': 6.039264e-05, 'epoch': 0.64}
2025-10-14 19:27:53 | {'loss': 1.5097, 'grad_norm': 3.1847550868988037, 'learning_rate': 7.552863999999999e-05, 'epoch': 0.8}
2025-10-14 19:28:00 | {'loss': 1.4757, 'grad_norm': 3.632985830307007, 'learning_rate': 7.437243979057592e-05, 'epoch': 0.96}
2025-10-14 19:30:16 | {'eval_loss': 1.4020020961761475, 'eval_rouge1': 0.4018385906398968, 'eval_rouge2': 0.2518859914469965, 'eval_rougeL': 0.3943102195998318, 'eval_rouge_sum': 1.0480348016867251, 'eval_runtime': 134.1643, 'eval_samples_per_second': 18.567, 'eval_steps_per_second': 1.163, 'epoch': 1.0}
2025-10-14 19:30:21 | {'loss': 1.2861, 'grad_norm': 3.31866192817688, 'learning_rate': 7.305167190226876e-05, 'epoch': 1.12}
2025-10-14 19:30:27 | {'loss': 1.2287, 'grad_norm': 4.67814826965332, 'learning_rate': 7.17309040139616e-05, 'epoch': 1.28}
2025-10-14 19:30:33 | {'loss': 1.2409, 'grad_norm': 3.266814708709717, 'learning_rate': 7.041013612565445e-05, 'epoch': 1.44}
2025-10-14 19:30:39 | {'loss': 1.2157, 'grad_norm': 2.889238119125366, 'learning_rate': 6.908936823734729e-05, 'epoch': 1.61}
2025-10-14 19:30:46 | {'loss': 1.2256, 'grad_norm': 4.1451921463012695, 'learning_rate': 6.776860034904013e-05, 'epoch': 1.77}
2025-10-14 19:30:51 | {'loss': 1.2462, 'grad_norm': 2.9153175354003906, 'learning_rate': 6.644783246073298e-05, 'epoch': 1.93}
2025-10-14 19:33:04 | {'eval_loss': 1.359453797340393, 'eval_rouge1': 0.4165532970179312, 'eval_rouge2': 0.2656334728001498, 'eval_rougeL': 0.4084363129820983, 'eval_rouge_sum': 1.0906230828001793, 'eval_runtime': 130.3468, 'eval_samples_per_second': 19.111, 'eval_steps_per_second': 1.197, 'epoch': 2.0}
2025-10-14 19:33:09 | {'loss': 1.0333, 'grad_norm': 3.061655282974243, 'learning_rate': 6.512706457242582e-05, 'epoch': 2.09}
2025-10-14 19:33:15 | {'loss': 0.8985, 'grad_norm': 3.2179012298583984, 'learning_rate': 6.380629668411867e-05, 'epoch': 2.25}
2025-10-14 19:33:21 | {'loss': 0.9119, 'grad_norm': 2.928719997406006, 'learning_rate': 6.248552879581151e-05, 'epoch': 2.41}
2025-10-14 19:33:27 | {'loss': 0.8908, 'grad_norm': 3.2767651081085205, 'learning_rate': 6.116476090750436e-05, 'epoch': 2.57}
2025-10-14 19:33:34 | {'loss': 0.9144, 'grad_norm': 3.1412246227264404, 'learning_rate': 5.98439930191972e-05, 'epoch': 2.73}
2025-10-14 19:33:40 | {'loss': 0.915, 'grad_norm': 3.0054304599761963, 'learning_rate': 5.852322513089005e-05, 'epoch': 2.89}
2025-10-14 19:35:58 | {'eval_loss': 1.3840093612670898, 'eval_rouge1': 0.43008830895641315, 'eval_rouge2': 0.27141685430928536, 'eval_rougeL': 0.42181746930134173, 'eval_rouge_sum': 1.1233226325670402, 'eval_runtime': 133.7749, 'eval_samples_per_second': 18.621, 'eval_steps_per_second': 1.166, 'epoch': 3.0}
2025-10-14 19:36:01 | {'loss': 0.8239, 'grad_norm': 2.774409055709839, 'learning_rate': 5.720245724258289e-05, 'epoch': 3.05}
2025-10-14 19:36:06 | {'loss': 0.6374, 'grad_norm': 3.3734679222106934, 'learning_rate': 5.588168935427574e-05, 'epoch': 3.21}
2025-10-14 19:36:12 | {'loss': 0.6516, 'grad_norm': 3.2008745670318604, 'learning_rate': 5.456092146596858e-05, 'epoch': 3.37}
2025-10-14 19:36:20 | {'loss': 0.6598, 'grad_norm': 3.5774877071380615, 'learning_rate': 5.324015357766143e-05, 'epoch': 3.53}
2025-10-14 19:36:26 | {'loss': 0.6645, 'grad_norm': 2.9580137729644775, 'learning_rate': 5.191938568935427e-05, 'epoch': 3.69}
2025-10-14 19:36:32 | {'loss': 0.6852, 'grad_norm': 3.0171010494232178, 'learning_rate': 5.0598617801047115e-05, 'epoch': 3.85}
2025-10-14 19:38:53 | {'eval_loss': 1.4668138027191162, 'eval_rouge1': 0.44379382987998445, 'eval_rouge2': 0.2855309681275745, 'eval_rougeL': 0.4356286486934623, 'eval_rouge_sum': 1.1649534467010212, 'eval_runtime': 135.6544, 'eval_samples_per_second': 18.363, 'eval_steps_per_second': 1.15, 'epoch': 4.0}
2025-10-14 19:38:55 | {'loss': 0.6682, 'grad_norm': 2.400099277496338, 'learning_rate': 4.927784991273996e-05, 'epoch': 4.01}
2025-10-14 19:39:01 | {'loss': 0.459, 'grad_norm': 3.1629528999328613, 'learning_rate': 4.795708202443281e-05, 'epoch': 4.17}
2025-10-14 19:39:08 | {'loss': 0.4727, 'grad_norm': 3.078026056289673, 'learning_rate': 4.6636314136125646e-05, 'epoch': 4.33}
2025-10-14 19:39:14 | {'loss': 0.4783, 'grad_norm': 3.04095196723938, 'learning_rate': 4.53155462478185e-05, 'epoch': 4.49}
2025-10-14 19:39:20 | {'loss': 0.487, 'grad_norm': 2.96452260017395, 'learning_rate': 4.399477835951134e-05, 'epoch': 4.65}
2025-10-14 19:39:26 | {'loss': 0.4937, 'grad_norm': 2.8334896564483643, 'learning_rate': 4.267401047120419e-05, 'epoch': 4.82}
2025-10-14 19:39:32 | {'loss': 0.4996, 'grad_norm': 2.5825841426849365, 'learning_rate': 4.135324258289703e-05, 'epoch': 4.98}
2025-10-14 19:41:49 | {'eval_loss': 1.5553083419799805, 'eval_rouge1': 0.4466007325884223, 'eval_rouge2': 0.2843916106163865, 'eval_rougeL': 0.4373567614283279, 'eval_rouge_sum': 1.168349104633137, 'eval_runtime': 136.0843, 'eval_samples_per_second': 18.305, 'eval_steps_per_second': 1.146, 'epoch': 5.0}
2025-10-14 19:41:56 | {'loss': 0.3568, 'grad_norm': 2.850301742553711, 'learning_rate': 4.0032474694589875e-05, 'epoch': 5.14}
2025-10-14 19:42:02 | {'loss': 0.3433, 'grad_norm': 2.6162078380584717, 'learning_rate': 3.8711706806282714e-05, 'epoch': 5.3}
2025-10-14 19:42:08 | {'loss': 0.3441, 'grad_norm': 3.290910243988037, 'learning_rate': 3.739093891797556e-05, 'epoch': 5.46}
2025-10-14 19:42:13 | {'loss': 0.3516, 'grad_norm': 2.5913336277008057, 'learning_rate': 3.6070171029668406e-05, 'epoch': 5.62}
2025-10-14 19:42:19 | {'loss': 0.3601, 'grad_norm': 2.5806360244750977, 'learning_rate': 3.474940314136125e-05, 'epoch': 5.78}
2025-10-14 19:42:25 | {'loss': 0.361, 'grad_norm': 2.448672294616699, 'learning_rate': 3.34286352530541e-05, 'epoch': 5.94}
2025-10-14 19:44:41 | {'eval_loss': 1.6425691843032837, 'eval_rouge1': 0.46153187313760696, 'eval_rouge2': 0.2942255618306621, 'eval_rougeL': 0.45055557519974737, 'eval_rouge_sum': 1.2063130101680164, 'eval_runtime': 132.6836, 'eval_samples_per_second': 18.774, 'eval_steps_per_second': 1.176, 'epoch': 6.0}
2025-10-14 19:44:46 | {'loss': 0.2901, 'grad_norm': 2.1732747554779053, 'learning_rate': 3.2107867364746944e-05, 'epoch': 6.1}
2025-10-14 19:44:52 | {'loss': 0.2499, 'grad_norm': 2.440507173538208, 'learning_rate': 3.078709947643979e-05, 'epoch': 6.26}
2025-10-14 19:44:58 | {'loss': 0.2538, 'grad_norm': 2.233182430267334, 'learning_rate': 2.9466331588132632e-05, 'epoch': 6.42}
2025-10-14 19:45:04 | {'loss': 0.2554, 'grad_norm': 2.359253406524658, 'learning_rate': 2.8145563699825478e-05, 'epoch': 6.58}
2025-10-14 19:45:10 | {'loss': 0.2598, 'grad_norm': 2.5094211101531982, 'learning_rate': 2.682479581151832e-05, 'epoch': 6.74}
2025-10-14 19:45:17 | {'loss': 0.2579, 'grad_norm': 2.236180543899536, 'learning_rate': 2.5504027923211166e-05, 'epoch': 6.9}
2025-10-14 19:47:40 | {'eval_loss': 1.6983880996704102, 'eval_rouge1': 0.4537445401933591, 'eval_rouge2': 0.290450652414152, 'eval_rougeL': 0.44404917218096085, 'eval_rouge_sum': 1.188244364788472, 'eval_runtime': 139.2985, 'eval_samples_per_second': 17.882, 'eval_steps_per_second': 1.12, 'epoch': 7.0}
2025-10-14 19:47:44 | {'loss': 0.229, 'grad_norm': 2.3651864528656006, 'learning_rate': 2.4183260034904012e-05, 'epoch': 7.06}
2025-10-14 19:47:50 | {'loss': 0.1817, 'grad_norm': 1.9906305074691772, 'learning_rate': 2.2862492146596854e-05, 'epoch': 7.22}
2025-10-14 19:47:55 | {'loss': 0.187, 'grad_norm': 2.308652400970459, 'learning_rate': 2.15417242582897e-05, 'epoch': 7.38}
2025-10-14 19:48:02 | {'loss': 0.1885, 'grad_norm': 2.6737892627716064, 'learning_rate': 2.0220956369982546e-05, 'epoch': 7.54}
2025-10-14 19:48:08 | {'loss': 0.1899, 'grad_norm': 2.2559666633605957, 'learning_rate': 1.8900188481675392e-05, 'epoch': 7.7}
2025-10-14 19:48:14 | {'loss': 0.189, 'grad_norm': 2.6655755043029785, 'learning_rate': 1.7579420593368238e-05, 'epoch': 7.87}
2025-10-14 19:50:29 | {'eval_loss': 1.747298002243042, 'eval_rouge1': 0.466029153182141, 'eval_rouge2': 0.2977145794057472, 'eval_rougeL': 0.45411194280563566, 'eval_rouge_sum': 1.217855675393524, 'eval_runtime': 130.3022, 'eval_samples_per_second': 19.117, 'eval_steps_per_second': 1.197, 'epoch': 8.0}
2025-10-14 19:50:31 | {'loss': 0.1868, 'grad_norm': 1.8934555053710938, 'learning_rate': 1.625865270506108e-05, 'epoch': 8.03}
2025-10-14 19:50:37 | {'loss': 0.142, 'grad_norm': 1.8668698072433472, 'learning_rate': 1.4937884816753926e-05, 'epoch': 8.19}
2025-10-14 19:50:43 | {'loss': 0.1419, 'grad_norm': 2.3466336727142334, 'learning_rate': 1.361711692844677e-05, 'epoch': 8.35}
2025-10-14 19:50:50 | {'loss': 0.1448, 'grad_norm': 2.0038816928863525, 'learning_rate': 1.2296349040139616e-05, 'epoch': 8.51}
2025-10-14 19:50:56 | {'loss': 0.1425, 'grad_norm': 1.861206293106079, 'learning_rate': 1.097558115183246e-05, 'epoch': 8.67}
2025-10-14 19:51:02 | {'loss': 0.1401, 'grad_norm': 2.0524091720581055, 'learning_rate': 9.654813263525306e-06, 'epoch': 8.83}
2025-10-14 19:51:08 | {'loss': 0.1434, 'grad_norm': 1.7751834392547607, 'learning_rate': 8.334045375218148e-06, 'epoch': 8.99}
2025-10-14 19:53:24 | {'eval_loss': 1.7964043617248535, 'eval_rouge1': 0.4608366948766175, 'eval_rouge2': 0.2946707763263409, 'eval_rougeL': 0.4500231589396722, 'eval_rouge_sum': 1.2055306301426305, 'eval_runtime': 135.7763, 'eval_samples_per_second': 18.346, 'eval_steps_per_second': 1.149, 'epoch': 9.0}
2025-10-14 19:53:31 | {'loss': 0.1168, 'grad_norm': 1.8784390687942505, 'learning_rate': 7.013277486910994e-06, 'epoch': 9.15}
2025-10-14 19:53:38 | {'loss': 0.1163, 'grad_norm': 1.7968668937683105, 'learning_rate': 5.692509598603838e-06, 'epoch': 9.31}
2025-10-14 19:53:44 | {'loss': 0.1139, 'grad_norm': 1.9529595375061035, 'learning_rate': 4.371741710296684e-06, 'epoch': 9.47}
2025-10-14 19:53:50 | {'loss': 0.1141, 'grad_norm': 1.8904802799224854, 'learning_rate': 3.0509738219895288e-06, 'epoch': 9.63}
2025-10-14 19:53:55 | {'loss': 0.116, 'grad_norm': 1.560590386390686, 'learning_rate': 1.7302059336823733e-06, 'epoch': 9.79}
2025-10-14 19:54:01 | {'loss': 0.111, 'grad_norm': 1.5882114171981812, 'learning_rate': 4.094380453752181e-07, 'epoch': 9.95}
2025-10-14 19:56:16 | {'eval_loss': 1.816977858543396, 'eval_rouge1': 0.462730006193417, 'eval_rouge2': 0.29645457261986635, 'eval_rougeL': 0.45270491137820545, 'eval_rouge_sum': 1.211889490191489, 'eval_runtime': 133.1155, 'eval_samples_per_second': 18.713, 'eval_steps_per_second': 1.172, 'epoch': 10.0}
2025-10-14 19:56:18 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 19:56:18 | {'train_runtime': 1735.4617, 'train_samples_per_second': 57.426, 'train_steps_per_second': 3.59, 'train_loss': 0.5944418359529915, 'epoch': 10.0}
2025-10-14 19:56:18 | 최종 모델 저장 중...
2025-10-14 19:56:18 | → 모델 저장 위치: experiments/20251014/20251014_183206_kobart_ultimate_kfold/fold_3/kfold/final_model
2025-10-14 19:56:18 | 최종 평가 중...
2025-10-14 19:58:31 | 최종 평가 결과:
2025-10-14 19:58:31 | eval_rouge1: 0.4660
2025-10-14 19:58:31 | eval_rouge2: 0.2977
2025-10-14 19:58:31 | eval_rougeL: 0.4541
2025-10-14 19:58:31 | eval_rouge_sum: 1.2179
2025-10-14 19:58:31 | ============================================================
2025-10-14 19:58:31 | ✅ 학습 완료!
2025-10-14 19:58:31 | ============================================================
2025-10-14 19:58:31 | ========================================
2025-10-14 19:58:31 | 📌 Fold 4/5 학습 시작
2025-10-14 19:58:31 | ========================================
2025-10-14 19:58:31 | 학습: 9966개
2025-10-14 19:58:31 | 검증: 2491개
2025-10-14 19:58:31 | 모델 타입: encoder_decoder
2025-10-14 19:58:31 | ============================================================
2025-10-14 19:58:31 | 모델 및 토크나이저 로딩 시작
2025-10-14 19:58:31 | ============================================================
2025-10-14 19:58:31 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 19:58:32 | 모델 로딩: digit82/kobart-summarization
2025-10-14 19:58:32 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 19:58:33 | → 디바이스: cuda
2025-10-14 19:58:33 | → 전체 파라미터: 123,859,968
2025-10-14 19:58:33 | → 학습 가능 파라미터: 123,859,968
2025-10-14 19:58:33 | ============================================================
2025-10-14 19:58:33 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 19:58:33 | ============================================================
2025-10-14 19:58:33 | ============================================================
2025-10-14 19:58:33 | 모델 학습 시작
2025-10-14 19:58:33 | ============================================================
2025-10-14 19:58:33 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 19:58:33 | 학습 진행 중...
2025-10-14 19:58:41 | {'loss': 2.0856, 'grad_norm': 4.015513896942139, 'learning_rate': 1.498464e-05, 'epoch': 0.16}
2025-10-14 19:58:47 | {'loss': 1.6381, 'grad_norm': 4.073381423950195, 'learning_rate': 3.012064e-05, 'epoch': 0.32}
2025-10-14 19:58:53 | {'loss': 1.5406, 'grad_norm': 3.619155168533325, 'learning_rate': 4.525663999999999e-05, 'epoch': 0.48}
2025-10-14 19:58:59 | {'loss': 1.5101, 'grad_norm': 3.736116409301758, 'learning_rate': 6.039264e-05, 'epoch': 0.64}
2025-10-14 19:59:04 | {'loss': 1.4959, 'grad_norm': 3.898545742034912, 'learning_rate': 7.552863999999999e-05, 'epoch': 0.8}
2025-10-14 19:59:11 | {'loss': 1.4955, 'grad_norm': 3.2150328159332275, 'learning_rate': 7.437243979057592e-05, 'epoch': 0.96}
2025-10-14 20:01:20 | {'eval_loss': 1.400126576423645, 'eval_rouge1': 0.39028215358971224, 'eval_rouge2': 0.24411347243221337, 'eval_rougeL': 0.3834714554373206, 'eval_rouge_sum': 1.0178670814592463, 'eval_runtime': 127.1941, 'eval_samples_per_second': 19.584, 'eval_steps_per_second': 1.226, 'epoch': 1.0}
2025-10-14 20:01:27 | {'loss': 1.2683, 'grad_norm': 3.220890522003174, 'learning_rate': 7.305167190226876e-05, 'epoch': 1.12}
2025-10-14 20:01:33 | {'loss': 1.2193, 'grad_norm': 7.796854496002197, 'learning_rate': 7.17309040139616e-05, 'epoch': 1.28}
2025-10-14 20:01:38 | {'loss': 1.2192, 'grad_norm': 3.513819456100464, 'learning_rate': 7.041013612565445e-05, 'epoch': 1.44}
2025-10-14 20:01:44 | {'loss': 1.2368, 'grad_norm': 3.185053825378418, 'learning_rate': 6.908936823734729e-05, 'epoch': 1.61}
2025-10-14 20:01:50 | {'loss': 1.2336, 'grad_norm': 3.1941978931427, 'learning_rate': 6.776860034904013e-05, 'epoch': 1.77}
2025-10-14 20:01:56 | {'loss': 1.2564, 'grad_norm': 3.9010565280914307, 'learning_rate': 6.644783246073298e-05, 'epoch': 1.93}
2025-10-14 20:04:15 | {'eval_loss': 1.3465527296066284, 'eval_rouge1': 0.4334551528356781, 'eval_rouge2': 0.27934794859464745, 'eval_rougeL': 0.4242587305564105, 'eval_rouge_sum': 1.137061831986736, 'eval_runtime': 136.4459, 'eval_samples_per_second': 18.256, 'eval_steps_per_second': 1.143, 'epoch': 2.0}
2025-10-14 20:04:20 | {'loss': 1.0342, 'grad_norm': 3.393756628036499, 'learning_rate': 6.512706457242582e-05, 'epoch': 2.09}
2025-10-14 20:04:26 | {'loss': 0.9067, 'grad_norm': 3.0563108921051025, 'learning_rate': 6.380629668411867e-05, 'epoch': 2.25}
2025-10-14 20:04:32 | {'loss': 0.8994, 'grad_norm': 3.230788469314575, 'learning_rate': 6.248552879581151e-05, 'epoch': 2.41}
2025-10-14 20:04:38 | {'loss': 0.9074, 'grad_norm': 3.8684163093566895, 'learning_rate': 6.116476090750436e-05, 'epoch': 2.57}
2025-10-14 20:04:45 | {'loss': 0.9294, 'grad_norm': 3.171818256378174, 'learning_rate': 5.98439930191972e-05, 'epoch': 2.73}
2025-10-14 20:04:51 | {'loss': 0.9172, 'grad_norm': 3.177459716796875, 'learning_rate': 5.852322513089005e-05, 'epoch': 2.89}
2025-10-14 20:07:05 | {'eval_loss': 1.3757500648498535, 'eval_rouge1': 0.4422255295113239, 'eval_rouge2': 0.2850123410974763, 'eval_rougeL': 0.4333250143289451, 'eval_rouge_sum': 1.1605628849377454, 'eval_runtime': 130.103, 'eval_samples_per_second': 19.146, 'eval_steps_per_second': 1.199, 'epoch': 3.0}
2025-10-14 20:07:09 | {'loss': 0.8536, 'grad_norm': 3.4708423614501953, 'learning_rate': 5.720245724258289e-05, 'epoch': 3.05}
2025-10-14 20:07:14 | {'loss': 0.6406, 'grad_norm': 2.9507133960723877, 'learning_rate': 5.588168935427574e-05, 'epoch': 3.21}
2025-10-14 20:07:20 | {'loss': 0.652, 'grad_norm': 3.399359941482544, 'learning_rate': 5.456092146596858e-05, 'epoch': 3.37}
2025-10-14 20:07:26 | {'loss': 0.6785, 'grad_norm': 3.2014801502227783, 'learning_rate': 5.324015357766143e-05, 'epoch': 3.53}
2025-10-14 20:07:33 | {'loss': 0.6832, 'grad_norm': 3.2150168418884277, 'learning_rate': 5.191938568935427e-05, 'epoch': 3.69}
2025-10-14 20:07:38 | {'loss': 0.7012, 'grad_norm': 3.4280858039855957, 'learning_rate': 5.0598617801047115e-05, 'epoch': 3.85}
2025-10-14 20:09:54 | {'eval_loss': 1.4652621746063232, 'eval_rouge1': 0.46154955664487535, 'eval_rouge2': 0.2910688998494408, 'eval_rougeL': 0.44838486417549905, 'eval_rouge_sum': 1.2010033206698152, 'eval_runtime': 131.0155, 'eval_samples_per_second': 19.013, 'eval_steps_per_second': 1.191, 'epoch': 4.0}
2025-10-14 20:09:57 | {'loss': 0.6732, 'grad_norm': 2.268998384475708, 'learning_rate': 4.927784991273996e-05, 'epoch': 4.01}
2025-10-14 20:10:03 | {'loss': 0.4662, 'grad_norm': 2.9186909198760986, 'learning_rate': 4.795708202443281e-05, 'epoch': 4.17}
2025-10-14 20:10:09 | {'loss': 0.4803, 'grad_norm': 2.937772274017334, 'learning_rate': 4.6636314136125646e-05, 'epoch': 4.33}
2025-10-14 20:10:16 | {'loss': 0.4904, 'grad_norm': 3.0472006797790527, 'learning_rate': 4.53155462478185e-05, 'epoch': 4.49}
2025-10-14 20:10:22 | {'loss': 0.4936, 'grad_norm': 2.902920961380005, 'learning_rate': 4.399477835951134e-05, 'epoch': 4.65}
2025-10-14 20:10:28 | {'loss': 0.5098, 'grad_norm': 2.8551764488220215, 'learning_rate': 4.267401047120419e-05, 'epoch': 4.82}
2025-10-14 20:10:34 | {'loss': 0.5049, 'grad_norm': 2.858456611633301, 'learning_rate': 4.135324258289703e-05, 'epoch': 4.98}
2025-10-14 20:12:47 | {'eval_loss': 1.5525739192962646, 'eval_rouge1': 0.46189273153175736, 'eval_rouge2': 0.296342151373692, 'eval_rougeL': 0.44872411598090123, 'eval_rouge_sum': 1.2069589988863507, 'eval_runtime': 131.8141, 'eval_samples_per_second': 18.898, 'eval_steps_per_second': 1.183, 'epoch': 5.0}
2025-10-14 20:12:53 | {'loss': 0.3619, 'grad_norm': 2.444366455078125, 'learning_rate': 4.0032474694589875e-05, 'epoch': 5.14}
2025-10-14 20:12:59 | {'loss': 0.3485, 'grad_norm': 2.618022918701172, 'learning_rate': 3.8711706806282714e-05, 'epoch': 5.3}
2025-10-14 20:13:06 | {'loss': 0.354, 'grad_norm': 3.3741767406463623, 'learning_rate': 3.739093891797556e-05, 'epoch': 5.46}
2025-10-14 20:13:11 | {'loss': 0.3595, 'grad_norm': 2.608546495437622, 'learning_rate': 3.6070171029668406e-05, 'epoch': 5.62}
2025-10-14 20:13:17 | {'loss': 0.3676, 'grad_norm': 3.2039437294006348, 'learning_rate': 3.474940314136125e-05, 'epoch': 5.78}
2025-10-14 20:13:23 | {'loss': 0.359, 'grad_norm': 2.8451366424560547, 'learning_rate': 3.34286352530541e-05, 'epoch': 5.94}
2025-10-14 20:15:36 | {'eval_loss': 1.613148808479309, 'eval_rouge1': 0.459617723775132, 'eval_rouge2': 0.2929058951342703, 'eval_rougeL': 0.4469948368291783, 'eval_rouge_sum': 1.1995184557385805, 'eval_runtime': 131.1214, 'eval_samples_per_second': 18.998, 'eval_steps_per_second': 1.19, 'epoch': 6.0}
2025-10-14 20:15:41 | {'loss': 0.2918, 'grad_norm': 2.320993185043335, 'learning_rate': 3.2107867364746944e-05, 'epoch': 6.1}
2025-10-14 20:15:48 | {'loss': 0.2514, 'grad_norm': 2.422257900238037, 'learning_rate': 3.078709947643979e-05, 'epoch': 6.26}
2025-10-14 20:15:53 | {'loss': 0.2564, 'grad_norm': 2.582587957382202, 'learning_rate': 2.9466331588132632e-05, 'epoch': 6.42}
2025-10-14 20:15:59 | {'loss': 0.26, 'grad_norm': 2.681806802749634, 'learning_rate': 2.8145563699825478e-05, 'epoch': 6.58}
2025-10-14 20:16:05 | {'loss': 0.2677, 'grad_norm': 2.720841407775879, 'learning_rate': 2.682479581151832e-05, 'epoch': 6.74}
2025-10-14 20:16:11 | {'loss': 0.2656, 'grad_norm': 2.498622179031372, 'learning_rate': 2.5504027923211166e-05, 'epoch': 6.9}
2025-10-14 20:18:27 | {'eval_loss': 1.6785134077072144, 'eval_rouge1': 0.4807697690627791, 'eval_rouge2': 0.30936680031876557, 'eval_rougeL': 0.46723284902275886, 'eval_rouge_sum': 1.2573694184043034, 'eval_runtime': 131.9167, 'eval_samples_per_second': 18.883, 'eval_steps_per_second': 1.183, 'epoch': 7.0}
2025-10-14 20:18:30 | {'loss': 0.2351, 'grad_norm': 1.8720415830612183, 'learning_rate': 2.4183260034904012e-05, 'epoch': 7.06}
2025-10-14 20:18:37 | {'loss': 0.1846, 'grad_norm': 2.2168915271759033, 'learning_rate': 2.2862492146596854e-05, 'epoch': 7.22}
2025-10-14 20:18:43 | {'loss': 0.1917, 'grad_norm': 2.360801935195923, 'learning_rate': 2.15417242582897e-05, 'epoch': 7.38}
2025-10-14 20:18:49 | {'loss': 0.1935, 'grad_norm': 2.071913480758667, 'learning_rate': 2.0220956369982546e-05, 'epoch': 7.54}
2025-10-14 20:18:54 | {'loss': 0.1945, 'grad_norm': 2.23781418800354, 'learning_rate': 1.8900188481675392e-05, 'epoch': 7.7}
2025-10-14 20:19:00 | {'loss': 0.1949, 'grad_norm': 2.4518866539001465, 'learning_rate': 1.7579420593368238e-05, 'epoch': 7.87}
2025-10-14 20:21:17 | {'eval_loss': 1.7340099811553955, 'eval_rouge1': 0.46826544052284486, 'eval_rouge2': 0.29868840468024777, 'eval_rougeL': 0.4558115304108569, 'eval_rouge_sum': 1.2227653756139496, 'eval_runtime': 132.5105, 'eval_samples_per_second': 18.799, 'eval_steps_per_second': 1.177, 'epoch': 8.0}
2025-10-14 20:21:21 | {'loss': 0.1895, 'grad_norm': 2.1918907165527344, 'learning_rate': 1.625865270506108e-05, 'epoch': 8.03}
2025-10-14 20:21:27 | {'loss': 0.1438, 'grad_norm': 2.255923271179199, 'learning_rate': 1.4937884816753926e-05, 'epoch': 8.19}
2025-10-14 20:21:33 | {'loss': 0.1441, 'grad_norm': 2.4605064392089844, 'learning_rate': 1.361711692844677e-05, 'epoch': 8.35}
2025-10-14 20:21:39 | {'loss': 0.1479, 'grad_norm': 2.1610875129699707, 'learning_rate': 1.2296349040139616e-05, 'epoch': 8.51}
2025-10-14 20:21:44 | {'loss': 0.1439, 'grad_norm': 1.9787824153900146, 'learning_rate': 1.097558115183246e-05, 'epoch': 8.67}
2025-10-14 20:21:50 | {'loss': 0.1467, 'grad_norm': 1.9546297788619995, 'learning_rate': 9.654813263525306e-06, 'epoch': 8.83}
2025-10-14 20:21:57 | {'loss': 0.1468, 'grad_norm': 2.0323660373687744, 'learning_rate': 8.334045375218148e-06, 'epoch': 8.99}
2025-10-14 20:24:09 | {'eval_loss': 1.7691515684127808, 'eval_rouge1': 0.4678998449072714, 'eval_rouge2': 0.29922331054304724, 'eval_rougeL': 0.45465943694804306, 'eval_rouge_sum': 1.2217825923983616, 'eval_runtime': 132.2246, 'eval_samples_per_second': 18.839, 'eval_steps_per_second': 1.18, 'epoch': 9.0}
2025-10-14 20:24:16 | {'loss': 0.115, 'grad_norm': 1.8302321434020996, 'learning_rate': 7.013277486910994e-06, 'epoch': 9.15}
2025-10-14 20:24:22 | {'loss': 0.1178, 'grad_norm': 1.7510954141616821, 'learning_rate': 5.692509598603838e-06, 'epoch': 9.31}
2025-10-14 20:24:28 | {'loss': 0.1148, 'grad_norm': 1.6742048263549805, 'learning_rate': 4.371741710296684e-06, 'epoch': 9.47}
2025-10-14 20:24:33 | {'loss': 0.1163, 'grad_norm': 1.6603686809539795, 'learning_rate': 3.0509738219895288e-06, 'epoch': 9.63}
2025-10-14 20:24:40 | {'loss': 0.1202, 'grad_norm': 4.5453996658325195, 'learning_rate': 1.7302059336823733e-06, 'epoch': 9.79}
2025-10-14 20:24:46 | {'loss': 0.1141, 'grad_norm': 1.5937423706054688, 'learning_rate': 4.094380453752181e-07, 'epoch': 9.95}
2025-10-14 20:26:56 | {'eval_loss': 1.7978765964508057, 'eval_rouge1': 0.47220528527320077, 'eval_rouge2': 0.3020008127902838, 'eval_rougeL': 0.45852000458153186, 'eval_rouge_sum': 1.2327261026450165, 'eval_runtime': 128.4766, 'eval_samples_per_second': 19.389, 'eval_steps_per_second': 1.214, 'epoch': 10.0}
2025-10-14 20:26:57 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 20:26:57 | {'train_runtime': 1702.6134, 'train_samples_per_second': 58.534, 'train_steps_per_second': 3.659, 'train_loss': 0.5995971490636491, 'epoch': 10.0}
2025-10-14 20:26:57 | 최종 모델 저장 중...
2025-10-14 20:26:58 | → 모델 저장 위치: experiments/20251014/20251014_183206_kobart_ultimate_kfold/fold_4/kfold/final_model
2025-10-14 20:26:58 | 최종 평가 중...
2025-10-14 20:29:12 | 최종 평가 결과:
2025-10-14 20:29:12 | eval_rouge1: 0.4808
2025-10-14 20:29:12 | eval_rouge2: 0.3094
2025-10-14 20:29:12 | eval_rougeL: 0.4672
2025-10-14 20:29:12 | eval_rouge_sum: 1.2574
2025-10-14 20:29:12 | ============================================================
2025-10-14 20:29:12 | ✅ 학습 완료!
2025-10-14 20:29:12 | ============================================================
2025-10-14 20:29:12 | ========================================
2025-10-14 20:29:12 | 📌 Fold 5/5 학습 시작
2025-10-14 20:29:12 | ========================================
2025-10-14 20:29:12 | 학습: 9966개
2025-10-14 20:29:12 | 검증: 2491개
2025-10-14 20:29:12 | 모델 타입: encoder_decoder
2025-10-14 20:29:12 | ============================================================
2025-10-14 20:29:12 | 모델 및 토크나이저 로딩 시작
2025-10-14 20:29:12 | ============================================================
2025-10-14 20:29:12 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-14 20:29:12 | 모델 로딩: digit82/kobart-summarization
2025-10-14 20:29:13 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-14 20:29:14 | → 디바이스: cuda
2025-10-14 20:29:14 | → 전체 파라미터: 123,859,968
2025-10-14 20:29:14 | → 학습 가능 파라미터: 123,859,968
2025-10-14 20:29:14 | ============================================================
2025-10-14 20:29:14 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-14 20:29:14 | ============================================================
2025-10-14 20:29:14 | ============================================================
2025-10-14 20:29:14 | 모델 학습 시작
2025-10-14 20:29:14 | ============================================================
2025-10-14 20:29:14 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-14 20:29:14 | 학습 진행 중...
2025-10-14 20:29:20 | {'loss': 2.0728, 'grad_norm': 4.572504997253418, 'learning_rate': 1.498464e-05, 'epoch': 0.16}
2025-10-14 20:29:26 | {'loss': 1.6087, 'grad_norm': 4.131191730499268, 'learning_rate': 3.012064e-05, 'epoch': 0.32}
2025-10-14 20:29:31 | {'loss': 1.5654, 'grad_norm': 3.6993584632873535, 'learning_rate': 4.525663999999999e-05, 'epoch': 0.48}
2025-10-14 20:29:38 | {'loss': 1.5205, 'grad_norm': 3.667991876602173, 'learning_rate': 6.039264e-05, 'epoch': 0.64}
2025-10-14 20:29:44 | {'loss': 1.492, 'grad_norm': 3.7065422534942627, 'learning_rate': 7.552863999999999e-05, 'epoch': 0.8}
2025-10-14 20:29:49 | {'loss': 1.468, 'grad_norm': 3.6956686973571777, 'learning_rate': 7.437243979057592e-05, 'epoch': 0.96}
2025-10-14 20:31:56 | {'eval_loss': 1.4086072444915771, 'eval_rouge1': 0.4061375182601659, 'eval_rouge2': 0.2563636638432602, 'eval_rougeL': 0.39936280665119034, 'eval_rouge_sum': 1.0618639887546164, 'eval_runtime': 125.297, 'eval_samples_per_second': 19.881, 'eval_steps_per_second': 1.245, 'epoch': 1.0}
2025-10-14 20:32:02 | {'loss': 1.281, 'grad_norm': 3.3435261249542236, 'learning_rate': 7.305167190226876e-05, 'epoch': 1.12}
2025-10-14 20:32:08 | {'loss': 1.2126, 'grad_norm': 3.459592342376709, 'learning_rate': 7.17309040139616e-05, 'epoch': 1.28}
2025-10-14 20:32:14 | {'loss': 1.2283, 'grad_norm': 3.2379393577575684, 'learning_rate': 7.041013612565445e-05, 'epoch': 1.44}
2025-10-14 20:32:20 | {'loss': 1.2341, 'grad_norm': 2.989885091781616, 'learning_rate': 6.908936823734729e-05, 'epoch': 1.61}
2025-10-14 20:32:27 | {'loss': 1.2174, 'grad_norm': 3.030444860458374, 'learning_rate': 6.776860034904013e-05, 'epoch': 1.77}
2025-10-14 20:32:32 | {'loss': 1.2207, 'grad_norm': 3.185495615005493, 'learning_rate': 6.644783246073298e-05, 'epoch': 1.93}
2025-10-14 20:34:52 | {'eval_loss': 1.3528505563735962, 'eval_rouge1': 0.4175965983855044, 'eval_rouge2': 0.2645768608381295, 'eval_rougeL': 0.40863913518515105, 'eval_rouge_sum': 1.090812594408785, 'eval_runtime': 136.7869, 'eval_samples_per_second': 18.211, 'eval_steps_per_second': 1.14, 'epoch': 2.0}
2025-10-14 20:34:57 | {'loss': 1.0316, 'grad_norm': 3.316622495651245, 'learning_rate': 6.512706457242582e-05, 'epoch': 2.09}
2025-10-14 20:35:02 | {'loss': 0.8847, 'grad_norm': 2.9339776039123535, 'learning_rate': 6.380629668411867e-05, 'epoch': 2.25}
2025-10-14 20:35:09 | {'loss': 0.8909, 'grad_norm': 3.4836151599884033, 'learning_rate': 6.248552879581151e-05, 'epoch': 2.41}
2025-10-14 20:35:15 | {'loss': 0.902, 'grad_norm': 3.6112987995147705, 'learning_rate': 6.116476090750436e-05, 'epoch': 2.57}
2025-10-14 20:35:21 | {'loss': 0.909, 'grad_norm': 3.3875911235809326, 'learning_rate': 5.98439930191972e-05, 'epoch': 2.73}
2025-10-14 20:35:27 | {'loss': 0.9132, 'grad_norm': 3.7673535346984863, 'learning_rate': 5.852322513089005e-05, 'epoch': 2.89}
2025-10-14 20:37:35 | {'eval_loss': 1.3876978158950806, 'eval_rouge1': 0.424126689410742, 'eval_rouge2': 0.26824130099926335, 'eval_rougeL': 0.4148134267947299, 'eval_rouge_sum': 1.1071814172047352, 'eval_runtime': 124.5019, 'eval_samples_per_second': 20.008, 'eval_steps_per_second': 1.253, 'epoch': 3.0}
2025-10-14 20:37:39 | {'loss': 0.841, 'grad_norm': 3.2024035453796387, 'learning_rate': 5.720245724258289e-05, 'epoch': 3.05}
2025-10-14 20:37:44 | {'loss': 0.6309, 'grad_norm': 3.0192222595214844, 'learning_rate': 5.588168935427574e-05, 'epoch': 3.21}
2025-10-14 20:37:50 | {'loss': 0.6464, 'grad_norm': 2.8526265621185303, 'learning_rate': 5.456092146596858e-05, 'epoch': 3.37}
2025-10-14 20:37:57 | {'loss': 0.6652, 'grad_norm': 2.757248878479004, 'learning_rate': 5.324015357766143e-05, 'epoch': 3.53}
2025-10-14 20:38:03 | {'loss': 0.6673, 'grad_norm': 2.9511120319366455, 'learning_rate': 5.191938568935427e-05, 'epoch': 3.69}
2025-10-14 20:38:09 | {'loss': 0.6696, 'grad_norm': 3.233764886856079, 'learning_rate': 5.0598617801047115e-05, 'epoch': 3.85}
2025-10-14 20:40:30 | {'eval_loss': 1.4668715000152588, 'eval_rouge1': 0.4403089777564073, 'eval_rouge2': 0.27930105919720394, 'eval_rougeL': 0.4288140977994528, 'eval_rouge_sum': 1.1484241347530642, 'eval_runtime': 136.3173, 'eval_samples_per_second': 18.274, 'eval_steps_per_second': 1.144, 'epoch': 4.0}
2025-10-14 20:40:33 | {'loss': 0.6659, 'grad_norm': 2.529043197631836, 'learning_rate': 4.927784991273996e-05, 'epoch': 4.01}
2025-10-14 20:40:40 | {'loss': 0.4656, 'grad_norm': 2.961411476135254, 'learning_rate': 4.795708202443281e-05, 'epoch': 4.17}
2025-10-14 20:40:46 | {'loss': 0.4692, 'grad_norm': 2.950249195098877, 'learning_rate': 4.6636314136125646e-05, 'epoch': 4.33}
2025-10-14 20:40:51 | {'loss': 0.4807, 'grad_norm': 2.59729266166687, 'learning_rate': 4.53155462478185e-05, 'epoch': 4.49}
2025-10-14 20:40:57 | {'loss': 0.483, 'grad_norm': 3.237591505050659, 'learning_rate': 4.399477835951134e-05, 'epoch': 4.65}
2025-10-14 20:41:03 | {'loss': 0.4909, 'grad_norm': 2.8462445735931396, 'learning_rate': 4.267401047120419e-05, 'epoch': 4.82}
2025-10-14 20:41:09 | {'loss': 0.4996, 'grad_norm': 3.450397253036499, 'learning_rate': 4.135324258289703e-05, 'epoch': 4.98}
2025-10-14 20:43:17 | {'eval_loss': 1.5565416812896729, 'eval_rouge1': 0.4445457520182998, 'eval_rouge2': 0.28165847961095164, 'eval_rougeL': 0.4324793868510268, 'eval_rouge_sum': 1.1586836184802782, 'eval_runtime': 127.5809, 'eval_samples_per_second': 19.525, 'eval_steps_per_second': 1.223, 'epoch': 5.0}
2025-10-14 20:43:25 | {'loss': 0.3575, 'grad_norm': 2.3769407272338867, 'learning_rate': 4.0032474694589875e-05, 'epoch': 5.14}
2025-10-14 20:43:30 | {'loss': 0.3452, 'grad_norm': 2.8072948455810547, 'learning_rate': 3.8711706806282714e-05, 'epoch': 5.3}
2025-10-14 20:43:36 | {'loss': 0.3417, 'grad_norm': 2.415868043899536, 'learning_rate': 3.739093891797556e-05, 'epoch': 5.46}
2025-10-14 20:43:42 | {'loss': 0.3557, 'grad_norm': 2.673332691192627, 'learning_rate': 3.6070171029668406e-05, 'epoch': 5.62}
2025-10-14 20:43:47 | {'loss': 0.3589, 'grad_norm': 2.4418182373046875, 'learning_rate': 3.474940314136125e-05, 'epoch': 5.78}
2025-10-14 20:43:53 | {'loss': 0.361, 'grad_norm': 2.7628366947174072, 'learning_rate': 3.34286352530541e-05, 'epoch': 5.94}
2025-10-14 20:46:14 | {'eval_loss': 1.6340279579162598, 'eval_rouge1': 0.45480094204310484, 'eval_rouge2': 0.28783957383642594, 'eval_rougeL': 0.4434800879812743, 'eval_rouge_sum': 1.1861206038608052, 'eval_runtime': 138.9642, 'eval_samples_per_second': 17.925, 'eval_steps_per_second': 1.123, 'epoch': 6.0}
2025-10-14 20:46:19 | {'loss': 0.286, 'grad_norm': 2.2344064712524414, 'learning_rate': 3.2107867364746944e-05, 'epoch': 6.1}
2025-10-14 20:46:25 | {'loss': 0.2467, 'grad_norm': 2.301880121231079, 'learning_rate': 3.078709947643979e-05, 'epoch': 6.26}
2025-10-14 20:46:31 | {'loss': 0.2521, 'grad_norm': 3.0521860122680664, 'learning_rate': 2.9466331588132632e-05, 'epoch': 6.42}
2025-10-14 20:46:37 | {'loss': 0.2612, 'grad_norm': 3.7656304836273193, 'learning_rate': 2.8145563699825478e-05, 'epoch': 6.58}
2025-10-14 20:46:44 | {'loss': 0.2567, 'grad_norm': 2.392571449279785, 'learning_rate': 2.682479581151832e-05, 'epoch': 6.74}
2025-10-14 20:46:50 | {'loss': 0.2613, 'grad_norm': 2.4284512996673584, 'learning_rate': 2.5504027923211166e-05, 'epoch': 6.9}
2025-10-14 20:49:05 | {'eval_loss': 1.6945096254348755, 'eval_rouge1': 0.451093332970472, 'eval_rouge2': 0.2853368492401519, 'eval_rougeL': 0.43858067613781077, 'eval_rouge_sum': 1.1750108583484347, 'eval_runtime': 131.1443, 'eval_samples_per_second': 18.994, 'eval_steps_per_second': 1.19, 'epoch': 7.0}
2025-10-14 20:49:08 | {'loss': 0.2318, 'grad_norm': 1.9687789678573608, 'learning_rate': 2.4183260034904012e-05, 'epoch': 7.06}
2025-10-14 20:49:14 | {'loss': 0.1836, 'grad_norm': 2.065286159515381, 'learning_rate': 2.2862492146596854e-05, 'epoch': 7.22}
2025-10-14 20:49:20 | {'loss': 0.188, 'grad_norm': 2.235182285308838, 'learning_rate': 2.15417242582897e-05, 'epoch': 7.38}
2025-10-14 20:49:25 | {'loss': 0.1881, 'grad_norm': 2.5701868534088135, 'learning_rate': 2.0220956369982546e-05, 'epoch': 7.54}
2025-10-14 20:49:32 | {'loss': 0.1904, 'grad_norm': 2.572035074234009, 'learning_rate': 1.8900188481675392e-05, 'epoch': 7.7}
2025-10-14 20:49:38 | {'loss': 0.1878, 'grad_norm': 2.9094059467315674, 'learning_rate': 1.7579420593368238e-05, 'epoch': 7.87}
2025-10-14 20:51:55 | {'eval_loss': 1.7482651472091675, 'eval_rouge1': 0.46056561959597436, 'eval_rouge2': 0.29408044251759435, 'eval_rougeL': 0.4490621217456171, 'eval_rouge_sum': 1.2037081838591859, 'eval_runtime': 132.7715, 'eval_samples_per_second': 18.762, 'eval_steps_per_second': 1.175, 'epoch': 8.0}
2025-10-14 20:51:58 | {'loss': 0.1794, 'grad_norm': 1.873727798461914, 'learning_rate': 1.625865270506108e-05, 'epoch': 8.03}
2025-10-14 20:52:03 | {'loss': 0.1401, 'grad_norm': 1.4966294765472412, 'learning_rate': 1.4937884816753926e-05, 'epoch': 8.19}
2025-10-14 20:52:09 | {'loss': 0.1443, 'grad_norm': 1.8579751253128052, 'learning_rate': 1.361711692844677e-05, 'epoch': 8.35}
2025-10-14 20:52:16 | {'loss': 0.1461, 'grad_norm': 1.986762523651123, 'learning_rate': 1.2296349040139616e-05, 'epoch': 8.51}
2025-10-14 20:52:22 | {'loss': 0.1423, 'grad_norm': 1.9250414371490479, 'learning_rate': 1.097558115183246e-05, 'epoch': 8.67}
2025-10-14 20:52:28 | {'loss': 0.14, 'grad_norm': 1.8503514528274536, 'learning_rate': 9.654813263525306e-06, 'epoch': 8.83}
2025-10-14 20:52:34 | {'loss': 0.1408, 'grad_norm': 2.2489163875579834, 'learning_rate': 8.334045375218148e-06, 'epoch': 8.99}
2025-10-14 20:54:49 | {'eval_loss': 1.7825475931167603, 'eval_rouge1': 0.46302095895902284, 'eval_rouge2': 0.2951404056054723, 'eval_rougeL': 0.45046202297086946, 'eval_rouge_sum': 1.2086233875353645, 'eval_runtime': 134.1067, 'eval_samples_per_second': 18.575, 'eval_steps_per_second': 1.163, 'epoch': 9.0}
2025-10-14 20:54:56 | {'loss': 0.1136, 'grad_norm': 1.716342568397522, 'learning_rate': 7.013277486910994e-06, 'epoch': 9.15}
2025-10-14 20:55:02 | {'loss': 0.1124, 'grad_norm': 1.6662102937698364, 'learning_rate': 5.692509598603838e-06, 'epoch': 9.31}
2025-10-14 20:55:08 | {'loss': 0.1151, 'grad_norm': 1.7876299619674683, 'learning_rate': 4.371741710296684e-06, 'epoch': 9.47}
2025-10-14 20:55:14 | {'loss': 0.1137, 'grad_norm': 1.7441736459732056, 'learning_rate': 3.0509738219895288e-06, 'epoch': 9.63}
2025-10-14 20:55:19 | {'loss': 0.1138, 'grad_norm': 1.6399850845336914, 'learning_rate': 1.7302059336823733e-06, 'epoch': 9.79}
2025-10-14 20:55:25 | {'loss': 0.1107, 'grad_norm': 1.6542418003082275, 'learning_rate': 4.094380453752181e-07, 'epoch': 9.95}
2025-10-14 20:57:38 | {'eval_loss': 1.8124338388442993, 'eval_rouge1': 0.46031307608593597, 'eval_rouge2': 0.2933662804990186, 'eval_rougeL': 0.4487966949199752, 'eval_rouge_sum': 1.2024760515049298, 'eval_runtime': 130.9707, 'eval_samples_per_second': 19.02, 'eval_steps_per_second': 1.191, 'epoch': 10.0}
2025-10-14 20:57:39 | There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].
2025-10-14 20:57:39 | {'train_runtime': 1704.941, 'train_samples_per_second': 58.454, 'train_steps_per_second': 3.654, 'train_loss': 0.5927346402148373, 'epoch': 10.0}
2025-10-14 20:57:39 | 최종 모델 저장 중...
2025-10-14 20:57:40 | → 모델 저장 위치: experiments/20251014/20251014_183206_kobart_ultimate_kfold/fold_5/kfold/final_model
2025-10-14 20:57:40 | 최종 평가 중...
2025-10-14 20:59:54 | 최종 평가 결과:
2025-10-14 20:59:54 | eval_rouge1: 0.4630
2025-10-14 20:59:54 | eval_rouge2: 0.2951
2025-10-14 20:59:54 | eval_rougeL: 0.4505
2025-10-14 20:59:54 | eval_rouge_sum: 1.2086
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:54 | ✅ 학습 완료!
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:54 | ✅ K-FOLD 교차검증 완료!
2025-10-14 20:59:54 | 📊 평균 성능:
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:54 | 💾 결과 저장: experiments/20251014/20251014_183206_kobart_ultimate_kfold/kfold_results.json
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:54 | ✅ 학습 완료!
2025-10-14 20:59:54 | 📁 결과 저장: experiments/20251014/20251014_183206_kobart_ultimate_kfold
2025-10-14 20:59:54 | ============================================================
2025-10-14 20:59:55 | wandb: updating run metadata
2025-10-14 20:59:56 | wandb: uploading summary, console lines 541-557
2025-10-14 20:59:56 | wandb: 🚀 View run 1014-1832-kobart_ultimate_kfold at: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization/runs/vsrvwfcc
wandb: ⭐️ View project at: https://wandb.ai/kimsunmin0227-hufs/dialogue-summarization
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
2025-10-14 20:59:56 | wandb: Find logs at: ./wandb/wandb/run-20251014_183207-vsrvwfcc/logs
2025-10-14 20:59:56 | ✅ WandB 세션 종료
2025-10-14 20:59:56 | >> 로그 리디렉션 중료.
