2025-10-12 23:39:50 | >> 표준 출력 및 오류를 로그 파일로 리디렉션 시작
2025-10-12 23:39:52 | 📊 FULL 모드 실행 중...
2025-10-12 23:39:52 | ============================================================
2025-10-12 23:39:52 | = FULL PIPELINE 실행 시작
2025-10-12 23:39:52 | =대상 모델: kobart, llama-3.2-korean-3b, qwen3-4b, solar-10.7b, polyglot-ko-12.8b, kullm-v2
2025-10-12 23:39:52 | =앙상블 앙상블 전략: stacking
2025-10-12 23:39:52 | = TTA 사용: True
2025-10-12 23:39:52 | ============================================================
2025-10-12 23:39:52 | [1/6] 데이터 로딩...
2025-10-12 23:39:52 | ✅ 학습 데이터: 12457개
2025-10-12 23:39:52 | ✅ 검증 데이터: 499개
2025-10-12 23:39:52 | ⚙️ max_train_samples 적용: 학습 데이터 2000개로 제한
2025-10-12 23:39:52 | [2/6] 다중 모델 학습 (6 모델)...
2025-10-12 23:39:52 | ==================================================
2025-10-12 23:39:52 | 모델 1/6: kobart
2025-10-12 23:39:52 | ==================================================
2025-10-12 23:39:52 | 모델 타입: encoder_decoder
2025-10-12 23:39:52 | ============================================================
2025-10-12 23:39:52 | 모델 및 토크나이저 로딩 시작
2025-10-12 23:39:52 | ============================================================
2025-10-12 23:39:52 | 토크나이저 로딩: digit82/kobart-summarization
2025-10-12 23:39:52 | 모델 로딩: digit82/kobart-summarization
2025-10-12 23:39:53 | You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.
2025-10-12 23:39:54 | → 디바이스: cuda
2025-10-12 23:39:54 | → 전체 파라미터: 123,859,968
2025-10-12 23:39:54 | → 학습 가능 파라미터: 123,859,968
2025-10-12 23:39:54 | ============================================================
2025-10-12 23:39:54 | ✅ 모델 및 토크나이저 로딩 완료
2025-10-12 23:39:54 | ============================================================
2025-10-12 23:39:54 | ============================================================
2025-10-12 23:39:54 | 모델 학습 시작
2025-10-12 23:39:54 | ============================================================
2025-10-12 23:39:54 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:39:54 | 학습 진행 중...
2025-10-12 23:39:54 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-12 23:39:54 | 1%|          | 1/125 [00:00<01:02,  1.97it/s]
2025-10-12 23:39:54 | 2%|▏         | 3/125 [00:00<00:22,  5.53it/s]
2025-10-12 23:39:55 | 4%|▍         | 5/125 [00:00<00:14,  8.55it/s]
2025-10-12 23:39:55 | 6%|▌         | 7/125 [00:00<00:10, 10.91it/s]
2025-10-12 23:39:55 | 7%|▋         | 9/125 [00:00<00:09, 12.66it/s]
2025-10-12 23:39:55 | 9%|▉         | 11/125 [00:01<00:08, 14.09it/s]
2025-10-12 23:39:55 | 10%|█         | 13/125 [00:01<00:07, 15.13it/s]
2025-10-12 23:39:55 | 12%|█▏        | 15/125 [00:01<00:06, 15.78it/s]
2025-10-12 23:39:55 | 14%|█▎        | 17/125 [00:01<00:06, 16.22it/s]
2025-10-12 23:39:55 | 15%|█▌        | 19/125 [00:01<00:06, 16.52it/s]
2025-10-12 23:39:55 | 17%|█▋        | 21/125 [00:01<00:06, 16.65it/s]
2025-10-12 23:39:56 | 18%|█▊        | 23/125 [00:01<00:06, 16.82it/s]
2025-10-12 23:39:56 | 20%|██        | 25/125 [00:01<00:05, 17.26it/s]
2025-10-12 23:39:56 | 22%|██▏       | 27/125 [00:02<00:05, 17.38it/s]
2025-10-12 23:39:56 | 23%|██▎       | 29/125 [00:02<00:05, 17.47it/s]
2025-10-12 23:39:56 | 25%|██▍       | 31/125 [00:02<00:05, 17.31it/s]
2025-10-12 23:39:56 | 26%|██▋       | 33/125 [00:02<00:05, 17.20it/s]
2025-10-12 23:39:56 | 28%|██▊       | 35/125 [00:02<00:05, 17.05it/s]
2025-10-12 23:39:57 | 30%|██▉       | 37/125 [00:03<00:17,  4.91it/s]
2025-10-12 23:39:57 | 31%|███       | 39/125 [00:03<00:13,  6.24it/s]
2025-10-12 23:39:58 | 33%|███▎      | 41/125 [00:03<00:10,  7.70it/s]
2025-10-12 23:39:58 | 34%|███▍      | 43/125 [00:03<00:08,  9.24it/s]
2025-10-12 23:39:58 | 36%|███▌      | 45/125 [00:04<00:07, 10.74it/s]
2025-10-12 23:39:58 | 38%|███▊      | 47/125 [00:04<00:06, 11.92it/s]
2025-10-12 23:39:58 | 39%|███▉      | 49/125 [00:04<00:05, 12.96it/s]
2025-10-12 23:39:58 | 41%|████      | 51/125 [00:04<00:05, 14.00it/s]
2025-10-12 23:39:58 | 42%|████▏     | 53/125 [00:04<00:04, 14.79it/s]
2025-10-12 23:39:58 | 44%|████▍     | 55/125 [00:04<00:04, 15.29it/s]
2025-10-12 23:39:59 | 46%|████▌     | 57/125 [00:04<00:04, 15.49it/s]
2025-10-12 23:39:59 | 47%|████▋     | 59/125 [00:04<00:04, 15.79it/s]
2025-10-12 23:39:59 | 49%|████▉     | 61/125 [00:04<00:03, 16.13it/s]
2025-10-12 23:39:59 | 50%|█████     | 63/125 [00:05<00:03, 16.37it/s]
2025-10-12 23:39:59 | 52%|█████▏    | 65/125 [00:05<00:03, 16.54it/s]
2025-10-12 23:39:59 | 54%|█████▎    | 67/125 [00:05<00:03, 16.65it/s]
2025-10-12 23:39:59 | 55%|█████▌    | 69/125 [00:05<00:03, 16.81it/s]
2025-10-12 23:39:59 | 57%|█████▋    | 71/125 [00:05<00:03, 16.65it/s]
2025-10-12 23:39:59 | 58%|█████▊    | 73/125 [00:05<00:03, 16.74it/s]
2025-10-12 23:40:00 | 60%|██████    | 75/125 [00:05<00:02, 16.69it/s]
2025-10-12 23:40:00 | 62%|██████▏   | 77/125 [00:05<00:02, 16.75it/s]
2025-10-12 23:40:00 | 63%|██████▎   | 79/125 [00:06<00:02, 16.83it/s]
2025-10-12 23:40:00 | 65%|██████▍   | 81/125 [00:06<00:02, 16.90it/s]
2025-10-12 23:40:00 | 66%|██████▋   | 83/125 [00:06<00:02, 16.81it/s]
2025-10-12 23:40:00 | 68%|██████▊   | 85/125 [00:06<00:02, 16.98it/s]
2025-10-12 23:40:00 | 70%|██████▉   | 87/125 [00:06<00:02, 17.01it/s]
2025-10-12 23:40:00 | 71%|███████   | 89/125 [00:06<00:02, 17.04it/s]
2025-10-12 23:40:01 | 73%|███████▎  | 91/125 [00:06<00:02, 16.90it/s]
2025-10-12 23:40:01 | 74%|███████▍  | 93/125 [00:06<00:01, 16.83it/s]
2025-10-12 23:40:01 | 76%|███████▌  | 95/125 [00:07<00:01, 16.89it/s]
2025-10-12 23:40:01 | 78%|███████▊  | 97/125 [00:07<00:01, 17.03it/s]
2025-10-12 23:40:01 | 79%|███████▉  | 99/125 [00:07<00:01, 17.07it/s]
2025-10-12 23:40:01 | {'loss': 2.6874, 'grad_norm': 5.762125015258789, 'learning_rate': 9.9e-07, 'epoch': 0.8}
2025-10-12 23:40:01 | 80%|████████  | 100/125 [00:07<00:01, 17.07it/s]
2025-10-12 23:40:01 | 81%|████████  | 101/125 [00:07<00:01, 16.95it/s]
2025-10-12 23:40:01 | 82%|████████▏ | 103/125 [00:07<00:01, 16.89it/s]
2025-10-12 23:40:01 | 84%|████████▍ | 105/125 [00:07<00:01, 17.25it/s]
2025-10-12 23:40:01 | 86%|████████▌ | 107/125 [00:07<00:01, 17.44it/s]
2025-10-12 23:40:02 | 87%|████████▋ | 109/125 [00:07<00:00, 17.54it/s]
2025-10-12 23:40:02 | 89%|████████▉ | 111/125 [00:07<00:00, 17.46it/s]
2025-10-12 23:40:02 | 90%|█████████ | 113/125 [00:08<00:00, 17.63it/s]
2025-10-12 23:40:02 | 92%|█████████▏| 115/125 [00:08<00:00, 17.68it/s]
2025-10-12 23:40:02 | 94%|█████████▎| 117/125 [00:08<00:00, 17.45it/s]
2025-10-12 23:40:02 | 95%|█████████▌| 119/125 [00:08<00:00, 17.10it/s]
2025-10-12 23:40:02 | 97%|█████████▋| 121/125 [00:08<00:00, 17.38it/s]
2025-10-12 23:40:02 | 98%|█████████▊| 123/125 [00:08<00:00, 17.56it/s]
2025-10-12 23:40:02 | 100%|██████████| 125/125 [00:08<00:00, 17.24it/s]
2025-10-12 23:40:03 | 0%|          | 0/32 [00:00<?, ?it/s]
2025-10-12 23:40:03 | [A
2025-10-12 23:40:04 | 6%|▋         | 2/32 [00:00<00:13,  2.26it/s]
2025-10-12 23:40:04 | [A
2025-10-12 23:40:05 | 9%|▉         | 3/32 [00:01<00:18,  1.57it/s]
2025-10-12 23:40:05 | [A
2025-10-12 23:40:06 | 12%|█▎        | 4/32 [00:02<00:19,  1.41it/s]
2025-10-12 23:40:06 | [A
2025-10-12 23:40:07 | 16%|█▌        | 5/32 [00:03<00:22,  1.22it/s]
2025-10-12 23:40:07 | [A
2025-10-12 23:40:08 | 19%|█▉        | 6/32 [00:04<00:20,  1.24it/s]
2025-10-12 23:40:08 | [A
2025-10-12 23:40:09 | 22%|██▏       | 7/32 [00:05<00:20,  1.21it/s]
2025-10-12 23:40:09 | [A
2025-10-12 23:40:10 | 25%|██▌       | 8/32 [00:06<00:19,  1.21it/s]
2025-10-12 23:40:10 | [A
2025-10-12 23:40:10 | 28%|██▊       | 9/32 [00:06<00:19,  1.20it/s]
2025-10-12 23:40:10 | [A
2025-10-12 23:40:11 | 31%|███▏      | 10/32 [00:07<00:18,  1.21it/s]
2025-10-12 23:40:11 | [A
2025-10-12 23:40:12 | 34%|███▍      | 11/32 [00:08<00:17,  1.19it/s]
2025-10-12 23:40:12 | [A
2025-10-12 23:40:13 | 38%|███▊      | 12/32 [00:09<00:16,  1.20it/s]
2025-10-12 23:40:13 | [A
2025-10-12 23:40:14 | 41%|████      | 13/32 [00:10<00:15,  1.20it/s]
2025-10-12 23:40:14 | [A
2025-10-12 23:40:15 | 44%|████▍     | 14/32 [00:11<00:14,  1.21it/s]
2025-10-12 23:40:15 | [A
2025-10-12 23:40:15 | 100%|██████████| 125/125 [00:20<00:00, 17.24it/s]
2025-10-12 23:40:15 | 47%|████▋     | 15/32 [00:11<00:14,  1.21it/s]
2025-10-12 23:40:15 | [A
2025-10-12 23:40:16 | 50%|█████     | 16/32 [00:12<00:12,  1.24it/s]
2025-10-12 23:40:16 | [A
2025-10-12 23:40:17 | 53%|█████▎    | 17/32 [00:13<00:11,  1.27it/s]
2025-10-12 23:40:17 | [A
2025-10-12 23:40:18 | 56%|█████▋    | 18/32 [00:14<00:11,  1.22it/s]
2025-10-12 23:40:18 | [A
2025-10-12 23:40:19 | 59%|█████▉    | 19/32 [00:15<00:10,  1.24it/s]
2025-10-12 23:40:19 | [A
2025-10-12 23:40:19 | 62%|██████▎   | 20/32 [00:16<00:10,  1.20it/s]
2025-10-12 23:40:19 | [A
2025-10-12 23:40:20 | 66%|██████▌   | 21/32 [00:16<00:09,  1.20it/s]
2025-10-12 23:40:20 | [A
2025-10-12 23:40:21 | 69%|██████▉   | 22/32 [00:17<00:08,  1.21it/s]
2025-10-12 23:40:21 | [A
2025-10-12 23:40:22 | 72%|███████▏  | 23/32 [00:18<00:07,  1.19it/s]
2025-10-12 23:40:22 | [A
2025-10-12 23:40:23 | 75%|███████▌  | 24/32 [00:19<00:06,  1.22it/s]
2025-10-12 23:40:23 | [A
2025-10-12 23:40:24 | 78%|███████▊  | 25/32 [00:20<00:05,  1.22it/s]
2025-10-12 23:40:24 | [A
2025-10-12 23:40:24 | 81%|████████▏ | 26/32 [00:20<00:04,  1.21it/s]
2025-10-12 23:40:24 | [A
2025-10-12 23:40:25 | 84%|████████▍ | 27/32 [00:21<00:04,  1.19it/s]
2025-10-12 23:40:25 | [A
2025-10-12 23:40:26 | 88%|████████▊ | 28/32 [00:22<00:03,  1.18it/s]
2025-10-12 23:40:26 | [A
2025-10-12 23:40:27 | 91%|█████████ | 29/32 [00:23<00:02,  1.18it/s]
2025-10-12 23:40:27 | [A
2025-10-12 23:40:28 | 94%|█████████▍| 30/32 [00:24<00:01,  1.19it/s]
2025-10-12 23:40:28 | [A
2025-10-12 23:40:29 | 97%|█████████▋| 31/32 [00:25<00:00,  1.20it/s]
2025-10-12 23:40:29 | [A
2025-10-12 23:40:29 | 100%|██████████| 32/32 [00:25<00:00,  1.38it/s]
2025-10-12 23:40:29 | [A
2025-10-12 23:40:29 | ❌ kobart 학습 실패: OverflowError: out of range integral type conversion attempted
2025-10-12 23:40:29 | 오류 로그 저장: experiments/20251012/20251012_233950_test_full_pipeline_batch16/errors/kobart_error.log
2025-10-12 23:40:29 | ==================================================
2025-10-12 23:40:29 | 모델 2/6: llama-3.2-korean-3b
2025-10-12 23:40:29 | ==================================================
2025-10-12 23:40:29 | 모델 타입: causal_lm
2025-10-12 23:40:29 | Loading Causal LM: Bllossom/llama-3.2-Korean-Bllossom-3B
2025-10-12 23:40:29 | 모델 로딩 중...
2025-10-12 23:40:29 | `torch_dtype` is deprecated! Use `dtype` instead!
2025-10-12 23:40:29 | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
2025-10-12 23:40:29 | [A[A
2025-10-12 23:40:31 | Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.84s/it]
2025-10-12 23:40:31 | [A[A
2025-10-12 23:40:32 | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10it/s]
2025-10-12 23:40:32 | [A[A
2025-10-12 23:40:32 | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]
2025-10-12 23:40:32 | 토크나이저 로딩 중...
2025-10-12 23:40:33 | 패딩 토큰 설정: <|eot_id|>
2025-10-12 23:40:33 | LoRA 설정 적용 중...
2025-10-12 23:40:33 | 🔍 자동 탐지된 target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
2025-10-12 23:40:33 | ✅ LoRA 적용 완료
2025-10-12 23:40:33 | 학습 가능 파라미터: 24,313,856 (0.75%)
2025-10-12 23:40:33 | 전체 파라미터: 3,237,063,680
2025-10-12 23:40:33 | Input require grads 활성화 (LoRA + Gradient Checkpointing)
2025-10-12 23:40:33 | ✅ Gradient Checkpointing 활성화
2025-10-12 23:40:33 | ✅ Causal LM 로드 완료
2025-10-12 23:40:33 | ============================================================
2025-10-12 23:40:33 | 모델 학습 시작
2025-10-12 23:40:33 | ============================================================
2025-10-12 23:40:33 | WandB 로그인 상태: ieyeppo-job
2025-10-12 23:40:34 | wandb: Currently logged in as: ieyeppo-job (kimsunmin0227-hufs) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-10-12 23:40:34 | wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
2025-10-12 23:40:35 | wandb: setting up run 4v9ny9it
2025-10-12 23:40:35 | wandb: Tracking run with wandb version 0.22.2
2025-10-12 23:40:35 | wandb: Run data is saved locally in /home/ieyeppo/AI_Lab/natural-language-processing-competition/wandb/wandb/run-20251012_234034-4v9ny9it
wandb: Run `wandb offline` to turn off syncing.
2025-10-12 23:40:35 | wandb: Syncing run 1012-2340-llama_3.2_3b_qlora
2025-10-12 23:40:35 | wandb: ⭐️ View project at https://wandb.ai/ieyeppo/nlp-competition
2025-10-12 23:40:35 | wandb: 🚀 View run at https://wandb.ai/ieyeppo/nlp-competition/runs/4v9ny9it
2025-10-12 23:40:35 | wandb: Detected [openai] in use.
2025-10-12 23:40:35 | wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-10-12 23:40:35 | wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-10-12 23:40:35 | 📋 실험명: 1012-2340-llama_3.2_3b_qlora
2025-10-12 23:40:35 | 🔗 WandB URL: https://wandb.ai/ieyeppo/nlp-competition/runs/4v9ny9it
2025-10-12 23:40:35 | /home/ieyeppo/AI_Lab/natural-language-processing-competition/src/training/trainer.py:218: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
2025-10-12 23:40:35 | The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-10-12 23:40:35 | 학습 진행 중...
2025-10-12 23:40:35 | The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
2025-10-12 23:40:35 | 100%|██████████| 125/125 [00:41<00:00,  3.04it/s]
2025-10-12 23:40:35 | [A
2025-10-12 23:40:35 | 0%|          | 0/125 [00:00<?, ?it/s]
2025-10-12 23:40:47 | 1%|          | 1/125 [00:11<24:23, 11.80s/it]
2025-10-12 23:40:54 | 2%|▏         | 2/125 [00:19<19:10,  9.35s/it]
2025-10-12 23:41:11 | 3%|▎         | 4/125 [00:35<17:31,  8.69s/it]
2025-10-12 23:41:19 | 4%|▍         | 5/125 [00:43<16:36,  8.31s/it]
2025-10-12 23:41:26 | 5%|▍         | 6/125 [00:51<16:02,  8.08s/it]
2025-10-12 23:41:34 | 6%|▌         | 7/125 [00:59<15:40,  7.97s/it]
2025-10-12 23:41:50 | 7%|▋         | 9/125 [01:15<15:32,  8.04s/it]
