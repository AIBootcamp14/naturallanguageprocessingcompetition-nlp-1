# âš™ï¸ í†µí•© ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œ

## ğŸ¯ ê°œìš”
ì»´í“¨í„° ë¹„ì „ í”„ë¡œì íŠ¸ì—ì„œ ê²€ì¦ëœ ìœ ì—°í•œ ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œì„ NLP ëŒ€í™” ìš”ì•½ íƒœìŠ¤í¬ì— ìµœì í™”í•˜ì—¬ ì ìš©

## ğŸ—ï¸ ì‹¤í–‰ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    subgraph "ëª…ë ¹í–‰ ì¸í„°í˜ì´ìŠ¤"
        A[python train.py] --> B[ArgumentParser]
        B --> C[Config ë¡œë“œ]
    end

    subgraph "ì‹¤í–‰ ëª¨ë“œ ì„ íƒ"
        C --> D{Mode?}
        D -->|single| E[ë‹¨ì¼ ëª¨ë¸]
        D -->|kfold| F[K-Fold CV]
        D -->|multi| G[ë‹¤ì¤‘ ëª¨ë¸]
        D -->|optuna| H[í•˜ì´í¼íŒŒë¼ë¯¸í„°]
        D -->|full| I[ì „ì²´ íŒŒì´í”„ë¼ì¸]
    end

    subgraph "ì˜µì…˜ ì²˜ë¦¬"
        E --> J[ê¸°ë³¸ í•™ìŠµ]
        F --> K[êµì°¨ ê²€ì¦]
        G --> L[ì•™ìƒë¸”]
        H --> M[ìµœì í™”]
        I --> N[ëª¨ë“  ì˜µì…˜]
    end

    subgraph "ì¶”ê°€ ì˜µì…˜"
        J --> O{TTA?}
        K --> O
        L --> O
        M --> O
        N --> O
        O -->|Yes| P[í…ìŠ¤íŠ¸ ì¦ê°•]
        O -->|No| Q[ê¸°ë³¸ ì¶”ë¡ ]
    end

    subgraph "ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§"
        P --> R[WandB]
        Q --> R
        R --> S[ì‹œê°í™”]
        R --> T[ì²´í¬í¬ì¸íŠ¸]
    end

    style D fill:#f96,stroke:#333,stroke-width:4px
    style O fill:#9ff,stroke:#333,stroke-width:4px
    style N fill:#ff9,stroke:#333,stroke-width:4px
```

## ğŸš€ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### train.py - í†µí•© ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
```python
#!/usr/bin/env python3
"""
NLP ëŒ€í™” ìš”ì•½ í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ì»´í“¨í„° ë¹„ì „ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ NLPì— ìµœì í™”
"""

import argparse
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from src.logging import Logger, WandBLogger
from src.utils.gpu_optimization import team_gpu_check, auto_batch_size
from src.utils.config import load_config, set_seed
from src.utils.visualizations import create_training_visualizations

from src.trainers import (
    SingleModelTrainer,
    KFoldTrainer,
    MultiModelEnsembleTrainer,
    OptunaOptimizer,
    FullPipelineTrainer
)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ - ìœ ì—°í•œ ì‹¤í–‰ ì˜µì…˜',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # ==================== ê¸°ë³¸ ì„¤ì • ====================
    parser.add_argument(
        '--mode',
        type=str,
        default='single',
        choices=['single', 'kfold', 'multi_model', 'optuna', 'full'],
        help='''ì‹¤í–‰ ëª¨ë“œ ì„ íƒ:
        single: ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ ì‹¤í—˜)
        kfold: K-Fold êµì°¨ ê²€ì¦ (ì•ˆì •ì„±)
        multi_model: ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” (ì„±ëŠ¥)
        optuna: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ìë™í™”)
        full: ì „ì²´ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ì œì¶œ)'''
    )

    parser.add_argument(
        '--config',
        type=str,
        default='configs/train_config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )

    parser.add_argument(
        '--experiment_name',
        type=str,
        default=None,
        help='ì‹¤í—˜ëª… (ìë™ ìƒì„±: {mode}_{model}_{timestamp})'
    )

    # ==================== ëª¨ë¸ ì„ íƒ ====================
    parser.add_argument(
        '--models',
        type=str,
        nargs='+',
        default=['solar-10.7b'],
        choices=[
            'solar-10.7b',
            'polyglot-ko-12.8b',
            'kullm-v2',
            'koalpaca',
            'ldcc-solar',
            'all'  # ëª¨ë“  ëª¨ë¸
        ],
        help='ì‚¬ìš©í•  ëª¨ë¸ (multi_model ëª¨ë“œì—ì„œ ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)'
    )

    parser.add_argument(
        '--use_lora',
        action='store_true',
        help='LoRA ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)'
    )

    parser.add_argument(
        '--lora_r',
        type=int,
        default=16,
        help='LoRA rank'
    )

    # ==================== í•™ìŠµ ì„¤ì • ====================
    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='ì—í­ ìˆ˜'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='ë°°ì¹˜ í¬ê¸° (None: ìë™ íƒìƒ‰)'
    )

    parser.add_argument(
        '--learning_rate',
        type=float,
        default=2e-5,
        help='í•™ìŠµë¥ '
    )

    parser.add_argument(
        '--gradient_accumulation_steps',
        type=int,
        default=1,
        help='Gradient accumulation steps'
    )

    # ==================== K-Fold ì„¤ì • ====================
    parser.add_argument(
        '--k_folds',
        type=int,
        default=5,
        help='K-Fold ìˆ˜ (kfold ëª¨ë“œ)'
    )

    parser.add_argument(
        '--fold_seed',
        type=int,
        default=42,
        help='Fold ë¶„í•  ì‹œë“œ'
    )

    # ==================== ì•™ìƒë¸” ì„¤ì • ====================
    parser.add_argument(
        '--ensemble_strategy',
        type=str,
        default='weighted_avg',
        choices=[
            'averaging',
            'weighted_avg',
            'majority_vote',
            'stacking',
            'blending',
            'rouge_weighted'
        ],
        help='ì•™ìƒë¸” ì „ëµ'
    )

    parser.add_argument(
        '--ensemble_weights',
        type=float,
        nargs='+',
        default=None,
        help='ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ìë™ ìµœì í™” ê°€ëŠ¥)'
    )

    # ==================== TTA ì„¤ì • ====================
    parser.add_argument(
        '--use_tta',
        action='store_true',
        help='Test Time Augmentation ì‚¬ìš©'
    )

    parser.add_argument(
        '--tta_strategies',
        type=str,
        nargs='+',
        default=['paraphrase'],
        choices=['paraphrase', 'reorder', 'synonym', 'mask'],
        help='TTA ì „ëµ'
    )

    parser.add_argument(
        '--tta_num_aug',
        type=int,
        default=3,
        help='TTA ì¦ê°• ìˆ˜'
    )

    # ==================== Optuna ì„¤ì • ====================
    parser.add_argument(
        '--optuna_trials',
        type=int,
        default=100,
        help='Optuna ì‹œë„ íšŸìˆ˜'
    )

    parser.add_argument(
        '--optuna_timeout',
        type=int,
        default=7200,
        help='Optuna ì œí•œ ì‹œê°„ (ì´ˆ)'
    )

    parser.add_argument(
        '--optuna_sampler',
        type=str,
        default='tpe',
        choices=['tpe', 'gp', 'random', 'cmaes'],
        help='Optuna ìƒ˜í”ŒëŸ¬'
    )

    parser.add_argument(
        '--optuna_pruner',
        type=str,
        default='median',
        choices=['median', 'percentile', 'hyperband'],
        help='Optuna ê°€ì§€ì¹˜ê¸°'
    )

    parser.add_argument(
        '--optuna_n_jobs',
        type=int,
        default=1,
        help='ë³‘ë ¬ ì‹¤í–‰ ìˆ˜'
    )

    # ==================== ìƒì„± ì„¤ì • ====================
    parser.add_argument(
        '--temperature',
        type=float,
        default=0.5,
        help='ìƒì„± temperature'
    )

    parser.add_argument(
        '--top_p',
        type=float,
        default=0.9,
        help='Top-p (nucleus) sampling'
    )

    parser.add_argument(
        '--num_beams',
        type=int,
        default=4,
        help='Beam search í¬ê¸°'
    )

    parser.add_argument(
        '--max_new_tokens',
        type=int,
        default=100,
        help='ìµœëŒ€ ìƒì„± í† í°'
    )

    # ==================== ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ====================
    parser.add_argument(
        '--use_wandb',
        action='store_true',
        help='WandB ì‚¬ìš©'
    )

    parser.add_argument(
        '--wandb_project',
        type=str,
        default='dialogue-summarization',
        help='WandB í”„ë¡œì íŠ¸ëª…'
    )

    parser.add_argument(
        '--log_level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='ë¡œê·¸ ë ˆë²¨'
    )

    parser.add_argument(
        '--save_visualizations',
        action='store_true',
        help='ì‹œê°í™” ì €ì¥'
    )

    # ==================== ê¸°íƒ€ ì˜µì…˜ ====================
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='ëœë¤ ì‹œë“œ'
    )

    parser.add_argument(
        '--gpu_check',
        action='store_true',
        help='GPU í˜¸í™˜ì„± ì²´í¬'
    )

    parser.add_argument(
        '--auto_batch',
        action='store_true',
        help='ìë™ ë°°ì¹˜ í¬ê¸° íƒìƒ‰'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ (ì ì€ ë°ì´í„°)'
    )

    parser.add_argument(
        '--resume_from',
        type=str,
        default=None,
        help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ'
    )

    return parser.parse_args()

def setup_environment(args):
    """í™˜ê²½ ì„¤ì •"""
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)

    # GPU ì²´í¬
    if args.gpu_check:
        print("=" * 50)
        team_gpu_check.check_gpu_compatibility()
        print("=" * 50)

    # ìë™ ë°°ì¹˜ í¬ê¸°
    if args.auto_batch and args.batch_size is None:
        print("ğŸ” ìµœì  ë°°ì¹˜ í¬ê¸° íƒìƒ‰ ì¤‘...")
        args.batch_size = auto_batch_size.find_optimal_batch_size(
            model_name=args.models[0],
            max_length=1024
        )
        print(f"âœ… ìµœì  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")

    # ì‹¤í—˜ëª… ìë™ ìƒì„±
    if args.experiment_name is None:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = args.models[0].replace('-', '_')
        args.experiment_name = f"{args.mode}_{model_name}_{timestamp}"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(f"experiments/{args.experiment_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    args.output_dir = str(output_dir)

    # ë¡œê±° ì„¤ì •
    logger = Logger(
        log_path=output_dir / "train.log",
        print_also=True
    )
    logger.start_redirect()

    # WandB ì„¤ì •
    wandb_logger = None
    if args.use_wandb:
        wandb_logger = WandBLogger(
            project_name=args.wandb_project,
            experiment_name=args.experiment_name,
            config=vars(args)
        )

    return logger, wandb_logger

def get_trainer(args, logger, wandb_logger):
    """ëª¨ë“œì— ë”°ë¥¸ Trainer ì„ íƒ"""
    trainer_map = {
        'single': SingleModelTrainer,
        'kfold': KFoldTrainer,
        'multi_model': MultiModelEnsembleTrainer,
        'optuna': OptunaOptimizer,
        'full': FullPipelineTrainer
    }

    trainer_class = trainer_map[args.mode]
    return trainer_class(args, logger, wandb_logger)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    args = parse_arguments()

    print("=" * 50)
    print("ğŸš€ NLP ëŒ€í™” ìš”ì•½ í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“‹ ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ğŸ¤– ëª¨ë¸: {', '.join(args.models)}")
    print("=" * 50)

    # í™˜ê²½ ì„¤ì •
    logger, wandb_logger = setup_environment(args)

    try:
        # Trainer ìƒì„±
        trainer = get_trainer(args, logger, wandb_logger)

        # í•™ìŠµ ì‹¤í–‰
        print(f"\nğŸ“Š {args.mode.upper()} ëª¨ë“œ ì‹¤í–‰ ì¤‘...")
        results = trainer.train()

        # ê²°ê³¼ ì €ì¥
        trainer.save_results(results)

        # ì‹œê°í™”
        if args.save_visualizations:
            print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
            create_training_visualizations(
                fold_results=results,
                model_name=args.models[0],
                output_dir=args.output_dir
            )

        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {args.output_dir}")

        # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥
        if 'best_rouge' in results:
            print(f"ğŸ† ìµœê³  ROUGE-F1: {results['best_rouge']:.4f}")

    except Exception as e:
        logger.write(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", print_error=True)
        raise

    finally:
        # ì •ë¦¬
        logger.stop_redirect()
        logger.close()
        if wandb_logger:
            wandb_logger.finish()

if __name__ == "__main__":
    main()
```

## ğŸ“‹ ì‹¤í–‰ ì˜ˆì‹œ ëª¨ìŒ

### 1. ë¹ ë¥¸ ì‹¤í—˜ ëª¨ë“œ
```bash
# ë‹¨ì¼ ëª¨ë¸ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python train.py \
    --mode single \
    --models solar-10.7b \
    --epochs 1 \
    --debug

# LoRAë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
python train.py \
    --mode single \
    --models polyglot-ko-12.8b \
    --use_lora \
    --lora_r 8 \
    --epochs 3
```

### 2. ì•ˆì •ì  ê²€ì¦ ëª¨ë“œ
```bash
# 5-Fold êµì°¨ ê²€ì¦
python train.py \
    --mode kfold \
    --models solar-10.7b \
    --k_folds 5 \
    --epochs 5 \
    --use_wandb

# 3-Fold + TTA
python train.py \
    --mode kfold \
    --models kullm-v2 \
    --k_folds 3 \
    --use_tta \
    --tta_strategies paraphrase reorder
```

### 3. ê³ ì„±ëŠ¥ ì•™ìƒë¸” ëª¨ë“œ
```bash
# 3ê°œ ëª¨ë¸ ì•™ìƒë¸”
python train.py \
    --mode multi_model \
    --models solar-10.7b polyglot-ko-12.8b kullm-v2 \
    --ensemble_strategy weighted_avg \
    --epochs 5

# 5ê°œ ëª¨ë¸ + TTA ì•™ìƒë¸”
python train.py \
    --mode multi_model \
    --models all \
    --ensemble_strategy stacking \
    --use_tta \
    --tta_num_aug 5
```

### 4. ìë™ ìµœì í™” ëª¨ë“œ
```bash
# Optuna ê¸°ë³¸ ìµœì í™”
python train.py \
    --mode optuna \
    --models solar-10.7b \
    --optuna_trials 50 \
    --optuna_timeout 3600

# ë³‘ë ¬ Optuna (4 GPU)
python train.py \
    --mode optuna \
    --models polyglot-ko-12.8b \
    --optuna_trials 200 \
    --optuna_n_jobs 4 \
    --optuna_sampler tpe
```

### 5. í’€ íŒŒì´í”„ë¼ì¸ (ëŒ€íšŒ ìµœì¢…)
```bash
# ëª¨ë“  ì˜µì…˜ í™œì„±í™”
python train.py \
    --mode full \
    --models all \
    --k_folds 5 \
    --ensemble_strategy stacking \
    --use_tta \
    --tta_strategies paraphrase reorder synonym \
    --optuna_trials 100 \
    --use_wandb \
    --save_visualizations
```

## ğŸ¨ Config íŒŒì¼ ì‹œìŠ¤í…œ

### configs/train_config.yaml
```yaml
# ê¸°ë³¸ ì„¤ì •
experiment:
  name: ${experiment_name}
  seed: 42
  output_dir: experiments/${experiment_name}

# ë°ì´í„° ì„¤ì •
data:
  train_path: data/raw/train.csv
  dev_path: data/raw/dev.csv
  test_path: data/raw/test.csv
  max_length: 1024
  preprocessing:
    remove_noise: true
    simplify_tokens: true
    augmentation:
      enable: false
      strategies: []

# ëª¨ë¸ ì„¤ì •
model:
  name: ${model_name}
  use_lora: false
  lora_config:
    r: 16
    alpha: 32
    dropout: 0.1
  quantization:
    use_8bit: false
    use_4bit: false

# í•™ìŠµ ì„¤ì •
training:
  epochs: 3
  batch_size: 8
  learning_rate: 2e-5
  gradient_accumulation_steps: 1
  warmup_ratio: 0.1
  weight_decay: 0.01
  scheduler: cosine
  fp16: true

# ìƒì„± ì„¤ì •
generation:
  max_new_tokens: 100  # ìµœì ê°’: 100 (99.6% ì™„ì„±ë„ ë‹¬ì„±)
  min_new_tokens: 30
  temperature: 0.5
  top_p: 0.9
  num_beams: 4
  repetition_penalty: 1.5  # ìµœì ê°’: 1.5 (ì ì ˆí•œ ì–µì œ)
  no_repeat_ngram_size: 3  # ìµœì ê°’: 3 (ë°˜ë³µ ë°©ì§€)
  length_penalty: 1.0

# í‰ê°€ ì„¤ì •
evaluation:
  metrics:
    - rouge
    - bleu
  save_predictions: true

# ë¡œê¹… ì„¤ì •
logging:
  level: INFO
  use_wandb: false
  wandb_project: dialogue-summarization
  save_checkpoints: true
  checkpoint_interval: 1000
```

## ğŸ”„ ëª¨ë“œë³„ ì‹¤í–‰ íë¦„

### Single Mode íë¦„
```mermaid
graph LR
    A[ë°ì´í„° ë¡œë“œ] --> B[ëª¨ë¸ ì´ˆê¸°í™”]
    B --> C[í•™ìŠµ]
    C --> D[í‰ê°€]
    D --> E[ì €ì¥]
```

### K-Fold Mode íë¦„
```mermaid
graph LR
    A[ë°ì´í„° ë¶„í• ] --> B[Fold 1 í•™ìŠµ]
    B --> C[Fold 2 í•™ìŠµ]
    C --> D[...]
    D --> E[Fold K í•™ìŠµ]
    E --> F[ì•™ìƒë¸”]
    F --> G[ìµœì¢… í‰ê°€]
```

### Multi-Model Mode íë¦„
```mermaid
graph LR
    A[ëª¨ë¸ 1 í•™ìŠµ] --> D[ì•™ìƒë¸”]
    B[ëª¨ë¸ 2 í•™ìŠµ] --> D
    C[ëª¨ë¸ N í•™ìŠµ] --> D
    D --> E[ê°€ì¤‘ì¹˜ ìµœì í™”]
    E --> F[ìµœì¢… ì˜ˆì¸¡]
```

### Optuna Mode íë¦„
```mermaid
graph LR
    A[Trial ìƒì„±] --> B[íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§]
    B --> C[ëª¨ë¸ í•™ìŠµ]
    C --> D[ì„±ëŠ¥ í‰ê°€]
    D --> E{ìµœì ?}
    E -->|No| A
    E -->|Yes| F[ìµœì  íŒŒë¼ë¯¸í„°]
```

## ğŸ“Š ê²°ê³¼ ì €ì¥ êµ¬ì¡°

```
experiments/
â””â”€â”€ {experiment_name}/
    â”œâ”€â”€ train.log                 # í•™ìŠµ ë¡œê·¸
    â”œâ”€â”€ config.yaml               # ì‹¤í–‰ ì„¤ì •
    â”œâ”€â”€ checkpoints/              # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
    â”‚   â”œâ”€â”€ best_model.pt
    â”‚   â””â”€â”€ fold_*/
    â”œâ”€â”€ predictions/              # ì˜ˆì¸¡ ê²°ê³¼
    â”‚   â”œâ”€â”€ train_predictions.csv
    â”‚   â””â”€â”€ test_predictions.csv
    â”œâ”€â”€ visualizations/           # ì‹œê°í™”
    â”‚   â”œâ”€â”€ 01_fold_f1_performance.png
    â”‚   â”œâ”€â”€ 02_fold_accuracy_comparison.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ optuna/                   # Optuna ê²°ê³¼
    â”‚   â”œâ”€â”€ study.db
    â”‚   â””â”€â”€ best_params.json
    â””â”€â”€ results.json             # ìµœì¢… ê²°ê³¼
```

## ğŸ’¡ ì‹¤í–‰ íŒ

### ë©”ëª¨ë¦¬ ê´€ë¦¬
```bash
# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
--use_lora --lora_r 8
--gradient_accumulation_steps 4
--batch_size 4

# ìë™ ë°°ì¹˜ í¬ê¸° íƒìƒ‰
--auto_batch --gpu_check
```

### ì‹¤í—˜ ê´€ë¦¬
```bash
# ì‹¤í—˜ ì¬ê°œ
--resume_from experiments/previous_exp/checkpoints/last.pt

# ë””ë²„ê·¸ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
--debug --epochs 1 --optuna_trials 5
```

### ì„±ëŠ¥ ìµœì í™”
```bash
# Mixed Precision
--fp16

# ë³‘ë ¬ ì²˜ë¦¬
--optuna_n_jobs 4

# ìºì‹± í™œìš©
--use_cache
```

## ğŸ† ì¶”ì²œ ì‹¤í–‰ ìˆœì„œ

### ëŒ€íšŒ ì œì¶œìš© ìµœì  í”„ë¡œì„¸ìŠ¤
```bash
# 1ë‹¨ê³„: ë¹ ë¥¸ ì‹¤í—˜ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
python train.py --mode single --models all --epochs 1 --debug

# 2ë‹¨ê³„: ìµœê³  ëª¨ë¸ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
python train.py --mode optuna --models solar-10.7b --optuna_trials 100

# 3ë‹¨ê³„: ìµœì  íŒŒë¼ë¯¸í„°ë¡œ K-Fold ê²€ì¦
python train.py --mode kfold --k_folds 5 --config best_config.yaml

# 4ë‹¨ê³„: ìƒìœ„ 3ê°œ ëª¨ë¸ ì•™ìƒë¸”
python train.py --mode multi_model --models top3 --ensemble_strategy stacking

# 5ë‹¨ê³„: ìµœì¢… ì œì¶œ (í’€ íŒŒì´í”„ë¼ì¸ + TTA)
python train.py --mode full --use_tta --save_visualizations
```