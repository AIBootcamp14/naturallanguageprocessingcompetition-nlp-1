# 🔧 기술 요구사항

## 💻 개발 환경

### Python 환경
- **Python 버전**: 3.11.9
- **가상환경**: pyenv + virtualenv
- **환경 이름**: nlp_py3_11_9

### GPU 환경
- **GPU 서버**: AI Stages 제공 서버
- **운영 기간**: 2025.09.26 ~ 2025.11.13
- **CUDA**: 11.8 이상
- **GPU 메모리**: 최소 16GB 권장

## 📦 주요 라이브러리

### 핵심 라이브러리
```python
# 딥러닝 프레임워크
torch>=2.0.0
transformers>=4.30.0
pytorch-lightning>=2.0.0
accelerate>=0.20.0
peft>=0.4.0  # LoRA/QLoRA용

# 한국어 NLP
konlpy>=0.6.0
soynlp>=0.0.493
kss>=4.0.0

# 평가 지표
rouge>=1.0.1
nltk>=3.8
sacrebleu>=2.3.0

# 데이터 처리
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# 시각화
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0
wordcloud>=1.9.0

# 실험 관리
wandb>=0.15.0
mlflow>=2.3.0
optuna>=3.1.0

# API
openai>=1.0.0  # Solar API용
requests>=2.31.0

# 유틸리티
tqdm>=4.65.0
pyyaml>=6.0
python-dotenv>=1.0.0
jupyterlab>=4.0.0
argparse  # 실행 옵션 시스템용
```

### 모델별 추가 요구사항

#### SOLAR-10.7B (최우선 추천)
```python
# Hugging Face 모델
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("upstage/SOLAR-10.7B-Instruct-v1.0")
```

#### Polyglot-Ko-12.8B
```python
# Hugging Face 모델
model = AutoModelForCausalLM.from_pretrained("EleutherAI/polyglot-ko-12.8b")
```

#### KULLM-v2
```python
model = AutoModelForCausalLM.from_pretrained("nlpai-lab/kullm-v2")
```

#### Solar API
```python
pip install openai
# API Key: up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT
```

## 🗂️ 데이터 요구사항

### 저장 공간
- **원본 데이터**: ~50MB
- **전처리 데이터**: ~100MB
- **증강 데이터**: ~500MB
- **모델 체크포인트**: ~5GB per model
- **총 권장 공간**: 50GB 이상

### 데이터 형식
- **입력**: CSV (fname, dialogue, summary, topic)
- **출력**: CSV (fname, summary)
- **인코딩**: UTF-8
- **구분자**: 쉼표(,)

## ⚙️ 시스템 요구사항

### 최소 사양
- **CPU**: 8 cores
- **RAM**: 32GB
- **GPU**: RTX 3090 (24GB)
- **Storage**: 100GB SSD

### 권장 사양
- **CPU**: 16+ cores
- **RAM**: 64GB+
- **GPU**: A100 (40GB+) 또는 RTX 4090 (24GB)
- **Storage**: 500GB+ NVMe SSD

### 모델별 GPU 메모리 요구사항
| 모델 | 기본 | LoRA | QLoRA (4bit) |
|------|------|------|--------------|
| SOLAR-10.7B | 24GB | 16GB | 8GB |
| Polyglot-Ko-12.8B | 26GB | 18GB | 10GB |
| KULLM-v2 (7B) | 16GB | 12GB | 6GB |
| 5-모델 앙상블 | 48GB+ | 32GB | 20GB |

## 🔐 보안 요구사항

### API 키 관리
```python
# .env 파일 사용
UPSTAGE_API_KEY=up_rMJWNzzBi6YsD47RhwXPWZrZ0JKsT
WANDB_API_KEY=your_wandb_key
```

### Git 설정
```bash
# .gitignore
*.csv
*.pkl
*.pth
*.bin
.env
__pycache__/
wandb/
mlruns/
```

## 📝 코드 스타일 가이드

### Python 스타일
- **PEP 8** 준수
- **Type Hints** 사용
- **Docstring** 작성 (Google Style)

### 예시
```python
from typing import List, Dict, Tuple

def preprocess_dialogue(
    dialogue: str,
    max_length: int = 512
) -> Tuple[str, Dict[str, int]]:
    """
    대화문을 전처리합니다.

    Args:
        dialogue: 원본 대화문
        max_length: 최대 토큰 길이

    Returns:
        전처리된 대화문과 통계 정보
    """
    # 구현 내용
    pass
```

## 🧪 테스트 요구사항

### 단위 테스트
```python
# tests/test_preprocessing.py
import pytest

def test_remove_noise():
    input_text = "안녕\\n하세요"
    expected = "안녕\n하세요"
    assert remove_noise(input_text) == expected
```

### 통합 테스트
- 데이터 로딩 → 전처리 → 모델 → 평가
- End-to-End 파이프라인 검증

## 📊 성능 요구사항

### 학습 속도
- **Batch 처리**: < 1초/batch
- **Epoch 시간**: < 30분
- **전체 학습**: < 10시간

### 추론 속도
- **단일 샘플**: < 100ms
- **배치 (32)**: < 2초
- **전체 테스트셋 (250)**: < 5분

### 메모리 사용량
- **학습 시**: < 20GB GPU Memory
- **추론 시**: < 8GB GPU Memory

## 🔄 배포 요구사항

### Docker 환경
```dockerfile
FROM pytorch/pytorch:2.0.0-cuda11.8-cudnn8-runtime

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "main.py"]
```

### 제출 형식
```bash
# 최종 제출 파일 구조
submission/
├── output.csv        # 예측 결과
├── code/            # 실행 코드
├── model/           # 모델 체크포인트
└── README.md        # 실행 방법
```