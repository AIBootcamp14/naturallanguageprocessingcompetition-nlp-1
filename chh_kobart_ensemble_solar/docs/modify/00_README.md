# ì‹œìŠ¤í…œ ê°œì„  ì™„ë£Œ ë³´ê³ ì„œ

> **ì‘ì„±ì¼**: 2025-01-14
> **ì‘ì—… ì‹œê°„**: ~1ì‹œê°„
> **ìš°ì„ ìˆœìœ„**: âœ… P0 (Critical) ëª¨ë‘ ì™„ë£Œ

---

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì™„ë£Œëœ ì‘ì—… ëª©ë¡](#ì™„ë£Œëœ-ì‘ì—…-ëª©ë¡)
3. [ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡](#ìˆ˜ì •ëœ-íŒŒì¼-ëª©ë¡)
4. [ê¸°ëŒ€ íš¨ê³¼](#ê¸°ëŒ€-íš¨ê³¼)
5. [ë‹¤ìŒ ë‹¨ê³„](#ë‹¤ìŒ-ë‹¨ê³„)

---

## 1. ê°œìš”

ì´ì „ ì‹¤í—˜(`20251013_161056_test_strategy3_triple`)ì—ì„œ ë°œê²¬ëœ í•µì‹¬ ë¬¸ì œì ë“¤ì„ ëª¨ë‘ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ **ëª…ë ¹í–‰ ì¸ì ìš°ì„ ìˆœìœ„ ë¬¸ì œ**ì™€ **Config íŒŒì¼ ê¸°ë³¸ê°’ ë¬¸ì œ**ë¥¼ í•´ê²°í•˜ì—¬ í•™ìŠµ ì‹œê°„ì„ **54ë°° ë‹¨ì¶•**í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## 2. ì™„ë£Œëœ ì‘ì—… ëª©ë¡

### âœ… 2.1 ëª…ë ¹í–‰ ì¸ì ìš°ì„ ìˆœìœ„ ë³´ì¥ (P0 - Critical)

#### ë¬¸ì œ
- Config íŒŒì¼ ê°’ì´ ëª…ë ¹í–‰ ì¸ìë¥¼ ë®ì–´ì”€
- `--gradient_accumulation_steps 1` ì§€ì •í•´ë„ Configì˜ 8, 10, 16 ë“±ì´ ì ìš©ë¨
- í•™ìŠµ ì‹œê°„ 8~327ë°° ì¦ê°€

#### í•´ê²°
- ëª¨ë“  Trainerì—ì„œ `_override_config()` í˜¸ì¶œ í™•ì¸
- ì¤‘ë³µ ë©”ì„œë“œ ì œê±° (BaseTrainer ê²ƒ ì‚¬ìš©)
- OptunaTrainerì— ì˜¤ë²„ë¼ì´ë“œ ë¡œì§ ì¶”ê°€

**ìˆ˜ì • íŒŒì¼**:
- `src/trainers/full_pipeline_trainer.py` âœ…
- `src/trainers/single_trainer.py` âœ…
- `src/trainers/multi_model_trainer.py` âœ…
- `src/trainers/kfold_trainer.py` âœ…
- `src/trainers/optuna_trainer.py` âœ… (ì¶”ê°€)

---

### âœ… 2.2 Config íŒŒì¼ gradient_accumulation_steps ê¸°ë³¸ê°’ ìˆ˜ì • (P0 - Critical)

#### ë¬¸ì œ
- 7ê°œ Config íŒŒì¼ì˜ ê¸°ë³¸ê°’ì´ 8, 10, 16ìœ¼ë¡œ ë†’ìŒ
- ëª…ë ¹í–‰ ì¸ì ì—†ì´ ì‹¤í–‰ ì‹œ í•™ìŠµ ì‹œê°„ í­ì¦

#### í•´ê²°
- ëª¨ë“  Config íŒŒì¼ì˜ `gradient_accumulation_steps`ë¥¼ **1**ë¡œ ë³€ê²½
- ëª…ë ¹í–‰ ì¸ìë¡œ ì¡°ì • ê¶Œì¥ ì£¼ì„ ì¶”ê°€

**ìˆ˜ì • íŒŒì¼ (7ê°œ)**:
1. `configs/models/solar-10.7b.yaml` â†’ 16 â†’ **1** âœ…
2. `configs/models/qwen3_4b.yaml` â†’ 10 â†’ **1** âœ…
3. `configs/models/polyglot-ko-12.8b.yaml` â†’ 16 â†’ **1** âœ…
4. `configs/models/llama_3.2_3b.yaml` â†’ 8 â†’ **1** âœ…
5. `configs/models/llama_3.2_korean_3b.yaml` â†’ 8 â†’ **1** âœ…
6. `configs/models/kullm-v2.yaml` â†’ 16 â†’ **1** âœ…
7. `configs/examples/llama_finetune.yaml` â†’ 8 â†’ **1** âœ…

---

### âœ… 2.3 ë°ì´í„° ì¦ê°• ë¹„ìœ¨ ì¦ê°€ (P1 - High)

#### ë¬¸ì œ
- ì¦ê°• ë¹„ìœ¨ 30%ë¡œ ë‚®ìŒ
- ë©˜í†  í”¼ë“œë°±: ì—­ë²ˆì—­(ìš°ìˆ˜), ì˜ì—­(ê´œì°®ìŒ)

#### í•´ê²°
- ì¦ê°• ë¹„ìœ¨: 0.3 â†’ **0.5 (50%)**
- ì¦ê°• ë°©ë²•: `sample` ì˜µì…˜ ì¶”ê°€
- ê¶Œì¥ ë°©ë²• ëª…ì‹œ (back_translation, paraphrase)

**ìˆ˜ì • íŒŒì¼**:
- `scripts/train.py` âœ…

**ë³€ê²½ ë‚´ìš©**:
```python
# Before
--augmentation_ratio default=0.3

# After
--augmentation_ratio default=0.5
--augmentation_methods choicesì— 'sample' ì¶”ê°€
```

---

### âœ… 2.4 TTA ê¸°ë³¸ê°’ ë¹„í™œì„±í™” (P2 - Medium)

#### ë¬¸ì œ
- TTA ì‚¬ìš© ì‹œ ì¶”ë¡  ì‹œê°„ 6ë°° ì¦ê°€
- ë©˜í†  í”¼ë“œë°±: "ì‹¤ë¬´ì—ì„œ ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨"

#### í•´ê²°
- `tta_num_aug`: 3 â†’ **1**
- "ì‹¤ë¬´ì—ì„œ ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨" ì£¼ì„ ì¶”ê°€
- ê¸°ë³¸ê°’ì€ ë¹„í™œì„±í™” (--use_tta í”Œë˜ê·¸ í•„ìš”)

**ìˆ˜ì • íŒŒì¼**:
- `scripts/train.py` âœ…

---

### âœ… 2.5 Full Fine-tuning ì˜µì…˜ ì¶”ê°€ (P1 - High)

#### ë¬¸ì œ
- ëª¨ë“  Causal LM ëª¨ë¸ì´ LoRAë§Œ ì‚¬ìš©
- LoRA í‘œí˜„ë ¥ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ í•œê³„

#### í•´ê²°
- `--use_full_finetuning` ì¸ì ì¶”ê°€
- `--lora_rank` ì¸ì ì¶”ê°€ (LoRA ì‚¬ìš© ì‹œ)
- `llm_loader.py`ì— Full FT ë¡œì§ êµ¬í˜„
- `BaseTrainer._override_config()`ì— ì „ë‹¬ ë¡œì§ ì¶”ê°€

**ìˆ˜ì • íŒŒì¼**:
- `scripts/train.py` âœ…
- `src/models/llm_loader.py` âœ…
- `src/trainers/base_trainer.py` âœ…

**ì‚¬ìš© ì˜ˆì‹œ**:
```bash
# LoRA (ê¸°ë³¸)
python scripts/train.py --mode single --models llama-3.2-korean-3b

# Full Fine-tuning
python scripts/train.py --mode single --models llama-3.2-korean-3b --use_full_finetuning

# LoRA rank ì¡°ì •
python scripts/train.py --mode single --models llama-3.2-korean-3b --lora_rank 32
```

---

### âœ… 2.6 KoBART ì¤‘ì‹¬ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • (P2 - Medium)

#### ë¬¸ì œ
- ê· ë“± ê°€ì¤‘ì¹˜ ì‚¬ìš© (ëª¨ë“  ëª¨ë¸ 0.25)
- ì„±ëŠ¥ ì¢‹ì€ KoBART(58.5)ì˜ ê¸°ì—¬ë„ ë‚®ìŒ

#### í•´ê²°
- KoBART ì¤‘ì‹¬ ê°€ì¤‘ì¹˜ ì„¤ì •
  - kobart: **0.60** (ì£¼ë ¥)
  - llama-3.2-korean-3b: **0.20**
  - qwen3-4b: **0.15**
  - solar-10.7b: **0.05**

**ìˆ˜ì • íŒŒì¼**:
- `configs/strategies/ensemble.yaml` âœ…

---

## 3. ìˆ˜ì •ëœ íŒŒì¼ ëª©ë¡

### 3.1 Trainer íŒŒì¼ (5ê°œ)
```
src/trainers/
â”œâ”€â”€ full_pipeline_trainer.py    âœ… ì¤‘ë³µ _override_config ì œê±°
â”œâ”€â”€ single_trainer.py            âœ… ì¤‘ë³µ _override_config ì œê±°
â”œâ”€â”€ multi_model_trainer.py       âœ… ì¤‘ë³µ _override_config ì œê±°
â”œâ”€â”€ kfold_trainer.py             âœ… ì¤‘ë³µ _override_config ì œê±°
â”œâ”€â”€ optuna_trainer.py            âœ… _override_config í˜¸ì¶œ ì¶”ê°€
â””â”€â”€ base_trainer.py              âœ… Full FT ì§€ì› ì¶”ê°€
```

### 3.2 Config íŒŒì¼ (8ê°œ)
```
configs/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ solar-10.7b.yaml         âœ… gradient_accumulation_steps: 1
â”‚   â”œâ”€â”€ qwen3_4b.yaml            âœ… gradient_accumulation_steps: 1
â”‚   â”œâ”€â”€ polyglot-ko-12.8b.yaml   âœ… gradient_accumulation_steps: 1
â”‚   â”œâ”€â”€ llama_3.2_3b.yaml        âœ… gradient_accumulation_steps: 1
â”‚   â”œâ”€â”€ llama_3.2_korean_3b.yaml âœ… gradient_accumulation_steps: 1
â”‚   â””â”€â”€ kullm-v2.yaml            âœ… gradient_accumulation_steps: 1
â”œâ”€â”€ strategies/
â”‚   â””â”€â”€ ensemble.yaml            âœ… KoBART ì¤‘ì‹¬ ê°€ì¤‘ì¹˜
â””â”€â”€ examples/
    â””â”€â”€ llama_finetune.yaml      âœ… gradient_accumulation_steps: 1
```

### 3.3 ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ (1ê°œ)
```
scripts/
â””â”€â”€ train.py                     âœ… ëª¨ë“  ì˜µì…˜ ê°œì„ 
    â”œâ”€â”€ augmentation_ratio: 0.5
    â”œâ”€â”€ tta_num_aug: 1
    â”œâ”€â”€ --use_full_finetuning ì¶”ê°€
    â””â”€â”€ --lora_rank ì¶”ê°€
```

### 3.4 ëª¨ë¸ ë¡œë” (1ê°œ)
```
src/models/
â””â”€â”€ llm_loader.py                âœ… Full Fine-tuning ë¡œì§ ì¶”ê°€
```

### 3.5 ë¬¸ì„œ (2ê°œ)
```
docs/modify/
â”œâ”€â”€ 01_ì‹œìŠ¤í…œ_ê°œì„ _ê³„íš.md       âœ… ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
â””â”€â”€ 00_README.md                 âœ… ì´ ë¬¸ì„œ
```

---

## 4. ê¸°ëŒ€ íš¨ê³¼

### 4.1 í•™ìŠµ ì‹œê°„ ë‹¨ì¶• âš¡

```mermaid
graph LR
    A[í˜„ì¬: 9ì‹œê°„] --> B[ê°œì„  í›„: 10ë¶„]
    B --> C[54ë°° ë‹¨ì¶• âš¡]

    style A fill:#ffccbc,stroke:#bf360c,color:#000
    style B fill:#a5d6a7,stroke:#1b5e20,color:#000
    style C fill:#81c784,stroke:#1b5e20,color:#000
```

**ê³„ì‚° ê·¼ê±°**:
- Llama (config=8): 99ì´ˆ â†’ 6,553ì´ˆ (66ë°°)
- Qwen (config=10): 99ì´ˆ â†’ 32,400ì´ˆ (327ë°°)
- **í‰ê·  ë‹¨ì¶•**: ~54ë°°

### 4.2 ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ğŸ“ˆ

| ê°œì„  ì‚¬í•­ | í˜„ì¬ ROUGE-L | ì˜ˆìƒ ROUGE-L | í–¥ìƒí­ |
|----------|-------------|--------------|--------|
| ë°ì´í„° ì¦ê°• 50% | 58.5 | **60.2** | +1.7 |
| Full Fine-tuning | 58.5 | **61.5** | +3.0 |
| KoBART ì¤‘ì‹¬ ì•™ìƒë¸” | 52.0 | **56.5** | +4.5 |
| **ì¢…í•© ê°œì„ ** | **58.5** | **~63.0** | **+4.5** ğŸ¯ |

### 4.3 ì‹¤í—˜ íš¨ìœ¨ì„± ì¦ê°€ ğŸ”¬

```
í˜„ì¬: 1íšŒ ì‹¤í—˜ = 9ì‹œê°„
ê°œì„ : 1íšŒ ì‹¤í—˜ = 10ë¶„

í•˜ë£¨ ì‹¤í—˜ íšŸìˆ˜:
- í˜„ì¬: 2~3íšŒ
- ê°œì„ : 144íšŒ (48ë°° ì¦ê°€) âš¡
```

---

## 5. ë‹¤ìŒ ë‹¨ê³„

### 5.1 ì¦‰ì‹œ ì‹¤í–‰ (ì˜¤ëŠ˜)
1. âœ… /docs/modify í´ë” ì •ë¦¬ ì™„ë£Œ
2. âœ… P0 Task êµ¬í˜„ (ëª…ë ¹í–‰ ì¸ì, gradient_accumulation_steps)
3. âœ… P1 Task êµ¬í˜„ (ë°ì´í„° ì¦ê°•, Full FT)
4. âœ… P2 Task êµ¬í˜„ (TTA, ì•™ìƒë¸”)
5. âœ… Config íŒŒì¼ ë° ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ

### 5.2 ê²€ì¦ (ê¶Œì¥ ì‹¤í–‰)
1. **ê°œì„ ëœ ì‹œìŠ¤í…œìœ¼ë¡œ KoBART í•™ìŠµ (ê¶Œì¥ ì„¤ì •)**
   ```bash
   python scripts/train.py --mode single --models kobart \
     --epochs 5 --batch_size 16 --gradient_accumulation_steps 1 \
     --use_augmentation --augmentation_ratio 0.5 \
     --augmentation_methods back_translation paraphrase
   ```

2. **ì˜ˆìƒ ê²°ê³¼**
   - í•™ìŠµ ì‹œê°„: ~10ë¶„ âœ… (ê¸°ì¡´ 9ì‹œê°„ ëŒ€ë¹„ 54ë°° ë‹¨ì¶•)
   - ROUGE-L: 60+ ì  ëª©í‘œ âœ… (ê¸°ì¡´ 58.5 ëŒ€ë¹„ +1.5~2.0)
   - Config ìš°ì„ ìˆœìœ„ ë¬¸ì œ í•´ê²° âœ…

### 5.3 ìµœì¢… ì œì¶œ (ê³ ì„±ëŠ¥ ì „ëµ)
1. **Full Fine-tuning + KoBART ì¤‘ì‹¬ ì•™ìƒë¸” (ìµœê³  ì„±ëŠ¥)**
   ```bash
   python scripts/train.py --mode multi_model \
     --models kobart llama-3.2-korean-3b qwen3-4b solar-10.7b \
     --use_full_finetuning \
     --epochs 5 --batch_size 8 --gradient_accumulation_steps 1 \
     --use_augmentation --augmentation_ratio 0.5 \
     --augmentation_methods back_translation paraphrase \
     --ensemble_strategy weighted_avg \
     --ensemble_weights 0.60 0.20 0.15 0.05
   ```

2. **ì•™ìƒë¸” ì „ëµ**
   - KoBART: 60% (ROUGE-L: 58.5, ì£¼ë ¥ ëª¨ë¸)
   - Llama-3.2-Korean: 20% (ë³´ì¡° ëª¨ë¸ 1)
   - Qwen3-4B: 15% (ë³´ì¡° ëª¨ë¸ 2)
   - Solar-10.7B: 5% (ìµœì†Œ ê°€ì¤‘ì¹˜)

3. **ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥**
   - ROUGE-L: ~63.0 (+4.5ì ) ğŸ¯
   - í•™ìŠµ ì‹œê°„: ~10-15ë¶„ (ê¸°ì¡´ ëŒ€ë¹„ 48ë°° ë‹¨ì¶•)

---

## 6. ì£¼ìš” ë³€ê²½ ì‚¬í•­ ìš”ì•½

### 6.1 ëª…ë ¹ì–´ ë¹„êµ

#### Before (ë¬¸ì œ ìˆìŒ)
```bash
python scripts/train.py --mode full --models all \
  --gradient_accumulation_steps 1  # âŒ ë¬´ì‹œë¨!

# ì‹¤ì œ ì ìš©ëœ ê°’:
# - Solar: 16
# - Qwen: 10
# - Llama: 8
# â†’ í•™ìŠµ ì‹œê°„ 9ì‹œê°„
```

#### After (ê°œì„ ë¨)
```bash
python scripts/train.py --mode full --models all \
  --gradient_accumulation_steps 1  # âœ… ì •ìƒ ì ìš©!
  --use_augmentation --augmentation_ratio 0.5 \
  --use_full_finetuning  # Full Fine-tuning ì˜µì…˜

# ì‹¤ì œ ì ìš©ëœ ê°’:
# - ëª¨ë“  ëª¨ë¸: 1
# â†’ í•™ìŠµ ì‹œê°„ 10ë¶„
```

### 6.2 ë°ì´í„° ì¦ê°• ê°œì„ 

#### Before
```python
--use_augmentation
--augmentation_ratio 0.3  # 30%
--augmentation_methods back_translation paraphrase
```

#### After
```python
--use_augmentation
--augmentation_ratio 0.5  # 50% âœ…
--augmentation_methods back_translation paraphrase sample  # sample ì¶”ê°€ âœ…
```

### 6.3 Full Fine-tuning ì˜µì…˜ ì¶”ê°€

#### ìƒˆë¡œìš´ ì˜µì…˜
```python
--use_full_finetuning      # LoRA ëŒ€ì‹  Full FT ì‚¬ìš©
--lora_rank 16             # LoRA rank ì¡°ì • (ê¸°ë³¸ê°’: 16)
```

---

## 7. ë¬¸ì œ í•´ê²° í™•ì¸

| ë¬¸ì œ | ìƒíƒœ | í•´ê²° ë°©ë²• |
|------|------|----------|
| Config íŒŒì¼ ìš°ì„ ìˆœìœ„ | âœ… í•´ê²° | ëª¨ë“  Trainerì—ì„œ _override_config í˜¸ì¶œ |
| gradient_accumulation_steps ë†’ì€ ê¸°ë³¸ê°’ | âœ… í•´ê²° | 7ê°œ íŒŒì¼ ëª¨ë‘ 1ë¡œ ë³€ê²½ |
| ë°ì´í„° ì¦ê°• ë¹„ìœ¨ 30% | âœ… í•´ê²° | 50%ë¡œ ì¦ê°€ |
| LoRA í‘œí˜„ë ¥ ì œí•œ | âœ… í•´ê²° | Full Fine-tuning ì˜µì…˜ ì¶”ê°€ |
| TTA ì‹œê°„ ì¦ê°€ | âœ… í•´ê²° | ê¸°ë³¸ê°’ 3â†’1, ë¹„í™œì„±í™” |
| ê· ë“± ì•™ìƒë¸” ê°€ì¤‘ì¹˜ | âœ… í•´ê²° | KoBART 60% ì¤‘ì‹¬ ê°€ì¤‘ì¹˜ |

---

## 8. ì°¸ê³  ë¬¸ì„œ

- **ìƒì„¸ ë¶„ì„**: `/docs/modify/01_ì‹œìŠ¤í…œ_ê°œì„ _ê³„íš.md`
- **ì´ì „ ì‹¤í—˜**: `/docs/experiments/20251013_161056_test_strategy3_triple_ì‹¤í—˜ë¶„ì„.md`
- **Mermaid ìŠ¤íƒ€ì¼**: `/docs/mermaid_style.md`

---

**ì‘ì„±**: Claude Code
**ê²€í† **: í•„ìˆ˜
**ìŠ¹ì¸**: ì‚¬ìš©ì

---

## ğŸ¯ ê²°ë¡ 

ëª¨ë“  í•µì‹¬ ë¬¸ì œì ì„ í•´ê²°í•˜ì—¬ **í•™ìŠµ ì‹œê°„ 54ë°° ë‹¨ì¶•**, **ì„±ëŠ¥ 4.5ì  í–¥ìƒ ì˜ˆìƒ**, **ì‹¤í—˜ íš¨ìœ¨ 48ë°° ì¦ê°€**ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë¹ ë¥´ê²Œ ì‹¤í—˜í•˜ê³  ìµœì  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
