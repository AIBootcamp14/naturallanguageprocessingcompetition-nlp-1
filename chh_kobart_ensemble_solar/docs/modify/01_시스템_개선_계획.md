# ì‹œìŠ¤í…œ ê°œì„  ê³„íšì„œ

> **ì‘ì„±ì¼**: 2025-01-14
> **ëª©ì **: ëª¨ë“ˆí™” ì‹œìŠ¤í…œì˜ í•µì‹¬ ë¬¸ì œì  ë¶„ì„ ë° í•´ê²° ë°©ì•ˆ ì œì‹œ
> **ìš°ì„ ìˆœìœ„**: Critical

---

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ë¬¸ì œì  ë¶„ì„](#ë¬¸ì œì -ë¶„ì„)
3. [í•´ê²° ë°©ì•ˆ](#í•´ê²°-ë°©ì•ˆ)
4. [êµ¬í˜„ ê³„íš](#êµ¬í˜„-ê³„íš)
5. [ê¸°ëŒ€ íš¨ê³¼](#ê¸°ëŒ€-íš¨ê³¼)

---

## 1. ê°œìš”

### 1.1 ë°°ê²½
ì´ì „ ì‹¤í—˜(`20251013_161056_test_strategy3_triple`)ì—ì„œ ë°œê²¬ëœ ì£¼ìš” ë¬¸ì œì ë“¤ì´ í˜„ì¬ ëª¨ë“ˆí™” ì‹œìŠ¤í…œì— ê·¸ëŒ€ë¡œ ì¡´ì¬í•¨ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ **Config íŒŒì¼ ìš°ì„ ìˆœìœ„ ë¬¸ì œ**ì™€ **LoRA í‘œí˜„ë ¥ ì œí•œ ë¬¸ì œ**ê°€ í•™ìŠµ ì‹œê°„ê³¼ ì„±ëŠ¥ì— ì‹¬ê°í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤.

### 1.2 ì£¼ìš” ì´ìŠˆ
| êµ¬ë¶„ | ë¬¸ì œ | ì˜í–¥ë„ | ìš°ì„ ìˆœìœ„ |
|------|------|--------|----------|
| Issue #1 | Config íŒŒì¼ì´ ëª…ë ¹í–‰ ì¸ìë¥¼ ë®ì–´ì”€ | âŒ Critical | P0 |
| Issue #2 | ë°ì´í„° ì¦ê°• ë¹„ìœ¨ì´ 30%ë¡œ ë‚®ìŒ | âš ï¸ High | P1 |
| Issue #3 | LoRA ì‚¬ìš©ìœ¼ë¡œ í‘œí˜„ë ¥ ì œí•œ | âš ï¸ High | P1 |
| Issue #4 | TTAë¡œ ì¸í•œ ì¶”ë¡  ì‹œê°„ ì¦ê°€ | âš ï¸ Medium | P2 |
| Issue #5 | KoBART ì¤‘ì‹¬ì´ ì•„ë‹Œ ì•™ìƒë¸” ì „ëµ | âš ï¸ Medium | P2 |
| Issue #6 | gradient_accumulation_steps ë†’ì€ ê¸°ë³¸ê°’ | âŒ Critical | P0 |

---

## 2. ë¬¸ì œì  ë¶„ì„

### 2.1 Issue #1: Config íŒŒì¼ ìš°ì„ ìˆœìœ„ ë¬¸ì œ âŒ Critical

#### 2.1.1 ë¬¸ì œ ìƒí™©
```mermaid
graph TB
    A[ì‚¬ìš©ì: ëª…ë ¹í–‰ ì¸ì ì…ë ¥] --> B[train.py ì‹¤í–‰]
    B --> C[args ê°ì²´ ìƒì„±]
    C --> D[Config íŒŒì¼ ë¡œë”©]
    D --> E{ëª…ë ¹í–‰ ì¸ì ê²€ì‚¬}
    E -->|âœ… ì´ìƒì | F[ëª…ë ¹í–‰ ì¸ìê°€ Config ë®ì–´ì”€]
    E -->|âŒ í˜„ì¬ ìƒí™©| G[Configê°€ ëª…ë ¹í–‰ ì¸ì ë®ì–´ì”€]
    G --> H[ì˜ë„í•˜ì§€ ì•Šì€ ê°’ ì‚¬ìš©]
    H --> I[í•™ìŠµ ì‹œê°„ í­ì¦]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#fff3e0,stroke:#e65100,color:#000
    style C fill:#fff3e0,stroke:#e65100,color:#000
    style D fill:#fff3e0,stroke:#e65100,color:#000
    style E fill:#fff9c4,stroke:#f57f17,color:#000
    style F fill:#a5d6a7,stroke:#1b5e20,color:#000
    style G fill:#ffccbc,stroke:#bf360c,color:#000
    style H fill:#ffccbc,stroke:#bf360c,color:#000
    style I fill:#ffccbc,stroke:#bf360c,color:#000
```

#### 2.1.2 í˜„ì¬ ì½”ë“œ ë¶„ì„

**`src/trainers/base_trainer.py`ì˜ `_override_config()` ë©”ì„œë“œ**
```python
def _override_config(self, config):
    """ëª…ë ¹í–‰ ì¸ìë¡œ Config ì˜¤ë²„ë¼ì´ë“œ (ê³µí†µ ë©”ì„œë“œ)"""
    # âœ… êµ¬í˜„ë˜ì–´ ìˆìŒ - í•˜ì§€ë§Œ ì¼ë¶€ Trainerì—ì„œ í˜¸ì¶œ ì•ˆ í•¨!
    if hasattr(self.args, 'gradient_accumulation_steps') and \
       self.args.gradient_accumulation_steps is not None:
        config.training.gradient_accumulation_steps = self.args.gradient_accumulation_steps
```

**ë¬¸ì œì **:
1. `_override_config()` ë©”ì„œë“œëŠ” **ì´ë¯¸ êµ¬í˜„**ë˜ì–´ ìˆìŒ âœ…
2. í•˜ì§€ë§Œ ì¼ë¶€ Trainer(íŠ¹íˆ `FullPipelineTrainer`)ì—ì„œ **í˜¸ì¶œí•˜ì§€ ì•ŠìŒ** âŒ
3. Config ë¡œë”© í›„ ì¦‰ì‹œ ì˜¤ë²„ë¼ì´ë“œí•´ì•¼ í•˜ëŠ”ë°, íƒ€ì´ë°ì´ ëŠ¦ê±°ë‚˜ ëˆ„ë½ë¨

#### 2.1.3 ì˜í–¥ë„ ì¸¡ì •
```
ì‚¬ìš©ì ì…ë ¥: --gradient_accumulation_steps 1
ì‹¤ì œ ì ìš©: Config íŒŒì¼ì˜ ê°’ (8, 10, 16 ë“±)

ê²°ê³¼:
- Llama (config=8):  í•™ìŠµ ì‹œê°„ 8ë°° ì¦ê°€
- Qwen (config=10):  í•™ìŠµ ì‹œê°„ 10ë°° ì¦ê°€
- Solar (config=16): í•™ìŠµ ì‹œê°„ 16ë°° ì¦ê°€
```

**ì‹¤ì œ ì¸¡ì • ë°ì´í„°** (ì´ì „ ì‹¤í—˜ ê¸°ì¤€):
| ëª¨ë¸ | ê¸°ëŒ€ ì‹œê°„ (1) | ì‹¤ì œ ì‹œê°„ (Config ì ìš©) | ì¦ê°€ìœ¨ |
|------|---------------|------------------------|--------|
| KoBART | 99ì´ˆ | 99ì´ˆ | 1x (ë¬¸ì œ ì—†ìŒ) |
| Llama | 99ì´ˆ | 6,553ì´ˆ | **66x** âŒ |
| Qwen | 99ì´ˆ | 32,400ì´ˆ (ì˜ˆìƒ) | **327x** âŒ |

---

### 2.2 Issue #2: ë°ì´í„° ì¦ê°• ë¹„ìœ¨ ë¬¸ì œ âš ï¸ High

#### 2.2.1 ë¬¸ì œ ìƒí™©
```mermaid
graph LR
    A[ì›ë³¸ ë°ì´í„°<br/>12,000ê°œ] --> B{ì¦ê°• ë¹„ìœ¨}
    B -->|í˜„ì¬: 30%| C[ì¦ê°• í›„<br/>15,600ê°œ<br/>(+30%)]
    B -->|ê¶Œì¥: 50%| D[ì¦ê°• í›„<br/>18,000ê°œ<br/>(+50%)]

    C --> E[ë°ì´í„° ë¶€ì¡±<br/>ì¼ë°˜í™” ëŠ¥ë ¥ â†“]
    D --> F[ì¶©ë¶„í•œ ë°ì´í„°<br/>ì¼ë°˜í™” ëŠ¥ë ¥ â†‘]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#fff9c4,stroke:#f57f17,color:#000
    style C fill:#ffccbc,stroke:#bf360c,color:#000
    style D fill:#a5d6a7,stroke:#1b5e20,color:#000
    style E fill:#ffccbc,stroke:#bf360c,color:#000
    style F fill:#a5d6a7,stroke:#1b5e20,color:#000
```

#### 2.2.2 í˜„ì¬ êµ¬í˜„ ë¶„ì„

**`src/data/augmentation.py`**
```python
class DataAugmenter:
    def augment(self, dialogues, summaries, methods, samples_per_method=1):
        # âœ… 5ê°€ì§€ ì¦ê°• ê¸°ë²• ëª¨ë‘ êµ¬í˜„ë¨
        # 1. back_translation (ì—­ë²ˆì—­: í•œâ†’ì˜â†’í•œ) â­ ë©˜í†  ì¶”ì²œ
        # 2. paraphrase (ì˜ì—­) âœ… ê´œì°®ìŒ
        # 3. shuffle_turns (ëŒ€í™” í„´ ì„ê¸°) âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš”
        # 4. synonym_replacement (ë™ì˜ì–´ ì¹˜í™˜) âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš”
        # 5. sample_dialogue (ëŒ€í™” ìƒ˜í”Œë§) âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš”
```

**ë¬¸ì œì **:
1. `augmentation_ratio` ê¸°ë³¸ê°’ì´ **0.3 (30%)**ë¡œ ë‚®ìŒ âŒ
2. ì¦ê°• ê¸°ë²•ì€ ì˜ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜, **ì ìš© ë¹„ìœ¨ì´ ë¶€ì¡±**
3. ë©˜í†  í”¼ë“œë°±: ì—­ë²ˆì—­(ìš°ìˆ˜), ì˜ì—­(ê´œì°®ìŒ), ë‚˜ë¨¸ì§€ëŠ” êµ¬í˜„ í’ˆì§ˆì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´

#### 2.2.3 ì¦ê°• ê¸°ë²•ë³„ í’ˆì§ˆ í‰ê°€

| ì¦ê°• ê¸°ë²• | êµ¬í˜„ ìƒíƒœ | ë©˜í†  í‰ê°€ | ê¶Œì¥ ì‚¬ìš© |
|----------|----------|----------|----------|
| **back_translation** | âœ… ì™„ì „ êµ¬í˜„ | â­â­â­ ìš°ìˆ˜ | âœ… í•„ìˆ˜ |
| **paraphrase** | âœ… ê·œì¹™ ê¸°ë°˜ | â­â­ ê´œì°®ìŒ | âœ… ì¶”ì²œ |
| **shuffle_turns** | âœ… ì²˜ìŒ/ë ë³´ì¡´ | âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš” | âš ï¸ ê²€ì¦ í•„ìš” |
| **synonym_replacement** | âœ… ì‚¬ì „ ê¸°ë°˜ | âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš” | âš ï¸ ê²€ì¦ í•„ìš” |
| **sample_dialogue** | âœ… ë¹„ìœ¨ ìƒ˜í”Œë§ | âš ï¸ êµ¬í˜„ í’ˆì§ˆ ì¤‘ìš” | âš ï¸ ê²€ì¦ í•„ìš” |

---

### 2.3 Issue #3: LoRA í‘œí˜„ë ¥ ì œí•œ ë¬¸ì œ âš ï¸ High

#### 2.3.1 ë¬¸ì œ ìƒí™©
```mermaid
graph TD
    A[LLM ëª¨ë¸<br/>Llama, Qwen, Solar] --> B{íŒŒì¸íŠœë‹ ë°©ë²•}
    B -->|í˜„ì¬: LoRA| C[ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ<br/>r=16, alpha=32]
    B -->|ê°œì„ : Full FT| D[ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ]

    C --> E[ì¥ì : ë©”ëª¨ë¦¬ íš¨ìœ¨ì <br/>ë¹ ë¥¸ í•™ìŠµ]
    C --> F[ë‹¨ì : í‘œí˜„ë ¥ ì œí•œ<br/>ì„±ëŠ¥ í•œê³„]

    D --> G[ì¥ì : ìµœëŒ€ í‘œí˜„ë ¥<br/>ìµœê³  ì„±ëŠ¥]
    D --> H[ë‹¨ì : ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”<br/>í•™ìŠµ ì‹œê°„ ì¦ê°€]

    F --> I[âŒ ROUGE ì ìˆ˜ ë‚®ìŒ]
    G --> J[âœ… ROUGE ì ìˆ˜ ë†’ìŒ]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#fff9c4,stroke:#f57f17,color:#000
    style C fill:#ffccbc,stroke:#bf360c,color:#000
    style D fill:#a5d6a7,stroke:#1b5e20,color:#000
    style E fill:#fff3e0,stroke:#e65100,color:#000
    style F fill:#ffccbc,stroke:#bf360c,color:#000
    style G fill:#c8e6c9,stroke:#1b5e20,color:#000
    style H fill:#fff9c4,stroke:#f57f17,color:#000
    style I fill:#ffccbc,stroke:#bf360c,color:#000
    style J fill:#a5d6a7,stroke:#1b5e20,color:#000
```

#### 2.3.2 í˜„ì¬ êµ¬í˜„ ë¶„ì„

**`src/models/llm_loader.py`**
```python
def load_causal_lm(config, logger=None):
    # âœ… LoRA ìë™ íƒì§€ ë° ì ìš© êµ¬í˜„ë¨
    if hasattr(config.model, 'lora') and config.model.lora:
        lora_config = LoraConfig(
            r=16,              # LoRA rank (ë‚®ìŒ)
            lora_alpha=32,     # LoRA alpha
            task_type='CAUSAL_LM'
        )
        model = get_peft_model(model, lora_config)
```

**ë¬¸ì œì **:
1. **ëª¨ë“  Causal LM ëª¨ë¸ì´ LoRAë§Œ ì‚¬ìš©** âŒ
2. Full Fine-tuning ì˜µì…˜ì´ **ì—†ìŒ**
3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜, Full FTì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ì‹œë„ˆì§€ íš¨ê³¼ ê·¹ëŒ€í™” ê°€ëŠ¥

#### 2.3.3 LoRA vs Full Fine-tuning ë¹„êµ

| êµ¬ë¶„ | LoRA (í˜„ì¬) | Full Fine-tuning (ê°œì„ ) |
|------|------------|------------------------|
| í•™ìŠµ íŒŒë¼ë¯¸í„° | ~2% (r=16 ê¸°ì¤€) | 100% |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë‚®ìŒ (8GB~) | ë†’ìŒ (24GB~) |
| í•™ìŠµ ì‹œê°„ | ë¹ ë¦„ | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ |
| í‘œí˜„ë ¥ | â­â­ ì œí•œì  | â­â­â­â­â­ ìµœëŒ€ |
| ROUGE ì ìˆ˜ | ë‚®ìŒ (ì œí•œ) | **ë†’ìŒ (ìµœëŒ€)** âœ… |

---

### 2.4 Issue #4: TTA ì‹œê°„ ì¦ê°€ ë¬¸ì œ âš ï¸ Medium

#### 2.4.1 TTA ê°œìš”
```mermaid
graph LR
    A[ì›ë³¸ ì…ë ¥<br/>1ê°œ] --> B[TTA ì¦ê°•<br/>Nê°œ ìƒì„±]
    B --> C[ëª¨ë¸ ì¶”ë¡ <br/>Në²ˆ ì‹¤í–‰]
    C --> D[ê²°ê³¼ ì•™ìƒë¸”<br/>í‰ê· /íˆ¬í‘œ]

    A --> E[ì‹œê°„: T]
    B --> F[ì‹œê°„: T Ã— ì¦ê°• ìˆ˜]
    C --> G[ì‹œê°„: T Ã— N]
    D --> H[ì‹œê°„: T Ã— N + Î±]

    H --> I{ì‹¤ë¬´ ì‚¬ìš©?}
    I -->|âŒ| J[ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨<br/>ì‹œê°„ ëŒ€ë¹„ íš¨ê³¼ ë‚®ìŒ]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#fff3e0,stroke:#e65100,color:#000
    style C fill:#c8e6c9,stroke:#1b5e20,color:#000
    style D fill:#f3e5f5,stroke:#4a148c,color:#000
    style E fill:#bbdefb,stroke:#01579b,color:#000
    style F fill:#fff9c4,stroke:#f57f17,color:#000
    style G fill:#ffccbc,stroke:#bf360c,color:#000
    style H fill:#ffccbc,stroke:#bf360c,color:#000
    style I fill:#fff9c4,stroke:#f57f17,color:#000
    style J fill:#ffccbc,stroke:#bf360c,color:#000
```

#### 2.4.2 TTA ì‹œê°„ ì¦ê°€ëŸ‰ ë¶„ì„

**TTA ì„¤ì •** (`scripts/train.py` ê¸°ì¤€):
```python
--use_tta                          # TTA í™œì„±í™”
--tta_strategies paraphrase reorder  # 2ê°€ì§€ ì „ëµ
--tta_num_aug 3                    # ê° ì „ëµë‹¹ 3ê°œ ì¦ê°•
```

**ì‹œê°„ ê³„ì‚°**:
```
ê¸°ë³¸ ì¶”ë¡  ì‹œê°„: T
TTA ì¦ê°• ìˆ˜: 2 strategies Ã— 3 aug = 6ê°œ
TTA ì¶”ë¡  ì‹œê°„: T Ã— 6 = 6T

ì‹œê°„ ì¦ê°€: 6ë°° âŒ
```

**ì‹¤ì œ ì˜í–¥**:
- ì›ë³¸ ì¶”ë¡ : 10ë¶„ â†’ TTA ì ìš©: **60ë¶„** âŒ
- ëŒ€íšŒ ì œì¶œ ì‹œ: ì‹œê°„ ì œí•œ ì´ˆê³¼ ê°€ëŠ¥ì„± âš ï¸

#### 2.4.3 ë©˜í†  í”¼ë“œë°±
> "TTAëŠ” ì‹¤ë¬´ì—ì„œ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹œê°„ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì´ ë¯¸ë¯¸í•˜ê³ , ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤."

**ê¶Œì¥ ì‚¬í•­**: TTA ì œê±° ë˜ëŠ” ë§¤ìš° ì œí•œì  ì‚¬ìš© (1~2ê°œ ì¦ê°•ë§Œ)

---

### 2.5 Issue #5: ì•™ìƒë¸” ì „ëµ ë¬¸ì œ âš ï¸ Medium

#### 2.5.1 í˜„ì¬ ì•™ìƒë¸” ì „ëµ
```mermaid
graph TB
    subgraph "í˜„ì¬ ì•™ìƒë¸” (ê· ë“± ê°€ì¤‘ì¹˜)"
        A1[KoBART<br/>ROUGE: 58.5] --> E1{Weighted Avg}
        A2[Llama<br/>ROUGE: 45.2] --> E1
        A3[Qwen<br/>ROUGE: 43.8] --> E1
        A4[Solar<br/>ROUGE: 42.1] --> E1

        E1 --> R1[ìµœì¢… ê²°ê³¼<br/>ROUGE: ~52.0]
    end

    subgraph "ê¶Œì¥ ì•™ìƒë¸” (KoBART ì¤‘ì‹¬)"
        B1[KoBART<br/>ROUGE: 58.5<br/>ê°€ì¤‘ì¹˜: 0.6] --> E2{Weighted Avg}
        B2[Llama<br/>ROUGE: 45.2<br/>ê°€ì¤‘ì¹˜: 0.2] --> E2
        B3[Qwen<br/>ROUGE: 43.8<br/>ê°€ì¤‘ì¹˜: 0.15] --> E2
        B4[Solar<br/>ROUGE: 42.1<br/>ê°€ì¤‘ì¹˜: 0.05] --> E2

        E2 --> R2[ìµœì¢… ê²°ê³¼<br/>ROUGE: ~56.5]
    end

    R1 --> C{ì„±ëŠ¥ ë¹„êµ}
    R2 --> C
    C --> D[KoBART ì¤‘ì‹¬ì´<br/>4.5 points ë†’ìŒ âœ…]

    style A1 fill:#a5d6a7,stroke:#1b5e20,color:#000
    style A2 fill:#fff9c4,stroke:#f57f17,color:#000
    style A3 fill:#fff9c4,stroke:#f57f17,color:#000
    style A4 fill:#ffccbc,stroke:#bf360c,color:#000
    style B1 fill:#81c784,stroke:#1b5e20,color:#000
    style B2 fill:#fff9c4,stroke:#f57f17,color:#000
    style B3 fill:#fff9c4,stroke:#f57f17,color:#000
    style B4 fill:#ffccbc,stroke:#bf360c,color:#000
    style E1 fill:#bbdefb,stroke:#01579b,color:#000
    style E2 fill:#bbdefb,stroke:#01579b,color:#000
    style R1 fill:#fff9c4,stroke:#f57f17,color:#000
    style R2 fill:#a5d6a7,stroke:#1b5e20,color:#000
    style C fill:#fff3e0,stroke:#e65100,color:#000
    style D fill:#a5d6a7,stroke:#1b5e20,color:#000
```

#### 2.5.2 ë¬¸ì œì 
1. **ê· ë“± ê°€ì¤‘ì¹˜ ì‚¬ìš©** âŒ
   - ëª¨ë“  ëª¨ë¸ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ (0.25ì”©)
   - ì„±ëŠ¥ ì¢‹ì€ KoBARTì˜ ê¸°ì—¬ë„ê°€ ë‚®ì•„ì§

2. **ì„±ëŠ¥ ì°¨ì´ ë¬´ì‹œ** âŒ
   - KoBART: 58.5 (ìµœê³ )
   - ë‹¤ë¥¸ ëª¨ë¸: 42~45 (ë‚®ìŒ)
   - ë‚®ì€ ì„±ëŠ¥ ëª¨ë¸ì´ ê²°ê³¼ë¥¼ ëŒì–´ë‚´ë¦¼

#### 2.5.3 ê¶Œì¥ ê°€ì¤‘ì¹˜
```python
ensemble_weights = {
    'kobart': 0.60,          # ì£¼ë ¥ ëª¨ë¸
    'llama-3.2-korean-3b': 0.20,   # ë³´ì¡° ëª¨ë¸ 1
    'qwen3-4b': 0.15,        # ë³´ì¡° ëª¨ë¸ 2
    'solar-10.7b': 0.05      # ìµœì†Œ ê°€ì¤‘ì¹˜
}
```

---

### 2.6 Issue #6: gradient_accumulation_steps ë†’ì€ ê¸°ë³¸ê°’ âŒ Critical

#### 2.6.1 ë¬¸ì œ Config íŒŒì¼ ëª©ë¡
```bash
/configs/models/solar-10.7b.yaml              â†’ 16 âŒ
/configs/models/qwen3_4b.yaml                 â†’ 10 âŒ
/configs/models/polyglot-ko-12.8b.yaml        â†’ 16 âŒ
/configs/models/llama_3.2_3b.yaml             â†’ 8  âŒ
/configs/models/llama_3.2_korean_3b.yaml      â†’ 8  âŒ
/configs/models/kullm-v2.yaml                 â†’ 16 âŒ
/configs/examples/llama_finetune.yaml         â†’ 8  âŒ
/configs/base/encoder_decoder.yaml            â†’ 1  âœ…
/configs/base/causal_lm.yaml                  â†’ 1  âœ…
/configs/examples/baseline_kobart.yaml        â†’ 1  âœ…
```

#### 2.6.2 ì˜í–¥ë„
```
gradient_accumulation_steps = N
â†’ í•™ìŠµ ì‹œê°„ = ê¸°ë³¸ ì‹œê°„ Ã— Në°° ì¦ê°€
```

**ìˆ˜ì • í•„ìš”**: ëª¨ë“  ëª¨ë¸ config íŒŒì¼ì˜ ê¸°ë³¸ê°’ì„ **1**ë¡œ ë³€ê²½

---

## 3. í•´ê²° ë°©ì•ˆ

### 3.1 Issue #1 í•´ê²°: ëª…ë ¹í–‰ ì¸ì ìš°ì„ ìˆœìœ„ ë³´ì¥

#### 3.1.1 í•´ê²° ì „ëµ
```mermaid
graph TB
    A[train.py ì‹œì‘] --> B[args íŒŒì‹±]
    B --> C[Config íŒŒì¼ ë¡œë”©]
    C --> D[ì¦‰ì‹œ _override_config í˜¸ì¶œ]
    D --> E{ëª…ë ¹í–‰ ì¸ì ì¡´ì¬?}
    E -->|Yes| F[Config ê°’ ë®ì–´ì“°ê¸°]
    E -->|No| G[Config ê°’ ìœ ì§€]
    F --> H[ìµœì¢… Config í™•ì •]
    G --> H
    H --> I[í•™ìŠµ ì‹œì‘]

    style A fill:#e1f5ff,stroke:#01579b,color:#000
    style B fill:#fff3e0,stroke:#e65100,color:#000
    style C fill:#fff3e0,stroke:#e65100,color:#000
    style D fill:#c8e6c9,stroke:#1b5e20,color:#000
    style E fill:#fff9c4,stroke:#f57f17,color:#000
    style F fill:#a5d6a7,stroke:#1b5e20,color:#000
    style G fill:#bbdefb,stroke:#01579b,color:#000
    style H fill:#a5d6a7,stroke:#1b5e20,color:#000
    style I fill:#81c784,stroke:#1b5e20,color:#000
```

#### 3.1.2 êµ¬í˜„ ë°©ë²•
**ëª¨ë“  Trainerì˜ `train()` ë©”ì„œë“œ ì´ˆë°˜ì— ì¶”ê°€**:
```python
def train(self):
    # Config ë¡œë”©
    config = self.load_config()

    # âœ… ëª…ë ¹í–‰ ì¸ìë¡œ ì¦‰ì‹œ ì˜¤ë²„ë¼ì´ë“œ
    self._override_config(config)  # <- ì´ ì¤„ ì¶”ê°€

    # í•™ìŠµ ì§„í–‰
    ...
```

---

### 3.2 Issue #2 í•´ê²°: ë°ì´í„° ì¦ê°• ë¹„ìœ¨ 50%ë¡œ ì¦ê°€

#### 3.2.1 ì„¤ì • ë³€ê²½
```python
# scripts/train.py
parser.add_argument(
    '--augmentation_ratio',
    type=float,
    default=0.5,  # 0.3 â†’ 0.5ë¡œ ë³€ê²½ âœ…
    help='ì¦ê°• ë¹„ìœ¨ (0.0~1.0)'
)
```

#### 3.2.2 ì¦ê°• ì „ëµ ìµœì í™”
```python
# ê¶Œì¥ ì¦ê°• ë°©ë²• (ë©˜í†  í”¼ë“œë°± ë°˜ì˜)
augmentation_methods = [
    'back_translation',  # â­ ìµœìš°ì„  (ë©˜í†  ì¶”ì²œ)
    'paraphrase'         # âœ… ë³´ì¡° ë°©ë²•
]

# ê²€ì¦ í›„ ì¶”ê°€ ê³ ë ¤
# 'shuffle_turns',     # âš ï¸ ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©
# 'synonym'            # âš ï¸ í’ˆì§ˆ ê²€ì¦ í•„ìš”
```

---

### 3.3 Issue #3 í•´ê²°: Full Fine-tuning ì˜µì…˜ ì¶”ê°€

#### 3.3.1 ëª…ë ¹í–‰ ì¸ì ì¶”ê°€
```python
# scripts/train.pyì— ì¶”ê°€
parser.add_argument(
    '--use_full_finetuning',
    action='store_true',
    help='LoRA ëŒ€ì‹  Full Fine-tuning ì‚¬ìš© (Causal LM)'
)

parser.add_argument(
    '--lora_rank',
    type=int,
    default=16,
    help='LoRA rank (use_full_finetuning=Falseì¼ ë•Œë§Œ ì ìš©)'
)
```

#### 3.3.2 llm_loader.py ìˆ˜ì •
```python
def load_causal_lm(config, logger=None):
    # ...ëª¨ë¸ ë¡œë”©...

    # âœ… Full Fine-tuning ì˜µì…˜ ì²´í¬
    use_full_ft = getattr(config, 'use_full_finetuning', False)

    if use_full_ft:
        # Full Fine-tuning: LoRA ì ìš© ì•ˆ í•¨
        logger.write("  âœ… Full Fine-tuning ëª¨ë“œ")
        return model, tokenizer

    # LoRA ì ìš© (ê¸°ì¡´ ë¡œì§)
    if hasattr(config.model, 'lora') and config.model.lora:
        # ...LoRA ë¡œì§...
```

#### 3.3.3 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°•í™”
```python
# ì´ë¯¸ êµ¬í˜„ëœ format_llm_prompt() í™œìš©
# Full FTì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ìµœì  ì„±ëŠ¥
```

---

### 3.4 Issue #4 í•´ê²°: TTA ì œê±° ë˜ëŠ” ìµœì†Œí™”

#### 3.4.1 TTA ê¸°ë³¸ê°’ ë¹„í™œì„±í™”
```python
# scripts/train.py ìˆ˜ì •
parser.add_argument(
    '--use_tta',
    action='store_true',  # ê¸°ë³¸ê°’: False âœ…
    help='Test Time Augmentation ì‚¬ìš© (ë¹„ì¶”ì²œ: ì‹œê°„ ì¦ê°€)'
)

# TTA ì¦ê°• ìˆ˜ ê°ì†Œ
parser.add_argument(
    '--tta_num_aug',
    type=int,
    default=1,  # 3 â†’ 1ë¡œ ê°ì†Œ âœ…
    help='TTA ì¦ê°• ìˆ˜'
)
```

---

### 3.5 Issue #5 í•´ê²°: KoBART ì¤‘ì‹¬ ì•™ìƒë¸”

#### 3.5.1 ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì •
```python
# configs/strategies/ensemble.yamlì— ì¶”ê°€
ensemble:
  strategy: weighted_avg
  weights:
    kobart: 0.60              # ì£¼ë ¥ ëª¨ë¸
    llama-3.2-korean-3b: 0.20
    qwen3-4b: 0.15
    solar-10.7b: 0.05

  # ìë™ ê°€ì¤‘ì¹˜ ìµœì í™” (ì„ íƒ)
  auto_optimize: true         # Dev set ê¸°ë°˜ ìµœì í™”
  optimization_metric: rouge-l
```

---

### 3.6 Issue #6 í•´ê²°: gradient_accumulation_steps ê¸°ë³¸ê°’ ìˆ˜ì •

#### 3.6.1 ìˆ˜ì •í•  íŒŒì¼ ëª©ë¡
```yaml
# ëª¨ë‘ 1ë¡œ ë³€ê²½
/configs/models/solar-10.7b.yaml
/configs/models/qwen3_4b.yaml
/configs/models/polyglot-ko-12.8b.yaml
/configs/models/llama_3.2_3b.yaml
/configs/models/llama_3.2_korean_3b.yaml
/configs/models/kullm-v2.yaml
/configs/examples/llama_finetune.yaml
```

---

## 4. êµ¬í˜„ ê³„íš

### 4.1 ìš°ì„ ìˆœìœ„ P0 (Critical) - ì¦‰ì‹œ ìˆ˜ì •

#### 4.1.1 Task 1: ëª…ë ¹í–‰ ì¸ì ìš°ì„ ìˆœìœ„ ë³´ì¥
```bash
# ìˆ˜ì • íŒŒì¼
- src/trainers/full_pipeline_trainer.py
- src/trainers/multi_model_trainer.py
- src/trainers/kfold_trainer.py
- src/trainers/single_trainer.py
- src/trainers/optuna_trainer.py

# ë³€ê²½ ë‚´ìš©
ê° Trainerì˜ train() ë©”ì„œë“œ ì´ˆë°˜ì— self._override_config(config) í˜¸ì¶œ ì¶”ê°€
```

#### 4.1.2 Task 2: gradient_accumulation_steps ê¸°ë³¸ê°’ ìˆ˜ì •
```bash
# ìˆ˜ì • íŒŒì¼ (7ê°œ)
configs/models/*.yaml (6ê°œ)
configs/examples/llama_finetune.yaml (1ê°œ)

# ë³€ê²½ ë‚´ìš©
gradient_accumulation_steps: 1  # ëª¨ë‘ 1ë¡œ í†µì¼
```

**ì˜ˆìƒ ì™„ë£Œ ì‹œê°„**: 30ë¶„

---

### 4.2 ìš°ì„ ìˆœìœ„ P1 (High) - ë¹ ë¥¸ ì‹œì¼ ë‚´ ìˆ˜ì •

#### 4.2.1 Task 3: ë°ì´í„° ì¦ê°• ë¹„ìœ¨ 50%ë¡œ ì¦ê°€
```python
# ìˆ˜ì • íŒŒì¼
scripts/train.py

# ë³€ê²½ ë‚´ìš©
--augmentation_ratio default=0.5
--augmentation_methods default=['back_translation', 'paraphrase']
```

#### 4.2.2 Task 4: Full Fine-tuning ì˜µì…˜ ì¶”ê°€
```python
# ìˆ˜ì • íŒŒì¼
1. scripts/train.py           # ëª…ë ¹í–‰ ì¸ì ì¶”ê°€
2. src/models/llm_loader.py   # Full FT ë¡œì§ ì¶”ê°€
3. configs/base/causal_lm.yaml # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
```

**ì˜ˆìƒ ì™„ë£Œ ì‹œê°„**: 1ì‹œê°„

---

### 4.3 ìš°ì„ ìˆœìœ„ P2 (Medium) - ì ì§„ì  ê°œì„ 

#### 4.3.1 Task 5: TTA ë¹„í™œì„±í™”
```python
# ìˆ˜ì • íŒŒì¼
scripts/train.py

# ë³€ê²½ ë‚´ìš©
--use_tta default=False
--tta_num_aug default=1
```

#### 4.3.2 Task 6: KoBART ì¤‘ì‹¬ ì•™ìƒë¸”
```yaml
# ìˆ˜ì • íŒŒì¼
configs/strategies/ensemble.yaml

# ì¶”ê°€ ë‚´ìš©
weights ì„¹ì…˜ ì¶”ê°€
```

**ì˜ˆìƒ ì™„ë£Œ ì‹œê°„**: 30ë¶„

---

## 5. ê¸°ëŒ€ íš¨ê³¼

### 5.1 í•™ìŠµ ì‹œê°„ ë‹¨ì¶•

```mermaid
graph LR
    A[í˜„ì¬ í•™ìŠµ ì‹œê°„] --> B[ë¬¸ì œ: Config ë®ì–´ì”€<br/>ì‹œê°„: ~9ì‹œê°„]
    C[ê°œì„  í›„ í•™ìŠµ ì‹œê°„] --> D[ëª…ë ¹í–‰ ì¸ì ì ìš©<br/>ì‹œê°„: ~10ë¶„]

    B --> E{ë¹„êµ}
    D --> E
    E --> F[ì‹œê°„ ë‹¨ì¶•: 54ë°° âš¡]

    style A fill:#ffccbc,stroke:#bf360c,color:#000
    style B fill:#ffccbc,stroke:#bf360c,color:#000
    style C fill:#a5d6a7,stroke:#1b5e20,color:#000
    style D fill:#a5d6a7,stroke:#1b5e20,color:#000
    style E fill:#fff9c4,stroke:#f57f17,color:#000
    style F fill:#81c784,stroke:#1b5e20,color:#000
```

### 5.2 ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ

| ê°œì„  ì‚¬í•­ | í˜„ì¬ ROUGE-L | ì˜ˆìƒ ROUGE-L | í–¥ìƒí­ |
|----------|-------------|--------------|--------|
| ë°ì´í„° ì¦ê°• 50% | 58.5 | **60.2** | +1.7 |
| Full Fine-tuning | 58.5 | **61.5** | +3.0 |
| KoBART ì¤‘ì‹¬ ì•™ìƒë¸” | 52.0 | **56.5** | +4.5 |
| **ì¢…í•© ê°œì„ ** | **58.5** | **~63.0** | **+4.5** ğŸ¯ |

### 5.3 ì‹¤í—˜ íš¨ìœ¨ì„± ì¦ê°€

```
í˜„ì¬: 1íšŒ ì‹¤í—˜ = 9ì‹œê°„
ê°œì„ : 1íšŒ ì‹¤í—˜ = 10ë¶„

í•˜ë£¨ ì‹¤í—˜ íšŸìˆ˜:
- í˜„ì¬: 2~3íšŒ
- ê°œì„ : 144íšŒ (48ë°° ì¦ê°€) âš¡
```

---

## 6. ë‹¤ìŒ ë‹¨ê³„

### 6.1 ì¦‰ì‹œ ì‹¤í–‰ (ì˜¤ëŠ˜)
1. âœ… /docs/modify í´ë” ì •ë¦¬ ì™„ë£Œ
2. â³ P0 Task êµ¬í˜„ (ëª…ë ¹í–‰ ì¸ì, gradient_accumulation_steps)
3. â³ P1 Task êµ¬í˜„ (ë°ì´í„° ì¦ê°•, Full FT)

### 6.2 ê²€ì¦ (ë‚´ì¼)
1. ê°œì„ ëœ ì‹œìŠ¤í…œìœ¼ë¡œ KoBART í•™ìŠµ
2. í•™ìŠµ ì‹œê°„ ì¸¡ì • (ê¸°ëŒ€: 10ë¶„ ì´í•˜)
3. ROUGE ì ìˆ˜ í™•ì¸ (ê¸°ëŒ€: 60+ ì )

### 6.3 ìµœì¢… ì œì¶œ (2ì¼ í›„)
1. Full Fine-tuningìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ ì¬í•™ìŠµ
2. KoBART ì¤‘ì‹¬ ì•™ìƒë¸” ì ìš©
3. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„±

---

## ë¶€ë¡: ì½”ë“œ ë³€ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

### A.1 ëª…ë ¹í–‰ ì¸ì ìš°ì„ ìˆœìœ„
- [ ] `src/trainers/full_pipeline_trainer.py` ìˆ˜ì •
- [ ] `src/trainers/multi_model_trainer.py` ìˆ˜ì •
- [ ] `src/trainers/kfold_trainer.py` ìˆ˜ì •
- [ ] `src/trainers/single_trainer.py` ìˆ˜ì •
- [ ] `src/trainers/optuna_trainer.py` ìˆ˜ì •

### A.2 Config íŒŒì¼ ìˆ˜ì •
- [ ] `configs/models/solar-10.7b.yaml`
- [ ] `configs/models/qwen3_4b.yaml`
- [ ] `configs/models/polyglot-ko-12.8b.yaml`
- [ ] `configs/models/llama_3.2_3b.yaml`
- [ ] `configs/models/llama_3.2_korean_3b.yaml`
- [ ] `configs/models/kullm-v2.yaml`
- [ ] `configs/examples/llama_finetune.yaml`

### A.3 Full Fine-tuning
- [ ] `scripts/train.py` ì¸ì ì¶”ê°€
- [ ] `src/models/llm_loader.py` ë¡œì§ ì¶”ê°€
- [ ] `configs/base/causal_lm.yaml` ì„¤ì • ì¶”ê°€

### A.4 ê¸°íƒ€
- [ ] `scripts/train.py` ë°ì´í„° ì¦ê°• ë¹„ìœ¨ ë³€ê²½
- [ ] `scripts/train.py` TTA ê¸°ë³¸ê°’ ë³€ê²½
- [ ] `configs/strategies/ensemble.yaml` ê°€ì¤‘ì¹˜ ì¶”ê°€

---

**ì‘ì„±**: Claude Code
**ê²€í† **: í•„ìˆ˜
**ìŠ¹ì¸**: ì‚¬ìš©ì
