# 04. 모델 학습 및 추론 시스템

## 목차
- [Part 1: 모델 로딩](#part-1-모델-로딩)
  - [모델 로더 개요](#모델-로더-개요)
  - [ModelLoader 클래스](#modelloader-클래스)
  - [모델 로더 사용 방법](#모델-로더-사용-방법)
  - [디바이스 관리](#디바이스-관리)
  - [특수 토큰 처리](#특수-토큰-처리)
- [Part 2: 학습 시스템](#part-2-학습-시스템)
  - [학습 시스템 개요](#학습-시스템-개요)
  - [ModelTrainer 클래스](#modeltrainer-클래스)
  - [학습 시스템 사용 방법](#학습-시스템-사용-방법)
  - [학습 인자 설정](#학습-인자-설정)
  - [WandB 통합](#wandb-통합)
  - [체크포인트 관리](#체크포인트-관리)
- [Part 3: 추론 시스템](#part-3-추론-시스템)
  - [추론 시스템 개요](#추론-시스템-개요)
  - [Predictor 클래스](#predictor-클래스)
  - [추론 시스템 사용 방법](#추론-시스템-사용-방법)
  - [생성 파라미터](#생성-파라미터)
  - [제출 파일 생성](#제출-파일-생성)
- [Part 4: LLM 파인튜닝](#part-4-llm-파인튜닝)
  - [LLM 파인튜닝 개요](#llm-파인튜닝-개요)
  - [LoRA Loader](#lora-loader)
    - [주요 기능](#주요-기능)
    - [주의사항 및 트러블슈팅](#주의사항-및-트러블슈팅)
  - [LLM Dataset](#llm-dataset)
  - [LLM Trainer](#llm-trainer)
  - [LLM 실행 명령어](#llm-실행-명령어)

---

# Part 1: 모델 로딩

## 모델 로더 개요

### 목적
- HuggingFace 사전학습 모델 자동 로드
- 토크나이저 초기화 및 특수 토큰 추가
- 디바이스 자동 감지 및 배치
- 임베딩 크기 자동 조정

### 핵심 기능
- ✅ Config 기반 모델 로딩
- ✅ 특수 토큰 자동 추가 및 임베딩 리사이즈
- ✅ GPU/CPU 자동 감지
- ✅ 모델 파라미터 정보 출력
- ✅ Logger 통합 지원

---

## ModelLoader 클래스

### 파일 위치
```
src/models/model_loader.py
```

### 클래스 구조

```python
class ModelLoader:
    def __init__(self, config: DictConfig, logger=None):
        """모델 로더 초기화"""

    def _get_device(self) -> torch.device:
        """사용할 디바이스 결정 (GPU/CPU)"""

    def load_tokenizer(self) -> PreTrainedTokenizer:
        """토크나이저 로드 및 특수 토큰 추가"""

    def load_model(self, tokenizer=None) -> PreTrainedModel:
        """사전학습 모델 로드"""

    def load_model_and_tokenizer(self) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """모델과 토크나이저를 함께 로드"""
```

---

## 모델 로더 사용 방법

### 1. 기본 사용법

```python
# ==================== 모델 로더 기본 사용 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer

# ---------------------- Config 로드 ---------------------- #
# baseline_kobart 설정 파일 로드
config = load_config("baseline_kobart")

# ---------------------- 모델 및 토크나이저 로드 ---------------------- #
# Config 기반으로 사전학습 모델 및 토크나이저 자동 로드
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- 모델 정보 출력 ---------------------- #
# 전체 파라미터 수 계산 및 출력
print(f"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}")
```

### 2. Logger와 함께 사용

```python
# ==================== Logger와 함께 모델 로딩 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path
from src.models import load_model_and_tokenizer

# ---------------------- Logger 초기화 ---------------------- #
# 로그 파일 경로 생성 및 Logger 인스턴스 생성
log_path = create_log_path("train", "model_load.log")
logger = Logger(log_path, print_also=True)

# ---------------------- Logger를 전달하여 모델 로드 ---------------------- #
# Logger를 전달하면 모델 로딩 과정이 로그 파일에 기록됨
model, tokenizer = load_model_and_tokenizer(config, logger=logger)
```

**출력 예시:**
```
============================================================
모델 및 토크나이저 로딩 시작
============================================================
토크나이저 로딩: digit82/kobart-summarization
  → 특수 토큰 7개 추가됨
  → pad_token 설정: </s>

모델 로딩: digit82/kobart-summarization
  → 임베딩 크기 조정: 51200 → 51207
  → 디바이스: cuda
  → 전체 파라미터: 123,859,968
  → 학습 가능 파라미터: 123,859,968
============================================================
✅ 모델 및 토크나이저 로딩 완료
============================================================
```

### 3. ModelLoader 클래스 직접 사용

```python
# ==================== ModelLoader 클래스 직접 사용 예시 ==================== #

# ---------------------- 프로젝트 모듈 임포트 ---------------------- #
from src.models.model_loader import ModelLoader

# ---------------------- ModelLoader 인스턴스 생성 ---------------------- #
# Config 및 Logger를 전달하여 ModelLoader 인스턴스 생성
loader = ModelLoader(config, logger=logger)

# ---------------------- 단계별 로드 ---------------------- #
# 1. 토크나이저만 먼저 로드
tokenizer = loader.load_tokenizer()
# 2. 토크나이저를 전달하여 모델 로드
model = loader.load_model(tokenizer)

# ---------------------- 또는 한 번에 로드 ---------------------- #
# 모델과 토크나이저를 동시에 로드
model, tokenizer = loader.load_model_and_tokenizer()
```

---

## 디바이스 관리

### 디바이스 자동 감지

ModelLoader는 다음 우선순위로 디바이스를 결정합니다:

1. **Config 설정 확인**
   ```yaml
   # configs/base/default.yaml
   # ------------------------------- 학습 디바이스 설정 ------------------------------- #
   training:
     device: "cuda"                                      # 디바이스 설정 (cuda, cpu, cuda:0, cuda:1 등)
   ```

2. **자동 감지 (Config 없는 경우)**
   ```python
   # ---------------------- 디바이스 자동 감지 ---------------------- #
   # CUDA 사용 가능 여부에 따라 자동으로 디바이스 결정
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   ```

### 특정 GPU 지정

```yaml
# configs/experiments/my_experiment.yaml
# ------------------------------- 특정 GPU 지정 ------------------------------- #
training:
  device: "cuda:1"                                      # 두 번째 GPU 사용 (인덱스 1번)
```

### 디바이스 관련 동작

```python
# ==================== 디바이스 관리 예시 ==================== #

# ---------------------- 디바이스 결정 ---------------------- #
# Config 설정 또는 자동 감지를 통해 사용할 디바이스 결정
device = loader._get_device()

# ---------------------- 모델을 디바이스로 이동 ---------------------- #
# 결정된 디바이스(GPU 또는 CPU)로 모델 전송
model = model.to(device)

# ---------------------- GPU 사용 불가 시 경고 ---------------------- #
# CUDA가 설정되었으나 사용 불가능합니다. CPU를 사용합니다.
```

---

## 특수 토큰 처리

### 특수 토큰 자동 추가

Config에서 특수 토큰 리스트를 정의하면 자동으로 추가됩니다:

```yaml
# configs/base/encoder_decoder.yaml
# ------------------------------- 특수 토큰 설정 ------------------------------- #
tokenizer:
  special_tokens:                                       # 추가할 특수 토큰 리스트
    - '#Person1#'                                       # 대화 참여자 1
    - '#Person2#'                                       # 대화 참여자 2
    - '#PhoneNumber#'                                   # 전화번호 마스킹 토큰
    - '#Address#'                                       # 주소 마스킹 토큰
    - '#DateAndTime#'                                   # 날짜/시간 마스킹 토큰
```

### 처리 과정

1. **토크나이저 로드**
   ```python
   # ---------------------- 사전학습 토크나이저 로드 ---------------------- #
   # HuggingFace Hub에서 체크포인트에 해당하는 토크나이저 로드
   tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)
   ```

2. **특수 토큰 추가**
   ```python
   # ---------------------- 특수 토큰 추가 ---------------------- #
   # Config에 정의된 특수 토큰 리스트 가져오기
   special_tokens = list(config.model.special_tokens)
   # 토크나이저에 특수 토큰 추가
   num_added = tokenizer.add_special_tokens({
       'additional_special_tokens': special_tokens
   })
   # 출력: "특수 토큰 7개 추가됨"
   ```

3. **패딩 토큰 설정** (BART 계열 모델)
   ```python
   # ---------------------- 패딩 토큰 설정 ---------------------- #
   # 패딩 토큰이 없는 경우 EOS 토큰을 패딩 토큰으로 지정
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

4. **모델 임베딩 리사이즈**
   ```python
   # ---------------------- 모델 임베딩 크기 조정 ---------------------- #
   # 현재 토크나이저 어휘 크기 확인
   vocab_size = len(tokenizer)
   # 모델의 기존 어휘 크기 확인
   model_vocab_size = model.config.vocab_size

   # -------------- 어휘 크기 불일치 시 임베딩 리사이즈 -------------- #
   # 특수 토큰 추가로 어휘 크기가 증가한 경우 처리
   if vocab_size != model_vocab_size:
       model.resize_token_embeddings(vocab_size)
       # 출력: "임베딩 크기 조정: 51200 → 51207"
   ```

### 임베딩 크기 조정 이유

특수 토큰을 추가하면 토크나이저 어휘 크기가 증가하므로, 모델의 임베딩 레이어 크기도 함께 늘려야 합니다.

**변경 전:**
- 토크나이저 어휘: 51,200개
- 모델 임베딩: 51,200개

**변경 후:**
- 토크나이저 어휘: 51,207개 (+7)
- 모델 임베딩: 51,207개 (+7)

---

# Part 2: 학습 시스템

## 학습 시스템 개요

### 목적
- HuggingFace Seq2SeqTrainer 래핑
- Config 기반 학습 자동화
- WandB 로깅 통합
- ROUGE 평가 자동 실행
- 체크포인트 자동 관리

### 핵심 기능
- ✅ Seq2SeqTrainer 자동 설정
- ✅ WandB 로깅 통합
- ✅ ROUGE 자동 평가
- ✅ Early Stopping 지원
- ✅ 최상 모델 자동 저장
- ✅ Logger 통합 지원

---

## ModelTrainer 클래스

### 파일 위치
```
src/training/trainer.py
```

### 클래스 구조

```python
class ModelTrainer:
    def __init__(self, config, model, tokenizer, train_dataset,
                 eval_dataset=None, use_wandb=True, logger=None):
        """학습 시스템 초기화"""

    def _create_training_args(self) -> Seq2SeqTrainingArguments:
        """HuggingFace 학습 인자 생성"""

    def compute_metrics(self, eval_preds) -> Dict[str, float]:
        """평가 메트릭 계산 (ROUGE)"""

    def _create_trainer(self) -> Seq2SeqTrainer:
        """HuggingFace Seq2SeqTrainer 생성"""

    def train(self) -> Dict[str, Any]:
        """모델 학습 실행"""

    def evaluate(self) -> Dict[str, float]:
        """모델 평가 실행"""
```

---

## 학습 시스템 사용 방법

### 1. 기본 사용법

```python
# ==================== ModelTrainer 기본 사용 예시 ==================== #

# ---------------------- 필수 모듈 임포트 ---------------------- #
from src.config import load_config
from src.models import load_model_and_tokenizer
from src.data import DialogueSummarizationDataset
from src.training import create_trainer
import pandas as pd

# ---------------------- Config 로드 ---------------------- #
# 베이스라인 KoBART 실험 설정 파일 로드
config = load_config("baseline_kobart")

# ---------------------- 모델 및 토크나이저 로드 ---------------------- #
# Config 기반으로 사전학습 모델과 토크나이저 자동 로드
model, tokenizer = load_model_and_tokenizer(config)

# ---------------------- 학습/검증 데이터 로드 ---------------------- #
# CSV 파일에서 학습 데이터 로드
train_df = pd.read_csv("data/raw/train.csv")
# CSV 파일에서 검증 데이터 로드
eval_df = pd.read_csv("data/raw/dev.csv")

# ---------------------- PyTorch Dataset 생성 ---------------------- #
# 학습용 Dataset 생성
train_dataset = DialogueSummarizationDataset(
    dialogues=train_df['dialogue'].tolist(),           # 대화 리스트
    summaries=train_df['summary'].tolist(),            # 요약 리스트
    tokenizer=tokenizer,                               # 토크나이저
    encoder_max_len=config.tokenizer.encoder_max_len,  # 인코더 최대 길이
    decoder_max_len=config.tokenizer.decoder_max_len   # 디코더 최대 길이
)

# 검증용 Dataset 생성
eval_dataset = DialogueSummarizationDataset(
    dialogues=eval_df['dialogue'].tolist(),            # 대화 리스트
    summaries=eval_df['summary'].tolist(),             # 요약 리스트
    tokenizer=tokenizer,                               # 토크나이저
    encoder_max_len=config.tokenizer.encoder_max_len,  # 인코더 최대 길이
    decoder_max_len=config.tokenizer.decoder_max_len   # 디코더 최대 길이
)

# ---------------------- Trainer 생성 ---------------------- #
# ModelTrainer 인스턴스 생성 및 WandB 로깅 활성화
trainer = create_trainer(
    config=config,                  # 실험 설정
    model=model,                    # 모델
    tokenizer=tokenizer,            # 토크나이저
    train_dataset=train_dataset,    # 학습 데이터셋
    eval_dataset=eval_dataset,      # 검증 데이터셋
    use_wandb=True                  # WandB 로깅 사용
)

# ---------------------- 학습 실행 ---------------------- #
# 모델 학습 시작 및 결과 저장
results = trainer.train()

# ---------------------- 결과 출력 ---------------------- #
# 최종 모델 저장 경로 출력
print(f"최종 모델 경로: {results['final_model_path']}")
# 검증 데이터셋에 대한 ROUGE Sum 점수 출력
print(f"ROUGE Sum: {results['eval_metrics']['eval_rouge_sum']:.4f}")
```

### 2. Logger와 함께 사용

```python
# ==================== Logger와 함께 ModelTrainer 사용 예시 ==================== #

# ---------------------- Logger 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger 초기화 ---------------------- #
# 로그 파일 경로 생성
log_path = create_log_path("train", "train.log")
# Logger 인스턴스 생성 (콘솔 출력도 활성화)
logger = Logger(log_path, print_also=True)
# 표준 출력 리다이렉션 시작 (print도 로그 파일로 기록)
logger.start_redirect()

# ---------------------- 예외 처리 블록 ---------------------- #
try:
    # -------------- Trainer 생성 (Logger 전달) -------------- #
    # Logger를 전달하여 학습 과정 로그 기록
    trainer = create_trainer(
        config=config,                  # 실험 설정
        model=model,                    # 모델
        tokenizer=tokenizer,            # 토크나이저
        train_dataset=train_dataset,    # 학습 데이터셋
        eval_dataset=eval_dataset,      # 검증 데이터셋
        use_wandb=True,                 # WandB 로깅 사용
        logger=logger                   # Logger 인스턴스
    )

    # -------------- 학습 실행 -------------- #
    # 모델 학습 시작 및 결과 저장
    results = trainer.train()

# ---------------------- 종료 처리 ---------------------- #
finally:
    # 표준 출력 리다이렉션 중지
    logger.stop_redirect()
    # Logger 파일 핸들러 닫기
    logger.close()
```

### 3. WandB 없이 학습

```python
# ==================== WandB 없이 학습 예시 ==================== #

# ---------------------- Trainer 생성 (WandB 비활성화) ---------------------- #
# WandB 로깅 없이 로컬에서만 학습 진행
trainer = create_trainer(
    config=config,                  # 실험 설정
    model=model,                    # 모델
    tokenizer=tokenizer,            # 토크나이저
    train_dataset=train_dataset,    # 학습 데이터셋
    eval_dataset=eval_dataset,      # 검증 데이터셋
    use_wandb=False                 # WandB 사용 안 함
)

# ---------------------- 학습 실행 ---------------------- #
# 모델 학습 시작 및 결과 저장
results = trainer.train()
```

---

## 학습 인자 설정

### Config에서 자동 생성

`src/training/trainer.py`의 `_create_training_args` 함수가 Config를 기반으로 학습 인자를 자동 생성합니다:

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 학습 하이퍼파라미터 설정 ------------------------------- #
training:
  epochs: 20                                            # 학습 에포크 수
  batch_size: 50                                        # 디바이스당 배치 크기
  learning_rate: 1e-5                                   # 학습률
  weight_decay: 0.01                                    # 가중치 감소 (L2 정규화)
  warmup_steps: 500                                     # 학습률 워밍업 스텝 수
  save_total_limit: 3                                   # 최대 저장 체크포인트 수
  logging_steps: 100                                    # 로깅 스텝 간격
```

### 생성된 학습 인자

```python
# ==================== Seq2SeqTrainingArguments 자동 생성 ==================== #

Seq2SeqTrainingArguments(
    # ---------------------- 출력 디렉토리 설정 ---------------------- #
    output_dir="outputs/baseline_kobart",       # 체크포인트 및 결과 저장 경로
    overwrite_output_dir=True,                  # 기존 디렉토리 덮어쓰기 허용

    # ---------------------- 학습 하이퍼파라미터 ---------------------- #
    num_train_epochs=20,                        # 학습 에포크 수
    per_device_train_batch_size=50,             # 학습 배치 크기 (디바이스당)
    per_device_eval_batch_size=50,              # 평가 배치 크기 (디바이스당)
    learning_rate=1e-5,                         # 학습률
    weight_decay=0.01,                          # 가중치 감소 (L2 정규화)
    warmup_steps=500,                           # 학습률 워밍업 스텝 수

    # ---------------------- 평가 및 저장 전략 ---------------------- #
    eval_strategy='epoch',                      # 에포크마다 평가 수행
    save_strategy='epoch',                      # 에포크마다 체크포인트 저장
    save_total_limit=3,                         # 최대 3개 체크포인트만 유지
    load_best_model_at_end=True,                # 학습 종료 시 최상 모델 자동 로드
    metric_for_best_model='eval_rouge_sum',     # 최상 모델 선정 기준 메트릭

    # ---------------------- 로깅 설정 ---------------------- #
    logging_dir="outputs/baseline_kobart/logs", # TensorBoard 로그 저장 경로
    logging_steps=100,                          # 100 스텝마다 로그 기록
    report_to=['wandb'] if use_wandb else [],   # WandB 로깅 활성화 여부

    # ---------------------- Seq2Seq 생성 설정 ---------------------- #
    predict_with_generate=True,                 # 평가 시 생성 모드 사용
    generation_max_length=100,                  # 생성 최대 길이
    generation_num_beams=4,                     # Beam search 빔 개수

    # ---------------------- 기타 최적화 설정 ---------------------- #
    fp16=torch.cuda.is_available(),             # GPU 사용 가능 시 FP16 자동 활성화
    dataloader_num_workers=4                    # 데이터 로딩 워커 수
)
```

---

## WandB 통합

### WandB 설정

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- WandB 로깅 설정 ------------------------------- #
wandb:
  enabled: true                                         # WandB 사용 여부
  project: "nlp-competition"                            # WandB 프로젝트명
  entity: "ieyeppo"                                     # WandB 엔티티 (사용자/팀)

# ------------------------------- 실험 설정 ------------------------------- #
experiment:
  name: "baseline_kobart"                               # 실험명
  tags:                                                 # 실험 태그 리스트
    - "baseline"                                        # 베이스라인 실험
    - "kobart"                                          # KoBART 모델
```

### 자동 로깅 항목

WandB에 자동으로 로깅되는 항목:

1. **학습 메트릭**
   - train_loss
   - train_runtime
   - train_samples_per_second
   - train_steps_per_second

2. **평가 메트릭**
   - eval_rouge1
   - eval_rouge2
   - eval_rougeL
   - eval_rouge_sum
   - eval_loss

3. **Config 정보**
   - 전체 Config 파라미터
   - 실험 태그
   - 모델 체크포인트 이름

4. **시스템 정보**
   - GPU 사용률
   - 메모리 사용량
   - CPU 사용률

### WandB 초기화 및 종료

```python
# ==================== WandB Logger 생명주기 관리 ==================== #

# ---------------------- WandB Logger 초기화 ---------------------- #
# WandB 사용이 활성화된 경우에만 초기화
if use_wandb and config.wandb.enabled:
    self.wandb_logger = WandbLogger(
        project_name=config.wandb.project,          # WandB 프로젝트명
        entity=config.wandb.entity,                 # WandB 엔티티 (사용자/팀)
        experiment_name=config.experiment.name,     # 실험명
        config=dict(config),                        # Config를 딕셔너리로 변환하여 저장
        tags=config.experiment.tags                 # 실험 태그 리스트
    )

# ---------------------- WandB Run 시작 ---------------------- #
# 학습 시작 시 WandB Run 초기화
self.wandb_logger.init_run()

# ---------------------- WandB Run 종료 ---------------------- #
# 학습 종료 시 WandB Run 종료 및 최종 로그 저장
self.wandb_logger.finish()
```

---

## 체크포인트 관리

### 자동 저장

학습 중 자동으로 체크포인트가 저장됩니다:

```
outputs/baseline_kobart/
├── checkpoint-500/          # 500 스텝 체크포인트
│   ├── config.json
│   ├── pytorch_model.bin
│   └── trainer_state.json
├── checkpoint-1000/         # 1000 스텝 체크포인트
├── checkpoint-1500/         # 1500 스텝 체크포인트
└── final_model/             # 최종 모델
    ├── config.json
    ├── pytorch_model.bin
    ├── tokenizer_config.json
    └── special_tokens_map.json
```

### 최상 모델 자동 로드

```python
# ==================== 최상 모델 자동 로드 설정 ==================== #

# ---------------------- Config 설정 ---------------------- #
save_strategy='epoch',                         # 에포크마다 체크포인트 저장
load_best_model_at_end=True,                   # 최상 모델 자동 로드
metric_for_best_model='eval_rouge_sum'         # ROUGE Sum 기준으로 최상 모델 선정
```

학습 종료 후 자동으로 가장 높은 ROUGE Sum을 달성한 체크포인트가 로드됩니다.

### 체크포인트 개수 제한

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 체크포인트 저장 제한 ------------------------------- #
training:
  save_total_limit: 3                                   # 최대 3개 체크포인트만 유지 (디스크 공간 절약)
```

오래된 체크포인트는 자동으로 삭제되어 디스크 공간을 절약합니다.

---

# Part 3: 추론 시스템

## 추론 시스템 개요

### 목적
- 학습된 모델로 대화 요약 예측
- 배치 추론 및 진행 표시
- 제출 파일 자동 생성
- 생성 파라미터 유연한 설정

### 핵심 기능
- ✅ 단일/배치 추론 지원
- ✅ DataFrame 직접 처리
- ✅ 제출 파일 자동 생성
- ✅ 생성 파라미터 오버라이드
- ✅ Logger 통합 지원

---

## Predictor 클래스

### 파일 위치
```
src/inference/predictor.py
```

### 클래스 구조

```python
class Predictor:
    def __init__(self, model, tokenizer, config=None, device=None, logger=None):
        """추론 시스템 초기화"""

    def _setup_generation_config(self) -> Dict:
        """생성 파라미터 설정"""

    def predict_single(self, dialogue: str, **generation_kwargs) -> str:
        """단일 대화 요약 예측"""

    def predict_batch(self, dialogues: List[str], batch_size=32,
                     show_progress=True, **generation_kwargs) -> List[str]:
        """배치 대화 요약 예측"""

    def predict_dataframe(self, df: pd.DataFrame, batch_size=32,
                          show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """DataFrame에 대해 예측 수행"""

    def create_submission(self, test_df: pd.DataFrame, output_path: str,
                         batch_size=32, show_progress=True, **generation_kwargs) -> pd.DataFrame:
        """제출 파일 생성"""
```

---

## 추론 시스템 사용 방법

### 1. 기본 사용법 (단일 예측)

```python
# ==================== Predictor 기본 사용 예시 (단일 예측) ==================== #

# ---------------------- Transformers 모듈 임포트 ---------------------- #
import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ---------------------- 학습된 모델 및 토크나이저 로드 ---------------------- #
# 저장된 최종 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# 저장된 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- GPU로 모델 이동 ---------------------- #
# CUDA 사용 가능 시 모델을 GPU로 이동 (추론 속도 대폭 향상)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ---------------------- Predictor 인스턴스 생성 ---------------------- #
# 모델과 토크나이저를 전달하여 Predictor 생성
# device를 전달하여 GPU 사용 보장
predictor = create_predictor(model, tokenizer, device=device)

# ---------------------- 단일 대화 예측 ---------------------- #
# 예측할 대화 텍스트
dialogue = "#Person1#: 안녕하세요 #Person2#: 네 안녕하세요"
# 대화 요약 생성
summary = predictor.predict_single(dialogue)

# ---------------------- 결과 출력 ---------------------- #
# 생성된 요약 출력
print(f"예측 요약: {summary}")
```

### 2. 배치 예측

```python
# ==================== Predictor 배치 예측 예시 ==================== #

# ---------------------- 예측할 대화 리스트 준비 ---------------------- #
dialogues = [
    "#Person1#: 저녁 뭐 먹을까? #Person2#: 김치찌개 어때?",
    "#Person1#: 내일 회의 몇 시야? #Person2#: 3시로 잡혔어",
    "#Person1#: 날씨 좋네 #Person2#: 산책 가자"
]

# ---------------------- 배치 예측 실행 ---------------------- #
# 여러 대화를 한 번에 처리
summaries = predictor.predict_batch(
    dialogues,              # 대화 리스트
    batch_size=32,          # 배치 크기
    show_progress=True      # 진행률 표시 활성화
)

# ---------------------- 결과 출력 ---------------------- #
# 각 대화와 생성된 요약을 순회하며 출력
for dialogue, summary in zip(dialogues, summaries):
    print(f"대화: {dialogue}")
    print(f"요약: {summary}\n")
```

### 3. DataFrame 예측

```python
# ==================== Predictor DataFrame 예측 예시 ==================== #

# ---------------------- Pandas 모듈 임포트 ---------------------- #
import pandas as pd

# ---------------------- 테스트 데이터 로드 ---------------------- #
# CSV 파일에서 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- DataFrame 예측 실행 ---------------------- #
# DataFrame에 대해 배치 예측 수행
result_df = predictor.predict_dataframe(
    test_df,                # 테스트 데이터프레임
    batch_size=32,          # 배치 크기
    show_progress=True      # 진행률 표시 활성화
)

# ---------------------- 결과 확인 ---------------------- #
# 파일명, 대화, 요약 컬럼의 상위 5개 행 출력
print(result_df[['fname', 'dialogue', 'summary']].head())
```

### 4. 제출 파일 생성

```python
# ==================== 제출 파일 자동 생성 예시 ==================== #

# ---------------------- 제출 파일 생성 실행 ---------------------- #
# 테스트 데이터에 대해 예측하고 제출 형식으로 저장
submission_df = predictor.create_submission(
    test_df=test_df,                            # 테스트 데이터프레임
    output_path="submissions/submission.csv",   # 제출 파일 저장 경로
    batch_size=32,                              # 배치 크기
    show_progress=True                          # 진행률 표시 활성화
)

# ---------------------- 결과 출력 ---------------------- #
# 제출 파일 생성 완료 메시지
print(f"제출 파일 생성 완료: submissions/submission.csv")
# 총 예측 샘플 수 출력
print(f"샘플 수: {len(submission_df)}")
```

---

## 생성 파라미터

### ⚠️ 중요: max_length vs max_new_tokens

**문제점:** `max_length`는 **입력+출력 전체 길이**를 제한하므로, 입력이 길면 출력이 잘립니다.

**해결책:** `max_new_tokens`는 **출력만의 길이**를 제한하여 일관된 요약 길이를 보장합니다.

```python
# ==================== max_length의 문제점 ==================== #

# ❌ 잘못된 방법: max_length 사용
outputs = model.generate(
    input_ids,
    max_length=100  # 입력(80토큰) + 출력(?) = 100
)
# 결과: 출력은 20토큰만 생성됨 → 문장 끊김 발생! ❌

# ✅ 올바른 방법: max_new_tokens 사용
outputs = model.generate(
    input_ids,
    max_new_tokens=200,  # 입력 길이와 무관하게 출력 200토큰 생성
    min_new_tokens=30    # 최소 30토큰 보장
)
# 결과: 항상 일관된 길이의 완전한 요약 생성! ✅
```

### 📌 동시 사용 시 우선순위

**중요:** `max_length`와 `max_new_tokens`를 **동시에 지정하면** `max_new_tokens`가 우선순위를 가집니다.

```python
# ==================== 동시 사용 시 동작 방식 ==================== #

# ⚠️ 두 파라미터를 동시에 지정한 경우
outputs = model.generate(
    input_ids,
    max_length=100,       # ❌ 무시됨
    max_new_tokens=200    # ✅ 이 값이 사용됨
)

# HuggingFace Transformers 공식 동작:
# 1. max_new_tokens가 지정되면 max_length는 **무시**됩니다
# 2. max_new_tokens가 None이면 max_length를 사용합니다
# 3. 둘 다 지정하면 경고 메시지와 함께 max_new_tokens가 적용됩니다

# 경고 메시지 예시:
# "Both `max_new_tokens` and `max_length` have been set.
#  `max_new_tokens` will take precedence."
```

### 📋 파라미터 비교표

| 파라미터 | 적용 범위 | 입력 길이 의존성 | 출력 일관성 | 권장 여부 |
|---------|----------|----------------|------------|-----------|
| `max_length` | **입력+출력 전체** | ❌ 의존함 | ❌ 가변적 | ❌ 비권장 |
| `max_new_tokens` | **출력만** | ✅ 독립적 | ✅ 일관적 | ✅ 권장 |
| `min_new_tokens` | **출력만 (최소)** | ✅ 독립적 | ✅ 보장됨 | ✅ 권장 |

### 🔍 Config 파일에서의 설정 예시

```yaml
# configs/base/encoder_decoder.yaml
inference:
  # ⚠️ 세 가지 옵션 모두 설정 가능
  generate_max_new_tokens: 200    # ✅ 우선순위 1: 출력 200토큰
  generate_min_new_tokens: 30     # ✅ 우선순위 1: 최소 30토큰
  generate_max_length: 512        # ⚠️ 무시됨 (max_new_tokens가 있으므로)
```

**실제 동작:**
- `max_new_tokens=200` 사용됨 → 출력 200토큰 생성
- `min_new_tokens=30` 사용됨 → 최소 30토큰 보장
- `max_length=512` 무시됨 → max_new_tokens가 우선

**왜 max_length를 Config에 남겨두나요?**
- **하위 호환성**: 이전 코드와의 호환성 유지
- **안전 장치**: max_new_tokens가 None일 때 대비
- **명시적 문서화**: 전체 길이 제한을 명시적으로 표시

### ⚠️ 경고 vs 오류: 삭제 필요 여부

**Q: max_length를 삭제해야 하나요?**

**A: 선택 사항입니다. 삭제해도 되고 그냥 둬도 됩니다!**

```python
# ==================== max_length 삭제 여부 ==================== #

# ✅ 옵션 1: max_length 유지 (권장)
inference:
  generate_max_new_tokens: 200    # 사용됨
  generate_min_new_tokens: 30     # 사용됨
  generate_max_length: 512        # 무시되지만 문서화/Fallback 목적

# ✅ 옵션 2: max_length 삭제 (깔끔)
inference:
  generate_max_new_tokens: 200    # 사용됨
  generate_min_new_tokens: 30     # 사용됨
  # max_length 없음 → 문제 없음!
```

**둘 다 정상 작동합니다!**

---

### 🔍 경고 메시지 상세 설명

**Q: 경고 메시지가 발생하면 오류인가요? 실행이 중단되나요?**

**A: 아닙니다! 경고는 오류가 아닙니다. 프로그램은 정상 실행됩니다.**

```python
# ==================== 경고 vs 오류 비교 ==================== #

# ⚠️ 경고(Warning) - 노란색 메시지
outputs = model.generate(
    input_ids,
    max_length=100,       # 둘 다 있으면 경고 발생
    max_new_tokens=200
)
# 결과:
# - 노란색 경고 메시지 출력:
#   "Both `max_new_tokens` and `max_length` have been set.
#    `max_new_tokens` will take precedence."
# - ✅ 프로그램 계속 실행됨
# - ✅ max_new_tokens=200 사용됨
# - ✅ 정상 종료됨

# ❌ 오류(Error) - 빨간색 메시지
outputs = model.generate(
    input_ids,
    max_new_tokens=-100   # 잘못된 값
)
# 결과:
# - 빨간색 오류 메시지 출력:
#   "ValueError: max_new_tokens must be positive"
# - ❌ 프로그램 즉시 중단됨
# - ❌ 예외 처리 필요
```

### 📊 경고 vs 오류 비교표

| 구분 | 경고(Warning) | 오류(Error) |
|-----|-------------|-----------|
| **색상** | ⚠️ 노란색 | ❌ 빨간색 |
| **프로그램 실행** | ✅ 계속 실행됨 | ❌ 즉시 중단됨 |
| **예외 처리** | 불필요 | 필수 (try-except) |
| **심각도** | 낮음 (알림) | 높음 (치명적) |
| **발생 상황** | 권장하지 않는 사용법 | 잘못된 사용법 |
| **예시** | 두 파라미터 동시 사용 | 음수 값, 잘못된 타입 |

### 💡 실무 권장 사항

**1️⃣ max_length를 유지하는 경우 (권장)**

```yaml
# configs/base/encoder_decoder.yaml
inference:
  generate_max_new_tokens: 200    # ✅ 주 파라미터
  generate_min_new_tokens: 30     # ✅ 보조 파라미터
  generate_max_length: 512        # ⚠️ Fallback/문서화용 (경고 발생 가능)
```

**장점:**
- ✅ 하위 호환성 유지
- ✅ max_new_tokens=None일 때 자동 Fallback
- ✅ 전체 길이 제한 문서화
- ⚠️ 경고 메시지 발생 가능 (무해함)

**2️⃣ max_length를 삭제하는 경우**

```yaml
# configs/base/encoder_decoder.yaml
inference:
  generate_max_new_tokens: 200    # ✅ 주 파라미터
  generate_min_new_tokens: 30     # ✅ 보조 파라미터
  # max_length 없음
```

**장점:**
- ✅ 경고 메시지 없음 (깔끔)
- ✅ 혼란 방지
- ⚠️ Fallback 없음 (max_new_tokens=None이면 기본값 사용)

### 🎯 결론

```python
# ==================== 최종 권장 사항 ==================== #

# 선택 1: max_length 유지 (안전)
# - 경고 메시지 발생할 수 있지만 무해함
# - Fallback 안전장치 있음
# - 권장: 프로젝트 초기 또는 하위 호환성 중요 시

# 선택 2: max_length 삭제 (깔끔)
# - 경고 없음
# - 더 명확한 설정
# - 권장: 프로젝트 안정화 후 또는 명확성 선호 시

# ✅ 중요: 둘 다 정상 작동합니다!
# ✅ 경고는 오류가 아니므로 프로그램은 계속 실행됩니다!
```

### 💡 권장 사용 패턴

```python
# ==================== 권장 사용 패턴 ==================== #

# ✅ 패턴 1: max_new_tokens만 사용 (가장 권장)
outputs = model.generate(
    input_ids,
    max_new_tokens=200,
    min_new_tokens=30,
    num_beams=5
)

# ✅ 패턴 2: Config에서 자동 로드 (프로젝트 표준)
# Config 파일에 generate_max_new_tokens 설정
# 코드에서 getattr로 자동 로드
max_new_tokens = getattr(args, 'max_new_tokens', 200)
outputs = model.generate(input_ids, max_new_tokens=max_new_tokens)

# ❌ 패턴 3: 두 파라미터 동시 사용 (비권장)
# 경고 메시지가 발생하며 혼란을 야기함
outputs = model.generate(
    input_ids,
    max_length=100,      # 무시됨
    max_new_tokens=200   # 실제 사용됨
)
```

### 기본 생성 파라미터

```python
# ==================== 기본 생성 파라미터 설정 ==================== #

# ---------------------- 생성 파라미터 딕셔너리 ---------------------- #
default_config = {
    'max_new_tokens': 100,          # 최적값: 100 (99.6% 완성도 달성)
    'min_new_tokens': 30,           # ✅ 생성할 최소 토큰 수 (문장 끊김 방지)
    'num_beams': 5,                 # Beam search 빔 개수
    'early_stopping': True,         # EOS 토큰 생성 시 조기 종료
    'no_repeat_ngram_size': 3,      # n-gram 반복 방지 크기
    'length_penalty': 1.0,          # 길이 페널티 (1.0: 중립)
    'repetition_penalty': 1.5,      # 최적값: 1.5 (적절한 억제)
}
```

### Config에서 자동 로드

```yaml
# configs/experiments/baseline_kobart.yaml
# ------------------------------- 추론 생성 파라미터 설정 ------------------------------- #
inference:
  generate_max_new_tokens: 100                          # 최적값: 100 (99.6% 완성도 달성)
  generate_min_new_tokens: 30                           # ✅ 생성할 최소 토큰 수 (권장)
  num_beams: 5                                          # Beam search 빔 개수
  early_stopping: true                                  # EOS 토큰 생성 시 조기 종료
  no_repeat_ngram_size: 3                               # 최적값: 3 (반복 방지)
  length_penalty: 1.0                                   # 길이 페널티 (1.0: 중립)
  repetition_penalty: 1.5                               # 최적값: 1.5 (적절한 억제)
```

### CLI 명령어로 오버라이드

```bash
# ==================== CLI에서 생성 파라미터 지정 ==================== #

# ---------------------- 권장 설정 (한국어 요약) ---------------------- #
python scripts/train.py \
  --mode full \
  --models kobart \
  --max_new_tokens 200 \        # ✅ 한국어는 200 권장 (영어의 2-3배)
  --min_new_tokens 30 \         # ✅ 최소 길이 보장
  --num_beams 5 \
  --repetition_penalty 1.2 \
  --length_penalty 1.0 \
  --no_repeat_ngram_size 3
```

### 런타임 오버라이드

```python
# ==================== 생성 파라미터 런타임 오버라이드 예시 ==================== #

# ---------------------- Config 값 무시하고 파라미터 지정 ---------------------- #
# 예측 실행 시점에 생성 파라미터를 동적으로 변경
summaries = predictor.predict_batch(
    dialogues,                  # 대화 리스트
    batch_size=16,              # 배치 크기
    max_new_tokens=100,         # 최적값: 100 (99.6% 완성도 달성)
    min_new_tokens=30,          # ✅ 오버라이드: 최소 30토큰 보장
    num_beams=8,                # 오버라이드: Beam 개수 증가
    repetition_penalty=1.5      # 최적값: 1.5 (적절한 억제)
)
```

### 주요 생성 파라미터 설명

| 파라미터 | 권장값 | 설명 |
|---------|-------|------|
| `max_new_tokens` ⭐ | **100** | **생성할 최대 토큰 수** (최적값: 99.6% 완성도 달성) |
| `min_new_tokens` ⭐ | **30** | **생성할 최소 토큰 수** (문장 끊김 방지) |
| `num_beams` | 5 | Beam search 빔 개수 (높을수록 품질↑, 속도↓) |
| `repetition_penalty` | 1.5 | 반복 억제 강도 (최적값: 1.5, 적절한 억제) |
| `early_stopping` | True | EOS 토큰 생성 시 즉시 종료 |
| `no_repeat_ngram_size` | 3 | n-gram 반복 방지 크기 |
| `length_penalty` | 1.0 | 길이 페널티 (>1: 긴 문장 선호, <1: 짧은 문장 선호) |
| `temperature` | 1.0 | 샘플링 온도 (낮을수록 결정적, 높을수록 다양) |
| `top_k` | 50 | Top-k 샘플링 |
| `top_p` | 1.0 | Nucleus 샘플링 |

### 한국어 요약에 최적화된 설정

```python
# ==================== 한국어 요약 최적 설정 ==================== #

# ---------------------- 고품질 요약 설정 ---------------------- #
optimal_config = {
    # ✅ 길이 제어
    'max_new_tokens': 100,      # 최적값: 100 (99.6% 완성도 달성)
    'min_new_tokens': 30,       # 너무 짧은 요약 방지

    # ✅ 품질 향상
    'num_beams': 5,             # 5개 후보 중 최선 선택
    'repetition_penalty': 1.5,  # 최적값: 1.5 (적절한 억제)
    'no_repeat_ngram_size': 3,  # 최적값: 3 (반복 방지)

    # ✅ 종료 조건
    'early_stopping': True,     # EOS 토큰 생성 시 즉시 종료
    'length_penalty': 1.0,      # 중립적 길이 선호
}

# ---------------------- 예측 실행 ---------------------- #
summaries = predictor.predict_batch(
    dialogues,
    **optimal_config
)
```

### 문장 끊김 문제 해결 가이드

#### 문제 증상
- 요약이 중간에 끊김: "Person1과 Person2는 저녁 약속을 잡았다. 그들은 김치찌개를 먹기로"
- 문장 부호 없이 종료: "Person1과 Person2는 내일 3시에 회의를 하기로 했다"
- 불완전한 단어: "Person1은 #Pe"

#### 해결 방법

**1단계: max_new_tokens 사용**
```python
# ❌ 기존 (문제 발생)
outputs = model.generate(input_ids, max_length=100)

# ✅ 수정 (문제 해결)
outputs = model.generate(
    input_ids,
    max_new_tokens=100,  # 최적값: 100 (99.6% 완성도 달성)
    min_new_tokens=30    # 최소 길이 보장
)
```

**2단계: 후처리 추가**
```python
def postprocess_summary(text):
    """요약문 후처리"""
    import re
    text = text.strip()

    # 1. 불완전한 플레이스홀더 제거
    text = re.sub(r'\s+#[A-Za-z가-힣]{0,10}$', '', text)

    # 2. 짧은 마지막 단어 제거
    parts = text.rsplit(' ', 1)
    if len(parts) == 2 and len(parts[1]) <= 3:
        if not parts[1].endswith(('.', '!', '?')):
            text = parts[0]

    # 3. 문장 종결 보장
    if text and text[-1] not in '.!?':
        text += '.'

    return text
```

**3단계: CLI 명령어 사용**
```bash
# 모든 모드에서 올바른 파라미터 사용
python scripts/train.py \
  --mode full \
  --models kobart \
  --max_new_tokens 100 \  # 최적값: 100 (99.6% 완성도 달성)
  --min_new_tokens 30 \
  --num_beams 5
```

---

## 제출 파일 생성

### 기본 사용법

```python
# ==================== 제출 파일 생성 기본 사용법 ==================== #

# ---------------------- 필수 모듈 임포트 ---------------------- #
import torch
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from src.inference import create_predictor

# ---------------------- 학습된 모델 로드 ---------------------- #
# 최종 모델 로드
model = AutoModelForSeq2SeqLM.from_pretrained("outputs/baseline_kobart/final_model")
# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("outputs/baseline_kobart/final_model")

# ---------------------- GPU로 모델 이동 ---------------------- #
# CUDA 사용 가능 시 모델을 GPU로 이동 (추론 속도 대폭 향상)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ---------------------- Predictor 생성 ---------------------- #
# 모델과 토크나이저를 전달하여 Predictor 생성
# device를 전달하여 GPU 사용 보장
predictor = create_predictor(model, tokenizer, device=device)

# ---------------------- 테스트 데이터 로드 ---------------------- #
# CSV 파일에서 테스트 데이터 로드
test_df = pd.read_csv("data/raw/test.csv")

# ---------------------- 제출 파일 생성 ---------------------- #
# 테스트 데이터에 대해 예측하고 제출 형식으로 저장
submission_df = predictor.create_submission(
    test_df=test_df,                            # 테스트 데이터프레임
    output_path="submissions/submission.csv",   # 제출 파일 저장 경로
    batch_size=32,                              # 배치 크기
    show_progress=True                          # 진행률 표시 활성화
)
```

### Logger와 함께 사용

```python
# ==================== Logger와 함께 제출 파일 생성 예시 ==================== #

# ---------------------- Logger 모듈 임포트 ---------------------- #
from src.logging.logger import Logger
from src.utils.core.common import create_log_path

# ---------------------- Logger 초기화 ---------------------- #
# 추론용 로그 파일 경로 생성
log_path = create_log_path("inference", "inference.log")
# Logger 인스턴스 생성 (콘솔 출력도 활성화)
logger = Logger(log_path, print_also=True)
# 표준 출력 리다이렉션 시작
logger.start_redirect()

# ---------------------- 예외 처리 블록 ---------------------- #
try:
    # -------------- Predictor 생성 (Logger 전달) -------------- #
    # Logger를 전달하여 추론 과정 로그 기록
    predictor = create_predictor(model, tokenizer, logger=logger)

    # -------------- 제출 파일 생성 -------------- #
    # 테스트 데이터에 대해 예측 수행
    submission_df = predictor.create_submission(
        test_df=test_df,                            # 테스트 데이터프레임
        output_path="submissions/submission.csv",   # 제출 파일 저장 경로
        batch_size=32,                              # 배치 크기
        show_progress=True                          # 진행률 표시 활성화
    )

# ---------------------- 종료 처리 ---------------------- #
finally:
    # 표준 출력 리다이렉션 중지
    logger.stop_redirect()
    # Logger 파일 핸들러 닫기
    logger.close()
```

### 출력 예시

```
============================================================
제출 파일 생성 시작
============================================================

샘플 수: 2500
Predicting: 100%|██████████| 79/79 [02:15<00:00,  1.71s/it]

✅ 제출 파일 저장 완료: submissions/submission.csv
============================================================
```

### 제출 파일 형식

```csv
fname,summary
test_001,두 사람이 저녁 약속을 잡았다
test_002,회의 시간을 3시로 정했다
test_003,내일 점심 메뉴는 김치찌개다
...
```

---

# Part 4: LLM 파인튜닝

## LLM 파인튜닝 개요

### 목적
- Causal LM (Llama, Qwen) 파인튜닝
- QLoRA 4-bit 양자화 지원
- LoRA (Low-Rank Adaptation) 지원
- Instruction/Chat Format 지원

### 핵심 기능
- ✅ QLoRA 4-bit 양자화
- ✅ LoRA 파라미터 효율적 학습
- ✅ Chat template 토큰 자동 추가
- ✅ Prompt truncation 방지
- ✅ Instruction Tuning 데이터 증강

---

## LoRA Loader

### 파일 위치
```
src/models/lora_loader.py
```

### 클래스 구조

```python
class LoRALoader:
    def __init__(self, config, logger=None)
    def load_model_and_tokenizer(use_lora=True, use_qlora=False)
    def _load_tokenizer(checkpoint)
    def _create_bnb_config()
    def _load_causal_lm(checkpoint, bnb_config)
    def _add_chat_tokens(model, tokenizer)
    def _apply_lora(model)
    def _configure_tokenizer(tokenizer)
```

### 주요 기능

#### 1. QLoRA 4-bit 양자화
```python
# ==================== QLoRA 4-bit 양자화 설정 ==================== #

# ---------------------- BitsAndBytes 양자화 Config 생성 ---------------------- #
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                          # 4-bit 양자화 활성화
    bnb_4bit_use_double_quant=True,             # 이중 양자화 사용 (메모리 절약)
    bnb_4bit_quant_type="nf4",                  # NF4 양자화 타입
    bnb_4bit_compute_dtype=torch.float16        # ⚠️ FP16 권장 (BF16은 PyTorch AMP 호환 문제 발생 가능)
)
```

#### 2. LoRA 설정
```python
# ==================== LoRA (Low-Rank Adaptation) 설정 ==================== #

# ---------------------- LoRA Config 생성 ---------------------- #
lora_config = LoraConfig(
    r=16,                    # LoRA rank (저랭크 행렬 차원)
    lora_alpha=32,           # alpha 스케일링 파라미터 (일반적으로 r * 2)
    target_modules=[
        # Attention 레이어
        "q_proj", "k_proj", "v_proj", "o_proj",
        # MLP 레이어
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,       # LoRA 레이어 드롭아웃
    task_type=TaskType.CAUSAL_LM  # Causal Language Modeling 태스크
)
```

#### 3. Chat Template 토큰 추가
```python
# ==================== Chat Template 특수 토큰 추가 ==================== #

# ---------------------- Llama 모델용 Chat 토큰 ---------------------- #
# Llama 3 Chat 템플릿 특수 토큰
chat_tokens = ["<|start_header_id|>", "<|end_header_id|>", "<|eot_id|>"]

# ---------------------- Qwen 모델용 Chat 토큰 ---------------------- #
# Qwen Chat 템플릿 특수 토큰
chat_tokens = ["<|im_start|>", "<|im_end|>"]

# ---------------------- 토크나이저에 특수 토큰 추가 ---------------------- #
# Chat 템플릿 토큰을 토크나이저에 추가
tokenizer.add_special_tokens({'additional_special_tokens': chat_tokens})
# 토큰 추가로 인한 임베딩 크기 조정
model.resize_token_embeddings(len(tokenizer))
```

#### 4. Prompt Truncation 방지
```python
# ==================== Prompt Truncation 방지 설정 ==================== #

# ---------------------- Left Padding/Truncation 설정 ---------------------- #
# Causal LM에서 프롬프트 보존을 위해 왼쪽부터 패딩 및 잘라내기
tokenizer.padding_side = "left"         # 왼쪽에서 패딩
tokenizer.truncation_side = "left"      # 왼쪽에서 잘라내기 (프롬프트 끝부분 보존)
```

### 5. 주의사항 및 트러블슈팅

#### ⚠️ BFloat16 AMP 호환성 문제 (Critical)

**문제**: PyTorch AMP GradScaler는 BFloat16을 지원하지 않음

**에러 메시지**:
```
NotImplementedError: "_amp_foreach_non_finite_check_and_unscale_cuda" not implemented for 'BFloat16'
```

**원인**:
- `src/models/lora_loader.py:108`에서 기본적으로 `torch.bfloat16`을 사용
- 학습 중 gradient clipping 단계에서 AMP GradScaler가 BFloat16 텐서를 처리하지 못함
- `_amp_foreach_non_finite_check_and_unscale_cuda` 커널이 Float16만 지원

**해결 방법**:
`src/models/lora_loader.py:108-113`을 다음과 같이 수정:

```python
# 변경 전
compute_dtype = torch.bfloat16
if 'qwen' in self.config.model.checkpoint.lower():
    compute_dtype = torch.float16
    self._log("  - Qwen 모델: fp16 사용")
else:
    self._log("  - Llama 모델: bf16 사용")

# 변경 후
# PyTorch AMP GradScaler는 BFloat16을 지원하지 않으므로 모든 모델에 Float16 사용
compute_dtype = torch.float16
self._log("  - QLoRA compute dtype: fp16 (AMP 호환)")
```

**영향을 받는 모델**: Llama, Solar, Polyglot 등 Qwen이 아닌 모든 Causal LM

**검증**: 실험 `20251012_131535_test_full_pipeline_quick`에서 확인됨

#### ⚠️ Device Map Offload 문제

**문제**: 대형 모델(12.8B 이상) 로딩 시 디스크 오프로드 폴더 미지정

**에러 메시지**:
```
ValueError: The current `device_map` had weights offloaded to the disk.
Please provide an `offload_folder` for them.
```

**원인**:
- GPU 메모리 부족 시 `device_map="auto"`가 자동으로 디스크 오프로드 시도
- `offload_folder` 파라미터 미지정

**해결 방법 1**: offload_folder 추가 (권장)
```python
# src/models/llm_loader.py:58
from pathlib import Path
offload_dir = Path(config.experiment.get('output_dir', 'outputs')) / 'offload'
offload_dir.mkdir(parents=True, exist_ok=True)

model = AutoModelForCausalLM.from_pretrained(
    config.model.checkpoint,
    quantization_config=quantization_config,
    device_map="auto",
    offload_folder=str(offload_dir),  # 디스크 오프로드 폴더 지정
    torch_dtype=torch.float16,
    trust_remote_code=True
)
```

**해결 방법 2**: safetensors 설치
```bash
pip install safetensors
```

**영향을 받는 모델**: KULLM-v2 (12.8B), Solar-10.7B 등 대형 모델

**검증**: 실험 `20251012_131535_test_full_pipeline_quick`에서 KULLM-v2 로딩 실패 확인됨

#### ⚠️ token_type_ids 경고

**문제**: Causal LM 추론 시 불필요한 경고 메시지

**경고 메시지**:
```
The following `model_kwargs` are not used by the model: ['token_type_ids']
```

**원인**:
- Causal LM은 `token_type_ids`를 사용하지 않음
- Tokenizer가 자동으로 생성하지만 모델이 무시

**해결 방법**:
추론 코드에서 `token_type_ids` 제거:

```python
# src/inference/predictor.py 또는 관련 추론 코드
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# token_type_ids 제거 (Causal LM은 사용하지 않음)
if 'token_type_ids' in inputs:
    del inputs['token_type_ids']

outputs = model.generate(**inputs, ...)
```

---

## LLM Dataset

### 파일 위치
```
src/data/llm_dataset.py
```

### 클래스 구조

```python
class LLMSummarizationDataset(Dataset):
    def __init__(dialogues, summaries, tokenizer,
                 encoder_max_len=1024, decoder_max_len=200,
                 format_type="instruction")
    def __getitem__(idx)
    def _format_instruction(dialogue, summary)
    def _format_chat(dialogue, summary)

class InstructionAugmentedDataset(Dataset):
    # 5가지 instruction 템플릿으로 데이터 증강
```

### 데이터 포맷

#### Instruction Format
```
### Instruction:
다음 대화를 간결하게 요약해주세요.

### Input:
{dialogue}

### Response:
{summary}
```

#### Chat Format (Llama)
```
<|start_header_id|>system<|end_header_id|>
당신은 대화를 요약하는 전문가입니다.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
다음 대화를 요약해주세요:
{dialogue}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
{summary}<|eot_id|>
```

---

## LLM Trainer

### 파일 위치
```
src/training/llm_trainer.py
```

### 클래스 구조

```python
class LLMTrainer:
    def __init__(config, model, tokenizer, train_dataset, eval_dataset)
    def train()
    def evaluate()
    def _create_training_args()
    def _create_trainer()
```

### 학습 설정 (QLoRA 최적화)

```python
# ==================== LLM 학습 인자 설정 (QLoRA 최적화) ==================== #

# ---------------------- TrainingArguments 생성 ---------------------- #
training_args = TrainingArguments(
    # -------------- 기본 학습 설정 -------------- #
    num_train_epochs=3,                         # 학습 에포크 수
    per_device_train_batch_size=8,              # 디바이스당 배치 크기
    gradient_accumulation_steps=8,              # 그래디언트 누적 스텝 (effective batch=64)
    learning_rate=2e-5,                         # 학습률
    lr_scheduler_type="cosine",                 # Cosine 학습률 스케줄러
    warmup_ratio=0.1,                           # 워밍업 비율 (전체 스텝의 10%)
    weight_decay=0.1,                           # 가중치 감소 (L2 정규화)
    max_grad_norm=1.2,                          # 그래디언트 클리핑 임계값

    # -------------- QLoRA 최적화 설정 -------------- #
    optim="paged_adamw_32bit",                  # Paged AdamW 옵티마이저 (메모리 효율적)
    fp16=True,                                  # ⚠️ FP16 권장 (BF16은 PyTorch AMP 호환 문제 발생 가능)
    gradient_checkpointing=True,                # 그래디언트 체크포인팅 (메모리 절약)

    # -------------- 평가 및 저장 전략 -------------- #
    eval_strategy='epoch',                      # 에포크마다 평가
    save_strategy='epoch',                      # 에포크마다 저장
    metric_for_best_model='eval_loss',          # 최상 모델 선정 기준 (Causal LM은 loss 사용)
    greater_is_better=False                     # Loss는 낮을수록 좋음
)
```

---

## LLM 실행 명령어

### 1. 기본 LLM 학습 (Llama-3.2-3B + QLoRA)

```bash
# ==================== 기본 LLM 학습 명령어 ==================== #

# ---------------------- Llama-3.2-3B 모델 학습 (QLoRA 사용) ---------------------- #
# QLoRA를 활용하여 메모리 효율적으로 Llama-3.2-3B 모델 파인튜닝
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora
```

**결과 파일:**
- 모델: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_llama_3.2_3b/final_model/`
- 학습 로그: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_llama_3.2_3b/train.log`
- 로그 백업: `logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_llama_3.2_3b.log`

### 2. Qwen 모델 학습

```bash
# ==================== Qwen 모델 학습 명령어 ==================== #

# ---------------------- Qwen3-4B 모델 학습 (QLoRA 사용) ---------------------- #
# QLoRA를 활용하여 Qwen3-4B 모델 파인튜닝
python scripts/train_llm.py --experiment qwen3_4b --use_qlora
```

**결과 파일:**
- 모델: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_qwen3_4b/final_model/`
- 학습 로그: `experiments/YYYYMMDD/YYYYMMDD_HHMMSS_single_qwen3_4b/train.log`
- 로그 백업: `logs/YYYYMMDD/train/YYYYMMDD_HHMMSS_single_qwen3_4b.log`

### 3. Instruction Tuning (데이터 5배 증강)

```bash
# ==================== Instruction Tuning 학습 명령어 ==================== #

# ---------------------- 데이터 증강을 활용한 Llama-3.2-3B 학습 ---------------------- #
# 5가지 Instruction 템플릿으로 데이터를 5배 증강하여 학습
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --use_instruction_augmentation
```

**효과:**
- 학습 데이터: 12,457개 → 62,285개 (5배)
- 5가지 instruction 템플릿 적용

### 4. 디버그 모드 (빠른 테스트)

```bash
# ==================== 디버그 모드 학습 명령어 ==================== #

# ---------------------- 빠른 테스트를 위한 디버그 모드 실행 ---------------------- #
# 소량의 데이터로 빠르게 파이프라인을 검증
python scripts/train_llm.py --experiment llama_3.2_3b --use_qlora --debug
```

**디버그 모드 설정:**
- 데이터: 학습 50개, 검증 10개
- 에포크: 1회
- 배치 크기: 2
- WandB: 비활성화

---

## Config 파일

### Causal LM 기본 설정
**파일:** `configs/base/causal_lm.yaml`

```yaml
# ------------------------------- 모델 설정 ------------------------------- #
model:
  type: causal_lm                                         # 모델 타입 (Causal Language Model)
  checkpoint: "Bllossom/llama-3.2-Korean-Bllossom-3B"    # HuggingFace 모델 체크포인트

# ------------------------------- LoRA 설정 ------------------------------- #
lora:
  r: 16                                                   # LoRA rank (저랭크 행렬 차원)
  alpha: 32                                               # alpha 스케일링 파라미터
  target_modules:                                         # LoRA 적용 대상 모듈
    - "q_proj"                                            # Query projection
    - "k_proj"                                            # Key projection
    - "v_proj"                                            # Value projection
    - "o_proj"                                            # Output projection
    - "gate_proj"                                         # Gate projection (MLP)
    - "up_proj"                                           # Up projection (MLP)
    - "down_proj"                                         # Down projection (MLP)
  dropout: 0.05                                           # LoRA 레이어 드롭아웃 비율
  use_qlora: true                                         # QLoRA 4-bit 양자화 사용

# ------------------------------- 토크나이저 설정 ------------------------------- #
tokenizer:
  encoder_max_len: 1024                                   # 인코더 최대 길이 (Prompt truncation 방지)
  decoder_max_len: 200                                    # 디코더 최대 길이

# ------------------------------- 학습 설정 ------------------------------- #
training:
  epochs: 3                                               # 학습 에포크 수
  batch_size: 8                                           # 디바이스당 배치 크기
  gradient_accumulation_steps: 8                          # 그래디언트 누적 스텝 (effective batch=64)
  learning_rate: 2e-5                                     # 학습률
  lr_scheduler_type: "cosine"                             # 학습률 스케줄러 타입
  warmup_ratio: 0.1                                       # 워밍업 비율
  weight_decay: 0.1                                       # 가중치 감소 (L2 정규화)
  max_grad_norm: 1.2                                      # 그래디언트 클리핑 임계값
  gradient_checkpointing: true                            # 그래디언트 체크포인팅 사용
```

### GPU 메모리 사용량

| 모델 | 파라미터 | QLoRA (4-bit) | Full Fine-tuning |
|------|----------|---------------|------------------|
| Llama-3.2-3B | 3B | **8GB** | 24GB |
| Qwen3-4B | 4B | **10GB** | 32GB |
| Llama-3-8B | 8B | **16GB** | 64GB |

### 성능 목표

#### Zero-shot 성능 (검증됨)

| 모델 | ROUGE-1 | ROUGE-2 | ROUGE-L | **ROUGE Sum** |
|------|---------|---------|---------|---------------|
| Llama-3.2-Korean-3B | 26.96 | 11.08 | 24.22 | **49.52** (1위) |
| Qwen3-4B | 24.22 | 9.23 | 21.79 | **45.02** (4위) |

#### 파인튜닝 목표

| 모델 | Zero-shot | 파인튜닝 목표 | 개선 목표 |
|------|-----------|---------------|-----------|
| Llama-3.2-Korean-3B | 49.52 | **95+** | +45 포인트 |
| Qwen3-4B | 45.02 | **95+** | +50 포인트 |

---

## 관련 파일

**소스 코드:**
- `src/models/model_loader.py` - ModelLoader 클래스
- `src/models/lora_loader.py` - LoRA Loader
- `src/training/trainer.py` - ModelTrainer 클래스
- `src/training/llm_trainer.py` - LLM Trainer
- `src/inference/predictor.py` - Predictor 클래스
- `src/data/llm_dataset.py` - LLM Dataset

**Config:**
- `configs/base/default.yaml` - 기본 설정
- `configs/base/encoder_decoder.yaml` - Seq2Seq 설정
- `configs/base/causal_lm.yaml` - Causal LM 설정
- `configs/models/kobart.yaml` - KoBART 설정
- `configs/models/llama_3.2_3b.yaml` - Llama 설정
- `configs/models/qwen3_4b.yaml` - Qwen 설정

**스크립트:**
- `scripts/train.py` - 학습 스크립트
- `scripts/train_llm.py` - LLM 학습 스크립트
- `scripts/inference.py` - 추론 스크립트

**관련 문서:**
- [01_시작_가이드.md](./01_시작_가이드.md) - 빠른 시작 가이드
- [02_핵심_시스템.md](./02_핵심_시스템.md) - 핵심 시스템 및 Config
- [06_데이터_파이프라인.md](./06_데이터_파이프라인.md) - 데이터 처리 및 증강
- [08_평가_최적화.md](./08_평가_최적화.md) - 평가 및 최적화
- [04_명령어_옵션_완전_가이드.md](./04_명령어_옵션_완전_가이드.md) - 전체 명령어 가이드
