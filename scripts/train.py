#!/usr/bin/env python3
# ==================== NLP ëŒ€í™” ìš”ì•½ í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
NLP ëŒ€í™” ìš”ì•½ í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
PRD 14ë²ˆ "ì‹¤í–‰ ì˜µì…˜ ì‹œìŠ¤í…œ" êµ¬í˜„

ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ëª¨ë¸
    python scripts/train.py --mode single --models kobart

    # K-Fold êµì°¨ê²€ì¦
    python scripts/train.py --mode kfold --models solar-10.7b --k_folds 5

    # ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
    python scripts/train.py --mode multi_model --models kobart llama-3.2-3b

    # Optuna ìµœì í™”
    python scripts/train.py --mode optuna --optuna_trials 50

    # í’€ íŒŒì´í”„ë¼ì¸
    python scripts/train.py --mode full --models all --use_tta
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
import argparse
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.logging.logger import Logger
from src.utils.config.seed import set_seed


# ==================== ì¸ì íŒŒì‹± ==================== #
def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='NLP ëŒ€í™” ìš”ì•½ ëª¨ë¸ í•™ìŠµ - ìœ ì—°í•œ ì‹¤í–‰ ì˜µì…˜',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # ==================== ê¸°ë³¸ ì„¤ì • ====================
    parser.add_argument(
        '--mode',
        type=str,
        default='single',
        choices=['single', 'kfold', 'multi_model', 'optuna', 'full'],
        help='''ì‹¤í–‰ ëª¨ë“œ ì„ íƒ:
        single: ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ ì‹¤í—˜)
        kfold: K-Fold êµì°¨ ê²€ì¦ (ì•ˆì •ì„±)
        multi_model: ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸” (ì„±ëŠ¥)
        optuna: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ìë™í™”)
        full: ì „ì²´ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ì œì¶œ)'''
    )

    parser.add_argument(
        '--config',
        type=str,
        default='configs/train_config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ'
    )

    parser.add_argument(
        '--experiment_name',
        type=str,
        default=None,
        help='ì‹¤í—˜ëª… (ìë™ ìƒì„±: {mode}_{model}_{timestamp})'
    )

    # ==================== ëª¨ë¸ ì„ íƒ ====================
    parser.add_argument(
        '--models',
        type=str,
        nargs='+',
        default=['kobart'],
        choices=[
            'kobart',
            'solar-10.7b',
            'polyglot-ko-12.8b',
            'llama-3.2-korean-3b',
            'qwen3-4b',
            'kullm-v2',
            'all'  # ëª¨ë“  ëª¨ë¸
        ],
        help='ì‚¬ìš©í•  ëª¨ë¸ (multi_model ëª¨ë“œì—ì„œ ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)'
    )

    # ==================== í•™ìŠµ ì„¤ì • ====================
    parser.add_argument(
        '--epochs',
        type=int,
        default=None,
        help='ì—í­ ìˆ˜ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--batch_size',
        type=int,
        default=None,
        help='ë°°ì¹˜ í¬ê¸° (None: config íŒŒì¼ ê°’ ì‚¬ìš© ë˜ëŠ” ìë™ íƒìƒ‰)'
    )

    parser.add_argument(
        '--learning_rate',
        type=float,
        default=None,
        help='í•™ìŠµë¥  (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    # ==================== ê³ ê¸‰ í•™ìŠµ ì„¤ì • ====================
    parser.add_argument(
        '--gradient_accumulation_steps',
        type=int,
        default=None,
        help='ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ ìˆ˜ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--warmup_ratio',
        type=float,
        default=None,
        help='Warmup ë¹„ìœ¨ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--weight_decay',
        type=float,
        default=None,
        help='ê°€ì¤‘ì¹˜ ê°ì‡  (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--max_grad_norm',
        type=float,
        default=None,
        help='ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--label_smoothing',
        type=float,
        default=None,
        help='ë ˆì´ë¸” ìŠ¤ë¬´ë”© (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    # ==================== ìƒì„± íŒŒë¼ë¯¸í„° ====================
    parser.add_argument(
        '--num_beams',
        type=int,
        default=None,
        help='Beam search ë¹” ê°œìˆ˜ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--temperature',
        type=float,
        default=None,
        help='ìƒì„± ì˜¨ë„ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--top_p',
        type=float,
        default=None,
        help='Nucleus sampling í™•ë¥  (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--top_k',
        type=int,
        default=None,
        help='Top-K sampling ê°œìˆ˜ (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--repetition_penalty',
        type=float,
        default=None,
        help='ë°˜ë³µ íŒ¨ë„í‹° (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--length_penalty',
        type=float,
        default=None,
        help='ê¸¸ì´ íŒ¨ë„í‹° (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    parser.add_argument(
        '--no_repeat_ngram_size',
        type=int,
        default=None,
        help='ë°˜ë³µ ê¸ˆì§€ n-gram í¬ê¸° (None: config íŒŒì¼ ê°’ ì‚¬ìš©)'
    )

    # ==================== K-Fold ì„¤ì • ====================
    parser.add_argument(
        '--k_folds',
        type=int,
        default=5,
        help='K-Fold ìˆ˜ (kfold ëª¨ë“œ)'
    )

    parser.add_argument(
        '--fold_seed',
        type=int,
        default=42,
        help='Fold ë¶„í•  ì‹œë“œ'
    )

    # ==================== ì•™ìƒë¸” ì„¤ì • ====================
    parser.add_argument(
        '--ensemble_strategy',
        type=str,
        default='weighted_avg',
        choices=[
            'averaging',
            'weighted_avg',
            'majority_vote',
            'stacking',
            'blending',
            'rouge_weighted'
        ],
        help='ì•™ìƒë¸” ì „ëµ'
    )

    parser.add_argument(
        '--ensemble_weights',
        type=float,
        nargs='+',
        default=None,
        help='ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (ìë™ ìµœì í™” ê°€ëŠ¥)'
    )

    # ==================== TTA ì„¤ì • ====================
    parser.add_argument(
        '--use_tta',
        action='store_true',
        help='Test Time Augmentation ì‚¬ìš©'
    )

    parser.add_argument(
        '--tta_strategies',
        type=str,
        nargs='+',
        default=['paraphrase'],
        choices=['paraphrase', 'reorder', 'synonym', 'mask'],
        help='TTA ì „ëµ'
    )

    parser.add_argument(
        '--tta_num_aug',
        type=int,
        default=3,
        help='TTA ì¦ê°• ìˆ˜'
    )

    # ==================== Optuna ì„¤ì • ====================
    parser.add_argument(
        '--optuna_trials',
        type=int,
        default=100,
        help='Optuna ì‹œë„ íšŸìˆ˜'
    )

    parser.add_argument(
        '--optuna_timeout',
        type=int,
        default=7200,
        help='Optuna ì œí•œ ì‹œê°„ (ì´ˆ)'
    )

    parser.add_argument(
        '--optuna_sampler',
        type=str,
        default='tpe',
        choices=['tpe', 'gp', 'random', 'cmaes'],
        help='Optuna ìƒ˜í”ŒëŸ¬'
    )

    parser.add_argument(
        '--optuna_pruner',
        type=str,
        default='median',
        choices=['median', 'percentile', 'hyperband'],
        help='Optuna ê°€ì§€ì¹˜ê¸°'
    )

    # ==================== ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ====================
    parser.add_argument(
        '--use_wandb',
        action='store_true',
        help='WandB ì‚¬ìš©'
    )

    parser.add_argument(
        '--wandb_project',
        type=str,
        default='dialogue-summarization',
        help='WandB í”„ë¡œì íŠ¸ëª…'
    )

    parser.add_argument(
        '--save_visualizations',
        action='store_true',
        help='ì‹œê°í™” ì €ì¥'
    )

    # ==================== ë°ì´í„° ì¦ê°• (PRD 04) ====================
    parser.add_argument(
        '--use_augmentation',
        action='store_true',
        help='ë°ì´í„° ì¦ê°• ì‚¬ìš©'
    )

    parser.add_argument(
        '--augmentation_methods',
        type=str,
        nargs='+',
        default=['back_translation', 'paraphrase'],
        choices=['back_translation', 'paraphrase', 'synonym', 'turn_shuffle'],
        help='ì¦ê°• ë°©ë²•'
    )

    parser.add_argument(
        '--augmentation_ratio',
        type=float,
        default=0.3,
        help='ì¦ê°• ë¹„ìœ¨ (0.0~1.0)'
    )

    # ==================== Solar API (PRD 09) ====================
    parser.add_argument(
        '--use_solar_api',
        action='store_true',
        help='Solar API ì‚¬ìš©'
    )

    parser.add_argument(
        '--solar_api_key',
        type=str,
        default=None,
        help='Solar API í‚¤ (í™˜ê²½ë³€ìˆ˜ SOLAR_API_KEY ì‚¬ìš© ê°€ëŠ¥)'
    )

    parser.add_argument(
        '--solar_model',
        type=str,
        default='solar-1-mini-chat',
        choices=['solar-1-mini-chat', 'solar-1-chat'],
        help='Solar ëª¨ë¸ ì„ íƒ'
    )

    # ==================== í”„ë¡¬í”„íŠ¸ ì „ëµ (PRD 15) ====================
    parser.add_argument(
        '--prompt_strategy',
        type=str,
        default='zero_shot_simple',
        choices=[
            'zero_shot_simple',
            'zero_shot_detailed',
            'few_shot_standard',
            'few_shot_diverse',
            'chain_of_thought',
            'role_playing',
            'self_consistency'
        ],
        help='í”„ë¡¬í”„íŠ¸ ì „ëµ'
    )

    # ==================== ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (PRD 16) ====================
    parser.add_argument(
        '--validate_data_quality',
        action='store_true',
        help='ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰'
    )

    parser.add_argument(
        '--quality_threshold',
        type=float,
        default=0.7,
        help='í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’'
    )

    # ==================== ì¶”ë¡  ìµœì í™” (PRD 17) ====================
    parser.add_argument(
        '--optimize_inference',
        action='store_true',
        help='ì¶”ë¡  ìµœì í™” ì ìš© (í•™ìŠµ í›„ ìë™ ì‹¤í–‰)'
    )

    parser.add_argument(
        '--optimization_method',
        type=str,
        default='quantization',
        choices=['quantization', 'onnx', 'tensorrt', 'pruning'],
        help='ìµœì í™” ë°©ë²•'
    )

    parser.add_argument(
        '--quantization_bits',
        type=int,
        choices=[4, 8, 16],
        default=8,
        help='ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ (4: INT4, 8: INT8, 16: FP16)'
    )

    parser.add_argument(
        '--use_onnx',
        action='store_true',
        help='ONNX ë³€í™˜ ì ìš©'
    )

    parser.add_argument(
        '--use_batch_optimization',
        action='store_true',
        help='ë°°ì¹˜ í¬ê¸° ìµœì í™”'
    )

    # ==================== ê¸°íƒ€ ì˜µì…˜ ====================
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='ëœë¤ ì‹œë“œ'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ (ì ì€ ë°ì´í„°)'
    )

    # ==================== ë°ì´í„° ê²½ë¡œ ====================
    parser.add_argument(
        '--train_data',
        type=str,
        default='data/raw/train.csv',
        help='í•™ìŠµ ë°ì´í„° ê²½ë¡œ'
    )

    parser.add_argument(
        '--dev_data',
        type=str,
        default='data/raw/dev.csv',
        help='ê²€ì¦ ë°ì´í„° ê²½ë¡œ'
    )

    # ==================== ì¶œë ¥ ê²½ë¡œ ====================
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (None: ìë™ ìƒì„±)'
    )

    return parser.parse_args()


# ==================== í™˜ê²½ ì„¤ì • ====================
def setup_environment(args):
    """í™˜ê²½ ì„¤ì •"""
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)

    # ì‹¤í—˜ëª… ìë™ ìƒì„±
    if args.experiment_name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = args.models[0].replace('-', '_') if args.models else 'default'
        args.experiment_name = f"{timestamp}_{args.mode}_{model_name}"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë‚ ì§œë³„ ë¶„ë¥˜)
    if args.output_dir is None:
        # ë‚ ì§œ í´ë” ìƒì„±
        date_folder = datetime.now().strftime("%Y%m%d")
        output_dir = Path(f"experiments/{date_folder}/{args.experiment_name}")
    else:
        output_dir = Path(args.output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)
    args.output_dir = str(output_dir)

    # ë¡œê±° ì„¤ì •
    log_path = output_dir / "train.log"
    logger = Logger(log_path, print_also=True)
    logger.start_redirect()

    return logger


# ==================== Trainer ì„ íƒ ====================
def get_trainer(args, logger):
    """ëª¨ë“œì— ë”°ë¥¸ Trainer ì„ íƒ"""
    if args.mode == 'single':
        from src.trainers import SingleModelTrainer
        return SingleModelTrainer(args, logger)

    elif args.mode == 'kfold':
        from src.trainers import KFoldTrainer
        return KFoldTrainer(args, logger)

    elif args.mode == 'multi_model':
        from src.trainers import MultiModelEnsembleTrainer
        return MultiModelEnsembleTrainer(args, logger)

    elif args.mode == 'optuna':
        from src.trainers import OptunaTrainer
        return OptunaTrainer(args, logger)

    elif args.mode == 'full':
        from src.trainers import FullPipelineTrainer
        return FullPipelineTrainer(args, logger)

    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {args.mode}")


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ì íŒŒì‹±
    args = parse_arguments()

    print("=" * 60)
    print("ğŸš€ NLP ëŒ€í™” ìš”ì•½ í•™ìŠµ ì‹œì‘")
    print(f"ğŸ“‹ ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ğŸ¤– ëª¨ë¸: {', '.join(args.models)}")
    print(f"ğŸ“ ì‹¤í—˜ëª…: {args.experiment_name or '(ìë™ ìƒì„±)'}")
    print("=" * 60)

    # í™˜ê²½ ì„¤ì •
    logger = setup_environment(args)

    try:
        # Trainer ìƒì„±
        trainer = get_trainer(args, logger)

        # í•™ìŠµ ì‹¤í–‰
        logger.write(f"\nğŸ“Š {args.mode.upper()} ëª¨ë“œ ì‹¤í–‰ ì¤‘...")
        results = trainer.train()

        # ê²°ê³¼ ì €ì¥
        trainer.save_results(results)

        # í•™ìŠµ ë¡œê·¸ ë³µì‚¬ (logs í´ë”ì—ë„ ì €ì¥)
        try:
            import shutil
            from src.utils.core.common import now

            # ë‚ ì§œ í´ë” ê²½ë¡œ
            date_folder = now('%Y%m%d')
            log_backup_dir = Path(f"logs/{date_folder}/train")
            log_backup_dir.mkdir(parents=True, exist_ok=True)

            # ì˜µì…˜ ì •ë³´ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
            timestamp = now('%Y%m%d_%H%M%S')
            model_name = args.models[0].replace('-', '_') if args.models else 'default'

            # ì˜µì…˜ íƒœê·¸ ìƒì„±
            options = []
            if args.batch_size and args.batch_size != 8:
                options.append(f"bs{args.batch_size}")
            if args.epochs and args.epochs != 3:
                options.append(f"ep{args.epochs}")
            if args.use_augmentation:
                options.append("aug")
            if args.use_tta:
                options.append("tta")
            if args.ensemble_strategy and args.mode == 'multi_model':
                options.append(args.ensemble_strategy)

            # íŒŒì¼ëª… ìƒì„±
            parts = [timestamp, args.mode, model_name]
            if options:
                parts.extend(options)

            log_filename = "_".join(parts) + ".log"
            log_backup_path = log_backup_dir / log_filename

            # ë¡œê·¸ íŒŒì¼ ë³µì‚¬
            source_log = Path(args.output_dir) / "train.log"
            if source_log.exists():
                shutil.copy2(source_log, log_backup_path)
                logger.write(f"\nğŸ“‹ í•™ìŠµ ë¡œê·¸ ë°±ì—…: {log_backup_path}")

        except Exception as e:
            logger.write(f"\nâš ï¸ ë¡œê·¸ ë°±ì—… ì‹¤íŒ¨: {e}")

        # ì¶”ë¡  ìµœì í™” (PRD 17) - ì˜µì…˜
        if args.optimize_inference:
            logger.write("\nğŸ”§ ì¶”ë¡  ìµœì í™” ì‹œì‘ (PRD 17)...")
            try:
                from src.inference import create_inference_optimizer

                # ìµœì í™” ëª¨ë“ˆ ìƒì„±
                optimizer = create_inference_optimizer(
                    optimization_method=args.optimization_method,
                    quantization_bits=args.quantization_bits,
                    use_onnx=args.use_onnx,
                    use_batch_optimization=args.use_batch_optimization,
                    logger=logger
                )

                # ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                model_path = None
                if 'model_path' in results:
                    model_path = results['model_path']
                elif 'model_results' in results and results['model_results']:
                    model_path = results['model_results'][0].get('model_path')

                if model_path:
                    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

                    # ëª¨ë¸ ë¡œë“œ
                    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
                    tokenizer = AutoTokenizer.from_pretrained(model_path)

                    # ìµœì í™” ì‹¤í–‰
                    optimization_results = optimizer.optimize(
                        model=model,
                        tokenizer=tokenizer,
                        output_dir=f"{args.output_dir}/optimized",
                        sample_texts=["ìƒ˜í”Œ ëŒ€í™”ì…ë‹ˆë‹¤."] * 10  # ìƒ˜í”Œ ë°ì´í„°
                    )

                    logger.write(f"  âœ… ì¶”ë¡  ìµœì í™” ì™„ë£Œ!")
                    logger.write(f"  ğŸ“ ìµœì í™” ëª¨ë¸: {optimization_results.get('model_path', 'N/A')}")
                else:
                    logger.write("  âš ï¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¶”ë¡  ìµœì í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            except ImportError as e:
                logger.write(f"  âš ï¸ ì¶”ë¡  ìµœì í™” ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            except Exception as e:
                logger.write(f"  âš ï¸ ì¶”ë¡  ìµœì í™” ì‹¤íŒ¨: {e}")

        # ì‹œê°í™” (ì˜µì…˜)
        if args.save_visualizations:
            logger.write("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
            try:
                from src.utils.visualizations import create_training_visualizations
                create_training_visualizations(
                    results=results,
                    output_dir=args.output_dir
                )
                logger.write("  âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ")
            except ImportError:
                logger.write("  âš ï¸ ì‹œê°í™” ëª¨ë“ˆ ì—†ìŒ (ì¶”í›„ êµ¬í˜„ ì˜ˆì •)")
            except Exception as e:
                logger.write(f"  âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

        print("\n" + "=" * 60)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {args.output_dir}")
        if args.optimize_inference:
            print("ğŸ”§ ì¶”ë¡  ìµœì í™” ì ìš©ë¨")
        print("=" * 60)

    except Exception as e:
        logger.write(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}", print_error=True)
        raise

    finally:
        # ì •ë¦¬
        logger.stop_redirect()
        logger.close()


# ==================== ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    main()
