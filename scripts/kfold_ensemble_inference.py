#!/usr/bin/env python3
# ==================== K-Fold ì•™ìƒë¸” ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
K-Fold ëª¨ë¸ ì•™ìƒë¸” ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/kfold_ensemble_inference.py \
        --experiment_dir experiments/20251014/20251014_183206_kobart_ultimate_kfold \
        --test_data data/raw/test.csv \
        --ensemble_method soft_voting \
        --use_pretrained_correction \
        --output submissions/kfold_ensemble.csv
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
import os
import argparse
from pathlib import Path
from datetime import datetime
import pickle

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    env_path = project_root / '.env'
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ ì„±ê³µ: {env_path}")
    else:
        print(f"âš ï¸  .env íŒŒì¼ ì—†ìŒ: {env_path}")
except ImportError:
    print("âš ï¸  python-dotenv ë¯¸ì„¤ì¹˜ - í™˜ê²½ ë³€ìˆ˜ ìˆ˜ë™ ì„¤ì • í•„ìš”")

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import warnings
import pandas as pd
import torch
import numpy as np
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from tqdm import tqdm

# Transformers ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings("ignore", message=".*max_new_tokens.*max_length.*")
warnings.filterwarnings("ignore", message=".*num_labels.*id2label.*")

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.config import load_config
from src.inference import create_predictor
from src.logging.logger import Logger
from src.utils.core.common import now, ensure_dir
from src.utils.gpu_optimization.team_gpu_check import get_gpu_info, check_gpu_tier


# ==================== ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ í•¨ìˆ˜ ==================== #
def save_inference_checkpoint(checkpoint_dir, stage, data, logger=None):
    """
    ì¶”ë¡  ì²´í¬í¬ì¸íŠ¸ ì €ì¥

    Args:
        checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        stage: ë‹¨ê³„ ì´ë¦„ ('kfold', 'hf_correction', 'solar_api')
        data: ì €ì¥í•  ë°ì´í„° (dict)
        logger: Logger ì¸ìŠ¤í„´ìŠ¤
    """
    checkpoint_dir = Path(checkpoint_dir)
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    checkpoint_file = checkpoint_dir / f"{stage}_checkpoint.pkl"

    try:
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(data, f)

        if logger:
            logger.write(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file}")
    except Exception as e:
        if logger:
            logger.write(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_inference_checkpoint(checkpoint_dir, stage, logger=None):
    """
    ì¶”ë¡  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

    Args:
        checkpoint_dir: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        stage: ë‹¨ê³„ ì´ë¦„ ('kfold', 'hf_correction', 'solar_api')
        logger: Logger ì¸ìŠ¤í„´ìŠ¤

    Returns:
        dict: ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°, ì—†ìœ¼ë©´ None
    """
    checkpoint_dir = Path(checkpoint_dir)
    checkpoint_file = checkpoint_dir / f"{stage}_checkpoint.pkl"

    if not checkpoint_file.exists():
        return None

    try:
        with open(checkpoint_file, 'rb') as f:
            data = pickle.load(f)

        if logger:
            logger.write(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_file}")

        return data
    except Exception as e:
        if logger:
            logger.write(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def remove_inference_checkpoint(checkpoint_dir, stage, logger=None):
    """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ"""
    checkpoint_dir = Path(checkpoint_dir)
    checkpoint_file = checkpoint_dir / f"{stage}_checkpoint.pkl"

    if checkpoint_file.exists():
        try:
            checkpoint_file.unlink()
            if logger:
                logger.write(f"ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {checkpoint_file}")
        except Exception as e:
            if logger:
                logger.write(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")


# ==================== K-Fold ì•™ìƒë¸” í´ë˜ìŠ¤ ==================== #
class KFoldEnsemblePredictor:
    """K-Fold ëª¨ë¸ ì•™ìƒë¸” ì˜ˆì¸¡ê¸°"""

    def __init__(self, fold_model_dirs, ensemble_method='soft_voting', logger=None):
        """
        Args:
            fold_model_dirs: Foldë³„ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
            ensemble_method: ì•™ìƒë¸” ë°©ë²• ('soft_voting', 'hard_voting', 'averaging')
            logger: Logger ì¸ìŠ¤í„´ìŠ¤
        """
        self.fold_model_dirs = fold_model_dirs
        self.ensemble_method = ensemble_method
        self.logger = logger
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ê° Fold ëª¨ë¸ ë¡œë“œ
        self.models = []
        self.tokenizers = []
        self._load_all_fold_models()

    def _log(self, msg):
        """ë¡œê¹… í—¬í¼"""
        if self.logger:
            self.logger.write(msg)
        else:
            print(msg)

    def _load_all_fold_models(self):
        """ëª¨ë“  Fold ëª¨ë¸ ë¡œë“œ"""
        self._log(f"\nğŸ”„ {len(self.fold_model_dirs)}ê°œ Fold ëª¨ë¸ ë¡œë”© ì¤‘...")

        for i, model_dir in enumerate(self.fold_model_dirs):
            self._log(f"  [Fold {i+1}/{len(self.fold_model_dirs)}] ë¡œë”©: {model_dir}")
            try:
                model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)
                tokenizer = AutoTokenizer.from_pretrained(model_dir)
                model = model.to(self.device)
                model.eval()

                self.models.append(model)
                self.tokenizers.append(tokenizer)
                self._log(f"    âœ… ì™„ë£Œ")
            except Exception as e:
                self._log(f"    âŒ ì‹¤íŒ¨: {e}")
                raise

        self._log(f"âœ… ì „ì²´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

    def predict_batch(
        self,
        dialogues,
        batch_size=16,
        show_progress=True,
        **generation_kwargs
    ):
        """
        ë°°ì¹˜ ì•™ìƒë¸” ì˜ˆì¸¡

        Args:
            dialogues: ì…ë ¥ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            show_progress: ì§„í–‰ë°” í‘œì‹œ ì—¬ë¶€
            **generation_kwargs: ìƒì„± íŒŒë¼ë¯¸í„°

        Returns:
            List[str]: ì•™ìƒë¸”ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        self._log(f"ğŸ”® K-Fold ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘ ({self.ensemble_method})")
        self._log(f"  - ìƒ˜í”Œ ìˆ˜: {len(dialogues)}")
        self._log(f"  - Fold ìˆ˜: {len(self.models)}")
        self._log(f"  - ë°°ì¹˜ í¬ê¸°: {batch_size}")

        # ê° Foldë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        all_fold_summaries = []

        for fold_idx, (model, tokenizer) in enumerate(zip(self.models, self.tokenizers)):
            self._log(f"\n[Fold {fold_idx+1}/{len(self.models)}] ì˜ˆì¸¡ ì¤‘...")

            # Predictor ìƒì„±
            predictor = create_predictor(
                model=model,
                tokenizer=tokenizer,
                device=self.device,
                logger=None  # ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€
            )

            # ë°°ì¹˜ ì˜ˆì¸¡
            fold_summaries = predictor.predict_batch(
                dialogues=dialogues,
                batch_size=batch_size,
                show_progress=show_progress,
                **generation_kwargs
            )

            all_fold_summaries.append(fold_summaries)
            self._log(f"  âœ… Fold {fold_idx+1} ì˜ˆì¸¡ ì™„ë£Œ ({len(fold_summaries)}ê°œ)")

        # ì•™ìƒë¸” ìˆ˜í–‰
        self._log(f"\nğŸ”„ ì•™ìƒë¸” ìˆ˜í–‰ ì¤‘ ({self.ensemble_method})...")
        ensemble_summaries = self._ensemble(all_fold_summaries)
        self._log(f"  âœ… ì•™ìƒë¸” ì™„ë£Œ\n")

        return ensemble_summaries

    def _ensemble(self, all_fold_summaries):
        """
        ì•™ìƒë¸” ìˆ˜í–‰

        Args:
            all_fold_summaries: Foldë³„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
                [[fold0_summary0, fold0_summary1, ...],
                 [fold1_summary0, fold1_summary1, ...],
                 ...]

        Returns:
            List[str]: ì•™ìƒë¸”ëœ ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        """
        n_samples = len(all_fold_summaries[0])
        ensemble_summaries = []

        for i in range(n_samples):
            # ië²ˆì§¸ ìƒ˜í”Œì˜ ëª¨ë“  Fold ìš”ì•½ ìˆ˜ì§‘
            sample_summaries = [fold_summaries[i] for fold_summaries in all_fold_summaries]

            if self.ensemble_method == 'soft_voting':
                # Soft Voting: ê°€ì¥ ê¸´ ìš”ì•½ ì„ íƒ (ì •ë³´ëŸ‰ ìµœëŒ€í™”)
                ensemble_summary = max(sample_summaries, key=len)

            elif self.ensemble_method == 'hard_voting':
                # Hard Voting: ê°€ì¥ ë¹ˆë²ˆí•œ ìš”ì•½ ì„ íƒ
                from collections import Counter
                counter = Counter(sample_summaries)
                ensemble_summary = counter.most_common(1)[0][0]

            elif self.ensemble_method == 'averaging':
                # Averaging: ì¤‘ê°„ ê¸¸ì´ ìš”ì•½ ì„ íƒ
                sample_summaries_sorted = sorted(sample_summaries, key=len)
                median_idx = len(sample_summaries_sorted) // 2
                ensemble_summary = sample_summaries_sorted[median_idx]

            else:
                # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ Fold ì‚¬ìš©
                ensemble_summary = sample_summaries[0]

            ensemble_summaries.append(ensemble_summary)

        return ensemble_summaries


# ==================== ë©”ì¸ í•¨ìˆ˜ ==================== #
def main():
    # -------------- ì¸ì íŒŒì‹± -------------- #
    parser = argparse.ArgumentParser(description="K-Fold ì•™ìƒë¸” ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--experiment_dir",
        type=str,
        required=True,
        help="K-Fold í•™ìŠµ ì‹¤í—˜ ë””ë ‰í† ë¦¬ (ì˜ˆ: experiments/20251014/20251014_183206_kobart_ultimate_kfold)"
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default="data/raw/test.csv",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ì œì¶œ íŒŒì¼ ì¶œë ¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±)"
    )
    parser.add_argument(
        "--ensemble_method",
        type=str,
        default="soft_voting",
        choices=["soft_voting", "hard_voting", "averaging"],
        help="ì•™ìƒë¸” ë°©ë²•"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ì¶”ë¡  ë°°ì¹˜ í¬ê¸°"
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=4,
        help="Beam search ë¹” ê°œìˆ˜"
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=100,
        help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
    )
    parser.add_argument(
        "--min_new_tokens",
        type=int,
        default=None,
        help="ìƒì„±í•  ìµœì†Œ í† í° ìˆ˜"
    )
    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=None,
        help="ë°˜ë³µ ì–µì œ ê°•ë„"
    )
    parser.add_argument(
        "--no_repeat_ngram_size",
        type=int,
        default=None,
        help="ë°˜ë³µ ê¸ˆì§€ n-gram í¬ê¸°"
    )
    parser.add_argument(
        "--length_penalty",
        type=float,
        default=None,
        help="ê¸¸ì´ í˜ë„í‹°"
    )
    parser.add_argument(
        "--use_pretrained_correction",
        action="store_true",
        help="HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ì‚¬ìš©"
    )
    parser.add_argument(
        "--correction_models",
        type=str,
        nargs="+",
        default=["gogamza/kobart-base-v2", "digit82/kobart-summarization"],
        help="ë³´ì •ì— ì‚¬ìš©í•  HuggingFace ëª¨ë¸ ë¦¬ìŠ¤íŠ¸"
    )
    parser.add_argument(
        "--correction_strategy",
        type=str,
        default="quality_based",
        choices=["quality_based", "threshold", "voting", "weighted"],
        help="ë³´ì • ì „ëµ"
    )
    parser.add_argument(
        "--correction_threshold",
        type=float,
        default=0.3,
        help="í’ˆì§ˆ ì„ê³„ê°’"
    )
    parser.add_argument(
        "--use_solar_api",
        action="store_true",
        help="Solar API ì•™ìƒë¸” ì‚¬ìš©"
    )
    parser.add_argument(
        "--solar_api_key",
        type=str,
        default=None,
        help="Solar API í‚¤ (í™˜ê²½ë³€ìˆ˜ SOLAR_API_KEY ì‚¬ìš© ê°€ëŠ¥)"
    )
    parser.add_argument(
        "--solar_model",
        type=str,
        default="solar-1-mini-chat",
        help="Solar ëª¨ë¸ ì„ íƒ"
    )
    parser.add_argument(
        "--solar_temperature",
        type=float,
        default=0.2,
        help="Solar API ìƒì„± ì˜¨ë„"
    )
    parser.add_argument(
        "--solar_batch_size",
        type=int,
        default=10,
        help="Solar API ë°°ì¹˜ í¬ê¸°"
    )
    parser.add_argument(
        "--solar_delay",
        type=float,
        default=1.0,
        help="Solar API ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)"
    )
    parser.add_argument(
        "--solar_use_voting",
        action="store_true",
        help="Solar API K-Fold ë°©ì‹ ë‹¤ì¤‘ ìƒ˜í”Œë§ ì‚¬ìš©"
    )
    parser.add_argument(
        "--solar_n_samples",
        type=int,
        default=3,
        help="Solar API ìƒ˜í”Œë§ íšŸìˆ˜ (voting ì‚¬ìš© ì‹œ)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ ì‹¤í–‰"
    )
    parser.add_argument(
        "--skip_kfold",
        action="store_true",
        help="K-Fold ì•™ìƒë¸” ê±´ë„ˆë›°ê¸° (ì²´í¬í¬ì¸íŠ¸ í•„ìˆ˜)"
    )
    parser.add_argument(
        "--kfold_checkpoint",
        type=str,
        default=None,
        help="ì¬ì‚¬ìš©í•  K-Fold ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì˜ˆ: experiments/.../checkpoints/kfold_checkpoint.pkl)"
    )

    args = parser.parse_args()

    # -------------- ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • -------------- #
    timestamp = now('%Y%m%d_%H%M%S')

    # ì˜µì…˜ íƒœê·¸ ìƒì„±
    options = ["kfold", args.ensemble_method]
    if args.batch_size != 16:
        options.append(f"bs{args.batch_size}")
    if args.max_new_tokens != 100:
        options.append(f"maxnew{args.max_new_tokens}")
    if args.use_pretrained_correction:
        options.append("hf")
    if args.use_solar_api:
        options.append("solar")

    # í´ë”ëª… ìƒì„±
    folder_name = "_".join([timestamp, "inference_kobart"] + options)
    date_folder = datetime.now().strftime("%Y%m%d")
    output_dir = Path(f"experiments/{date_folder}/{folder_name}")
    output_dir.mkdir(parents=True, exist_ok=True)

    # -------------- Logger ì´ˆê¸°í™” -------------- #
    log_path = output_dir / "inference.log"
    logger = Logger(log_path, print_also=True)
    logger.start_redirect()

    # âœ… ì‹¤í–‰ ëª…ë ¹ì–´ ì €ì¥
    from src.utils.core.path_resolver import save_command_to_experiment
    save_command_to_experiment(output_dir, verbose=False)

    try:
        logger.write("=" * 60)
        logger.write(f"K-Fold ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘")
        logger.write("=" * 60)

        # -------------- GPU ì •ë³´ ì¶œë ¥ -------------- #
        logger.write("\n[GPU ì •ë³´]")
        gpu_info = get_gpu_info()
        for key, value in gpu_info.items():
            logger.write(f"  {key}: {value}")

        gpu_tier = check_gpu_tier()
        logger.write(f"  GPU Tier: {gpu_tier}")

        # -------------- 1. Fold ëª¨ë¸ ê²½ë¡œ íƒìƒ‰ -------------- #
        logger.write(f"\n[1/4] Fold ëª¨ë¸ íƒìƒ‰: {args.experiment_dir}")
        experiment_dir = Path(args.experiment_dir)

        fold_model_dirs = []
        for fold_dir in sorted(experiment_dir.glob("fold_*")):
            # kfold/final_model ë˜ëŠ” default/final_model ì°¾ê¸°
            for subdir in ['kfold', 'default']:
                model_path = fold_dir / subdir / 'final_model'
                if model_path.exists():
                    fold_model_dirs.append(str(model_path))
                    logger.write(f"  âœ… {fold_dir.name}: {model_path}")
                    break

        if not fold_model_dirs:
            raise ValueError(f"Fold ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {experiment_dir}")

        logger.write(f"\n  ğŸ“Š ë°œê²¬ëœ Fold ìˆ˜: {len(fold_model_dirs)}")

        # -------------- 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ -------------- #
        logger.write(f"\n[2/4] í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {args.test_data}")
        test_df = pd.read_csv(args.test_data)
        logger.write(f"  âœ… í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_df)}ê°œ")

        # ëŒ€í™” ì¶”ì¶œ
        dialogues = test_df['dialogue'].tolist()

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        checkpoint_dir = output_dir / "checkpoints" if args.resume else None

        # -------------- 3. K-Fold ì•™ìƒë¸” ì˜ˆì¸¡ -------------- #
        summaries = None
        kfold_checkpoint = None

        # ì™¸ë¶€ ì²´í¬í¬ì¸íŠ¸ ì§€ì • ì‹œ ë¡œë“œ
        if args.kfold_checkpoint:
            logger.write(f"\n[3/6] ğŸ“‚ ì™¸ë¶€ K-Fold ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {args.kfold_checkpoint}")
            try:
                with open(args.kfold_checkpoint, 'rb') as f:
                    kfold_checkpoint = pickle.load(f)
                logger.write(f"  âœ… ë¡œë“œ ì„±ê³µ: {len(kfold_checkpoint['summaries'])}ê°œ ìš”ì•½")
                summaries = kfold_checkpoint['summaries']
            except Exception as e:
                logger.write(f"  âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise

        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        elif checkpoint_dir:
            kfold_checkpoint = load_inference_checkpoint(checkpoint_dir, 'kfold', logger)

        if kfold_checkpoint and not args.kfold_checkpoint:
            logger.write(f"\n[3/6] âœ… K-Fold ì•™ìƒë¸” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›")
            logger.write(f"  - ë³µì›ëœ ìš”ì•½ ìˆ˜: {len(kfold_checkpoint['summaries'])}")
            summaries = kfold_checkpoint['summaries']
        elif args.skip_kfold and summaries is not None:
            logger.write(f"\n[3/6] â­ï¸  K-Fold ì•™ìƒë¸” ê±´ë„ˆë›°ê¸° (ì™¸ë¶€ ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)")
        elif args.skip_kfold:
            raise ValueError("--skip_kfold ì‚¬ìš© ì‹œ --kfold_checkpoint ë˜ëŠ” --resumeì´ í•„ìš”í•©ë‹ˆë‹¤")
        elif summaries is None:
            logger.write(f"\n[3/6] K-Fold ì•™ìƒë¸” ì¶”ë¡  ì‹¤í–‰...")
            logger.write(f"  - ì•™ìƒë¸” ë°©ë²•: {args.ensemble_method}")
            logger.write(f"  - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")

            # K-Fold Ensemble Predictor ìƒì„±
            ensemble_predictor = KFoldEnsemblePredictor(
                fold_model_dirs=fold_model_dirs,
                ensemble_method=args.ensemble_method,
                logger=logger
            )

            # ìƒì„± íŒŒë¼ë¯¸í„° ì¤€ë¹„
            generation_kwargs = {'num_beams': args.num_beams}
            if args.max_new_tokens is not None:
                generation_kwargs['max_new_tokens'] = args.max_new_tokens
            if args.min_new_tokens is not None:
                generation_kwargs['min_new_tokens'] = args.min_new_tokens
            if args.repetition_penalty is not None:
                generation_kwargs['repetition_penalty'] = args.repetition_penalty
            if args.no_repeat_ngram_size is not None:
                generation_kwargs['no_repeat_ngram_size'] = args.no_repeat_ngram_size
            if args.length_penalty is not None:
                generation_kwargs['length_penalty'] = args.length_penalty

            # ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰
            summaries = ensemble_predictor.predict_batch(
                dialogues=dialogues,
                batch_size=args.batch_size,
                show_progress=True,
                **generation_kwargs
            )

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if checkpoint_dir:
                save_inference_checkpoint(
                    checkpoint_dir,
                    'kfold',
                    {
                        'summaries': summaries,
                        'ensemble_method': args.ensemble_method,
                        'generation_kwargs': generation_kwargs
                    },
                    logger
                )

                # CSV ì²´í¬í¬ì¸íŠ¸ë„ ì €ì¥ (ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡)
                kfold_csv_path = checkpoint_dir / "kfold_summaries.csv"
                pd.DataFrame({
                    'fname': test_df['fname'],
                    'summary': summaries
                }).to_csv(kfold_csv_path, index=False, encoding='utf-8')
                logger.write(f"ğŸ’¾ K-Fold CSV ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {kfold_csv_path}")

        # -------------- 4. HuggingFace ë³´ì • (ì„ íƒì ) -------------- #
        hf_checkpoint = None
        if checkpoint_dir and args.use_pretrained_correction:
            hf_checkpoint = load_inference_checkpoint(checkpoint_dir, 'hf_correction', logger)

        if hf_checkpoint:
            logger.write(f"\n[4/6] âœ… HuggingFace ë³´ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›")
            logger.write(f"  - ë³µì›ëœ ìš”ì•½ ìˆ˜: {len(hf_checkpoint['summaries'])}")
            summaries = hf_checkpoint['summaries']
        elif args.use_pretrained_correction:
            logger.write("\n[4/6] ğŸ”§ HuggingFace ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë³´ì • ì‹œì‘")
            logger.write(f"  - ë³´ì • ëª¨ë¸: {', '.join(args.correction_models)}")
            logger.write(f"  - ë³´ì • ì „ëµ: {args.correction_strategy}")
            logger.write(f"  - í’ˆì§ˆ ì„ê³„ê°’: {args.correction_threshold}")

            try:
                from src.correction.pretrained_corrector import PretrainedCorrector

                # ë³´ì •ê¸° ìƒì„±
                corrector = PretrainedCorrector(
                    model_names=args.correction_models,
                    correction_strategy=args.correction_strategy,
                    quality_threshold=args.correction_threshold,
                    logger=logger
                )

                # ë³´ì • ìˆ˜í–‰
                summaries = corrector.correct_batch(
                    dialogues=dialogues,
                    candidate_summaries=summaries,
                    batch_size=args.batch_size,
                    **generation_kwargs
                )

                logger.write("âœ… HuggingFace ë³´ì • ì™„ë£Œ")

                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if checkpoint_dir:
                    save_inference_checkpoint(
                        checkpoint_dir,
                        'hf_correction',
                        {
                            'summaries': summaries,
                            'correction_models': args.correction_models,
                            'correction_strategy': args.correction_strategy
                        },
                        logger
                    )

                    # CSV ì²´í¬í¬ì¸íŠ¸ë„ ì €ì¥
                    hf_csv_path = checkpoint_dir / "hf_correction_summaries.csv"
                    pd.DataFrame({
                        'fname': test_df['fname'],
                        'summary': summaries
                    }).to_csv(hf_csv_path, index=False, encoding='utf-8')
                    logger.write(f"ğŸ’¾ HuggingFace CSV ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {hf_csv_path}")
            except Exception as e:
                logger.write(f"âŒ HuggingFace ë³´ì • ì‹¤íŒ¨: {e}")
                logger.write("  âš ï¸  ë³´ì • ì—†ì´ ì§„í–‰")
        else:
            if not args.use_pretrained_correction:
                logger.write("\n[4/6] â­ï¸  HuggingFace ë³´ì • ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”)")

        # -------------- 5. Solar API ì•™ìƒë¸” (ì„ íƒì ) -------------- #
        solar_checkpoint = None
        if checkpoint_dir and args.use_solar_api:
            solar_checkpoint = load_inference_checkpoint(checkpoint_dir, 'solar_api', logger)

        if solar_checkpoint:
            logger.write(f"\n[5/6] âœ… Solar API ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›")
            logger.write(f"  - ë³µì›ëœ ìš”ì•½ ìˆ˜: {len(solar_checkpoint['summaries'])}")
            summaries = solar_checkpoint['summaries']
        elif args.use_solar_api:
            logger.write("\n[5/6] ğŸŒ Solar API ì•™ìƒë¸” ì‹œì‘")
            logger.write(f"  - ëª¨ë¸: {args.solar_model}")
            logger.write(f"  - Temperature: {args.solar_temperature}")
            logger.write(f"  - ë°°ì¹˜ í¬ê¸°: {args.solar_batch_size}")
            logger.write(f"  - ëŒ€ê¸° ì‹œê°„: {args.solar_delay}ì´ˆ")

            try:
                from src.api.solar_api import create_solar_api

                # Solar API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                solar_api = create_solar_api(
                    api_key=args.solar_api_key,
                    token_limit=512,
                    cache_dir=str(output_dir / "cache" / "solar"),
                    logger=logger
                )

                # Solar APIë¡œ ìš”ì•½ ìƒì„±
                if args.solar_use_voting:
                    logger.write(f"\n  Solar API ë°°ì¹˜ ìš”ì•½ ìƒì„± ì¤‘ (ğŸ”„ K-Fold ë°©ì‹ {args.solar_n_samples}íšŒ ìƒ˜í”Œë§)...")
                else:
                    logger.write(f"\n  Solar API ë°°ì¹˜ ìš”ì•½ ìƒì„± ì¤‘...")

                solar_summaries = solar_api.summarize_batch(
                    dialogues=dialogues,
                    batch_size=args.solar_batch_size,
                    delay=args.solar_delay,
                    use_voting=args.solar_use_voting,
                    n_samples=args.solar_n_samples
                )

                # KoBART ìš”ì•½ê³¼ Solar ìš”ì•½ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· )
                logger.write(f"\n  KoBARTì™€ Solar ì•™ìƒë¸” ìˆ˜í–‰ ì¤‘...")
                ensemble_summaries = []
                for kobart_summary, solar_summary in zip(summaries, solar_summaries):
                    # ê°„ë‹¨í•œ ì•™ìƒë¸” ì „ëµ: Solar ìš”ì•½ ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ KoBART ì‚¬ìš©
                    if solar_summary and len(solar_summary.strip()) > 10:
                        ensemble_summaries.append(solar_summary)
                    else:
                        ensemble_summaries.append(kobart_summary)

                summaries = ensemble_summaries
                logger.write("âœ… Solar API ì•™ìƒë¸” ì™„ë£Œ")

                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if checkpoint_dir:
                    save_inference_checkpoint(
                        checkpoint_dir,
                        'solar_api',
                        {
                            'summaries': summaries,
                            'solar_model': args.solar_model,
                            'solar_temperature': args.solar_temperature
                        },
                        logger
                    )

                    # CSV ì²´í¬í¬ì¸íŠ¸ë„ ì €ì¥
                    solar_csv_path = checkpoint_dir / "solar_api_summaries.csv"
                    pd.DataFrame({
                        'fname': test_df['fname'],
                        'summary': summaries
                    }).to_csv(solar_csv_path, index=False, encoding='utf-8')
                    logger.write(f"ğŸ’¾ Solar API CSV ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {solar_csv_path}")

            except ImportError as e:
                logger.write(f"âŒ Solar API ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                logger.write("  âš ï¸  Solar API ì—†ì´ ì§„í–‰")
            except Exception as e:
                logger.write(f"âŒ Solar API ì•™ìƒë¸” ì‹¤íŒ¨: {e}")
                logger.write("  âš ï¸  Solar API ì—†ì´ ì§„í–‰")
        else:
            if not args.use_solar_api:
                logger.write("\n[5/6] â­ï¸  Solar API ê±´ë„ˆë›°ê¸° (ë¹„í™œì„±í™”)")

        # ì œì¶œ DataFrame ìƒì„±
        submission_df = test_df[['fname']].copy()
        submission_df['summary'] = summaries

        # -------------- 6. íŒŒì¼ ì €ì¥ -------------- #
        logger.write("\n[6/6] ì œì¶œ íŒŒì¼ ì €ì¥ ì¤‘...")

        # ì¶œë ¥ ê²½ë¡œ ìë™ ìƒì„±
        if args.output is None:
            submission_dir = output_dir / "submission"
            submission_dir.mkdir(parents=True, exist_ok=True)
            args.output = str(submission_dir / f"{folder_name}.csv")

        # ì¶œë ¥ ê²½ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±
        ensure_dir(Path(args.output).parent)

        # 1) ì‹¤í—˜ í´ë”ì— ì €ì¥
        submission_df.to_csv(args.output, index=False, encoding='utf-8')
        logger.write(f"  âœ… ì œì¶œ íŒŒì¼ ìƒì„± (1): {args.output}")

        # 2) ì „ì—­ submissions í´ë”ì—ë„ ì €ì¥
        global_submission_dir = Path('submissions') / date_folder
        global_submission_dir.mkdir(parents=True, exist_ok=True)
        global_submission_path = global_submission_dir / f"{folder_name}.csv"
        submission_df.to_csv(global_submission_path, index=False, encoding='utf-8')
        logger.write(f"  âœ… ì œì¶œ íŒŒì¼ ìƒì„± (2): {global_submission_path}")

        # -------------- 7. ê²°ê³¼ ì¶œë ¥ -------------- #
        logger.write("\n" + "=" * 60)
        logger.write("ğŸ“Š ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.write("=" * 60)

        logger.write(f"\nâœ… ì‹¤í–‰ëœ ë‹¨ê³„:")
        logger.write(f"  1. K-Fold ì•™ìƒë¸” ({len(fold_model_dirs)}ê°œ ëª¨ë¸, {args.ensemble_method})")
        if args.use_pretrained_correction:
            logger.write(f"  2. HuggingFace ë³´ì • ({', '.join(args.correction_models)})")
        if args.use_solar_api:
            logger.write(f"  3. Solar API ì•™ìƒë¸” ({args.solar_model})")
        logger.write(f"  4. í›„ì²˜ë¦¬ (Predictor ë‚´ì¥)")
        logger.write(f"  5. ì œì¶œ íŒŒì¼ ìƒì„±")

        logger.write(f"\nğŸ“ˆ ê²°ê³¼ í†µê³„:")
        logger.write(f"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(submission_df)}")
        logger.write(f"  - í‰ê·  ìš”ì•½ ê¸¸ì´: {sum(len(s) for s in summaries) / len(summaries):.1f}ì")

        # ìƒ˜í”Œ ì¶œë ¥
        logger.write("\nğŸ“ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 3ê°œ):")
        for idx, row in submission_df.head(3).iterrows():
            logger.write(f"  [{row['fname']}]: {row['summary'][:80]}...")

        logger.write("\n" + "=" * 60)
        logger.write("ğŸ‰ K-Fold ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ!")
        logger.write(f"ğŸ“ ì œì¶œ íŒŒì¼: {args.output}")
        logger.write("=" * 60)

    except Exception as e:
        logger.write(f"\nâŒ ì¶”ë¡  ì˜¤ë¥˜ ë°œìƒ: {e}", print_error=True)
        logger.write_last_progress()
        raise

    finally:
        # Logger ì •ë¦¬
        logger.stop_redirect()
        logger.close()


# ==================== ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    main()
