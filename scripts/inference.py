# ==================== ì¶”ë¡  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ==================== #
"""
ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python scripts/inference.py --model outputs/baseline_kobart/final_model --output submissions/submission.csv
    python scripts/inference.py --model outputs/baseline_kobart/checkpoint-1000 --output submissions/test.csv
"""

# ---------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import sys
import argparse
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ---------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ---------------------- #
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# ---------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---------------------- #
from src.config import load_config
from src.inference import create_predictor
from src.logging.logger import Logger
from src.utils.core.common import create_log_path, now
from src.utils.gpu_optimization.team_gpu_check import get_gpu_info, check_gpu_tier


# ==================== ë©”ì¸ í•¨ìˆ˜ ==================== #
def main():
    # -------------- ì¸ì íŒŒì‹± -------------- #
    parser = argparse.ArgumentParser(description="ì¶”ë¡  ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì˜ˆ: outputs/baseline_kobart/final_model)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ì œì¶œ íŒŒì¼ ì¶œë ¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ ìë™ ìƒì„±: {date}_{time}_{mode}_{models}_{options}.csv)"
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default="data/raw/test.csv",
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="ì¶”ë¡  ë°°ì¹˜ í¬ê¸°"
    )
    parser.add_argument(
        "--num_beams",
        type=int,
        default=4,
        help="Beam search ë¹” ê°œìˆ˜"
    )
    parser.add_argument(
        "--experiment",
        type=str,
        default="baseline_kobart",
        help="ì‹¤í—˜ Config ì´ë¦„ (ìƒì„± íŒŒë¼ë¯¸í„° ë¡œë“œìš©)"
    )
    args = parser.parse_args()

    # -------------- Logger ì´ˆê¸°í™” -------------- #
    # ëª¨ë¸ëª… ì¶”ì¶œ
    if 'kobart' in args.model.lower():
        model_name_short = 'kobart'
    elif 'solar' in args.model.lower():
        model_name_short = 'solar'
    elif 'pegasus' in args.model.lower():
        model_name_short = 'pegasus'
    elif 'bart' in args.model.lower():
        model_name_short = 'bart'
    else:
        model_name_short = Path(args.model).name

    # ì˜µì…˜ íƒœê·¸ ìƒì„±
    timestamp = now('%Y%m%d_%H%M%S')
    options = []
    if args.batch_size != 32:
        options.append(f"bs{args.batch_size}")
    if args.num_beams != 4:
        options.append(f"beam{args.num_beams}")

    # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
    parts = [timestamp, model_name_short]
    if options:
        parts.extend(options)

    log_filename = "_".join(parts) + ".log"
    log_path = create_log_path("inference", log_filename)
    logger = Logger(log_path, print_also=True)
    logger.start_redirect()

    try:
        logger.write("=" * 60)
        logger.write(f"ì¶”ë¡  ì‹œì‘")
        logger.write("=" * 60)

        # -------------- GPU ì •ë³´ ì¶œë ¥ -------------- #
        logger.write("\n[GPU ì •ë³´]")
        gpu_info = get_gpu_info()
        for key, value in gpu_info.items():
            logger.write(f"  {key}: {value}")

        gpu_tier = check_gpu_tier()
        logger.write(f"  GPU Tier: {gpu_tier}")

        # -------------- 1. Config ë¡œë“œ (ì„ íƒì ) -------------- #
        logger.write("\n[1/5] Config ë¡œë”©...")
        try:
            config = load_config(args.experiment)
            logger.write(f"  âœ… Config ë¡œë“œ ì™„ë£Œ: {args.experiment}")
        except:
            logger.write("  âš ï¸ Config ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
            config = None

        # -------------- ì¶œë ¥ íŒŒì¼ëª… ìë™ ìƒì„± -------------- #
        if args.output is None:
            # ë‚ ì§œ ë° ì‹œê°„
            timestamp = now('%Y%m%d_%H%M%S')

            # ëª¨ë¸ëª… ì¶”ì¶œ
            model_path = Path(args.model)
            if 'kobart' in args.model.lower():
                model_name = 'kobart'
            elif 'pegasus' in args.model.lower():
                model_name = 'pegasus'
            elif 'bart' in args.model.lower():
                model_name = 'bart'
            else:
                model_name = model_path.name

            # ì˜µì…˜ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
            options = []
            if args.batch_size != 32:
                options.append(f"bs{args.batch_size}")
            if args.num_beams != 4:
                options.append(f"beam{args.num_beams}")

            # íŒŒì¼ëª… ìƒì„±
            parts = [timestamp, model_name]
            if options:
                parts.extend(options)

            filename = "_".join(parts) + ".csv"
            args.output = f"submissions/{filename}"

            logger.write(f"  ğŸ“ ìë™ ìƒì„±ëœ íŒŒì¼ëª…: {args.output}")

        # -------------- 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ -------------- #
        logger.write(f"\n[2/5] ëª¨ë¸ ë¡œë”©: {args.model}")
        model = AutoModelForSeq2SeqLM.from_pretrained(args.model)
        tokenizer = AutoTokenizer.from_pretrained(args.model)

        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ ì´ë™
        import torch
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        logger.write(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        logger.write(f"  ë””ë°”ì´ìŠ¤: {device}")
        logger.write(f"  ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")

        # -------------- 3. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ -------------- #
        logger.write(f"\n[3/5] í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {args.test_data}")
        test_df = pd.read_csv(args.test_data)
        logger.write(f"  âœ… í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_df)}ê°œ")

        # -------------- 4. Predictor ìƒì„± ë° ì¶”ë¡  -------------- #
        logger.write("\n[4/5] ì¶”ë¡  ì‹¤í–‰...")
        predictor = create_predictor(
            model=model,
            tokenizer=tokenizer,
            config=config,
            device=device,
            logger=logger
        )

        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission_df = predictor.create_submission(
            test_df=test_df,
            output_path=args.output,
            batch_size=args.batch_size,
            show_progress=True,
            num_beams=args.num_beams  # ì˜¤ë²„ë¼ì´ë“œ
        )

        # -------------- 5. ê²°ê³¼ ì¶œë ¥ -------------- #
        logger.write("\n[5/5] ì¶”ë¡  ì™„ë£Œ!")
        logger.write(f"  âœ… ì œì¶œ íŒŒì¼ ìƒì„±: {args.output}")
        logger.write(f"  ìƒ˜í”Œ ìˆ˜: {len(submission_df)}")

        # ìƒ˜í”Œ ì¶œë ¥
        logger.write("\n  ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 3ê°œ):")
        for idx, row in submission_df.head(3).iterrows():
            logger.write(f"    [{row['fname']}]: {row['summary'][:50]}...")

        logger.write("\n" + "=" * 60)
        logger.write("ğŸ‰ ì¶”ë¡  ì™„ë£Œ!")
        logger.write("=" * 60)

    finally:
        # Logger ì •ë¦¬
        logger.stop_redirect()
        logger.close()


# ==================== ì‹¤í–‰ë¶€ ==================== #
if __name__ == "__main__":
    main()
